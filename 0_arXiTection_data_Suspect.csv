Paper_Name,Example_A,Example_B,Example_C,Example_D,Answer,Label
Attention is All You Need,"The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train.","The most common methods for converting sequences using neural networks have complex recurrent or convolutional architectures with an encoder and decoder. The top models also link the encoder and decoder using an attention system. We put forward a new straightforward network design, the Transformer, built completely on attention mechanisms, removing recurrence and convolutions fully. Tests on two machine translation tasks display these models have higher quality while allowing more parallelization and needing far less training time.","The dominant neural network architectures for transforming sequences utilize intricate recurrent or convolutional networks with an encoder and decoder. The best models also connect the encoder and decoder via an attention component. We present a new simple network structure, the Transformer, constructed entirely of attention components, eliminating recurrence and convolutions completely. Experiments on two translation tasks prove these models have superior performance while enabling more parallelization and requiring significantly less training time.  ","The most common neural network models for sequence transduction have complicated recurrent or convolutional neural networks containing an encoder and decoder. The top-performing models also link the encoder and decoder using an attention module. We introduce a new straightforward network design, the Transformer, made up solely of attention modules, removing recurrence and convolutions fully. Tests on two translation tasks show these models have higher quality while permitting more parallelization and needing far less training time.",A,1
Attention is All You Need,"Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.","Our system obtains a BLEU score of 28.4 on the WMT 2014 English-to-German translation challenge, surpassing prior top outcomes, including ensemble models, by more than 2 BLEU. For the WMT 2014 English-to-French translation challenge, our system sets a new single-model state-of-the-art BLEU result of 41.8 after practicing for 3.5 days on eight GPUs, a small portion of the training expenses of the best models in the literature. We demonstrate that the Transformer generalizes effectively to other tasks by successfully utilizing it for English constituency parsing with both substantial and limited training information.","Our approach accomplishes a BLEU of 28.4 on the 2014 WMT English-to-German translation benchmark, outperforming preceding best marks, encompassing ensemble systems, by over 2 BLEU. On the 2014 WMT English-to-French translation benchmark, our approach establishes a new single-model best BLEU record of 41.8 after learning for 3.5 days on eight GPUs, a small fraction of the training costs of the top models in the literature. We exhibit that the Transformer widely applies to other undertakings by productively applying it to English constituency parsing with both ample and limited training data.","Our model gains a BLEU of 28.4 on the 2014 WMT English-to-German translation test, surpassing previous top scores, including ensemble models, by more than 2 BLEU. On the 2014 WMT English-to-French translation test, our model sets a new single-model state-of-the-art BLEU result of 41.8 after practicing for 3.5 days on eight GPUs, a small portion of the training costs of the best models in the literature. We prove that the Transformer widely applies to other tasks by successfully using it for English constituency parsing with both large and limited training data.",A,1
Attention is All You Need,"Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [35, 2, 5]. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [38, 24, 15]. Recurrent models typically factor computation along the symbol positions of the input and output sequences.","Neural networks with recurrent connections, especially long short-term memory and gated recurrent types, have proven to be the best techniques for sequence modeling tasks like language modeling and machine translation. Much work has continued to improve recurrent language models and encoder-decoder models. Recurrent networks usually break down computation over the symbol positions in the input and output sequences.","Neural networks with feedback loops, including long short-term memory and gated recurrent variants, are the premier approaches for sequence modeling and sequence transduction challenges such as language modeling and machine translation. Many efforts have kept pushing the limits of recurrent language models and encoder-decoder architectures. Recurrent networks typically separate computation by the symbol positions in the input and output sequences.  ","Neural networks with cyclic connections, particularly long short-term memory and gated recurrent forms, have clearly shown themselves to be the leading techniques for sequence modeling and sequence transduction tasks like language modeling and machine translation. Much progress has been made to advance recurrent language models and encoder-decoder architectures. Recurrent networks tend to divide up computation according to the symbol positions of the input and output sequences.",A,1
Attention is All You Need,"Recent work has achieved significant improvements in computational efficiency through factorization tricks [21] and conditional computation [32], while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains. Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [2, 19].","Recent research has made major gains in speed through factorization techniques [21] and conditional processing [32], while also boosting model accuracy for the latter. However, the limitation of sequential processing remains. Attention systems have grown into a vital component of powerful sequence modeling and transduction models for various jobs, letting modeling of dependencies irrespective of their distance in the input or output sequences [2, 19].","Recent studies have achieved big leaps in efficiency via factorization tricks [21] and conditional computation [32], while additionally enhancing model results for the latter. Nonetheless, the fundamental constraint of serial processing persists. Attention components have turned into a core piece of compelling sequence modeling and transduction models in many tasks, enabling modeling of relationships without consideration for their spacing in the input or output sequences [2, 19]. ","The latest work has realized considerable gains in speed through factorization techniques [21] and conditional calculation [32], while also improving model performance in the case of the latter. However, the fundamental restriction of step-by-step processing remains. Attention systems have evolved into an integral part of impressive sequence modeling and transduction models across various applications, permitting modeling of links irrespective of their position in the input or output sequences [2, 19].",A,1
Attention is All You Need,"In all but a few cases [27], however, such attention mechanisms are used in conjunction with a recurrent network. In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.","With just a few exceptions [27], attention mechanisms have typically been paired with recurrent networks. Here we present the Transformer, an architecture that avoids recurrence and instead fully depends on attention to create connections between input and output globally. Because it parallelizes well, the Transformer can surpass the current best translations after training for only twelve hours on eight P100 GPUs.","In nearly all cases [27], attention components have been used with recurrent networks. We introduce the Transformer, a model design that does not use recurrence but uses attention completely to make global links between input and output. Since it parallelizes better, the Transformer can achieve new state-of-the-art translation quality with just twelve hours of training on eight P100 GPUs. ","Except for a couple instances [27], attention mechanisms have been utilized along with recurrent networks. We put forth the Transformer, an architecture that eschews recurrence and wholly leverages attention to make global connections between input and output. Because it parallelizes much better, the Transformer is able to reach new heights in translation quality after training for just twelve hours on eight P100 GPUs.",A,1
Attention is All You Need,"The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difficult to learn dependencies between distant positions [12].","The aim of decreasing step-by-step calculation also constitutes the basis of the Extended Neural GPU [16], ByteNet [18] and ConvS2S [9]. All of these utilize convolutional neural networks as fundamental components, calculating hidden representations simultaneously for all input and output locations. In these models, the quantity of operations needed to connect signals from two arbitrary input or output positions increases with the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it tougher to learn connections between faraway positions [12].","The objective of reducing sequential processing likewise underpins the Extended Neural GPU [16], ByteNet [18] and ConvS2S [9]. All three harness convolutional neural networks as elementary building blocks, producing latent representations in parallel across all inputs and outputs. Here, the number of computations to bridge signals between any two input or output spots rises with their separation, scaling linearly for ConvS2S and logarithmically for ByteNet. This impedes learning associations between distant spots [12].  ","The goal of minimizing step-by-step operations also undergirds the Extended Neural GPU [16], ByteNet [18] and ConvS2S [9]. These all utilize convolutional neural networks as basic components, generating hidden representations simultaneously across all inputs and outputs. In these, the quantity of computations to relate signals between any two input or output locations grows with their distance, linearly for ConvS2S and logarithmically for ByteNet. This hinders learning links between far-flung locations [12].",A,1
Attention is All You Need,"Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4, 27, 28, 22]. End-to-end memory networks are based on a recurrent attention mechanism instead of sequencealigned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks [34]. To the best of our knowledge, however, the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequencealigned RNNs or convolution.","Self-regard, also known as inner-regard, is an attention system connecting different points of one sequence to create a depiction of that sequence. Self-regard has been effectively used in many tasks including reading comprehension, summarizing, inferring from text, and learning representations of sentences independent of task [4, 27, 28, 22]. End-to-end memory networks depend on a repetitive attention system instead of sequence-aligned repetition and have shown good performance on basic language question answering and language modeling [34]. However, as far as we know, the Transformer is the first transduction model that fully depends on self-regard to compute representations of its input and output without using sequence-aligned RNNs or convolution.","Self-focus, sometimes called intra-focus, is a focus mechanism relating different positions of a single sequence to generate a representation of the sequence. Self-focus has been successfully used in various tasks like reading comprehension, abstract summarization, textual implication, and learning task-independent sentence representations [4, 27, 28, 22]. End-to-end memory networks are based on a repetitive focus mechanism rather than sequence-aligned repetition and have proven effective on simple language question answering and language modeling tasks [34]. However, to our knowledge, the Transformer is the first transduction model that entirely depends on self-focus to compute representations of its input and output without using sequence-aligned RNNs or convolution.  ","Self-attention, also called inner-attention, is an attention system connecting different points of one sequence to create a depiction of that sequence. Self-attention has been effectively used in many tasks including reading comprehension, abstract summarizing, inferring from text, and learning representations of sentences independent of task [4, 27, 28, 22]. End-to-end memory networks depend on a repetitive attention system instead of sequence-aligned repetition and have proven effective on basic language question answering and language modeling tasks [34]. However, to our knowledge, the Transformer is the first transduction model that fully depends on self-attention to compute representations of its input and output without using sequence-aligned RNNs or convolution.",A,1
Attention is All You Need,"Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35]. Here, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence of continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output sequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive [10], consuming the previously generated symbols as additional input when generating the next. The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively.","The majority of the most effective neural sequence transduction systems have a structure consisting of an encoder and a decoder [5, 2, 35]. In these systems, the encoder changes an input series of symbolic representations (x1, ..., xn) into a series of constant depictions z = (z1, ..., zn). Given z, the decoder then produces an output series (y1, ..., ym) of symbols one at a time. At each step, the model is auto-regressive [10], using the previously created symbols as extra input when making the next symbol. The Transformer employs this general design using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown on the left and right halves of Figure 1, respectively.","Most of the top-performing neural sequence transduction architectures have an encoder-decoder form [5, 2, 35]. The encoder turns an input sequence of symbol representations (x1, ..., xn) into a sequence of continuous representations z = (z1, ..., zn). With z as input, the decoder generates an output sequence (y1, ..., ym) of symbols one at a time. At each generation step, the model is auto-regressive [10], taking in the previously generated symbols as extra context when producing the next symbol. The Transformer uses this high-level design with stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, as shown in the left and right halves of Figure 1.","A majority of the most effective neural sequence transduction systems utilize an encoder-decoder architecture [5, 2, 35]. In these models, the encoder converts an input series of symbol representations (x1, ..., xn) into a series of continuous latent representations z = (z1, ..., zn). Conditioned on z, the decoder then iteratively generates an output sequence (y1, ..., ym) of symbols one element at a time. At each generation step, the model is auto-regressive [10], leveraging the previously generated symbols as additional context when producing the next symbol. The Transformer employs this overall framework using stacked self-attention and pointwise fully connected layers for both the encoder and decoder, depicted in the left and right halves of Figure 1.",A,1
Attention is All You Need,"Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, positionwise fully connected feed-forward network. We employ a residual connection [11] around each of the two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension dmodel = 512.","The encoder has N = 6 identical layers stacked on top of each other. Every layer contains two sub-layers. The first sub-layer is a multi-head self-attention system, and the second sub-layer is a basic, positionwise fully connected feedforward network. We use a residual connection [11] around both sub-layers, followed by layer normalization [1]. In other words, the output of each sub-layer is LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself. To allow these residual connections, all sub-layers in the model, as well as the embedding layers, generate outputs with dimension dmodel = 512.","The encoder is made up of N = 6 identical layers stacked on top of each other. Each of these layers contains two smaller components. The first component is a multi-head self-attention mechanism, and the second component is a straightforward, positionwise fully connected feedforward network. We utilize a residual connection [11] around both of the smaller components, followed by layer normalization [1]. That means the output of each smaller component is LayerNorm(x + Subcomponent(x)), where Subcomponent(x) is the function implemented by the sub-layer itself. To enable these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of size dmodel = 512.  ","The encoder consists of a pile of N = 6 identical layers. Every layer has two sub-sections. The first sub-section is a multi-head self-attention system, and the second sub-section is a simple, positionwise fully connected feedforward network. We use a residual connection [11] around both sub-sections, followed by layer normalization [1]. In other words, the output of each sub-section is LayerNorm(x + Subsection(x)), where Subsection(x) is the function implemented by the sub-layer itself. To allow these residual connections, all sub-layers in the model, as well as the embedding layers, generate outputs of dimension dmodel = 512.",A,1
Attention is All You Need,"Decoder: The decoder is also composed of a stack of N = 6 identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization. We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i.","Decryptor: The decryptor also consists of a pile of N = 6 identical tiers. Besides the two sub-sections in each encoder tier, the decryptor inserts a third sub-section, which does multi-head focus on the production of the encoder pile. Comparable to the encoder, we use residual links around each of the sub-sections, followed by layer normalization. We also adapt the self-attention sub-section in the decryptor pile to stop positions from taking notice of next positions. This masking, together with the fact that the output embeddings are moved back by one position, guarantees that the forecasts for position i can depend exclusively on the recognized outputs at positions less than i.","Decoder: The decoder is also made up of a stack of N = 6 identical layers. In supplement to the two sub-divisions in each encoder layer, the decoder inserts a third sub-division, which does multi-head attention on the output of the encoder stack. Similar to the encoder, we use residual connections around each of the sub-divisions, followed by layer standardization. We also change the self-attention sub-division in the decoder stack to prevent positions from paying attention to following positions. This masking, combined with the fact that the output embeddings are counterbalanced by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i.  ","Interpreter: The interpreter is also composed of a pile of N = 6 identical tiers. In adding to the two sub-parts in each encoder tier, the interpreter inserts a third sub-part, which does multi-head focus on the production of the encoder pile. Alike to the encoder, we utilize residual links around each of the sub-parts, followed by layer normalization. We also adapt the self-attention sub-part in the interpreter pile to stop positions from observing subsequent positions. This masking, together with the fact that the output embeddings are balanced by one position, guarantees that the forecasts for position i can depend exclusively on the identified outputs at positions less than i.",A,1
Attention is All You Need,"The Transformer uses multi-head attention in three different ways: In ""encoder-decoder attention"" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [38, 2, 9]. The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder.","The Transformer utilizes multi-head consideration in three distinct manners: In ""encoder-decoder consideration"" layers, the inquiries originate from the past decoder layer, and the memory keys and qualities come from the yield of the encoder. This permits each position in the decoder to focus over all positions in the info succession. This copies the common encoder-decoder consideration components in sequence-to-sequence models like [38, 2, 9]. The encoder contains self-consideration layers. In a self-consideration layer all of the keys, qualities and inquiries come from a similar spot, for this situation, the yield of the past layer in the encoder. Each position in the encoder can focus to all positions in the past layer of the encoder.","The Transformer makes use of multi-head notice in three unique ways: In ""encoder-decoder notice"" layers, the questions come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This enables every position in the decoder to pay attention over all positions in the input sequence. This imitates the typical encoder-decoder notice mechanisms in sequence-to-sequence models like [38, 2, 9]. The encoder contains self-notice layers. In a self-notice layer all of the keys, values and questions come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can pay attention to all positions in the previous layer of the encoder.  ","The Transformer utilizes multi-head regard in three distinct manners: In ""encoder-decoder regard"" layers, the inquiries originate from the past decoder layer, and the memory keys and qualities come from the yield of the encoder. This permits each position in the decoder to look over all positions in the info succession. This copies the normal encoder-decoder regard components in sequence-to-sequence models like [38, 2, 9]. The encoder contains self-regard layers. In a self-regard layer all of the keys, qualities and inquiries come from a similar spot, for this situation, the yield of the past layer in the encoder. Each position in the encoder can look to all positions in the past layer of the encoder.",A,1
Attention is All You Need,"Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add ""positional encodings"" to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel as the embeddings, so that the two can be summed. There are many choices of positional encodings, learned and fixed [9].","Our model does not use recurrence or convolution, so to enable the model to utilize the sequence order, we need to provide details about the relative or absolute position of the tokens in the sequence. For this purpose, we append ""positional encodings"" to the input embeddings at the base of the encoder and decoder stacks. The positional encodings have the same dmodel dimension as the embeddings, allowing the two to be summed. There are many possible positional encodings, both learned and fixed [9].","Since our model has neither recurrence nor convolution, we must incorporate information about the tokens' relative or absolute position in the sequence to let the model exploit the order. We accomplish this by adding ""positional encodings"" to the input embeddings at the foundation of the encoder and decoder stacks. The positional encodings have the same dmodel dimension as the embeddings, permitting the two to be added together. There are numerous options for positional encodings, both learned and fixed [9].","Our model lacks recurrence and convolution, so to enable the model to leverage sequence order, we inject details about the tokens' relative or absolute position in the sequence. We do this by appending ""positional encodings"" to the input embeddings at the base of the encoder and decoder stacks. The positional encodings have the same dmodel size as the embeddings, allowing summation of the two. Many positional encoding options exist, both learned and fixed [9].",A,1
Attention is All You Need,"We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset k, P Epos+k can be represented as a linear function of P Epos. We also experimented with using learned positional embeddings [9] instead, and found that the two versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.","We selected this function because we theorized it would enable the model to readily learn to focus based on relative positions, since for any fixed offset k, P Epos+k can be depicted as a linear function of P Epos. We also tried using learned positional embeddings [9] instead, and discovered that the two versions generated nearly the same results (refer to Table 3 row (E)). We opted for the sinusoidal version since it may permit the model to extrapolate to sequence lengths longer than the ones come across during training.","We went with this function as we hypothesized it would make it easy for the model to learn to pay attention by relative positions. This is because for any constant offset k, P Epos+k can be represented as a linear function of P Epos. We also tested using learned positional embeddings [9], and found the two versions had almost identical results (see Table 3 row (E)). We chose the sinusoidal version since it might allow the model to extend to sequence lengths greater than those seen during training.  ","We selected this function because we thought it would enable the model to easily learn to focus on relative positions. This is because for any fixed offset k, P Epos+k can be shown as a linear function of P Epos. We also tried using learned positional embeddings [9], and saw the two versions had nearly the same results (look at Table 3 row (E)). We went with the sinusoidal version since it may let the model generalize to sequence lengths longer than those encountered during training.",A,1
Attention is All You Need,"In this section we compare various aspects of self-attention layers to the recurrent and convolutional layers commonly used for mapping one variable-length sequence of symbol representations (x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi , zi ∈ R d , such as a hidden layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we consider three desiderata. One is the total computational complexity per layer. Another is the amount of computation that can be parallelized, as measured by the minimum number of sequential operations required.","This part examines the different features of self-attention layers versus the recurrent and convolutional layers often used for converting one variable-length series of symbol representations (x1, ..., xn) into another sequence of the same length (z1, ..., zn), with xi , zi ∈ R d, like a hidden layer in a common sequence transduction encoder or decoder. We look at three desired qualities that motivate our use of self-attention. One is the overall computational complexity per layer. Another is the amount of computation that can be parallelized, as measured by the minimum number of sequential operations needed.","In this portion we analyze various properties of self-attention layers compared to the recurrent and convolutional layers frequently utilized for mapping one changeable-length sequence of symbol representations (x1, ..., xn) to a different sequence of the same size (z1, ..., zn), with xi , zi ∈ R d, such as a hidden layer in a typical sequence transduction encoder or decoder. We consider three criteria that motivate our use of self-attention. One criteria is the total computational complexity per layer. Another criteria is the amount of computation that can be parallelized, as measured by the minimum number of sequential operations required.  ","Here we review multiple facets of self-attention layers versus the recurrent and convolutional layers commonly employed for transforming one variable-length series of symbol representations (x1, ..., xn) into another sequence of equal size (z1, ..., zn), with xi , zi ∈ R d, like a hidden layer in a standard sequence transduction encoder or decoder. We examine three desired attributes that justify our use of self-attention. One attribute is the total computational complexity per layer. Another attribute is the amount of computation that can be parallelized, as gauged by the minimum number of sequential operations needed.",A,1
Attention is All You Need,"The third is the path length between long-range dependencies in the network. Learning long-range dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network. The shorter these paths between any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare the maximum path length between any two input and output positions in networks composed of the different layer types.","The third aspect is the distance between connections in the network that are far apart. Learning relationships between distant elements is a major difficulty in many sequence transformation jobs. A key aspect impacting the capacity to learn such connections is the span of the routes forward and backward signals need to cross in the network. The shorter these paths between any pairing of locations in the input and output sequences, the simpler it is to learn long-range dependencies [12]. Therefore, we also contrast the maximum route length between any two input and output positions in networks made up of the different layer varieties.","The third thing is the length of the path between associations in the network that are separated by a long distance. Mastering relationships between components that are far apart is a primary test in many sequence change tasks. One important factor influencing the skill to learn these connections is the extent of the paths forward and backward indicators need to go through in the network. The more compact these routes between any combination of spots in the input and output sequences, the easier it becomes to learn dependencies that are distant [12]. Consequently, we also compare the maximum path length between any two input and output positions in networks having the different layer types.  ","The third consideration is the distance of the path between links in the network that are far apart. Learning relationships between elements that are distant is a major challenge in many sequence alteration jobs. One key aspect affecting the capacity to learn these connections is the length of the paths forward and backward signals must traverse in the network. The more truncated these routes between any pairing of points in the input and output sequences, the simpler it becomes to learn dependencies that are long-range [12]. Therefore, we also contrast the maximum path length between any two input and output positions in networks containing the different layer varieties.",A,1
Attention is All You Need,"As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires O(n) sequential operations. In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece [38] and byte-pair [31] representations. To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size r in the input sequence centered around the respective output position. This would increase the maximum path length to O(n/r). We plan to investigate this approach further in future work.","The table indicates that a self-attention layer links all positions with a fixed number of sequential operations, while a recurrent layer needs O(n) sequential operations. Regarding computational intricacy, self-attention layers are quicker than recurrent layers when the sequence length n is smaller than the representation dimensionality d, which is frequently the case with sentence representations utilized by cutting-edge models in machine translations, like word-piece [38] and byte-pair [31] representations. To enhance computational efficiency for tasks with very long sequences, self-attention could be constrained to considering only a vicinity of size r in the input sequence focused around the particular output position. This would raise the maximum path length to O(n/r). We intend to further explore this method in future work.","As shown in the table, a self-attention layer connects all positions with a constant quantity of sequentially executed operations, while a recurrent layer requires O(n) sequential operations. In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence length n is less than the representation dimensionality d, which is often true for sentence representations used by state-of-the-art models in machine translation, like word-piece [38] and byte-pair [31] representations. To improve computational performance for tasks with very long sequences, self-attention could be limited to considering only a neighborhood of size r in the input sequence around the respective output position. This would increase the maximum path length to O(n/r). We plan to investigate this approach further in future work.","As indicated in the table, a self-attention layer links all positions with a fixed number of sequentially executed operations, whereas a recurrent layer necessitates O(n) sequential operations. Regarding computational complexity, self-attention layers are quicker than recurrent layers when the sequence length n is smaller than the representation dimensionality d, which is frequently the case with sentence representations utilized by cutting-edge models in machine translation, such as word-piece [38] and byte-pair [31] representations. To boost computational efficiency for tasks involving very long sequences, self-attention could be constrained to considering only a vicinity of size r in the input sequence centered around the respective output position. This would increase the maximum path length to O(n/r). We intend to explore this approach further in future work.",A,1
Attention is All You Need,"A single convolutional layer with kernel width k < n does not connect all pairs of input and output positions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels, or O(logk(n)) in the case of dilated convolutions [18], increasing the length of the longest paths between any two positions in the network. Convolutional layers are generally more expensive than recurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity considerably, to O(k · n · d + n · d^2 ). Even with k = n, however, the complexity of a separable convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer, the approach we take in our model.","A single layer of convolution with a kernel width k that is less than n does not connect all pairs of inputs and outputs. This requires stacking O(n/k) convolutional layers for contiguous kernels, or O(logk(n)) for dilated convolutions [18], lengthening the maximum paths between any two positions in the network. Convolutional layers are generally more expensive than recurrent layers, by a factor of k. However, separable convolutions [6] reduce the complexity considerably, to O(k · n · d + n · d^2). Even with k = n though, the complexity of a separable convolution equals the combination of a self-attention layer and a pointwise feedforward layer, our model's approach.","One convolution layer having kernel size k < n fails to link all input-output pairs. Stacking O(n/k) convolution layers is needed for contiguous kernels, or O(logk(n)) for dilated convolutions [18], increasing longest paths between positions. Convolution layers cost more than recurrent layers, by k times. But separable convolutions [6] cut complexity greatly, to O(k · n · d + n · d^2). Even if k = n, separable convolution complexity matches self-attention plus pointwise feedforward layers, our model's method.  ","A solitary convolution layer with kernel width k < n does not join all input and output spots. This necessitates piling O(n/k) convolution layers for contiguous kernels, or O(logk(n)) for expanded convolutions [18], extending the maximum ways between any two positions. Convolution layers are generally pricier than recurrent layers, by a k factor. In any case, separable convolutions [6] essentially decrease the intricacy, to O(k · n · d + n · d^2). Regardless of whether k = n, the intricacy of a separable convolution rises to the blend of a self-consideration layer and a point-wise feedforward layer, our model's methodology.",A,1
Attention is All You Need,"We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared sourcetarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens.","We used the typical WMT 2014 English-German data set made up of around 4.5 million sentence pairs to train our model. The sentences were encoded using byte-pair encoding [3], which utilizes a shared source-target vocabulary containing about 37000 tokens. For English-French, we leveraged the much larger WMT 2014 English-French data set with 36M sentences and separated tokens into a 32000 word-piece vocabulary [38]. Sentence pairs were grouped together by approximate sequence length. Each training batch was comprised of a collection of sentence pairs with roughly 25000 source tokens and 25000 target tokens.","Our training data consisted of the standard WMT 2014 English-German dataset with approximately 4.5 million sentence pairs. We encoded the sentences using byte-pair encoding [3], which uses a joint source-target vocabulary of around 37000 tokens. For English-French, we utilized the significantly larger 2014 English-French dataset from WMT with 36 million sentences, splitting the tokens into a 32000 word-piece vocabulary [38]. We batched sentence pairs together based on similar sequence length. Every training batch included a set of sentence pairs totaling about 25000 source tokens and 25000 target tokens.","We trained using the typical 2014 English-German data from WMT containing about 4.5 million sentence pairs. The sentences were encoded via byte-pair encoding [3], using a shared source-target vocabulary of approximately 37000 tokens. For English-French, we used the much larger 2014 English-French WMT dataset with 36 million sentences, dividing the tokens into a 32000 word-piece vocabulary [38]. Sentence pairs were grouped by approximate sequence length into batches. Each training batch had a collection of sentence pairs with around 25000 source tokens and 25000 target tokens.",A,1
Attention is All You Need,"We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We trained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the bottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps (3.5 days).","Our models were educated on a single computer with 8 NVIDIA P100 graphics processing units. Using the parameter settings described in the paper, each iteration of training took around 0.4 seconds for our foundational models. We trained the foundational models for 100,000 iterations total, which took 12 hours. For our large models (detailed on the last line of table 3), each iteration took 1.0 seconds. We trained the large models for 300,000 iterations (3.5 days).","We taught our algorithms on a single device equipped with 8 NVIDIA P100 GPUs. With the hyperparameters outlined in the document, every round of learning required about 0.4 seconds for our baseline models. The baseline models were educated over 100,000 rounds or 12 hours total. For our expansive models (depicted on the final row of table 3), each round lasted 1.0 seconds. We trained the expansive models for 300,000 rounds (3.5 days).  ","Our algorithms were developed on a single processor containing 8 NVIDIA P100 graphics chips. Utilizing the settings described in the article, each training pass took around 0.4 seconds for our elementary models. The elementary models were trained for 100,000 passes or 12 hours altogether. For our large-scale models (portrayed on the ending line of table 3), each pass took 1.0 seconds. We trained the large-scale models for 300,000 passes (3.5 days).",A,1
Attention is All You Need,"On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big) in Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0 BLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is listed in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models. On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0, outperforming all of the previously published single models, at less than 1/4 the training cost of the previous state-of-the-art model. The Transformer (big) model trained for English-to-French used dropout rate Pdrop = 0.1, instead of 0.3.","In the WMT 2014 English-to-German translation challenge, the large transformer architecture (Transformer (big) in Table 2) surpasses the top previously documented models (including ensembles) by over 2.0 BLEU, setting a new state-of-the-art BLEU result of 28.4. The setup of this model is shown in the last line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our baseline model beats all previously published models and ensembles, at a fraction of the training cost of any of the competitive models. On the WMT 2014 English-to-French translation challenge, our big model achieves a BLEU score of 41.0, outperforming all of the previously published single models, at less than 1/4 the training cost of the previous state-of-the-art model. The Transformer (big) model trained for English-to-French used dropout rate Pdrop = 0.1, instead of 0.3.","In the WMT 2014 English-to-German translation evaluation, the large transformer design (Transformer (big) in Table 2) exceeds the best previously documented models (including ensembles) by over 2.0 BLEU, establishing a new state-of-the-art BLEU outcome of 28.4. The parameters of this model are shown in the final line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our simple model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models. On the WMT 2014 English-to-French translation evaluation, our big model achieves a BLEU score of 41.0, outperforming all of the previously published single models, at less than 1/4 the training cost of the previous state-of-the-art model. The Transformer (big) model trained for English-to-French used dropout rate Pdrop = 0.1, instead of 0.3.","In the WMT 2014 English-to-German translation task, the large transformer architecture (Transformer (big) in Table 2) exceeds the top previously documented models (including ensembles) by over 2.0 BLEU, setting a new state-of-the-art BLEU result of 28.4. The configuration of this model is shown in the last line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our simple model beats all previously published models and ensembles, at a fraction of the training cost of any of the competitive models. On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0, outperforming all of the previously published single models, at less than 1/4 the training cost of the previous state-of-the-art model. The Transformer (big) model trained for English-to-French used dropout rate Pdrop = 0.1, instead of 0.3.",A,1
Attention is All You Need,"For the base models, we used a single model obtained by averaging the last 5 checkpoints, which were written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We used beam search with a beam size of 4 and length penalty α = 0.6 [38]. These hyperparameters were chosen after experimentation on the development set. We set the maximum output length during inference to input length + 50, but terminate early when possible [38]. Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature. We estimate the number of floating point operations used to train a model by multiplying the training time, the number of GPUs used, and an estimate of the sustained single-precision floating-point capacity of each GPU.","For the foundational prototypes, we utilized a sole exemplar formed by taking the mean of the final 5 checkpoints, scribbled at 10-minute interims. For the voluminous prototypes, we averaged the concluding 20 checkpoints. We exercised ray exploration with a ray magnitude of 4 and length penalty α = 0.6 [38]. These hyperparameters were chosen subsequent to experimentation on the evolution set. We fixed the maximal yield length during inference to input magnitude + 50, but conclude prematurely when plausible [38]. Table 2 summarizes our consequences and analyzes our translation caliber and training outlays to other exemplar architectures from the literature. We estimate the integer of floating point operations utilized to train a exemplar by multiplying the training time, the integer of GPUs utilized, and an estimate of the sustained single-precision floating-point capacity of each GPU.","Regarding the elementary models, we engaged a unitary example obtained by finding the mean of the final 5 checkpoints, inscribed at 10-minute intervals. Concerning the extensive models, we mediated the concluding 20 checkpoints. We wielded shaft exploration with a shaft size of 4 and length penalty α = 0.6 [38]. These hyperparameters were chosen succeeding experimentation on the growth set. We fixed the maximal turnout length during inference to input length + 50, but halt early when feasible [38]. Table 2 summarizes our fruits and compares our translation quality and training expenditures to other example architectures from the literature. We estimate the number of floating point operations used to train an example by multiplying the training time, the number of GPUs used, and an estimate of the sustained single-precision floating-point capacity of each GPU.","For the basic models, we engaged a single example obtained by averaging the final 5 checkpoints, documented at 10-minute gaps. For the sizable models, we mediated the last 20 checkpoints. We wielded beam search with a beam extent of 4 and length penalty α = 0.6 [38]. These hyperparameters were chosen after testing on the development set. We fixed the maximum yield length during inference to input extent + 50, but halt early when viable [38]. Table 2 summarizes our outcomes and analyzes our translation quality and training costs to other example architectures from the literature. We estimate the integer of floating point operations used to train an example by multiplying the training time, the integer of GPUs used, and an estimate of the sustained single-precision floating-point capacity of each GPU.",A,1
Attention is All You Need,"To evaluate the importance of different components of the Transformer, we varied our base model in different ways, measuring the change in performance on English-to-German translation on the development set, newstest2013. We used beam search as described in the previous section, but no checkpoint averaging. We present these results in Table 3. In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions, keeping the amount of computation constant, as described in Section 3.2.2. While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.","To assess the significance of the different parts of the Transformer, we modified our baseline model in various ways and measured how performance on English-to-German translation changed using the development set newstest2013. We utilized beam search as explained earlier, excluding checkpoint averaging. We show these findings in Table 3. In Table 3 rows (A), we change the quantity of attention heads and the attention key and value sizes, keeping the amount of computation the same, as discussed in Section 3.2.2. Although single-head attention is 0.9 BLEU inferior to the optimal configuration, quality also declines with too many heads.","To evaluate the relative importance of the various components of the Transformer, we altered our standard model in a number of ways and quantified the impact on performance on English-to-German translation using the development set newstest2013. We employed beam search as described previously, but without checkpoint averaging. We present these outcomes in Table 3. In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions, maintaining computational cost constant, as explained in Section 3.2.2. While single-head attention is 0.9 BLEU worse than the best setting, quality also decreases with too many heads.","To assess the significance of the different elements of the Transformer, we modified our baseline model in various ways and measured the change in performance on English-to-German translation using the development set newstest2013. We used beam search as explained before, excluding checkpoint averaging. We show these results in Table 3. In Table 3 rows (A), we alter the number of attention heads and the attention key and value sizes, keeping computational cost the same, as described in Section 3.2.2. Although single-head attention is 0.9 BLEU inferior to the optimal configuration, quality also declines with too many heads.",A,1
Attention is All You Need,"To evaluate if the Transformer can generalize to other tasks we performed experiments on English constituency parsing. This task presents specific challenges: the output is subject to strong structural constraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence models have not been able to attain state-of-the-art results in small-data regimes [37]. We trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the Penn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting, using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences [37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens for the semi-supervised setting.","To determine if the Transformer is able to generalize to other tasks, we conducted experiments on parsing English sentences into constituency trees. This task presents particular challenges: the output has strong structural constraints and is much longer than the input. Also, RNN sequence-to-sequence models have not achieved state-of-the-art results with small amounts of training data [37]. We trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the Penn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised manner, using the larger high-confidence and BerkleyParser datasets with approximately 17M sentences [37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens for the semi-supervised setting.","To test whether the Transformer can extend to other jobs, we did experiments on parsing English into constituency trees. This job has specific difficulties: the output has strong structural rules and is much longer than the input. Also, RNN sequence-to-sequence models have not gotten state-of-the-art results with little training data [37]. We trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) part of the Penn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised way, using the larger high-confidence and BerkleyParser datasets with about 17M sentences [37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens for the semi-supervised setting.","To evaluate whether the Transformer can generalize to other tasks, we conducted experiments on parsing English into constituency trees. This task has particular challenges: the output has strong structural constraints and is much longer than the input. Also, RNN sequence-to-sequence models have not achieved state-of-the-art results with small training data [37]. We trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the Penn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised manner, using the larger high-confidence and BerkleyParser datasets with approximately 17M sentences [37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens for the semi-supervised setting.",A,1
Attention is All You Need,"We performed only a small number of experiments to select the dropout, both attention and residual (section 5.4), learning rates and beam size on the Section 22 development set, all other parameters remained unchanged from the English-to-German base translation model. During inference, we increased the maximum output length to input length + 300. We used a beam size of 21 and α = 0.3 for both WSJ only and the semi-supervised setting.","We conducted just a few tests to choose the dropout, attention and residual connections (part 5.4), learning speeds and beam width using the Section 22 development set. All other settings stayed the same as the English-to-German base translation system. When making predictions, we expanded the maximum output length to input length + 300. We utilized a beam width of 21 and α = 0.3 for both the WSJ only and semi-supervised configurations.","We performed a small number of experiments to select the dropout, attention and residual connections (section 5.4), learning rates and beam size using the Section 22 development set. All other hyperparameters were unchanged from the English-to-German base translation model. During inference, we increased the max output length to input length + 300. We used a beam width of 21 and α = 0.3 for both the WSJ only and semi-supervised settings.","We carried out a limited number of tests to choose the dropout, attention and residual links (part 5.4), learning velocities and beam breadth utilizing the Section 22 development set. All other parameters stayed the same as the English-to-German base translation system. When making forecasts, we expanded the maximum output length to input length + 300. We employed a beam width of 21 and α = 0.3 for both the WSJ only and semi-supervised configurations.",A,1
Attention is All You Need,"Our results in Table 4 show that despite the lack of task-specific tuning our model performs surprisingly well, yielding better results than all previously reported models with the exception of the Recurrent Neural Network Grammar [8]. In contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the BerkeleyParser [29] even when training only on the WSJ training set of 40K sentences.","The data presented in Table 4 indicates that without any particular adjustment for the task, our model has unexpectedly strong performance, producing superior results to all other models reported except the Recurrent Neural Network Grammar [8]. Unlike RNN sequence-to-sequence models [37], the Transformer surpasses the BerkeleyParser [29] even when trained exclusively on the 40K sentence WSJ training set.","Our findings shown in Table 4 demonstrate that despite no task-specific fine-tuning, our system achieves remarkably good performance, generating better outcomes than all previously documented models excluding the Recurrent Neural Network Grammar [8]. In contrast with RNN sequence-to-sequence architectures [37], the Transformer beats the BerkeleyParser [29] even with training constrained to the 40K sentence WSJ training corpus.  ","The results in Table 4 exhibit that without any task-specific adaptation, our system attains unexpectedly robust performance, yielding superior results compared to all other formerly reported models besides the Recurrent Neural Network Grammar [8]. Unlike RNN sequence-to-sequence frameworks [37], the Transformer outperforms the BerkeleyParser [29] even when limited to training on the 40K sentence WSJ training set.",A,1
Attention is All You Need,"In this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention. For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art. In the former task our best model outperforms even all previously reported ensembles. We are excited about the future of attention-based models and plan to apply them to other tasks.","In this research, we introduced the Transformer, the first sequence transformation model fully relying on attention, substituting the recurrent layers most often utilized in encoder-decoder architectures with multi-headed self-attention. For translation tasks, the Transformer can be trained considerably faster than architectures depending on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we attain a new state-of-the-art performance. In the former task our best model surpasses even all previously documented ensembles. We are enthusiastic about the future of attention-based models and intend to use them for other tasks.","In this paper, we presented the Transformer, the first sequence transduction model entirely based on attention, swapping the recurrent layers most commonly employed in encoder-decoder architectures with multi-headed self-attention. For translation tasks, the Transformer can be trained significantly quicker than architectures utilizing recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state-of-the-art result. In the former task our top model outperforms even all previously reported ensembles. We are excited regarding the future of attention-based models and plan to apply them to other tasks.  ","In this study, we introduced the Transformer, the first sequence transduction model wholly based on attention, substituting the recurrent layers most often used in encoder-decoder architectures with multi-headed self-attention. For translation tasks, the Transformer can be trained considerably faster than architectures leveraging recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we attain new state-of-the-art performance. In the former task our best model surpasses even all previously documented ensembles. We are enthusiastic about the future of attention-based models and intend to utilize them for other tasks.",A,1
Bag of Tricks for Efficient Text Classification,"This paper explores a simple and efficient baseline for text classification. Our experiments show that our fast text classifier fastText is often on par with deep learning classifiers in terms of accuracy, and many orders of magnitude faster for training and evaluation. We can train fastText on more than one billion words in less than ten minutes using a standard multicore CPU, and classify half a million sentences among 312K classes in less than a minute. 1 Introduction Text classification is an important task in Natural Language Processing with many applications, such as web search, information retrieval, ranking and document classification (Deerwester et al., 1990; Pang and Lee, 2008).","This document investigates a straightforward and capable foundation for categorizing written language. Our trials demonstrate that our rapid text categorization method fastText is frequently on the same level as deep learning categorizers regarding precision, and multiple orders of magnitude quicker for teaching and assessment. We can educate fastText on over one billion terms in under ten minutes utilizing a standard multicore CPU, and classify half a million sentences among 312K categories in less than a minute. ","This paper looks into a simple and effective baseline for organizing text into categories. Our tests show that our fast text classification system fastText often matches deep learning systems in accuracy, and is many times faster for training and evaluation. We can train fastText on more than one billion words in under ten minutes using a normal multicore CPU, and categorize half a million sentences among 312K classes in under one minute.","This article studies a basic and proficient standard for text classification. Our experiments demonstrate that our rapid text classifier fastText frequently equals deep learning classifiers in correctness, and is multiple magnitudes faster for instruction and appraisal. We can educate fastText on over one billion terms in under ten minutes employing a common multicore CPU, and sort half a million sentences among 312K groups in less than sixty seconds.",A,1
Bag of Tricks for Efficient Text Classification,"Recently, models based on neural networks have become increasingly popular (Kim, 2014; Zhang and LeCun, 2015; Conneau et al., 2016). While these models achieve very good performance in practice, they tend to be relatively slow both at train and test time, limiting their use on very large datasets. Meanwhile, linear classifiers are often considered as strong baselines for text classification problems (Joachims, 1998; McCallum and Nigam, 1998; Fan et al., 2008). Despite their simplicity, they often obtain stateof-the-art performances if the right features are used (Wang and Manning, 2012).","In the past few years, models built using neural networks have gained a lot of popularity (Kim, 2014; Zhang and LeCun, 2015; Conneau et al., 2016). Although these models have shown very good performance, they tend to be quite slow during training and when making predictions, which restricts their applicability to very large datasets. On the other hand, linear classifiers are frequently viewed as strong baseline models for text classification tasks (Joachims, 1998; McCallum and Nigam, 1998; Fan et al., 2008). Even with their simple structure, they can achieve state-of-the-art results if the appropriate features are utilized (Wang and Manning, 2012).","Recently, models constructed from neural networks have become more and more common (Kim, 2014; Zhang and LeCun, 2015; Conneau et al., 2016). Despite demonstrating high performance, these models are relatively inefficient both during training and when making predictions, limiting their usefulness for very large datasets. In contrast, linear classifiers are often considered powerful baseline models for text classification problems (Joachims, 1998; McCallum and Nigam, 1998; Fan et al., 2008). Although simple, they can attain state-of-the-art performance when the right features are used (Wang and Manning, 2012).  ","In recent years, models built on neural networks have gained widespread adoption (Kim, 2014; Zhang and LeCun, 2015; Conneau et al., 2016). However, these models tend to be slow to train and make predictions, restricting their applicability to very large datasets. On the other hand, linear classifiers are frequently seen as strong foundational models for text classification tasks (Joachims, 1998; McCallum and Nigam, 1998; Fan et al., 2008). Despite their basic structure, they can achieve top results if appropriate features are employed (Wang and Manning, 2012).",A,1
Bag of Tricks for Efficient Text Classification,"They also have the potential to scale to very large corpus (Agarwal et al., 2014). In this work, we explore ways to scale these baselines to very large corpus with a large output space, in the context of text classification. Inspired by the recent work in efficient word representation learning (Mikolov et al., 2013; Levy et al., 2015), we show that linear models with a rank constraint and a fast loss approximation can train on a billion words within ten minutes, while achieving performance on par with the state-of-the-art.","These methods also can expand to cover an extremely large collection of data (Agarwal et al., 2014). In this research, we investigate approaches to enlarge these baseline models to very big collections of data with a substantial output area, in text categorization. Drawing inspiration from current advancements in efficient word representation learning (Mikolov et al., 2013; Levy et al., 2015), we demonstrate that linear models with a rank limitation and a fast loss estimation can learn from a billion words in ten minutes, while matching state-of-the-art performance.","They have the ability to grow to encompass a very large dataset (Agarwal et al., 2014). In this work, we explore techniques to increase these baseline models to massive datasets with a wide output space, for text classification tasks. Inspired by latest work in efficient word embedding learning (Mikolov et al., 2013; Levy et al., 2015), we show linear models with a rank constraint and quick loss function approximation can train on one billion words in ten minutes, achieving on par with state-of-the-art results.","These methods can also scale up to an extremely large body of text (Agarwal et al., 2014). In this research, we investigate ways to expand these baseline models to gigantic text collections with a wide output domain, for text classification. Drawing from recent advancements in efficient word embedding learning (Mikolov et al., 2013; Levy et al., 2015), we demonstrate linear models with a rank limit and fast loss estimation can learn from one billion words in ten minutes, while reaching state-of-the-art performance.",A,1
Bag of Tricks for Efficient Text Classification,"We evaluate the quality of our approach fastText1 on two different tasks, namely tag prediction and sentiment analysis. A simple and efficient baseline for sentence classification is to represent sentences as bag of words (BoW) and train a linear classifier, e.g., a logistic regression or an SVM (Joachims, 1998; Fan et al., 2008). However, linear classifiers do not share parameters among features and classes. This possibly limits their generalization in the context of large output space where some classes have very few examples.","We assess the performance of our fastText1 method on two different assignments - tag forecasting and sentiment review. A straightforward and capable foundation model for sentence sorting is to depict sentences as bag of words (BoW) and prepare a linear classifier, for example, a logistic regression or an SVM (Joachims, 1998; Fan et al., 2008). However, linear classifiers don't share parameters among features and classes. This possibly confines their generalization in the setting of large output space where some classes have very few examples.","We gauge the value of our fastText1 approach across two distinct tasks - tag prediction and sentiment analysis. A simple and effectual baseline technique for sentence classification is to model sentences as bag of words (BoW) and develop a linear classifier, like a logistic regression or SVM (Joachims, 1998; Fan et al., 2008). Though, linear classifiers don't share parameters between features and classes. This potentially limits their generalization in the context of a large output space where some classes have very few samples.  ","We measure the excellence of our fastText1 methodology on two separate jobs - tag forecasting and sentiment review. A basic and capable reference model for sentence sorting is to depict sentences as bag of words (BoW) and construct a linear classifier, such as a logistic regression or SVM (Joachims, 1998; Fan et al., 2008). However, linear classifiers don't share parameters among features and classes. This possibly constrains their generalization in the setting of a large output space where some classes have very sparse examples.",A,1
Bag of Tricks for Efficient Text Classification,"Common solutions to this problem are to factorize the linear classifier into low rank matrices (Schutze, 1992; Mikolov et al., 2013) or to use multilayer neural networks (Collobert and Weston, 2008; Zhang et al., 2015). Figure 1 shows a simple linear model with rank constraint. The first weight matrix A is a look-up table over the words. The word representations are then averaged into a text representation, which is in turn fed to a linear classifier. The features are embedded and averaged to form the hidden variable. This architecture is similar to the cbow model of Mikolov et al. (2013), where the middle word is replaced by a label.","Popular approaches to addressing this issue include decomposing the linear classifier into low rank matrices (Schutze, 1992; Mikolov et al., 2013) or utilizing multilayer neural networks (Collobert and Weston, 2008; Zhang et al., 2015). Figure 1 illustrates a basic linear model with rank limitation. The initial weight matrix A is a lookup table for the words. The word representations are then averaged into a text representation, which is then input to a linear classifier. The features are embedded and averaged to generate the hidden variable. This structure is comparable to the cbow model of Mikolov et al. (2013), where the middle word is substituted with a label.","Well-known solutions for this problem involve breaking down the linear classifier into low rank matrices (Schutze, 1992; Mikolov et al., 2013) or employing multi-layer neural networks (Collobert and Weston, 2008; Zhang et al., 2015). Figure 1 displays a straightforward linear model with rank constraint. The first weight matrix A is a reference table for the words. The word representations are then consolidated into a text representation, which is subsequently provided to a linear classifier. The features are inserted and consolidated to produce the hidden variable. This framework is similar to the cbow model of Mikolov et al. (2013), where the middle word is replaced with a label.  ","Common techniques to tackle this issue are to decompose the linear classifier into low rank matrices (Schutze, 1992; Mikolov et al., 2013) or use multi-layer neural networks (Collobert and Weston, 2008; Zhang et al., 2015). Figure 1 shows a basic linear model with rank restriction. The first weight matrix A is an index table for the words. The word representations are then combined into a text representation, which is then input into a linear classifier. The features are embedded and combined to generate the hidden variable. This design is comparable to the cbow model of Mikolov et al. (2013), where the middle word is substituted by a label.",A,1
Bag of Tricks for Efficient Text Classification,"We use the softmax function f to compute the probability distribution over the predefined classes. For a set of N documents, this leads to minimizing the negative loglikelihood over the classes: − 1 N X N n=1 yn log(f(BAxn)), where xn is the normalized bag of features of the nth document, yn the label, A and B the weight matrices. This model is trained asynchronously on multiple CPUs using stochastic gradient descent and a linearly decaying learning rate. When the number of classes is large, computing the linear classifier is computationally expensive. More precisely, the computational complexity is O(kh) where k is the number of classes and h the dimension of the text representation.","We utilize the softmax function f to determine the probability distribution across the pre-specified classes. For a group of N documents, this results in minimizing the negative log-likelihood over the classes: − 1 N ΣN n=1 yn log(f(BAxn)), where xn is the normalized bag of features of the nth document, yn the label, A and B the weight matrices. This model is trained asynchronously on multiple CPUs applying stochastic gradient descent and a linearly decreasing learning rate. When there are many classes, computing the linear classifier requires extensive computation. Specifically, the computational complexity is O(kh) where k is the number of classes and h is the dimension of the text representation.","We make use of the softmax function f to calculate the probability distribution over the pre-defined classes. For a collection of N documents, this leads to reducing the negative log-likelihood over the classes: − 1 N ΣN n=1 yn log(f(BAxn)), where xn is the normalized bag of features of the nth document, yn the label, A and B the weight matrices. This model is trained out of sync on several CPUs using stochastic gradient descent and a linearly decreasing learning rate. When there are numerous classes, calculating the linear classifier demands substantial computation. More exactly, the computational complexity is O(kh) where k is the quantity of classes and h is the dimension of the text representation.  ","We employ the softmax function f to determine the probability distribution across the pre-specified classes. For a set of N documents, this results in decreasing the negative log-likelihood over the classes: − 1 N ΣN n=1 yn log(f(BAxn)), where xn is the normalized bag of features of the nth document, yn the label, A and B the weight matrices. This model is trained asynchronously across multiple CPUs applying stochastic gradient descent and a linearly reducing learning rate. When there are many classes, computing the linear classifier necessitates significant computation. Specifically, the computational complexity is O(kh) where k is the number of classes and h is the dimension of the text representation.",A,1
Bag of Tricks for Efficient Text Classification,"In order to improve our running time, we use a hierarchical softmax (Goodman, 2001) based on the Huffman coding tree (Mikolov et al., 2013). During training, the computational complexity drops to O(h log2 (k)). The hierarchical softmax is also advantageous at test time when searching for the most likely class. Each node is associated with a probability that is the probability of the path from the root to that node.","To enhance our running speed, we utilize a hierarchical softmax (Goodman, 2001) founded on the Huffman encoding tree (Mikolov et al., 2013). While training, the computational intricacy decreases to O(h log2 (k)). The hierarchical softmax is also favorable during testing when looking for the most probable class. Each node has a probability which is the probability of the path from the root to that node.","In order to boost our execution velocity, we employ a hierarchical softmax (Goodman, 2001) based upon the Huffman coding tree (Mikolov et al., 2013). During learning, the computational complexity drops to O(h log2 (k)). The hierarchical softmax is also beneficial at evaluation time when searching for the most likely category. Every node has an associated probability which is the probability of the path from the root to that node.  ","To improve our running pace, we use a hierarchical softmax (Goodman, 2001) built on the Huffman encoding tree (Mikolov et al., 2013). While practicing, the computational difficulty reduces to O(h log2 (k)). The hierarchical softmax is also favorable during assessing when looking for the most expected group. Each node has a probability that is the probability of the route from the origin to that node.",A,1
Bag of Tricks for Efficient Text Classification,"This means that the probability of a node is always lower than the one of its parent. Exploring the tree with a depth first search and tracking the maximum probability among the leaves allows us to discard any branch associated with a small probability. In practice, we observe a reduction of the complexity to O(h log2 (k)) at test time. This approach is further extended to compute the T-top targets at the cost of O(log(T)), using a binary heap. Bag of words is invariant to word order but taking explicitly this order into account is often computationally very expensive. Instead, we use a bag of n-grams as additional features to capture some partial information about the local word order.","This signifies that a node's likelihood is always less than its parent's. Searching the tree depth-first while tracking the max probability of the leaves enables pruning branches with low probability. We see test time complexity fall to O(h log2 (k)). We extend this to find the T-top targets in O(log(T)) with a binary heap. Bag of words ignores word order, which is expensive to model explicitly. We add bag of n-grams as features to partially capture local order.","In other words, a node's chance is inferior to its parent's. Scouring the tree top-down and noting the best leaf probability lets us cut branches with small likelihood. We observe test time complexity of O(h log2 (k)). We further this to get the T-top targets in O(log(T)) using a binary heap. Bag of words disregards word order, though modeling it is computationally costly. We use bag of n-grams as extra features to partially capture local order.  ","To clarify, a node's probability is less than its parent's. Traversing the tree depth-first and tracking the maximum leaf probability enables pruning low probability branches. We see test time complexity of O(h log2 (k)). We extend this to find the T-top targets in O(log(T)) with a binary heap. Bag of words ignores word order, though modeling it is expensive. We add bag of n-grams as features to partially capture local word order.",A,1
Bag of Tricks for Efficient Text Classification,"This is very efficient in practice while achieving comparable results to methods that explicitly use the order (Wang and Manning, 2012). We maintain a fast and memory efficient mapping of the n-grams by using the hashing trick (Weinberger et al., 2009) with the same hashing function as in Mikolov et al. (2011) and 10M bins if we only used bigrams, and 100M otherwise.","This approach is very effective in real-world use and produces similar outcomes to techniques that directly leverage the sequence information (Wang and Manning, 2012). We implement a quick and memory efficient mapping of the n-grams through applying the hashing method (Weinberger et al., 2009) with the same hashing function as Mikolov et al. (2011) and 10M bins if we solely utilized bigrams, and 100M in other cases.","This technique is highly productive in practical applications and generates comparable performance to procedures that explicitly utilize the order (Wang and Manning, 2012). We sustain a fast and storage efficient representation of the n-grams by leveraging the hashing approach (Weinberger et al., 2009) with the identical hashing algorithm as Mikolov et al. (2011) and 10M containers if we exclusively used bigrams, and 100M otherwise.  ","This approach is very efficient in real applications and gives similar results to methods that directly use the sequence (Wang and Manning, 2012). We keep a fast and memory efficient mapping of the n-grams through the hashing technique (Weinberger et al., 2009) with the same hash function as Mikolov et al. (2011) and 10M buckets if we only employed bigrams, and 100M otherwise.",A,1
Bag of Tricks for Efficient Text Classification,"We evaluate fastText on two different tasks. First, we compare it to existing text classifers on the problem of sentiment analysis. Then, we evaluate its capacity to scale to large output space on a tag prediction dataset. Note that our model could be implemented with the Vowpal Wabbit library,2 but we observe in practice, that our tailored implementation is at least 2-5× faster. We employ the same 8 datasets and evaluation protocol of Zhang et al. (2015). We report the n-grams and TFIDF baselines from Zhang et al. (2015), as well as the character level convolutional model (char-CNN) of Zhang and LeCun (2015), the character based convolution recurrent network (char-CRNN) of (Xiao and Cho, 2016) and the very deep convolutional network (VDCNN) of Conneau et al. (2016).","We assess fastText on two separate tasks. First, we compare it with existing text classifiers on sentiment analysis. Then, we measure its ability to scale to a large output space using a tag prediction dataset. Note that our model could use the Vowpal Wabbit library, but we find in practice that our tailored implementation is at least 2-5x faster. We use the same 8 datasets and evaluation protocol from Zhang et al. (2015). We include the n-gram and TFIDF baselines from Zhang et al. (2015), and the character-level convolutional model (char-CNN) of Zhang and LeCun (2015), the character-based convolutional recurrent network (char-CRNN) of Xiao and Cho (2016), and the very deep convolutional network (VDCNN) of Conneau et al. (2016).","We test fastText on two tasks. Initially, we contrast it with current text classifiers for sentiment analysis. Afterward, we measure its ability to handle a large output space using a tag prediction dataset. Our model could utilize Vowpal Wabbit, but our customized implementation is 2-5x quicker in practice. We employ the same 8 datasets and evaluation method from Zhang et al. (2015). We provide the n-gram and TFIDF baselines from Zhang et al. (2015), plus the character-level convolutional model (char-CNN) of Zhang and LeCun (2015), the character-based convolutional recurrent network (char-CRNN) of Xiao and Cho (2016), and the very deep convolutional network (VDCNN) of Conneau et al. (2016).  ","We evaluate fastText using two tasks. First, we compare it to existing text classifiers for sentiment analysis. Second, we assess its capacity to handle a large output space with a tag prediction dataset. Our model could use Vowpal Wabbit, but our custom implementation is 2-5x faster in practice. We utilize the same 8 datasets and evaluation approach from Zhang et al. (2015). We include the n-gram and TFIDF baselines from Zhang et al. (2015), the character-level convolutional model (char-CNN) of Zhang and LeCun (2015), the character-based convolutional recurrent network (char-CRNN) of Xiao and Cho (2016), and the very deep convolutional network (VDCNN) of Conneau et al. (2016).",A,1
Bag of Tricks for Efficient Text Classification,"We report their main baselines as well as their two approaches based on recurrent networks (Conv-GRNN and LSTM-GRNN). We present the results in Figure 1. We use 10 hidden units and run fastText for 5 epochs with a learning rate selected on a validation set from {0.05, 0.1, 0.25, 0.5}. On this task, adding bigram information improves the performance by 1-4%. Overall our accuracy is slightly better than char-CNN and char-CRNN and, a bit worse than VDCNN. Note that we can increase the accuracy slightly by using more n-grams, for example with trigrams, the performance on Sogou goes up to 97.1%. Finally, Figure 3 shows that our method is competitive with the methods presented in Tang et al. (2015).","We describe their principal baselines as well as their two approaches founded on recurrent networks (Conv-GRNN and LSTM-GRNN). We exhibit the outcomes in Figure 1. We utilize 10 hidden units and execute fastText for 5 epochs with a learning rate chosen on a validation set from {0.05, 0.1, 0.25, 0.5}. On this task, appending bigram data improves the performance by 1-4%. By and large our accuracy is marginally superior to char-CNN and char-CRNN and, a bit more terrible than VDCNN. Note that we can expand the accuracy somewhat by utilizing more n-grams, for instance with trigrams, the performance on Sogou goes up to 97.1%. At last, Figure 3 shows that our technique is competitive with the techniques introduced in Tang et al. (2015).","We present their main baselines and their two methods using recurrent networks (Conv-GRNN and LSTM-GRNN). We display the results in Figure 1. We employ 10 hidden units and execute fastText for 5 epochs with a learning rate selected from {0.05, 0.1, 0.25, 0.5} using a validation set. In this task, adding bigram data improves performance by 1-4%. In general our accuracy is slightly better than char-CNN and char-CRNN and, a little worse than VDCNN. Note we can slightly increase accuracy by using more n-grams, for instance with trigrams, performance on Sogou rises to 97.1%. Finally, Figure 3 shows our method is competitive with the methods in Tang et al. (2015).  ","We communicate their primary baselines and their two approaches leveraging recurrent networks (Conv-GRNN and LSTM-GRNN). We visualize the outcomes in Figure 1. We utilize 10 hidden units and run fastText for 5 epochs with a learning rate chosen from {0.05, 0.1, 0.25, 0.5} using a validation set. For this task, incorporating bigram information enhances performance by 1-4%. Broadly our accuracy is marginally superior to char-CNN and char-CRNN and, somewhat inferior to VDCNN. Note we can slightly boost accuracy by leveraging more n-grams, for example with trigrams, performance on Sogou ascends to 97.1%. Ultimately, Figure 3 exhibits our method is competitive with the methods in Tang et al. (2015).",A,1
Bag of Tricks for Efficient Text Classification,"We tune the hyperparameters on the validation set and observe that using n-grams up to 5 leads to the best performance. Unlike Tang et al. (2015), fastText does not use pre-trained word embeddings, which can be explained the 1% difference in accuracy. The hyperparameters are chosen on the validation set. We report the test accuracy. Training time. Both char-CNN and VDCNN are trained on a NVIDIA Tesla K40 GPU, while our models are trained on a CPU using 20 threads. Table 2 shows that methods using convolutions are several orders of magnitude slower than fastText. While it is possible to have a 10× speed up for char-CNN by using more recent CUDA implementations of convolutions, fastText takes less than a minute to train on these datasets.","We adjust the hyperparameters on the validation set and see that utilizing n-grams up to 5 gives the best results. In contrast to Tang et al. (2015), fastText does not employ pre-trained word vectors, which clarifies the 1% change in precision. The hyperparameters are selected based on the validation set. We document the test accuracy. Training duration. Both char-CNN and VDCNN are educated on a NVIDIA Tesla K40 GPU, whereas our models are trained on a CPU utilizing 20 threads. Table 2 displays that techniques employing convolutions are multiple orders of magnitude slower than fastText. Although it's possible to get a 10× speed up for char-CNN by utilizing more current CUDA implementations of convolutions, fastText takes under a minute to train on these datasets.","We fine-tune the hyperparameters on the validation set and notice that using n-grams up to 5 produces the optimal performance. In contrast to Tang et al. (2015), fastText does not leverage pre-trained word embeddings, which explains the 1% difference in results. The hyperparameters are selected using the validation set. We report the test accuracy. Training time. Both char-CNN and VDCNN are learned on a NVIDIA Tesla K40 GPU, while our models are educated on a CPU using 20 threads. Table 2 shows that approaches using convolutions are several magnitudes slower than fastText. Although we can achieve a 10× speed up for char-CNN by leveraging more modern CUDA implementations of convolutions, fastText takes less than a minute to train on these datasets.  ","We optimize the hyperparameters on the validation set and see that applying n-grams up to 5 yields the best performance. Unlike Tang et al. (2015), fastText does not utilize pre-trained word representations, which accounts for the 1% disparity in precision. The hyperparameters are chosen based on the validation set. We present the test accuracy. Training duration. Both char-CNN and VDCNN are trained on a NVIDIA Tesla K40 GPU, whereas our models are trained on a CPU employing 20 threads. Table 2 indicates that techniques leveraging convolutions are multiple orders of magnitude slower than fastText. While we can attain a 10× speed up for char-CNN by using more current CUDA implementations of convolutions, fastText takes under a minute to train on these datasets.",A,1
Bag of Tricks for Efficient Text Classification,"The GRNNs method of Tang et al. (2015) takes around 12 hours per epoch on CPU with a single thread. Our speed- Input Prediction Tags taiyoucon 2011 digitals: individuals digital photos from the anime convention taiyoucon 2011 in mesa, arizona. if you know the model and/or the character, please comment. We show a few correct and incorrect tag predictions. up compared to neural network based methods increases with the size of the dataset, going up to at least a 15,000× speed-up. 3.2 Tag prediction Dataset and baselines.","The technique created by Tang and colleagues in 2015 requires approximately 12 hours for each iteration when implemented on a CPU utilizing a single thread. Our accelerated approach relative to neural network-dependent procedures becomes more significant as the data size increases, achieving at least a 15,000 times faster rate. ","The GRNNs approach designed by Tang's research team in 2015 needs around 12 hours per cycle on a single-threaded CPU. Our expedited method versus neural network methods scales up with larger datasets, getting a minimum of a 15,000x velocity improvement.","The 2015 GRNNs procedure of Tang et al. takes about 12 hours per round on a single-threaded CPU. Our sped up version compared to neural network-reliant techniques grows with bigger data sizes, reaching at least a 15,000x faster pace.",A,1
Bag of Tricks for Efficient Text Classification,"To test scalability of our approach, further evaluation is carried on the YFCC100M dataset (Thomee et al., 2016) which consists of almost 100M images with captions, titles and tags. We focus on predicting the tags according to the title and caption (we do not use the images). We remove the words and tags occurring less than 100 times and split the data into a train, validation and test set. The train set contains 91,188,648 examples (1.5B tokens). The validation has 930,497 examples and the test set 543,424. The vocabulary size is 297,141 and there are 312,116 unique tags. We will release a script that recreates this dataset so that our numbers could be reproduced. We report precision at 1.","In order to evaluate the scalability of our method, we performed more assessments using the YFCC100M dataset (Thomee et al., 2016). This dataset contains close to 100 million images along with captions, titles, and tags. We focused on predicting the tags using the title and caption only (without using the images). We removed words and tags that occurred less than 100 times and divided the data into training, validation, and test sets. The training set had 91,188,648 examples (1.5 billion tokens). The validation set had 930,497 examples and the test set had 543,424. There were 297,141 unique vocabulary words and 312,116 unique tags. We will make available a script that recreates this dataset so that our results can be reproduced. We report precision at 1.","To evaluate how well our approach scales, we conducted further experiments using the YFCC100M dataset (Thomee et al., 2016). This dataset has nearly 100 million images, each with captions, titles, and tags. We focused on predicting the tags from the title and caption only, without using the images. We filtered out words and tags occurring less than 100 times, and split the data into training, validation, and test sets. The training set had 91,188,648 examples (1.5 billion tokens). The validation set had 930,497 examples and the test set had 543,424. There was a vocabulary of 297,141 words and 312,116 unique tags. We will release a script to recreate this dataset so our results can be reproduced. We report precision at 1.","In order to test how well our approach handles large amounts of data, we did additional evaluation using the YFCC100M dataset (Thomee et al., 2016). This dataset contains close to 100 million images, each with a caption, title, and tags. We focused on predicting the tags from just the title and caption, without using the images. We removed words and tags occurring less than 100 times, and split the data into training, validation, and test sets. The training set contained 91,188,648 examples (1.5 billion tokens). The validation set had 930,497 examples and the test set had 543,424. There were 297,141 unique words in the vocabulary and 312,116 unique tags. We will provide a script to recreate this dataset so our results can be replicated. We report precision at 1.",A,1
Bag of Tricks for Efficient Text Classification,"We consider a frequency-based baseline which predicts the most frequent tag. We also compare with Tagspace (Weston et al., 2014), which is a tag prediction model similar to ours, but based on the Wsabie model of Weston et al. (2011). While the Tagspace model is described using convolutions, we consider the linear version, which achieves comparable performance but is much faster. Results and training time. Table 5 presents a comparison of fastText and the baselines.","We look at a baseline that guesses the most common tag as its prediction. We also make a comparison to Tagspace (Weston et al., 2014), which is a related tag prediction model to ours, but uses the Wsabie approach of Weston et al. (2011). Although Tagspace uses convolutions, we test the linear variant, which has similar performance but is much quicker. Outcomes and training duration. Table 5 shows a comparison of fastText and the baselines.","We evaluate a simple baseline that always predicts the most frequent tag. We also benchmark against Tagspace (Weston et al., 2014), a comparable tag prediction model to our approach, which utilizes the Wsabie framework of Weston et al. (2011). While Tagspace employs convolutions, we use the linear version, which has close performance but trains much faster. Results and learning time. Table 5 gives a comparison of fastText and the baselines.  ","We look at a naive baseline that always outputs the most common tag. We also contrast with Tagspace (Weston et al., 2014), a related tag prediction model to ours, which leverages the Wsabie method of Weston et al. (2011). Although Tagspace implements convolutions, we test the linear variant, which achieves similar accuracy but learns quicker. Outcomes and training speed. Table 5 presents a comparison of fastText against the baselines.",A,1
Bag of Tricks for Efficient Text Classification,"We also report the training time and test time. Test time is reported for a single thread, while training uses 20 threads for both models. and 200. Both models achieve a similar performance with a small hidden layer, but adding bigrams gives us a significant boost in accuracy. At test time, Tagspace needs to compute the scores for all the classes which makes it relatively slow, while our fast inference gives a significant speed-up when the number of classes is large (more than 300K here). Overall, we are more than an order of magnitude faster to obtain model with a better quality.","Additionally, we document the durations for training and testing. Testing time is for a single processor, while training utilizes 20 processors for both models. The two models attain comparable performance with a small concealed layer, but appending bigrams gives a substantial improvement in precision. During testing, Tagspace has to compute the valuations for all classes which makes it relatively slow, while our rapid deduction provides a major acceleration when the quantity of classes is large (over 300K here). On the whole, we are more than ten times quicker to get a model with superior quality.","Furthermore, we chronicle the periods for developing and assessing. Exam duration is for a single core, whereas preparation employs 20 cores for the two archetypes. The brace of paradigms gain analogous competence with a slight veiled stratum, however annexing bigrams gives a significant boost in correctness. At assay juncture, Tagspace must compute the valuations for all ranks which renders it relatively leaden, while our swift inference furnishes a foremost velocity when the figure of ranks is vast (above 300K here). By and large, we are more than tenfold faster to gain a prototype with superior caliber. ","Additionally, we document the timespans for instructing and evaluating. Examination interval is for a single processor, while education uses 20 processors for both examples. The two examples attain analogous capability with a small obscured layer, but attaching bigrams provides a significant improvement in precision. At test point, Tagspace must compute the valuations for all classes which makes it relatively slow, while our rapid deduction gives a major quickening when the amount of classes is huge (over 300K here). On the whole, we are more than tenfold quicker to obtain a model with superior quality.",A,1
Bag of Tricks for Efficient Text Classification,"The speedup of the test phase is even more significant (a 600× speedup). Table 4 shows some qualitative examples. 4 Discussion and conclusion In this work, we propose a simple baseline method for text classification. Unlike unsupervisedly trained word vectors from word2vec, our word features can be averaged together to form good sentence representations. In several tasks, fastText obtains performance on par with recently proposed methods inspired by deep learning, while being much faster.","The acceleration of the evaluation period is even more noteworthy (a 600 times acceleration). The table displays some qualitative instances. In this paper, we suggest a straightforward foundational technique for categorizing text. In contrast to word vectors trained without supervision from word2vec, our word characteristics can be combined to form good sentence representations. In several tasks, fastText achieves performance comparable to recently developed methods motivated by deep learning, while being much quicker.","The speeding up of the testing phase is even more impressive (a 600 fold speedup). The table shows some qualitative samples. In this work, we present a simple baseline approach for text classification. Unlike word embeddings trained in an unsupervised way from word2vec, our word features can be averaged to form good sentence representations. On several tasks, fastText attains performance on par with recently proposed methods inspired by deep learning, while being much faster. ","The hastening of the trial period is even more striking (a 600 times hastening). The table displays some qualitative examples. In this paper, we put forth a straightforward foundational technique for sorting text. In contrast to word vectors trained without supervision from word2vec, our word traits can be combined to form good sentence depictions. On several tasks, fastText accomplishes performance comparable to recently devised methods motivated by deep learning, while being much swifter.",A,1
Bag of Tricks for Efficient Text Classification,"Although deep neural networks have in theory much higher representational power than shallow models, it is not clear if simple text classification problems such as sentiment analysis are the right ones to evaluate them. We will publish our code so that the research community can easily build on top of our work. Acknowledgement. We thank Gabriel Synnaeve, Herv´e G´egou, Jason Weston and L´eon Bottou for their help and comments. We also thank Alexis Conneau, Duyu Tang and Zichao Zhang for providing us with information about their methods.","While deep neural networks theoretically have more representational capacity compared to shallow models, it is uncertain if elementary text classification tasks like sentiment analysis are suitable for assessing them. We will release our code so other researchers can conveniently extend our work. Thanks. We appreciate Gabriel Synnaeve, Herv ́e G ́egou, Jason Weston and L ́eon Bottou for their assistance and feedback. We also thank Alexis Conneau, Duyu Tang and Zichao Zhang for giving us details about their methods.","Although deep neural networks hypothetically have greater representational ability versus shallow models, it's unclear if basic text classification jobs like sentiment analysis are appropriate to evaluate them. We'll make our code available so the research community can easily build on our work. Gratitude. We are grateful to Gabriel Synnaeve, Herv ́e G ́egou, Jason Weston and L ́eon Bottou for their help and opinions. We also appreciate Alexis Conneau, Duyu Tang and Zichao Zhang for providing information regarding their methods.  ","While deep neural networks theoretically have superior representational power compared to shallow models, it is ambiguous whether simple text classification tasks such as sentiment analysis are suitable to assess them. We will publish our code so other researchers can conveniently extend our work. Thanks. We are thankful to Gabriel Synnaeve, Herv ́e G ́egou, Jason Weston and L ́eon Bottou for their assistance and feedback. We also appreciate Alexis Conneau, Duyu Tang and Zichao Zhang for giving us details about their approaches.",A,1
BERT,"We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.","We present a new language model named BERT, which is short for Bidirectional Encoder Representations from Transformers. In contrast to other recent language models (Peters et al., 2018a; Radford et al., 2018), BERT is intended to pre-train deep bidirectional representations using unlabeled text by simultaneously conditioning on context from both directions in all layers. Therefore, the pre-trained BERT model can be fine-tuned with just one extra output layer to produce state-of-the-art models for many different tasks, like question answering and language inference, without considerable task-specific architectural changes.","We put forward a novel language representation called BERT, which represents Bidirectional Encoder Representations from Transformers. Not like other latest language models (Peters et al., 2018a; Radford et al., 2018), BERT's design lets it pre-train deep bidirectional representations from unlabeled texts by together depending on both left and right contexts across all layers. Thus, the pre-trained BERT model can be adjusted with only one more output layer to make cutting-edge models for various tasks, such as question response and language deduction, without major task-specific structural modifications.  ","We present a new language model named BERT, shorthand for Bidirectional Encoder Representations from Transformers. In contrast with other recent language models (Peters et al., 2018a; Radford et al., 2018), BERT is engineered to pre-train deep bidirectional representations from unlabeled text by simultaneously using both left and right context in all layers. Consequently, the pre-trained BERT model can be fine-tuned by adding just one extra output layer to generate state-of-the-art models for many tasks, such as question answering and language inference, without large task-specific architectural changes.",A,1
BERT,"Language model pre-training has been shown to be effective for improving many natural language processing tasks (Dai and Le, 2015; Peters et al., 2018a; Radford et al., 2018; Howard and Ruder, 2018). These include sentence-level tasks such as natural language inference (Bowman et al., 2015; Williams et al., 2018) and paraphrasing (Dolan and Brockett, 2005), which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition  and question answering, where models are required to produce fine-grained output at the token level (Tjong Kim Sang and De Meulder, 2003; Rajpurkar et al., 2016)","Pre-training language models has proven beneficial for enhancing many natural language processing jobs (Dai and Le, 2015; Peters et al., 2018a; Radford et al., 2018; Howard and Ruder, 2018). These jobs include sentence-level tasks like natural language inference (Bowman et al., 2015; Williams et al., 2018) and paraphrasing (Dolan and Brockett, 2005), which try to predict relationships between sentences by examining them as a whole, as well as token-level tasks like named entity recognition and question answering, where models must generate detailed output at the token level (Tjong Kim Sang and De Meulder, 2003; Rajpurkar et al., 2016).","Language model pre-training has demonstrated its effectiveness at improving a variety of natural language processing activities (Dai and Le, 2015; Peters et al., 2018a; Radford et al., 2018; Howard and Ruder, 2018). These activities include sentence-level assignments such as natural language inference (Bowman et al., 2015; Williams et al., 2018) and paraphrasing (Dolan and Brockett, 2005), which attempt to predict connections between sentences by analyzing them in their entirety, and also token-level assignments like named entity recognition and question answering, where models need to produce fine-grained outputs at the token level (Tjong Kim Sang and De Meulder, 2003; Rajpurkar et al., 2016).  ","Pre-training language models before fine-tuning has proven successful at enhancing numerous natural language processing undertakings (Dai and Le, 2015; Peters et al., 2018a; Radford et al., 2018; Howard and Ruder, 2018). These undertakings consist of sentence-level objectives such as natural language inference (Bowman et al., 2015; Williams et al., 2018) and paraphrasing (Dolan and Brockett, 2005), which try to foresee relationships between sentences by scrutinizing them holistically, and also token-level objectives like named entity recognition and question answering, where models must generate detailed outputs at the token level (Tjong Kim Sang and De Meulder, 2003; Rajpurkar et al., 2016).",A,1
BERT,"There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning. The feature-based approach, such as ELMo (Peters et al., 2018a), uses task-specific architectures that include the pre-trained representations as additional features. The fine-tuning approach, such as the Generative Pre-trained Transformer (OpenAI GPT) (Radford et al., 2018), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters. The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations.","Currently there are two main ways to use pre-trained language models for specific tasks: extracting features and fine-tuning. The feature extraction method, like ELMo (Peters et al., 2018a), utilizes task-specific models that incorporate the pre-trained representations as extra inputs. The fine-tuning method, like OpenAI's GPT (Radford et al., 2018), adds minimal task-specific parameters and simply adjusts all pretrained weights by training on the downstream tasks. Both approaches employ the same objective during pre-training, using unidirectional language models to learn general linguistic representations.","There exist two prevailing techniques for leveraging pre-trained language models on downstream tasks: feature extraction and fine-tuning. The feature extraction technique, exemplified by ELMo (Peters et al., 2018a), constructs task-specific architectures that exploit the pre-trained representations as supplementary attributes. The fine-tuning technique, exemplified by OpenAI's Generative Pre-trained Transformer (GPT) (Radford et al., 2018), introduces minimal task-specific parameters and is trained on downstream tasks by merely fine-tuning all pre-trained parameters. Both techniques utilize the same objective function during pre-training, employing unidirectional language models to acquire general language representations.  ","Currently there are two main strategies to apply pre-trained language models to specific downstream tasks: extracting features and fine-tuning. The feature-based approach, like ELMo (Peters et al., 2018a), builds task-specific models that use the pre-trained representations as extra inputs. The fine-tuning approach, like OpenAI's GPT (Radford et al., 2018), introduces minimal task-specific parameters and simply adjusts all pre-trained weights by training on the downstream tasks. Both strategies use the same objective function during pre-training, employing unidirectional language models to learn general linguistic representations.",A,1
BERT,"We argue that current techniques restrict the power of the pre-trained representations, especially for the fine-tuning approaches. The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training. For example, in OpenAI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer (Vaswani et al., 2017). Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions.","We make the case that existing methods constrain the capability of the pre-trained representations, especially for fine-tuning techniques. The primary constraint is that standard language models only look in one direction, which limits the architectures that can be utilized during pre-training. For instance, in OpenAI GPT, the authors employ a left-to-right design, where each token can only focus on prior tokens in the self-attention layers of the Transformer (Vaswani et al., 2017). Such constraints are not ideal for sentence-level tasks, and could be very problematic when applying fine-tuning approaches to token-level tasks like question answering, where it is vital to include context from both directions.","We contend that current procedures restrict the power of the pre-trained depictions, particularly for the fine-tuning methodologies. The significant limitation is that standard language models are unilateral, and this confines the decision of models that can be utilized during pre-preparing. For instance, in OpenAI GPT, the creators utilize a left-to-right design, where each token can just focus on past tokens in the self-consideration layers of the Transformer (Vaswani et al., 2017). Such limitations are sub-ideal for sentence-level errands, and could be exceptionally destructive when applying fine-tuning based methodologies to token-level assignments like addressing, where it is crucial to join setting from the two bearings. ","We fight that current systems limit the capacity of the pre-prepared portrayals, particularly for the fine-tuning approaches. The significant constraint is that standard language models are one-directional, and this limits the decision of models that can be utilized during pre-preparing. For instance, in OpenAI GPT, the creators use a left-to-right design, where each token can just go to past tokens in the self-consideration layers of the Transformer (Vaswani et al., 2017). Such limitations are sub-ideal for sentence-level assignments, and could be exceptionally harming when applying fine-tuning based methodologies to token-level errands like addressing, where it is vital to join setting from the two headings.",A,1
BERT,"In this paper, we improve the fine-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers. BERT alleviates the previously mentioned unidirectionality constraint by using a “masked language model” (MLM) pre-training objective, inspired by the Cloze task (Taylor, 1953). The masked language model randomly masks some of the tokens from the input, and the objective is to predict the original vocabulary id of the masked word based only on its context. Unlike left-toright language model pre-training, the MLM objective enables the representation to fuse the left and the right context, which allows us to pretrain a deep bidirectional Transformer. In addition to the masked language model, we also use a “next sentence prediction” task that jointly pretrains text-pair representations. The contributions of our paper are as follows:","In this research, we enhance the fine-tuning methods by introducing BERT: Bidirectional Encoder Representations from Transformers. BERT gets around the previously stated one-directional limitation by utilizing a ""masked language model"" (MLM) pre-training goal, inspired by the Cloze task (Taylor, 1953). The masked language model arbitrarily hides some of the tokens from the input, and the goal is to predict the original vocabulary identification of the masked word based solely on its context. Unlike left-to-right language model pre-training, the MLM objective enables the representation to combine the left and right context, which enables us to pre-train a deep bidirectional Transformer. In addition to the masked language model, we also employ a ""next sentence prediction"" task that jointly pre-trains text-pair representations. The contributions of our research are as follows:","This paper improves fine-tuning techniques by presenting BERT: Bidirectional Encoder Representations from Transformers. BERT overcomes the earlier unidirectionality constraint by employing a ""masked language model"" (MLM) pre-training goal, based on the Cloze task (Taylor, 1953). The MLM randomly masks some input tokens, and the goal is to predict the original vocabulary id of the masked word using only context. Unlike left-to-right language model pre-training, the MLM objective allows the representation to integrate left and right context, enabling pre-training of a deep bidirectional Transformer. Additionally, a ""next sentence prediction"" task jointly pre-trains text-pair representations. Our contributions are:","In this work, we enhance fine-tuning methods through BERT: Bidirectional Encoder Representations from Transformers. BERT circumvents the prior unidirectionality limitation using a ""masked language model"" (MLM) pre-training objective, drawing on the Cloze task (Taylor, 1953). The MLM randomly masks input tokens, with the goal of predicting the original vocabulary id of the masked word from context alone. Unlike left-to-right language model pre-training, the MLM objective lets the representation fuse left and right context, allowing pre-training of a deep bidirectional Transformer. We also use a ""next sentence prediction"" task to jointly pre-train text-pair representations. Our contributions are:",A,1
BERT,"Learning widely applicable representations of words has been an active area of research for decades, including non-neural (Brown et al., 1992; Ando and Zhang, 2005; Blitzer et al., 2006) and neural (Mikolov et al., 2013; Pennington et al., 2014) methods. Pre-trained word embeddings are an integral part of modern NLP systems, offering significant improvements over embeddings learned from scratch (Turian et al., 2010). To pretrain word embedding vectors, left-to-right language modeling objectives have been used (Mnih and Hinton, 2009), as well as objectives to discriminate correct from incorrect words in left and right context (Mikolov et al., 2013)","Studying representations of words that can be applied in many contexts has been a dynamic research area for many years. This includes techniques that do not use neural networks (Brown et al., 1992; Ando and Zhang, 2005; Blitzer et al., 2006) as well as neural techniques (Mikolov et al., 2013; Pennington et al., 2014). Pre-trained word vectors are a key component of modern natural language processing systems, providing major gains compared to vectors learned from the beginning (Turian et al., 2010). To pre-train word embedding vectors, language modeling objectives from left to right have been utilized (Mnih and Hinton, 2009), along with objectives to tell apart accurate words from inaccurate words given left and right context (Mikolov et al., 2013).","Investigating widely useful representations of words has been a lively field of study for multiple decades. This comprises non-neural methods (Brown et al., 1992; Ando and Zhang, 2005; Blitzer et al., 2006) and neural network methods (Mikolov et al., 2013; Pennington et al., 2014). Pre-trained word embeddings are a fundamental piece of modern NLP systems, giving huge improvements over embeddings learned from nothing (Turian et al., 2010). To pre-train word embedding vectors, left-to-right language modeling goals have been employed (Mnih and Hinton, 2009), together with goals to differentiate correct words from incorrect words given left and right context (Mikolov et al., 2013).  ","Exploring representations of words that can be widely applied has been an energetic area of research for many years. This includes techniques not involving neural networks (Brown et al., 1992; Ando and Zhang, 2005; Blitzer et al., 2006) and techniques using neural networks (Mikolov et al., 2013; Pennington et al., 2014). Pre-trained word vectors are an essential component of modern natural language systems, providing major enhancements compared to vectors learned from scratch (Turian et al., 2010). To pre-train word embedding vectors, language modeling aims from left to right have been used (Mnih and Hinton, 2009), along with aims to distinguish accurate words from inaccurate words given surrounding left and right context (Mikolov et al., 2013).",A,1
BERT,"These approaches have been generalized to coarser granularities, such as sentence embeddings (Kiros et al., 2015; Logeswaran and Lee, 2018) or paragraph embeddings (Le and Mikolov, 2014). To train sentence representations, prior work has used objectives to rank candidate next sentences (Jernite et al., 2017; Logeswaran and Lee, 2018), left-to-right generation of next sentence words given a representation of the previous sentence (Kiros et al., 2015), or denoising autoencoder derived objectives (Hill et al., 2016).","These techniques have been extended to work with larger units, like representing whole sentences (Kiros et al., 2015; Logeswaran and Lee, 2018) or paragraphs (Le and Mikolov, 2014). To learn representations for sentences, previous studies have used goals like ranking plausible next sentences (Jernite et al., 2017; Logeswaran and Lee, 2018), generating the words in a next sentence from left to right conditioned on the previous sentence's representation (Kiros et al., 2015), or objectives derived from denoising autoencoders (Hill et al., 2016).","These methods have been generalized to operate on coarser chunks of text, such as full sentence vectors (Kiros et al., 2015; Logeswaran and Lee, 2018) or paragraph vectors (Le and Mikolov, 2014). Past work has trained sentence embeddings using objectives like ranking candidate subsequent sentences (Jernite et al., 2017; Logeswaran and Lee, 2018), generating the words in a next sentence from left to right given the previous sentence's representation (Kiros et al., 2015), or objectives based on denoising autoencoders (Hill et al., 2016).  ","These approaches have been extended to work on larger units of text, like representing entire sentences (Kiros et al., 2015; Logeswaran and Lee, 2018) or paragraphs (Le and Mikolov, 2014). Earlier research has trained sentence vectors using goals including ranking possible next sentences (Jernite et al., 2017; Logeswaran and Lee, 2018), sequentially generating the words in a following sentence from left to right conditioned on the prior sentence's representation (Kiros et al., 2015), or objectives derived from denoising autoencoders (Hill et al., 2016).",A,1
BERT,"ELMo and its predecessor (Peters et al., 2017, 2018a) generalize traditional word embedding research along a different dimension. They extract context-sensitive features from a left-to-right and a right-to-left language model. The contextual representation of each token is the concatenation of the left-to-right and right-to-left representations. When integrating contextual word embeddings with existing task-specific architectures, ELMo advances the state of the art for several major NLP benchmarks (Peters et al., 2018a) including question answering (Rajpurkar et al., 2016), sentiment analysis (Socher et al., 2013), and named entity recognition (Tjong Kim Sang and De Meulder, 2003). Melamud et al.","ELMo and its forerunner (Peters et al., 2017, 2018a) generalize conventional word embedding research along a different axis. They derive context-sensitive characteristics from a left-to-right and a right-to-left language archetype. The contextual illustration of each token is the fusion of the left-to-right and right-to-left portrayals. When combining contextual word embeddings with existing task-explicit architectures, ELMo propels the state of the art for several major NLP benchmarks (Peters et al., 2018a) including question solving (Rajpurkar et al., 2016), sentiment examination (Socher et al., 2013), and named entity identification (Tjong Kim Sang and De Meulder, 2003). Melamud et al.","ELMo and its precursor (Peters et al., 2017, 2018a) broaden traditional word embedding work in a different way. They extract context-sensitive features from a left-to-right and a right-to-left linguistic model. The contextual representation of each token is the combination of the left-to-right and right-to-left representations. When integrating contextual word embeddings into existing task-focused architectures, ELMo improves the state-of-the-art for several major NLP benchmarks (Peters et al., 2018a) including question replying (Rajpurkar et al., 2016), sentiment review (Socher et al., 2013), and named entity spotting (Tjong Kim Sang and De Meulder, 2003). Melamud et al.  ","ELMo and its predecessor (Peters et al., 2017, 2018a) expand conventional word embedding research in a different dimension. They derive context-sensitive traits from a left-to-right and a right-to-left language prototype. The contextual depiction of each token is the union of the left-to-right and right-to-left portrayals. When combining contextual word embeddings with current task-explicit architectures, ELMo advances the state of the art for several major NLP benchmarks (Peters et al., 2018a) including question answering (Rajpurkar et al., 2016), sentiment analysis (Socher et al., 2013), and named entity recognition (Tjong Kim Sang and De Meulder, 2003). Melamud et al.",A,1
BERT,"More recently, sentence or document encoders which produce contextual token representations have been pre-trained from unlabeled text and fine-tuned for a supervised downstream task (Dai and Le, 2015; Howard and Ruder, 2018; Radford et al., 2018). The advantage of these approaches is that few parameters need to be learned from scratch. At least partly due to this advantage, OpenAI GPT (Radford et al., 2018) achieved previously state-of-the-art results on many sentencelevel tasks from the GLUE benchmark (Wang et al., 2018a). Left-to-right language modeling and auto-encoder objectives have been used for pre-training such models (Howard and Ruder, 2018; Radford et al., 2018; Dai and Le, 2015).","In more current times, sentence or document encoders that generate contextual token representations have been pre-trained using unlabeled text and fine-tuned for a directed supervised downstream assignment (Dai and Le, 2015; Howard and Ruder, 2018; Radford et al., 2018). The benefit of these methods is that few parameters need to be learned from the beginning. At least somewhat due to this advantage, OpenAI GPT (Radford et al., 2018) attained previously best-in-class results on many sentence-level tasks from the GLUE benchmark (Wang et al., 2018a). Left-to-right language modeling and auto-encoder objectives have been utilized for pre-training such models (Howard and Ruder, 2018; Radford et al., 2018; Dai and Le, 2015).","Recently, sentence or document encoders that generate context-dependent token representations have been pre-trained using unlabeled text and fine-tuned for a particular supervised downstream task (Dai and Le, 2015; Howard and Ruder, 2018; Radford et al., 2018). The strength of these approaches is that few parameters need to be learned from scratch. At least partly owing to this advantage, OpenAI GPT (Radford et al., 2018) achieved previously top-ranking results on many sentence-level tasks from the GLUE benchmark (Wang et al., 2018a). Left-to-right language modeling and auto-encoder objectives have been employed for pre-training such models (Howard and Ruder, 2018; Radford et al., 2018; Dai and Le, 2015).  ","In the near past, sentence or document encoders which produce situational token representations have been pre-trained utilizing unlabeled text and fine-tuned for a directed supervised downstream assignment (Dai and Le, 2015; Howard and Ruder, 2018; Radford et al., 2018). The plus of these approaches is that few parameters need to be learned from square one. At least somewhat due to this advantage, OpenAI GPT (Radford et al., 2018) attained previously best-in-class results on many sentence-level tasks from the GLUE benchmark (Wang et al., 2018a). Left-to-right language modeling and auto-encoder objectives have been used for pre-training such models (Howard and Ruder, 2018; Radford et al., 2018; Dai and Le, 2015).",A,1
BERT,"We introduce BERT and its detailed implementation in this section. There are two steps in our framework: pre-training and fine-tuning. During pre-training, the model is trained on unlabeled data over different pre-training tasks. For finetuning, the BERT model is first initialized with the pre-trained parameters, and all of the parameters are fine-tuned using labeled data from the downstream tasks. Each downstream task has separate fine-tuned models, even though they are initialized with the same pre-trained parameters. The question-answering example in Figure 1 will serve as a running example for this section.","In this part, we present BERT and explain how it is implemented in detail. Our framework has two phases: pre-training and fine-tuning. During pre-training, the model is trained on data without labels across various pre-training tasks. For fine-tuning, we first initialize BERT with the pre-trained parameters, then refine all the parameters using labeled data from the specific downstream tasks. Each downstream task has its own fine-tuned models, despite starting from the same pre-trained parameters. The question-answering example in Figure 1 will be used as a running example here.","We describe BERT and its in-depth implementation in this section. There are two steps: pre-training and fine-tuning. In pre-training, the model trains on unlabeled data for different pre-training jobs. For fine-tuning, we first load the pre-trained parameters into BERT, then adjust all parameters using labeled data for the particular downstream tasks. Even though they start from the same pre-trained parameters, each downstream task has separate fine-tuned models. The question-answering instance in Figure 1 will serve as an ongoing example in this section.  ","We present BERT and its comprehensive implementation details in this section. Our approach has two phases: pre-training and fine-tuning. In pre-training, the model trains on data without labels across various pre-training objectives. For fine-tuning, we first initialize BERT with the pre-trained parameters, then refine all parameters using labeled data for the specific downstream tasks. While starting from the same pre-trained parameters, each downstream task gets its own fine-tuned models. The question-answering case in Figure 1 will be a running example here.",A,1
BERT,"Model Architecture BERT’s model architecture is a multi-layer bidirectional Transformer encoder based on the original implementation described in Vaswani et al. (2017) and released in the tensor2tensor library.1 Because the use of Transformers has become common and our implementation is almost identical to the original, we will omit an exhaustive background description of the model architecture and refer readers to Vaswani et al. (2017) as well as excellent guides such as “The Annotated Transformer.”2 In this work, we denote the number of layers (i.e., Transformer blocks) as L, the hidden size as H, and the number of self-attention heads as A. 3 We primarily report results on two model sizes: BERTBASE (L=12, H=768, A=12, Total Parameters=110M) and BERTLARGE (L=24, H=1024, A=16, Total Parameters=340M).","The architecture of BERT's model is constructed using multiple layers of a bidirectional Transformer encoder, which is based on the original design described in Vaswani et al. (2017) and implemented in the tensor2tensor library. Since the use of Transformers is now commonplace and our implementation closely resembles the original, we will omit a comprehensive background of the model architecture and refer readers to Vaswani et al. (2017) as well as excellent guides like ""The Annotated Transformer."" In our work, we denote the number of layers (i.e. Transformer blocks) as L, the hidden size as H, and the number of self-attention heads as A. We primarily report results on two model sizes: BERTBASE (L=12, H=768, A=12, Total Parameters=110M) and BERTLARGE (L=24, H=1024, A=16, Total Parameters=340M).","The model design of BERT employs multiple tiers of a bidirectional Transformer encoder, built upon the initial blueprint outlined in Vaswani et al. (2017) and coded in the tensor2tensor library. Since Transformers are now widely used and our implementation closely matches the original, we omit an exhaustive overview of the model design and refer readers to Vaswani et al. (2017) as well as great guides like ""The Annotated Transformer."" In our work, we signify the number of tiers (i.e. Transformer blocks) as L, the hidden dimension as H, and the quantity of self-attention heads as A. We primarily document outcomes on two model sizes: BERTBASE (L=12, H=768, A=12, Total Parameters=110M) and BERTLARGE (L=24, H=1024, A=16, Total Parameters=340M).","BERT's model structure consists of multiple layers of a bidirectional Transformer encoder, which was based on the initial implementation defined in Vaswani et al. (2017) and implemented in the tensor2tensor library. Since the use of Transformers is now widespread and our implementation is nearly identical to the original, we omit a detailed background of the model structure and refer readers to Vaswani et al. (2017) as well as great guides like ""The Annotated Transformer."" In our work, we denote the number of layers (i.e. Transformer blocks) as L, the hidden dimension as H, and the number of self-attention heads as A. We primarily present results on two model sizes: BERTBASE (L=12, H=768, A=12, Total Parameters=110M) and BERTLARGE (L=24, H=1024, A=16, Total Parameters=340M).",A,1
BERT,"In order to train a deep bidirectional representation, we simply mask some percentage of the input tokens at random, and then predict those masked tokens. We refer to this procedure as a “masked LM” (MLM), although it is often referred to as a Cloze task in the literature (Taylor, 1953). In this case, the final hidden vectors corresponding to the mask tokens are fed into an output softmax over the vocabulary, as in a standard LM. In all of our experiments, we mask 15% of all WordPiece tokens in each sequence at random. In contrast to denoising auto-encoders (Vincent et al., 2008), we only predict the masked words rather than reconstructing the entire input.","To teach a deep two-way representation, we just randomly hide some percent of the input tokens, and then guess those hidden tokens. We call this a ""masked LM"" (MLM), though it's often called a Cloze task in research (Taylor, 1953). Here, the final hidden vectors for the mask tokens go into a vocab softmax, like a normal LM. In all our tests, we randomly mask 15% of all WordPiece tokens per sequence. Unlike denoising autoencoders (Vincent et al., 2008), we only predict the masked words rather than remaking the whole input.","In order to develop a deep bidirectional representation, we simply conceal a random percentage of the input tokens, and then infer those concealed tokens. We term this procedure a ""masked LM"" (MLM), although it is frequently called a Cloze task in the literature (Taylor, 1953). In this case, the final hidden vectors matching the mask tokens are provided to an output softmax over the vocabulary, as in a standard LM. In all of our experiments, we randomly mask 15% of all WordPiece tokens in every sequence. In contrast to denoising autoencoders (Vincent et al., 2008), we only anticipate the masked words rather than reconstructing the whole input.","To train a deep two-way representation, we just hide some percent of the input tokens randomly, and then deduce those hidden tokens. We name this a ""masked LM"" (MLM), even though it's often termed a Cloze task in research (Taylor, 1953). Here, the final hidden vectors for the mask tokens enter a vocab softmax, as in a normal LM. In all our trials, we randomly mask 15% of all WordPiece tokens per sequence. Unlike denoising autoencoders (Vincent et al., 2008), we only deduce the masked words rather than recreating the whole input.",A,1
BERT,"Task #1: Masked LM Intuitively, it is reasonable to believe that a deep bidirectional model is strictly more powerful than either a left-to-right model or the shallow concatenation of a left-toright and a right-to-left model. Unfortunately, standard conditional language models can only be trained left-to-right or right-to-left, since bidirectional conditioning would allow each word to indirectly “see itself”, and the model could trivially predict the target word in a multi-layered context. ","The first assignment: Masked language model. It makes sense to think that a deep two-way model has more capability than either a left-to-right model or a superficial joining of a left-to-right and a right-to-left model. However, standard conditional language models can only be trained left-to-right or right-to-left, since two-way conditioning would let each word indirectly ""see itself"", and the model could easily predict the target word in a multi-layered situation.","Task number one: Masked language modeling. Intuitively, it seems reasonable to assume that a deep bidirectional model has more power than either a left-to-right model or the shallow combination of a left-to-right and a right-to-left model. But typical conditional language models can only be trained left-to-right or right-to-left, because bidirectional conditioning would allow each word to indirectly ""view itself"", and the model could simply predict the target word in a multi-layer context.  ","The first task: Masked language model. It makes sense to think that a deep two-way model is more powerful than either a left-to-right model or a superficial combination of a left-to-right and a right-to-left model. However, standard conditional language models can only be trained left-to-right or right-to-left, since two-way conditioning would enable each word to indirectly ""see itself"", and the model could easily predict the target word in a multi-layer situation.",A,1
BERT,"Task #2: Next Sentence Prediction (NSP) Many important downstream tasks such as Question Answering (QA) and Natural Language Inference (NLI) are based on understanding the relationship between two sentences, which is not directly captured by language modeling. In order to train a model that understands sentence relationships, we pre-train for a binarized next sentence prediction task that can be trivially generated from any monolingual corpus. Specifically, when choosing the sentences A and B for each pretraining example, 50% of the time B is the actual next sentence that follows A (labeled as IsNext), and 50% of the time it is a random sentence from the corpus (labeled as NotNext).","Assignment Number Two: Predicting the Following Sentence (NSP) Numerous essential downstream activities like Question Responding (QA) and Natural Language Implication (NLI) depend on grasping the connection between a pair of sentences, which language modeling does not directly capture. To train a model that comprehends sentence relationships, we pre-train for a binarized next sentence prediction task that can be easily generated from any single language corpus. In particular, when selecting the sentences A and B for each pretraining instance, 50% of the time B is the real subsequent sentence after A (labeled as IsNext), and 50% of the time it is an arbitrary sentence from the corpus (labeled as NotNext).","Work Item #2: Forecasting the Ensuing Utterance (NSP) Many crucial downstream jobs like Query Resolution (QA) and Natural Language Deduction (NLI) hinge on understanding the linkage between two utterances, which raw language modeling does not directly seize. To develop a model that comprehends sentence connections, we pre-train for a binarized next sentence prediction task that can be effortlessly spawned from any monolingual text corpus. Specifically, when picking the sentences A and B for each pretraining sample, 50% of the time B is the genuine next utterance following A (labeled as IsNext), and 50% of the time it is an arbitrary utterance from the corpus (labeled as NotNext).  ","Assignment Number 2: Anticipating the Subsequent Expression (NSP) Numerous pivotal downstream activities such as Interrogative Response (QA) and Natural Language Implication (NLI) depend on grasping the relationship between a couple of expressions, which plain language modeling does not directly capture. To build a model that understands sentence links, we pre-train for a binarized next sentence prediction task that can be readily created from any single language text corpus. In particular, when choosing the expressions A and B for each pretraining example, 50% of the time B is the real next expression after A (labeled as IsNext), and 50% of the time it is an arbitrary expression from the corpus (labeled as NotNext).",A,1
BERT,"Pre-training data The pre-training procedure largely follows the existing literature on language model pre-training. For the pre-training corpus we use the BooksCorpus (800M words) (Zhu et al., 2015) and English Wikipedia (2,500M words). For Wikipedia we extract only the text passages and ignore lists, tables, and headers. It is critical to use a document-level corpus rather than a shuffled sentence-level corpus such as the Billion Word Benchmark (Chelba et al., 2013) in order to extract long contiguous sequences.","Initial data preparation The initial data preparation process mostly follows previous research on language model initialization. As the initial training data we utilize the BooksCorpus (800 million words) (Zhu et al., 2015) and the text passages of English Wikipedia (2.5 billion words), excluding lists, tables, and headers. Using a document-level corpus rather than a shuffled sentence-level corpus like the Billion Word Benchmark (Chelba et al., 2013) is crucial for extracting long continuous sequences.","Preliminary training data gathering The preliminary training data collection largely adheres to existing work on language model pre-training. For the preliminary training texts we use the BooksCorpus (800 million terms) (Zhu et al., 2015) and the narrative sections of the English Wikipedia (2.5 billion terms), omitting lists, charts, and titles. It is essential to employ a document-level corpus rather than a jumbled sentence-level corpus such as the Billion Word Benchmark (Chelba et al., 2013) to obtain extended uninterrupted sequences.","Initial training data accumulation The initial training data accumulation largely follows established research on language model pre-training. For the initial training texts we utilize the BooksCorpus (800 million words) (Zhu et al., 2015) and the prose excerpts of the English Wikipedia (2.5 billion words), excluding lists, tables, and headers. Using a document-level corpus rather than a randomized sentence-level corpus such as the Billion Word Benchmark (Chelba et al., 2013) is vital for extracting lengthy continuous sequences.",A,1
BERT,"Fine-tuning is straightforward since the selfattention mechanism in the Transformer allows BERT to model many downstream tasks— whether they involve single text or text pairs—by swapping out the appropriate inputs and outputs. For applications involving text pairs, a common pattern is to independently encode text pairs before applying bidirectional cross attention, such as Parikh et al. (2016); Seo et al. (2017). BERT instead uses the self-attention mechanism to unify these two stages, as encoding a concatenated text pair with self-attention effectively includes bidirectional cross attention between two sentences.","Fine-tuning is simple because the self-attention system in the Transformer enables BERT to represent many downstream tasks - regardless of whether they include single text or text pairs - by exchanging the suitable inputs and outputs. For uses involving text pairs, a prevalent approach is to independently encode text pairs before applying bidirectional cross attention, like Parikh et al. (2016); Seo et al. (2017). Rather, BERT utilizes the self-attention mechanism to combine these two phases, as encoding a concatenated text pair with self-attention essentially incorporates bidirectional cross attention between two sentences.","Fine-tuning is easy since the self-attention component in the Transformer allows BERT to model various downstream tasks - single text or text pairs - by substituting the right inputs and outputs. For uses with text pairs, a common technique is to separately encode text pairs before applying bidirectional cross attention, such as Parikh et al. (2016); Seo et al. (2017). However, BERT uses the self-attention mechanism to unify these two steps, as encoding a concatenated text pair with self-attention effectively encompasses bidirectional cross attention between two sentences.  ","Fine-tuning is uncomplicated because the self-attention feature in the Transformer enables BERT to represent many downstream tasks - solitary text or text pairs - by switching the suitable inputs and outputs. For applications with text pairs, a prevalent approach is to independently encode text pairs before applying bidirectional cross attention, like Parikh et al. (2016); Seo et al. (2017). Instead, BERT harnesses the self-attention mechanism to combine these two phases, as encoding a concatenated text pair with self-attention fundamentally includes bidirectional cross attention between two sentences.",A,1
BERT,"We use a batch size of 32 and fine-tune for 3 epochs over the data for all GLUE tasks. For each task, we selected the best fine-tuning learning rate (among 5e-5, 4e-5, 3e-5, and 2e-5) on the Dev set. Additionally, for BERTLARGE we found that finetuning was sometimes unstable on small datasets, so we ran several random restarts and selected the best model on the Dev set. With random restarts, we use the same pre-trained checkpoint but perform different fine-tuning data shuffling and classifier layer initialization.","We utilize a batch amount of 32 and refine for 3 time periods over the information for all GLUE assignments. For each task, we chose the most effective refinement learning percentage (among 5e-5, 4e-5, 3e-5, and 2e-5) on the Development set. Furthermore, for BERTLARGE we discovered that finetuning was occasionally unstable on small datasets, so we executed several arbitrary restarts and selected the best model on the Development set. With arbitrary restarts, we employ the same pre-trained checkpoint but execute different fine-tuning data shuffling and classifier layer initialization.","We make use of a batch quantity of 32 and adjust for 3 epochs across the data for all GLUE jobs. For every job, we picked the optimum fine-tuning learning rate (among 5e-5, 4e-5, 3e-5, and 2e-5) on the Dev collection. Also, for BERTLARGE we realized that finetuning was sometimes erratic on small datasets, so we went through multiple random reinitiations and chose the top model on the Dev collection. With random reinitiations, we utilize the same pre-trained checkpoint but do different fine-tuning data shuffling and classifier layer initialization.  ","We utilize a batch amount of 32 and refine for 3 cycles over the data for all GLUE tasks. For each task, we selected the best refinement learning percentage (among 5e-5, 4e-5, 3e-5, and 2e-5) on the Development collection. Furthermore, for BERTLARGE we found that finetuning was occasionally variable on small datasets, so we went through several arbitrary restarts and picked the top model on the Development collection. With arbitrary restarts, we use the same pre-trained checkpoint but do different fine-tuning data shuffling and classifier layer initialization.",A,1
BERT,"Table 2 shows top leaderboard entries as well as results from top published systems (Seo et al., 2017; Clark and Gardner, 2018; Peters et al., 2018a; Hu et al., 2018). The top results from the SQuAD leaderboard do not have up-to-date public system descriptions available, and are allowed to use any public data when training their systems. We therefore use modest data augmentation in our system by first fine-tuning on TriviaQA (Joshi et al., 2017) befor fine-tuning on SQuAD","The second table displays the highest ranked entries on the leaderboard as well as outcomes from the best previously published systems (Seo et al., 2017; Clark and Gardner, 2018; Peters et al., 2018a; Hu et al., 2018). The top performing systems on the SQuAD leaderboard do not have current public descriptions of their systems available, and are permitted to utilize any public data when developing their systems. As a result, we implement minor data enhancement in our system by first tuning on TriviaQA (Joshi et al., 2017) before tuning on SQuAD.","Table number two exhibits the top ranked submissions on the leaderboard along with results from the most successful published systems (Seo et al., 2017; Clark and Gardner, 2018; Peters et al., 2018a; Hu et al., 2018). The highest performing entries on the SQuAD leaderboard do not have up-to-date public explanations of their systems accessible, and are allowed to employ any public information when constructing their systems. Therefore, we implement modest data expansion in our system by first adjusting on TriviaQA (Joshi et al., 2017) before adjusting on SQuAD.  ","The second table shows the highest scoring entries on the leaderboard and outcomes from the best previously released systems (Seo et al., 2017; Clark and Gardner, 2018; Peters et al., 2018a; Hu et al., 2018). The top performing submissions on the SQuAD leaderboard do not have current public descriptions of their systems available, and are permitted to use any public data when developing their systems. As a consequence, we implement minor data augmentation in our system by first fine-tuning on TriviaQA (Joshi et al., 2017) before fine-tuning on SQuAD.",A,1
BERT,"The SQuAD 2.0 task extends the SQuAD 1.1 problem definition by allowing for the possibility that no short answer exists in the provided paragraph, making the problem more realistic. We use a simple approach to extend the SQuAD v1.1 BERT model for this task. We treat questions that do not have an answer as having an answer span with start and end at the [CLS] token.","The SQuAD 2.0 challenge builds on the SQuAD 1.1 definition by permitting there to potentially not be a short response present in the given section, making the issue more practical. We utilize a straightforward technique to expand the SQuAD v1.1 BERT framework for this assignment. We regard inquiries that don't have a response as having an answer range with beginning and end at the [CLS] symbol.","The SQuAD 2.0 job expands the SQuAD 1.1 problem statement by allowing for the possibility that no brief solution is found in the provided passage, which makes the problem more true to life. We implement a simple method to extend the SQuAD v1.1 BERT system for this job. We treat questions without an answer as if they have an answer span starting and ending at the [CLS] token.","The SQuAD 2.0 undertaking broadens the SQuAD 1.1 issue characterization by permitting the chance that no short reply is available in the given section, making the issue more practical. We utilize a basic methodology to grow the SQuAD v1.1 BERT model for this undertaking. We consider questions without a reply as having an answer range beginning and finishing at the [CLS] token.",A,1
BERT,"The Situations With Adversarial Generations (SWAG) dataset contains 113k sentence-pair completion examples that evaluate grounded commonsense inference (Zellers et al., 2018). Given a sentence, the task is to choose the most plausible continuation among four choices. When fine-tuning on the SWAG dataset, we construct four input sequences, each containing the concatenation of the given sentence (sentence A) and a possible continuation (sentence B). ","The Situations With Adversarial Generations (SWAG) dataset has 113,000 sentence-pair completion samples that assess sensible commonsense deduction (Zellers et al., 2018). With a given sentence, the objective is to select the most probable continuation from four options. When fine-tuning on the SWAG dataset, we make four input sequences, each containing the combination of the provided sentence (sentence A) and a potential continuation (sentence B).","The Situations With Adversarial Generations (SWAG) dataset possesses 113,000 sentence-pair completion instances that evaluate grounded commonsense inference (Zellers et al., 2018). Provided a sentence, the goal is to choose the most plausible continuation out of four choices. When adapting on the SWAG dataset, we build four input sequences, each holding the fusion of the given sentence (sentence A) and a feasible continuation (sentence B).  ","The Situations With Adversarial Generations (SWAG) dataset has 113,000 sentence-pair completion examples that assess sensible commonsense reasoning (Zellers et al., 2018). Given a sentence, the aim is to select the most probable continuation from four options. When tuning on the SWAG dataset, we construct four input sequences, each containing the union of the provided sentence (sentence A) and a potential continuation (sentence B).",A,1
BERT,The only task-specific parameters introduced is a vector whose dot product with the [CLS] token representation C denotes a score for each choice which is normalized with a softmax layer. We fine-tune the model for 3 epochs with a learning rate of 2e-5 and a batch size of 16. Results are presented in Table 4. BERTLARGE outperforms the authors’ baseline ESIM+ELMo system by +27.1% and OpenAI GPT by 8.3%.,The sole job-related variables presented is a vector whose inner product with the [CLS] token depiction C indicates a score for each option which is standardized with a softmax layer. We refine the model for 3 epochs with a learning percentage of 2e-5 and a batch amount of 16. Outcomes are shown in Table 4. BERTLARGE surpasses the authors’ baseline ESIM+ELMo structure by +27.1% and OpenAI GPT by 8.3%.,The only assignment-specific parameters introduced is a vector whose dot product with the [CLS] token representation C denotes a score for each choice which is normalized with a softmax layer. We tune the model for 3 cycles with a learning rate of 2e-5 and a batch quantity of 16. Results are displayed in Table 4. BERTLARGE outperforms the writers' foundation ESIM+ELMo system by +27.1% and OpenAI GPT by 8.3%. ,The sole task-dependent variables presented is a vector whose dot product with the [CLS] token depiction C indicates a score for each option which is standardized with a softmax layer. We refine the model for 3 epochs with a learning percentage of 2e-5 and a batch amount of 16. Outcomes are exhibited in Table 4. BERTLARGE surpasses the authors’ baseline ESIM+ELMo structure by +27.1% and OpenAI GPT by 8.3%.,A,1
BERT,"A left-context-only model which is trained using a standard Left-to-Right (LTR) LM, rather than an MLM. The left-only constraint was also applied at fine-tuning, because removing it introduced a pre-train/fine-tune mismatch that degraded downstream performance. Additionally, this model was pre-trained without the NSP task. This is directly comparable to OpenAI GPT, but using our larger training dataset, our input representation, and our fine-tuning scheme.","A unidirectional model that was trained using a conventional Left-to-Right language model instead of a masked language model. The left-context restriction was also enforced during fine-tuning, as eliminating it led to a pre-training/fine-tuning inconsistency that worsened downstream results. Furthermore, this model was pre-trained without the next sentence prediction task. This can be directly contrasted with OpenAI GPT, but leveraging our larger training data, our input representation, and our fine-tuning methodology.","A model that can only see left context and was educated utilizing a standard Left-to-Right language model rather than a masked language model. The left-only limit was also imposed during fine-tuning, since removing it introduced a mismatch between pre-training and fine-tuning that degraded later performance. Additionally, this model was pre-trained without the next sentence prediction exercise. This can be directly compared to OpenAI's GPT, but uses our bigger training data set, our input representation, and our fine-tuning scheme. ","A model trained to only look left using a conventional Left-to-Right language model instead of a masked language model. The left-only constraint was also applied during fine-tuning, as eliminating it created a discrepancy between pre-training and fine-tuning that hurt later results. Furthermore, this model was pre-trained without the next sentence prediction task. This is directly comparable to OpenAI's GPT, but leverages our larger training dataset, our input representation, and our fine-tuning approach.",A,1
BERT,"For SQuAD it is intuitively clear that a LTR model will perform poorly at token predictions, since the token-level hidden states have no rightside context. In order to make a good faith attempt at strengthening the LTR system, we added a randomly initialized BiLSTM on top. This does significantly improve results on SQuAD, but the results are still far worse than those of the pretrained bidirectional models. The BiLSTM hurts performance on the GLUE tasks.","It is obvious that a left-to-right model will do badly at predicting tokens for SQuAD, because the token-level hidden states don't have any context from the right side. To try in good faith to improve the left-to-right system, we added a randomly initialized bidirectional LSTM on top. This does noticeably improve results on SQuAD, but the results are still much worse than those of the pre-trained bidirectional models. The bidirectional LSTM harms performance on the GLUE tasks.","For SQuAD, it is clear that a model that reads only left-to-right will be poor at predicting tokens, since the token-level hidden states lack rightside context. In an honest attempt to strengthen the left-to-right system, we supplemented it with an untrained bidirectional LSTM. This significantly boosts results on SQuAD, but the results are still far inferior to those of pretrained bidirectional models. The bidirectional LSTM degrades performance on the GLUE benchmarks.  ","It is evident that a left-to-right model will perform inadequately at token prediction for SQuAD, because the token-level hidden states have no context from the right side. In a good faith effort to improve the left-to-right system, we added an untrained bidirectional LSTM on top. This does meaningfully enhance results on SQuAD, but the results are still much worse than those of the pre-trained bidirectional models. The bidirectional LSTM impairs performance on the GLUE tasks.",A,1
BERT,"We recognize that it would also be possible to train separate LTR and RTL models and represent each token as the concatenation of the two models, as ELMo does. However: (a) this is twice as expensive as a single bidirectional model; (b) this is non-intuitive for tasks like QA, since the RTL model would not be able to condition the answer on the question; (c) this it is strictly less powerful than a deep bidirectional model, since it can use both left and right context at every layer.","We understand that it could also be feasible to develop separate left-to-right and right-to-left models and depict each token as the combination of the two models, as ELMo does. However: (a) this is two times more costly than a single bidirectional model; (b) this is counterintuitive for tasks like question answering, since the right-to-left model would be unable to base the answer on the question; (c) this is strictly less capable than a deep bidirectional model, since it cannot utilize both left and right context at every layer.","We acknowledge that it would also be possible to create distinct left-to-right and right-to-left systems and represent each token as the merging of the two systems, as ELMo does. However: (a) this is twice as expensive as a single two-way model; (b) this is unintuitive for tasks like QA, since the right-to-left system would be unable to condition the answer on the question; (c) this is strictly less powerful than an in-depth two-way model, since it cannot use both left and right context at each layer.","We realize that it would also be viable to develop separate left-to-right and right-to-left models and depict each token as the fusion of the two models, as ELMo does. However: (a) this is two times more costly than a single two-way model; (b) this is illogical for tasks like questioning answering, since the right-to-left model would be unable to base the answer on the question; (c) this is strictly less capable than an extensive two-way model, since it cannot leverage both left and right context at every layer.",A,1
BERT,"Results on selected GLUE tasks are shown in Table 6. In this table, we report the average Dev Set accuracy from 5 random restarts of fine-tuning. We can see that larger models lead to a strict accuracy improvement across all four datasets, even for MRPC which only has 3,600 labeled training examples, and is substantially different from the pre-training tasks. It is also perhaps surprising that we are able to achieve such significant improvements on top of models which are already quite large relative to the existing literature.","The performance numbers on some GLUE benchmarks are presented in Table 6. This table displays the mean validation set accuracy across 5 random initializations of fine-tuning. We observe that bigger models yield better accuracy on all four datasets, even for MRPC which has only 3,600 labeled training samples, and is very different from the pre-training objectives. It is also somewhat unexpected that we attain such large gains above models that are already quite big compared to previous work.","The results for selected GLUE tasks are given in Table 6. This table shows the average validation accuracy from 5 different random restarts of fine-tuning. We notice that larger models lead to better accuracy on all four datasets, even for MRPC which has only 3,600 labeled training instances, and is very different from the pre-training tasks. It is also perhaps surprising that we achieve such significant improvements on top of models which are already quite large compared to existing research.","The performance on some GLUE benchmarks is presented in Table 6. This table provides the mean dev set accuracy across 5 random initializations of fine-tuning. We can observe that bigger models result in higher accuracy across all four datasets, even for MRPC which has only 3,600 labeled training examples, and differs substantially from the pre-training objectives. It is also unexpected that we obtain such considerable gains above models that are already quite large relative to previous literature.",A,1
BERT,"It has long been known that increasing the model size will lead to continual improvements on large-scale tasks such as machine translation and language modeling, which is demonstrated by the LM perplexity of held-out training data shown in Table 6. However, we believe that this is the first work to demonstrate convincingly that scaling to extreme model sizes also leads to large improvements on very small scale tasks, provided that the model has been sufficiently pre-trained.","For a long time, people have understood that making machine learning models bigger improves performance on large jobs like translation and language modeling. This is shown in Table 6, which displays language model perplexity on held-out training data. However, we think this is the first research to clearly prove that going to really big model sizes also substantially boosts accuracy on tiny tasks, if the model is pre-trained enough.","It has been known for quite some time that expanding the scale of a model results in steady enhancements on large tasks such as machine translation and language modeling. This is exhibited by the language model perplexity of held-out training information found in Table 6. However, our belief is that this is the first piece of work to persuasively demonstrate that increasing to very large model sizes also causes significant improvements on very small tasks, assuming the model has gone through adequate pre-training. ","For many years, researchers have been aware that growing the size of a machine learning model leads to ongoing improvements on big jobs like translation and language modeling. You can see this in Table 6, which shows language model perplexity on held-out training data. But our view is that this is the first study to compellingly prove that moving to really enormous model sizes also produces big gains on tiny tasks, if the model has sufficiently pre-trained.",A,1
BERT,"All of the BERT results presented so far have used the fine-tuning approach, where a simple classification layer is added to the pre-trained model, and all parameters are jointly fine-tuned on a downstream task. However, the feature-based approach, where fixed features are extracted from the pretrained model, has certain advantages. First, not all tasks can be easily represented by a Transformer encoder architecture, and therefore require a task-specific model architecture to be added.","The BERT outcomes shown thus far have utilized fine-tuning, where a basic classification layer is appended to the pre-trained model, and all parameters are collectively fine-tuned on a downstream task. However, extracting fixed features from the pretrained model has certain benefits. First, not every task can be easily represented by a Transformer encoder architecture, so a task-specific model design needs to be added.","All prior BERT results used fine-tuning, adding a simple classification layer to the pre-trained model and jointly fine-tuning all parameters on a downstream task. However, extracting static features from the pretrained model has advantages. First, some tasks can't be easily modeled by a Transformer encoder, requiring a task-specific architecture. ","The BERT outputs so far used fine-tuning, appending a basic classification layer to the pre-trained model, and jointly fine-tuning all parameters on a downstream task. However, extracting immutable features from the pretrained model has upsides. First, certain tasks can't be readily depicted by a Transformer encoder design, necessitating a task-specific architecture.",A,1
BERT,"In this section, we compare the two approaches by applying BERT to the CoNLL-2003 Named Entity Recognition (NER) task (Tjong Kim Sang and De Meulder, 2003). In the input to BERT, we use a case-preserving WordPiece model, and we include the maximal document context provided by the data. Following standard practice, we formulate this as a tagging task but do not use a CRF layer in the output. We use the representation of the first sub-token as the input to the token-level classifier over the NER label set","This part examines the two methods by utilizing BERT for the CoNLL-2003 Named Entity Recognition (NER) task (Tjong Kim Sang and De Meulder, 2003). For BERT's input, we employ a case-preserving WordPiece model and incorporate the maximum document context given in the data. As is standard practice, we frame this as a tagging task but do not utilize a CRF layer in the output. We use the representation of the first sub-token as the input to the token-level classifier over the NER label set.","In this portion, we analyze the two techniques through applying BERT to the CoNLL-2003 Named Entity Recognition (NER) challenge (Tjong Kim Sang and De Meulder, 2003). For BERT's inputs, we use a case-keeping WordPiece model and include the maximum document context provided in the data. Per common practice, we formulate this as a tagging problem but do not employ a CRF layer in the output. We utilize the representation of the first sub-token as the input to the token-level classifier over the NER label set.  ","Here, we compare the two methods by using BERT for the CoNLL-2003 Named Entity Recognition (NER) task (Tjong Kim Sang and De Meulder, 2003). For BERT's inputs, we apply a case-retaining WordPiece model and incorporate the maximal document context present in the data. As is standard, we frame this as a tagging issue but do not employ a CRF layer in the output. We use the representation of the first sub-token as the input to the token-level classifier over the NER label set.",A,1
BERT,"To ablate the fine-tuning approach, we apply the feature-based approach by extracting the activations from one or more layers without fine-tuning any parameters of BERT. These contextual embeddings are used as input to a randomly initialized two-layer 768-dimensional BiLSTM before the classification layer. Results are presented in Table 7. BERTLARGE performs competitively with state-of-the-art methods. The best performing method concatenates the token representations from the top four hidden layers of the pre-trained Transformer, which is only 0.3 F1 behind fine-tuning the entire model. This demonstrates that BERT is effective for both finetuning and feature-based approaches.","To evaluate the fine-tuning method, we use the feature-extraction approach by taking out the activations from one or more tiers without adjusting any BERT parameters. These context-based embeddings are utilized as inputs to an arbitrarily started two-layer 768-dimensional BiLSTM before the categorization layer. Outcomes are exhibited in Table 7. BERTLARGE acts competitively with cutting edge techniques. The best performing technique concatenates the token representations from the top four concealed layers of the pre-prepared Transformer, which is just 0.3 F1 behind fine-tuning the whole model. This shows that BERT is compelling for both finetuning and feature-based methodologies.","To assess the fine-tuning procedure, we implement the feature-extraction method by extracting the activations from one or more levels without modifying any BERT parameters. These context-dependent embeddings are fed into a randomly initialized two-layer 768-dimensional BiLSTM before the classification layer. Results are presented in Table 7. BERTLARGE performs comparably with state-of-the-art approaches. The top performing approach joins together the token representations from the top four hidden layers of the pre-trained Transformer, which is only 0.3 F1 behind fine-tuning the whole model. This proves that BERT is effective for both finetuning and feature-extraction methods.  ","To evaluate the fine-tuning technique, we use the feature-based approach by taking out the activations from one or more strata without changing any BERT parameters. These context-reliant embeddings are utilized as inputs to an arbitrarily begun two-layer 768-dimensional BiLSTM before the categorization layer. Outcomes are shown in Table 7. BERTLARGE acts competitively with cutting edge strategies. The best performing strategy consolidates the token representations from the top four concealed layers of the pre-prepared Transformer, which is just 0.3 F1 behind fine-tuning the entire model. This exhibits that BERT is compelling for both finetuning and feature-based methodologies.",A,1
BERT,"Recent empirical improvements due to transfer learning with language models have demonstrated that rich, unsupervised pre-training is an integral part of many language understanding systems. In particular, these results enable even low-resource tasks to benefit from deep unidirectional architectures. Our major contribution is further generalizing these findings to deep bidirectional architectures, allowing the same pre-trained model to successfully tackle a broad set of NLP tasks.","The latest experimental advancements using transfer learning with language models have shown that ample, unsupervised pre-training is a key component of many language comprehension systems. Specifically, these findings allow even tasks with limited resources to benefit from deep one-directional architectures. Our main contribution is expanding these results to deep two-directional architectures, enabling the same pre-trained model to effectively address a wide range of natural language processing tasks.","Recent empirical enhancements utilizing transfer learning with language models have demonstrated that abundant, unsupervised pre-training is a vital part of many language understanding systems. In particular, these outcomes enable even tasks with scarce resources to gain from deep one-way architectures. Our major contribution is additionally generalizing these conclusions to deep two-way architectures, permitting the same pre-trained model to successfully tackle a wide variety of NLP tasks. ","The latest experimental improvements leveraging transfer learning with language models have shown that rich, unsupervised pre-training is an integral part of many language comprehension systems. Specifically, these findings allow even tasks with limited data to benefit from deep one-directional architectures. Our main contribution is further extending these results to deep bidirectional architectures, enabling the same pre-trained model to effectively handle a diverse range of natural language processing tasks.",A,1
Deep contextualized word representations,"Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pretrained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals. 1 Introduction Pre-trained word representations (Mikolov et al., 2013; Pennington et al., 2014) are a key component in many neural language understanding models. ","Our word vectors are derived from the inner workings of a deep two-way language model (biLM), which is pre-trained on a large collection of text. We demonstrate that these representations can be seamlessly incorporated into present models and substantially enhance the state-of-the-art across six tough NLP tasks, including question answering, textual entailment and sentiment analysis. We also provide an analysis exhibiting that revealing the deep internals of the pre-trained network is vital, enabling downstream models to mix various kinds of semi-supervision signals. 1 Introduction Pre-trained word representations (Mikolov et al., 2013; Pennington et al., 2014) are a key component in many neural language understanding models.","Our word vectors are obtained from the internal mechanisms of a deep bidirectional language model (biLM), which is pre-conditioned on a substantial text corpus. We illustrate that these representations can be easily added to current models and meaningfully improve the state-of-the-art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an examination showing that exposing the deep internals of the pre-conditioned network is critical, permitting downstream models to combine different types of semi-supervision cues. 1 Introduction Pre-trained word representations (Mikolov et al., 2013; Pennington et al., 2014) are a key component in many neural language understanding models.  ","Our word vectors are derived from the internal workings of a deep two-way language model (biLM), which is pre-trained on a large text dataset. We show that these representations can be seamlessly incorporated into existing models and substantially improve the state-of-the-art across six difficult NLP tasks, including question answering, textual entailment and sentiment analysis. We also provide an analysis demonstrating that revealing the deep internals of the pre-trained network is vital, allowing downstream models to integrate different types of semi-supervision signals. 1 Introduction Pre-trained word representations (Mikolov et al., 2013; Pennington et al., 2014) are a key component in many neural language understanding models.",A,1
Deep contextualized word representations,"However, learning high quality representations can be challenging. They should ideally model both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). In this paper, we introduce a new type of deep contextualized word representation that directly addresses both challenges, can be easily integrated into existing models, and significantly improves the state of the art in every considered case across a range of challenging language understanding problems. ","Nevertheless, acquiring excellent symbolic depictions can be tricky. Preferably, they ought to characterize both (1) intricate attributes of word usage (for instance, syntax and semantics), and (2) how these usages differ across linguistic situations (that is, to represent polysemy). In this paper, we present a new kind of profound contextualized word symbolism that straightforwardly tends to both difficulties, can be effortlessly coordinated into existing models, and essentially improves the state of the art in every considered case over an assortment of testing language understanding issues.","However, learning high-caliber representations is not easy. They should model the complex features of how words are used (e.g. syntax and semantics) as well as how these uses vary across different linguistic contexts (i.e. to capture polysemy). Here, we introduce a new type of deep contextualized word representation that tackles both challenges head-on. It can be seamlessly incorporated into current models and substantially boosts state-of-the-art performance on a range of tough language understanding tasks.","Nonetheless, acquiring high-quality depictions can be difficult. Ideally, they should portray both (1) intricate qualities of word utilization (for example, syntax and semantics), and (2) how these employments differ across etymological settings (that is, to demonstrate polysemy). In this paper, we present another sort of profound contextualized word portrayal that straightforwardly tends to the two difficulties, can be handily joined into existing models, and essentially improves the state of the craftsmanship in each considered case across an assortment of testing language understanding issues.",A,1
Deep contextualized word representations,"Our representations differ from traditional word type embeddings in that each token is assigned a representation that is a function of the entire input sentence. We use vectors derived from a bidirectional LSTM that is trained with a coupled language model (LM) objective on a large text corpus. For this reason, we call them ELMo (Embeddings from Language Models) representations. Unlike previous approaches for learning contextualized word vectors (Peters et al., 2017; McCann et al., 2017), ELMo representations are deep, in the sense that they are a function of all of the internal layers of the biLM. ","Our representations are different from standard word embeddings because each word is given a representation based on the full input sentence. We utilize vectors from a bidirectional LSTM trained with a combined language model goal on a large text dataset. Therefore, we name them ELMo representations. In contrast to past approaches for learning context-dependent word vectors (Peters et al., 2017; McCann et al., 2017), ELMo representations are deep, meaning they depend on all the internal layers of the biLM.","Our representations diverge from conventional word type embeddings since every token receives a depiction reliant on the whole input statement. We employ vectors derived from a bidirectional LSTM cultivated with a coupled language model aim on an expansive text corpus. For this rationale, we entitle them ELMo delineations. Dissimilar to preceding methodologies for acquiring contextualized word vectors (Peters et al., 2017; McCann et al., 2017), ELMo delineations are profound, in the sense that they are a capacity of all of the internal layers of the biLM.  ","Our representations differ from traditional word embeddings in that each token gets a depiction based on the full input sentence. We utilize vectors from a bidirectional LSTM trained with a combined language modeling goal on a large text dataset. Therefore, we term them ELMo representations. Unlike past approaches for acquiring context-dependent word vectors (Peters et al., 2017; McCann et al., 2017), ELMo representations are deep, meaning they depend on all the internal layers of the biLM.",A,1
Deep contextualized word representations,"More specifically, we learn a linear combination of the vectors stacked above each input word for each end task, which markedly improves performance over just using the top LSTM layer. Combining the internal states in this manner allows for very rich word representations. Using intrinsic evaluations, we show that the higher-level LSTM states capture context-dependent aspects of word meaning (e.g., they can be used without modification to perform well on supervised word sense disambiguation tasks) while lowerlevel states model aspects of syntax (e.g., they can be used to do part-of-speech tagging). ","In other words, we determine a linear mixture of the vectors piled on top of each input word for each final job, which substantially enhances performance over only utilizing the highest LSTM layer. Uniting the internal conditions in this way permits very abundant word representations. Employing intrinsic assessments, we demonstrate that the more advanced LSTM conditions grab context-dependent features of word meaning (for instance, they can be utilized without alteration to execute well on supervised word sense disambiguation assignments) while lower-level conditions model facets of syntax (for example, they can be leveraged to do part-of-speech tagging).","More specifically, we calculate a linear blend of the vectors stacked over every input word for each concluding task, which markedly improves results over just harnessing the topmost LSTM tier. Integrating the internal statuses in this fashion enables very rich word depictions. Using inherent evaluations, we establish that the higher LSTM statuses catch context-dependent aspects of word significance (e.g., they can be utilized as-is to perform well on supervised word sense disambiguation jobs) while lower-level statuses model aspects of syntax (e.g., they can be leveraged to do part-of-speech tagging).","In other words, we determine a linear combination of the vectors piled on top of each input word for each final task, which substantially improves performance over only leveraging the highest LSTM layer. Combining the internal states in this way allows for very abundant word representations. Using intrinsic assessments, we demonstrate that the more advanced LSTM states capture context-dependent facets of word meaning (for example, they can be utilized without modification to perform well on supervised word sense disambiguation tasks) while lower-level states model aspects of syntax (for instance, they can be used to do part-of-speech tagging).",A,1
Deep contextualized word representations,"Simultaneously exposing all of these signals is highly beneficial, allowing the learned models select the types of semi-supervision that are most useful for each end task. Extensive experiments demonstrate that ELMo representations work extremely well in practice. We first show that they can be easily added to existing models for six diverse and challenging language understanding problems, including textual entailment, question answering and sentiment analysis. The addition of ELMo representations alone significantly improves the state of the art in every case, including up to 20% relative error reductions. For tasks where direct comparisons are possible, ELMo outperforms CoVe (McCann et al., 2017), which computes contextualized representations using a neural machine translation encoder. ","Presenting all of these signals at the same time is very advantageous, as it allows the learned models to choose the types of semi-supervision that are most beneficial for each final task. Comprehensive experiments show that ELMo representations are extremely effective in practice. We first demonstrate that they can be seamlessly incorporated into present models for six varied and difficult language understanding tasks, including textual entailment, question answering and sentiment analysis. Just adding the ELMo representations significantly improves the state of the art in every case, with relative error reductions of up to 20%. For tasks where direct comparisons are feasible, ELMo surpasses CoVe (McCann et al., 2017), which generates contextualized representations utilizing a neural machine translation encoder.","Exposing all of these signals together is highly useful, permitting the learned models to select the semi-supervision types that are most valuable for each target task. Wide-ranging experiments prove that ELMo representations work incredibly well in reality. We first exhibit that they can be easily integrated into existing models for six diverse and challenging language understanding problems, like textual entailment, question answering and sentiment analysis. Merely incorporating the ELMo representations alone considerably improves the state of the art in every case, including up to 20% relative error decreases. For tasks where direct comparisons are possible, ELMo beats CoVe (McCann et al., 2017), which produces contextualized representations employing a neural machine translation encoder.","Presenting all of these signals at once is very beneficial, enabling the learned models to choose the semi-supervision forms that are most helpful for each final task. Extensive experiments demonstrate that ELMo representations are extremely effective in practice. We first show that they can be seamlessly added to current models for six varied and tough language understanding tasks, including textual entailment, question answering and sentiment analysis. Simply integrating the ELMo representations by itself significantly enhances the state of the art in every case, with relative error reductions of up to 20%. For tasks where direct comparisons are viable, ELMo is superior to CoVe (McCann et al., 2017), which generates contextualized representations using a neural machine translation encoder.",A,1
Deep contextualized word representations,"Finally, an analysis of both ELMo and CoVe reveals that deep representations outperform arXiv:1802.05365v2 [cs.CL] 22 Mar 2018 those derived from just the top layer of an LSTM. Our trained models and code are publicly available, and we expect that ELMo will provide similar gains for many other NLP problems.1 2 Related work Due to their ability to capture syntactic and semantic information of words from large scale unlabeled text, pretrained word vectors (Turian et al., 2010; Mikolov et al., 2013; Pennington et al., 2014) are a standard component of most state-ofthe-art NLP architectures, including for question answering (Liu et al., 2017), textual entailment (Chen et al., 2017) and semantic role labeling (He et al., 2017). However, these approaches for learning word vectors only allow a single contextindependent representation for each word. ","In conclusion, an examination of both ELMo and CoVe shows that representations derived from deeper layers outperform those from just the top layer of an LSTM. We have published our trained models and code, and we anticipate ELMo will give similar improvements on many other natural language processing tasks. Related work: Because of their capacity to learn syntactic and semantic information about words from large unlabeled corpora, pre-trained word vectors (Turian et al., 2010; Mikolov et al., 2013; Pennington et al., 2014) are a standard part of most state-of-the-art NLP systems, including for question answering (Liu et al., 2017), textual entailment (Chen et al., 2017) and semantic role labeling (He et al., 2017). However, these approaches for learning word vectors only enable a single context-independent representation per word.","In summary, analyzing both ELMo and CoVe demonstrates that representations from deeper layers are superior to those from just the top layer of an LSTM. We have published our models and code publicly, and we think ELMo will yield similar gains on many other natural language tasks. Prior work: Due to their ability to capture syntactic and semantic properties of words from large unlabeled text, pre-trained word vectors (Turian et al., 2010; Mikolov et al., 2013; Pennington et al., 2014) are a standard part of most cutting-edge NLP systems, including for question answering (Liu et al., 2017), textual entailment (Chen et al., 2017) and semantic role labeling (He et al., 2017). However, these word vector learning approaches only allow one context-independent representation per word.  ","To conclude, studying both ELMo and CoVe shows deep representations are better than those from just the top LSTM layer. We've released our models and code publicly, and expect ELMo will similarly improve many other NLP problems. Previous work: Since they can learn syntactic and semantic word properties from large unlabeled text, pre-trained word vectors (Turian et al., 2010; Mikolov et al., 2013; Pennington et al., 2014) are standard in most state-of-the-art NLP, like question answering (Liu et al., 2017), textual entailment (Chen et al., 2017) and semantic role labeling (He et al., 2017). However, these word vector approaches only enable one context-independent representation per word.",A,1
Deep contextualized word representations,"Previously proposed methods overcome some of the shortcomings of traditional word vectors by either enriching them with subword information (e.g., Wieting et al., 2016; Bojanowski et al., 2017) or learning separate vectors for each word sense (e.g., Neelakantan et al., 2014). Our approach also benefits from subword units through the use of character convolutions, and we seamlessly incorporate multi-sense information into downstream tasks without explicitly training to predict predefined sense classes. Other recent work has also focused on learning context-dependent representations. ","Earlier suggested techniques get around certain flaws of standard word vectors either by supplementing them with subword data (for example, Wieting et al., 2016; Bojanowski et al., 2017) or generating distinct vectors for each word meaning (for instance, Neelakantan et al., 2014). Our method also profits from subword parts through the application of character convolutions, and we effortlessly integrate multi-sense information into downstream tasks without explicitly teaching to predict predefined sense categories. Additional recent work has also concentrated on learning context-dependent representations.","Past proposed procedures conquer some of the deficiencies of conventional word vectors by enriching them with subword knowledge (e.g., Wieting et al., 2016; Bojanowski et al., 2017) or producing separate vectors for each word sense (e.g., Neelakantan et al., 2014). Our approach also benefits from subword units through the utilization of character convolutions, and we seamlessly incorporate multi-sense information into downstream tasks without explicitly instructing to predict predefined sense classes. Other latest work has also focused on learning context-dependent representations.","Earlier suggested techniques overcome certain shortcomings of traditional word vectors by supplementing them with subword data (for instance, Wieting et al., 2016; Bojanowski et al., 2017) or generating distinct vectors for each word meaning (for example, Neelakantan et al., 2014). Our method also gains from subword components through the use of character convolutions, and we seamlessly integrate multi-sense information into downstream tasks without explicitly teaching to predict predefined sense categories. Additional recent work has also concentrated on learning context-dependent representations.",A,1
Deep contextualized word representations,"context2vec (Melamud et al., 2016) uses a bidirectional Long Short Term Memory (LSTM; Hochreiter and Schmidhuber, 1997) to encode the context around a pivot word. Other approaches for learning contextual embeddings include the pivot word itself in the representation and are computed with the encoder of either a supervised neural machine translation (MT) system (CoVe; McCann et al., 2017) or an unsupervised language model (Peters et al., 2017). Both of these approaches benefit from large datasets, although the MT approach is limited by the size of parallel corpora. ","context2vec (Melamud et al., 2016) utilizes a bidirectional Long Short Term Memory neural network (Hochreiter and Schmidhuber, 1997) to encode the surroundings of a central word. Other techniques for learning contextual representations also include the central word itself in the depiction and are calculated with the encoder of either a supervised neural machine translation system (CoVe; McCann et al., 2017) or an unsupervised language model (Peters et al., 2017). Both of these methodologies benefit from large datasets, however the machine translation approach is constrained by the size of parallel corpora.","context2vec (Melamud et al., 2016) employs a bidirectional Long Short Term Memory neural network (Hochreiter and Schmidhuber, 1997) to encode the context around a focal word. Additional approaches for acquiring contextual representations also incorporate the focal word itself in the representation and are derived using the encoder of either a supervised neural machine translation model (CoVe; McCann et al., 2017) or an unsupervised language model (Peters et al., 2017). Both of these techniques benefit from substantial datasets, though the machine translation technique is limited by the extent of parallel corpora.  ","context2vec (Melamud et al., 2016) makes use of a bidirectional Long Short Term Memory neural network (Hochreiter and Schmidhuber, 1997) to encode the surroundings of a key word. Other methods for obtaining contextual representations also include the key word itself in the representation and are generated using the encoder of either a supervised neural machine translation system (CoVe; McCann et al., 2017) or an unsupervised language model (Peters et al., 2017). Both of these approaches take advantage of large datasets, although the machine translation approach is constrained by the size of parallel corpora.",A,1
Deep contextualized word representations,"In this paper, we take full advantage of access to plentiful monolingual data, and train our biLM on a corpus with approximately 30 million sentences (Chelba et al., 2014). We also generalize these approaches to deep contextual representations, which we show work well across a broad range of diverse NLP tasks. For example, introducing multi-task syntactic supervision (e.g., part-of-speech tags) at the lower levels of a deep LSTM can improve overall performance of higher level tasks such as dependency parsing (Hashimoto et al., 2017) or CCG super tagging (Søgaard and Goldberg, 2016). In an RNN-based encoder-decoder machine translation system, Belinkov et al. (2017) showed that the representations learned at the first layer in a 2- layer LSTM encoder are better at predicting POS tags then second layer. ","In this document, we make the most of access to abundant single-language information, and teach our biLM on a collection containing around 30 million sentences (Chelba et al., 2014). We also generalize these methodologies to profound contextual representations, which we demonstrate work admirably over an extensive scope of different NLP errands. For instance, presenting multi-task syntactic supervision (e.g., part-of-discourse labels) at the lower levels of a profound LSTM can improve by and large execution of higher level undertakings like reliance parsing (Hashimoto et al., 2017) or CCG super labeling (Søgaard and Goldberg, 2016). In a RNN-based encoder-decoder machine interpretation framework, Belinkov et al. (2017) demonstrated that the representations learned at the initial layer in a 2-layer LSTM encoder are better at anticipating POS labels then the second layer.","In this article, we capitalize on access to abundant single-language data, and educate our biLM on a corpus containing approximately 30 million sentences (Chelba et al., 2014). We additionally generalize these techniques to profound contextual representations, which we exhibit work admirably over a wide scope of various NLP errands. For instance, presenting multi-task syntactic supervision (e.g., part-of-discourse labels) at the lower levels of a profound LSTM can improve generally execution of higher level undertakings like reliance parsing (Hashimoto et al., 2017) or CCG super marking (Søgaard and Goldberg, 2016). In a RNN-based encoder-decoder machine interpretation framework, Belinkov et al. (2017) showed that the representations learned at the initial layer in a 2-layer LSTM encoder are better at anticipating POS labels then the second layer.","In this paper, we take full advantage of access to abundant single-language data, and train our biLM on a corpus containing around 30 million sentences (Chelba et al., 2014). We also generalize these techniques to deep contextual representations, which we demonstrate work well across a wide range of diverse NLP tasks. For example, introducing multi-task syntactic supervision (e.g., part-of-speech tags) at the lower levels of a deep LSTM can improve overall performance of higher level tasks like dependency parsing (Hashimoto et al., 2017) or CCG super tagging (Søgaard and Goldberg, 2016). In a RNN-based encoder-decoder machine translation system, Belinkov et al. (2017) showed that the representations learned at the first layer in a 2-layer LSTM encoder are better at predicting POS tags than the second layer.",A,1
Deep contextualized word representations,"Finally, the top layer of an LSTM for encoding word context (Melamud et al., 2016) has been shown to learn representations of word sense. We show that similar signals are also induced by the modified language model objective of our ELMo representations, and it can be very beneficial to learn models for downstream tasks that mix these different types of semi-supervision. Dai and Le (2015) and Ramachandran et al. (2017) pretrain encoder-decoder pairs using language models and sequence autoencoders and then fine tune with task specific supervision. ","In conclusion, the highest layer of an LSTM used for encoding word context (Melamud et al., 2016) has demonstrated an ability to learn representations of word sense. We illustrate that comparable signals are also created by the altered language model goal of our ELMo representations, and combining these different kinds of semi-supervision can be very advantageous for downstream task learning. Dai and Le (2015) and Ramachandran et al. (2017) pre-train encoder-decoder duos utilizing language models and sequence autoencoders followed by fine tuning with supervision particular to the task.","To summarize, the topmost layer of an LSTM utilized for encoding word context (Melamud et al., 2016) has shown an aptitude for learning representations of word sense. We exhibit that similar signals are also produced by the modified language modeling objective of our ELMo representations, and fusing these different semi-supervised techniques can be highly beneficial for learning downstream tasks. Dai and Le (2015) and Ramachandran et al. (2017) pre-train encoder-decoder pairs by using language models and sequence autoencoders then fine tune with task-specific supervision.","In closing, the highest layer of an LSTM used for encoding word context (Melamud et al., 2016) has demonstrated an ability to learn representations of word meaning. We show that comparable signals are also generated by the altered language model goal of our ELMo representations, and combining these different semi-supervised learnings can be very advantageous for learning downstream tasks. Dai and Le (2015) and Ramachandran et al. (2017) pre-train encoder-decoder teams using language models and sequence autoencoders then fine tune with supervision specific to the task.",A,1
Deep contextualized word representations,"In contrast, after pretraining the biLM with unlabeled data, we fix the weights and add additional taskspecific model capacity, allowing us to leverage large, rich and universal biLM representations for cases where downstream training data size dictates a smaller supervised model. 3 ELMo: Embeddings from Language Models Unlike most widely used word embeddings (Pennington et al., 2014), ELMo word representations are functions of the entire input sentence, as described in this section. They are computed on top of two-layer biLMs with character convolutions (Sec. 3.1), as a linear function of the internal network states (Sec. 3.2).","Conversely, after pre-training the bidirectional language model with unlabeled information, we stabilize the weights and supplement further task-specific model ability, enabling us to take advantage of large, rich and universal bidirectional language model representations for situations where downstream training data volume necessitates a smaller supervised model. ","In contrast, after pre-conditioning the two-directional language model with non-labeled data, we cement the loads and contribute extra assignment-particular model capacity, permitting us to leverage immense, wealthy and universal two-directional language model depictions for cases where downstream preparing information size commands a smaller supervised model.","However, after pre-teaching the two-way language model with unlabeled information, we set the weights and provide extra task-specific model potential, allowing us to use large, abundant and universal two-way language model representations for cases where downstream training data amount requires a smaller supervised model.",A,1
Deep contextualized word representations,"Recent state-of-the-art neural language models (Jozefowicz et al. ´ , 2016; Melis et al., 2017; Merity et al., 2017) compute a context-independent token representation x LM k (via token embeddings or a CNN over characters) then pass it through L layers of forward LSTMs. We tie the parameters for both the token representation (Θx) and Softmax layer (Θs) in the forward and backward direction while maintaining separate parameters for the LSTMs in each direction. Overall, this formulation is similar to the approach of Peters et al. (2017), with the exception that we share some weights between directions instead of using completely independent parameters. ","The latest and most advanced neural network models for language (Jozefowicz et al.  ́ , 2016; Melis et al., 2017; Merity et al., 2017) generate a representation for each word that does not depend on the context x LM k (using word embeddings or a CNN over individual characters). This representation is then processed through L layers of forward LSTMs. We tie the parameters for both the word representation (Θx) and the Softmax layer (Θs) in the forward and backward passes, but use separate parameters for the LSTMs in each direction. This approach is similar to Peters et al. (2017), except that we share some weights between directions instead of having completely independent parameters.","Cutting-edge neural network models for natural language (Jozefowicz et al.  ́ , 2016; Melis et al., 2017; Merity et al., 2017) create a context-independent representation for each token x LM k (through token embeddings or a convolutional neural network over characters). This token representation is fed through L layers of forward long short-term memory units. We constrain the parameters for the token representation (Θx) and Softmax layer (Θs) to be tied in the forward and backward passes, but allow separate parameters for the LSTMs in each direction. This formulation mirrors Peters et al. (2017), with the difference that we share some weights between directions rather than using fully independent parameters.","The most advanced neural language models recently developed (Jozefowicz et al.  ́ , 2016; Melis et al., 2017; Merity et al., 2017) generate a representation for each token x LM k that does not incorporate context (using token embeddings or a convolutional neural net on characters). This token representation goes through L layers of forward LSTM units. We tie the parameters for the token representation (Θx) and Softmax layer (Θs) in both the forward and backward passes, but keep separate parameters for the LSTMs in each direction. This is similar to Peters et al. (2017), except we share some weights between directions instead of having fully independent parameters.",A,1
Deep contextualized word representations,"In the next section, we depart from previous work by introducing a new approach for learning word representations that are a linear combination of the biLM layers. γ is of practical importance to aid the optimization process (see supplemental material for details). Considering that the activations of each biLM layer have a different distribution, in some cases it also helped to apply layer normalization (Ba et al., 2016) to each biLM layer before weighting. ","In the following part, we diverge from prior work by presenting a new technique for learning word representations that are a linear mix of the biLM layers. γ is practically important to help the optimization process (see extra material for specifics). Given that the activations of each biLM layer have a distinct distribution, in some situations it also assisted to use layer normalization (Ba et al., 2016) to each biLM layer before weighting.","In the next portion, we depart from earlier efforts by introducing a novel method for generating word embeddings that are a linear combination of the biLM layers. γ is of practical value to facilitate the optimization procedure (refer to supplementary information for particulars). Considering the activations of every biLM layer possess a different distribution, in certain cases it also proved beneficial to implement layer normalization (Ba et al., 2016) to each biLM layer prior to weighting.  ","In the following section, we diverge from previous undertakings by presenting a new system for producing word representations that are a linear mixture of the biLM layers. γ is of practical importance to promote the optimization process (see additional documentation for specifics). Given that the activations of each biLM layer have a unique distribution, in some situations it also helped to put into action layer normalization (Ba et al., 2016) to every biLM layer before weighting.",A,1
Deep contextualized word representations,"Given a pre-trained biLM and a supervised architecture for a target NLP task, it is a simple process to use the biLM to improve the task model. We simply run the biLM and record all of the layer representations for each word. Then, we let the end task model learn a linear combination of these representations, as described below. First consider the lowest layers of the supervised model without the biLM. Most supervised NLP models share a common architecture at the lowest layers, allowing us to add ELMo in a consistent, unified manner. ","With an already trained bi-directional language model and a supervised learning model for a specific natural language processing job, it is straightforward to use the biLM to enhance the task model. We just execute the biLM and log all of the layer depictions for each term. After that, we permit the final task model to learn a linear blend of these depictions, as depicted below. First think about the bottom layers of the supervised model without the biLM. Most supervised NLP models have a similar structure in the lower layers, letting us include ELMo in a steady, unified way.","Given a pre-trained bidirectional language model and an architecture for supervised learning on a target NLP task, it is easy to leverage the biLM to improve the task model. We run the biLM and keep track of the representations at each layer for every word. Then, we enable the end task model to learn a linear combination of these representations, as explained in the following. Consider first the lowest layers of the supervised model without biLM. Most supervised NLP models have the same basic architecture in the low layers, allowing us to incorporate ELMo consistently and uniformly.  ","With a pretrained bidirectional language model and a supervised learning architecture for a chosen NLP task, it is straightforward to use the biLM to enhance the task model. We execute the biLM and document all the layer representations for each word. We then let the final task model learn a linear mix of these representations, as described next. First think about the lowermost layers of the supervised model without biLM. Most supervised NLP models share a common low layer architecture, allowing us to add ELMo in a steady, unified fashion.",A,1
Deep contextualized word representations,"Given a sequence of tokens (t1, . . . , tN ), it is standard to form a context-independent token representation xk for each token position using pre-trained word embeddings and optionally character-based representations. Then, the model forms a context-sensitive representation hk, typically using either bidirectional RNNs, CNNs, or feed forward networks. To add ELMo to the supervised model, we first freeze the weights of the biLM and then concatenate the ELMo vector ELMotask k with xk and pass the ELMo enhanced representation [xk; ELMotask k ] into the task RNN. ","Considering a series of tokens (t1, . . . , tN), it is common practice to generate a context-independent representation xk for each token location utilizing pre-trained word vectors and potentially character-level representations. The model then forms a context-sensitive representation hk, often employing either bidirectional RNNs, CNNs, or feedforward networks. To integrate ELMo into the supervised model, we first fix the weights of the biLM and then join the ELMo vector ELMotask k to xk before passing the ELMo-enhanced representation [xk; ELMotask k] into the task RNN.","For a sequence of tokens (t1, . . . , tN), it is standard procedure to create a context-independent token embedding xk for each token position using pre-trained word embeddings and character embeddings where applicable. The model then generates a context-sensitive embedding hk, typically leveraging bidirectional RNNs, CNNs, or feedforward networks. To incorporate ELMo into the supervised model, we first freeze the biLM weights and concatenate the ELMo vector ELMotask k with xk before inputting the ELMo-augmented representation [xk; ELMotask k] into the task RNN.","Considering a series of tokens (t1, . . . , tN), it is conventional to construct a context-independent token representation xk for each token location by utilizing pre-trained word vectors and character-level vectors where relevant. The model then produces a context-sensitive representation hk, commonly applying bidirectional RNNs, CNNs, or feedforward networks. To add ELMo to the supervised model, we first fix the biLM weights and join the ELMo vector ELMotask k to xk prior to feeding the ELMo-enhanced representation [xk; ELMotask k] into the task RNN.",A,1
Deep contextualized word representations,"For some tasks (e.g., SNLI, SQuAD), we observe further improvements by also including ELMo at the output of the task RNN by introducing another set of output specific linear weights and replacing hk with [hk; ELMotask k ]. As the remainder of the supervised model remains unchanged, these additions can happen within the context of more complex neural models. For example, see the SNLI experiments in Sec. 4 where a bi-attention layer follows the biLSTMs, or the coreference resolution experiments where a clustering model is layered on top of the biLSTMs. ","Certain assignments (like SNLI and SQuAD) show additional progress when we also utilize ELMo at the output of the task RNN, introducing another collection of output particular linear weights and swapping hk with [hk; ELMotask k]. Since the rest of the monitored model stays the same, these supplements can develop within more intricate neural models. For instance, observe the SNLI trials in Section 4 where a bi-attention layer comes after the biLSTMs, or the coreference resolution trials where a clustering model is built on top of the biLSTMs.","For some jobs (such as SNLI and SQuAD), we see extra enhancements by incorporating ELMo at the end result of the task RNN too, presenting a different set of output specific linear coefficients and substituting hk with [hk; ELMotask k]. Because the remainder of the regulated model persists unchanged, these additions can transpire inside more complex neural networks. See the SNLI tests in Section 4, where a bi-attention layer follows the biLSTMs, or the coreference resolution tests where a clustering model is layered above the biLSTMs, for example.","Certain tasks (like SNLI and SQuAD) display further improvements when we also make use of ELMo at the output of the task RNN, bringing in another set of output particular linear weights and replacing hk with [hk; ELMotask k]. Since the rest of the supervised model stays unchanged, these additions can occur within more sophisticated neural networks. For instance, observe the SNLI experiments in Section 4 where a bi-attention layer comes after the biLSTMs, or the coreference resolution experiments where a clustering model is built on top of the biLSTMs.",A,1
Deep contextualized word representations,"Finally, we found it beneficial to add a moderate amount of dropout to ELMo (Srivastava et al., 2014) and in some cases to regularize the ELMo weights by adding λkwk 2 2 to the loss. This imposes an inductive bias on the ELMo weights to stay close to an average of all biLM layers. 3.4 Pre-trained bidirectional language model architecture The pre-trained biLMs in this paper are similar to the architectures in Jozefowicz et al. ´ (2016) and Kim et al. (2015), but modified to support joint training of both directions and add a residual connection between LSTM layers. ","In conclusion, we discovered that applying a moderate amount of dropout to ELMo (Srivastava et al., 2014) and sometimes regularizing the ELMo weights by incorporating λkwk^2_2 into the loss function was beneficial. This enforces an inductive bias on the ELMo weights to remain near an average of all biLM layers. The pre-trained biLMs in this paper have architectures akin to those in Jozefowicz et al. (2016) and Kim et al. (2015), however altered to facilitate joint training of both directions and append a residual link between LSTM layers.","To summarize, adding a moderate degree of dropout to ELMo (Srivastava et al., 2014) and in some situations regularizing the ELMo weights by introducing λkwk^2_2 to the loss proved advantageous. This imposes an inductive predisposition on the ELMo weights to persist near a mean of all biLM layers. The pre-trained biLMs described in this paper possess architectures similar to those detailed in Jozefowicz et al. (2016) and Kim et al. (2015), but modified to enable joint training of both directions and attach a residual connection between LSTM layers.  ","In closing, we established that applying a moderate measure of dropout to ELMo (Srivastava et al., 2014) and occasionally regularizing the ELMo weights by incorporating λkwk^2_2 into the loss was beneficial. This enforces an inductive inclination on the ELMo weights to linger near an average of all biLM layers. The pre-trained biLMs presented in this paper have architectures comparable to those documented in Jozefowicz et al. (2016) and Kim et al. (2015), however altered to facilitate collective training of both directions and append a residual link between LSTM layers.",A,1
Deep contextualized word representations,"We focus on large scale biLMs in this work, as Peters et al. (2017) highlighted the importance of using biLMs over forward-only LMs and large scale training. To balance overall language model perplexity with model size and computational requirements for downstream tasks while maintaining a purely character-based input representation, we halved all embedding and hidden dimensions from the single best model CNN-BIG-LSTM in Jozefowicz et al. ´ (2016). The final model uses L = 2 biLSTM layers with 4096 units and 512 dimension projections and a residual connection from the first to second layer. ","Our research concentrates on sizable bidirectional language models since Peters and colleagues (2017) emphasized the value of bidirectional over unidirectional language models and large-scale training. To find a balance between the overall perplexity of the language model, model size, computational needs for later tasks, and keeping a purely character-based input representation, we reduced all embedding and hidden dimensions by half compared to the best single model CNN-BIG-LSTM described by Jozefowicz and colleagues (2016). The final model utilizes 2 biLSTM layers with 4096 units and 512 dimension projections plus a residual connection from the first to the second layer.","We focus our work on large biLMs because Peters et al. (2017) showed the importance of using biLMs instead of LMs that only look forward, and of large scale training. To balance language model perplexity, model size, computational requirements for downstream tasks, and maintaining a character-only input representation, we halved all embedding and hidden dimensions compared to the best single model CNN-BIG-LSTM from Jozefowicz et al. (2016). The final model has 2 biLSTM layers with 4096 units and 512 dimension projections and a residual connection from layer 1 to layer 2.","Our research centers on big bidirectional language models since Peters and coauthors (2017) stressed the value of bidirectional over unidirectional language models and large-scale training. To balance overall language model perplexity, model size, computational needs for later tasks, and keeping a character-only input representation, we cut all embedding and hidden dimensions in half compared to the top single model CNN-BIG-LSTM described by Jozefowicz and colleagues (2016). The final model uses two biLSTM layers with 4096 units and 512 dimension projections plus a residual connection from the first to the second layer.",A,1
Deep contextualized word representations,"The context insensitive type representation uses 2048 character n-gram convolutional filters followed by two highway layers (Srivastava et al., 2015) and a linear projection down to a 512 representation. As a result, the biLM provides three layers of representations for each input token, including those outside the training set due to the purely character input. In contrast, traditional word embedding methods only provide one layer of representation for tokens in a fixed vocabulary. After training for 10 epochs on the 1B Word Benchmark (Chelba et al., 2014), the average forward and backward perplexities is 39.7, compared to 30.0 for the forward CNN-BIG-LSTM. ","The type representation that does not consider context uses 2048 character n-gram convolutional filters and then two highway layers (Srivastava et al., 2015) and a linear projection down to a 512 representation. Therefore, the biLM gives three layers of representations for each input token, even those not in the training set because of the purely character input. However, traditional word embedding techniques only give one layer of representation for tokens in a fixed vocabulary. After training for 10 epochs on the 1B Word Benchmark (Chelba et al., 2014), the average forward and backward perplexities is 39.7, compared to 30.0 for the forward CNN-BIG-LSTM.","The context insensitive type representation utilizes 2048 character n-gram convolutional filters followed by two highway networks (Srivastava et al., 2015) and a linear projection to a 512 representation. Consequently, the biLM provides three layers of representations for each input token, including those not in the training set due to the purely character input. In contrast, conventional word embedding methods only provide one layer of representation for tokens in a fixed vocabulary. After 10 epochs of training on the 1B Word Benchmark (Chelba et al., 2014), the average forward and backward perplexities are 39.7, compared to 30.0 for the forward CNN-BIG-LSTM.","The type representation that is oblivious to context employs 2048 character n-gram convolutional filters succeeded by two highway layers (Srivastava et al., 2015) and a linear projection to a 512 representation. Thus, the biLM furnishes three layers of representations for every input token, encompassing those outside the training set owing to the purely character input. However, old-fashioned word embedding techniques only furnish one layer of representation for tokens in a fixed vocabulary. Subsequent to training for 10 epochs on the 1B Word Benchmark (Chelba et al., 2014), the average forward and backward perplexities are 39.7, compared to 30.0 for the forward CNN-BIG-LSTM.",A,1
Deep contextualized word representations,"Generally, we found the forward and backward perplexities to be approximately equal, with the backward value slightly lower. Once pretrained, the biLM can compute representations for any task. In some cases, fine tuning the biLM on domain specific data leads to significant drops in perplexity and an increase in downstream task performance. This can be seen as a type of domain transfer for the biLM. As a result, in most cases we used a fine-tuned biLM in the downstream task. ","In most situations, we saw that the forward and backward perplexities were roughly the same, with the backward perplexity being a little lower. After pretraining, the biLM is able to generate representations for any task. Sometimes, tuning the biLM on data specific to a domain leads to big decreases in perplexity and better performance on downstream tasks. This can be viewed as a kind of domain transfer for the biLM. Therefore, in most cases we utilized a fine-tuned biLM for the downstream task.","Overall, we found that the forward and reverse perplexities were about equal, with the reverse perplexity slightly less. Once pre-trained, the biLM is capable of creating representations for any job. In certain cases, adapting the biLM to domain-specific information results in considerable drops in perplexity and improved downstream task results. This can be considered a type of domain transfer for the biLM. Consequently, we typically used an adjusted biLM for the downstream task.  ","On the whole, we saw that the forward and backward perplexities were roughly comparable, with the backward perplexity being marginally lower. After being pre-trained, the biLM is able to construct representations for any objective. Sometimes, tuning the biLM on data particular to a domain leads to significant decreases in perplexity and better performance on downstream objectives. This can be viewed as a type of domain transfer for the biLM. Thus, in most situations we employed a fine-tuned biLM for the downstream objective.",A,1
Deep contextualized word representations,"See supplemental material for details. Table 1 shows the performance of ELMo across a diverse set of six benchmark NLP tasks. In every task considered, simply adding ELMo establishes a new state-of-the-art result, with relative error reductions ranging from 6 - 20% over strong base models. This is a very general result across a diverse set model architectures and language understanding tasks. In the remainder of this section we provide high-level sketches of the individual task results; see the supplemental material for full experimental details. ","The supplementary information provides specifics. Table 1 displays ELMo's performance over six different benchmark natural language processing tasks. For each task tested, simply integrating ELMo produces the best result so far, with relative error decreases ranging from 6% to 20% compared to strong baseline models. This is a very broad result over many different model designs and language comprehension tasks. The rest of this section briefly summarizes the individual task outcomes; the supplementary material contains full experimental information.","The supplement has the particulars. Table 1 exhibits ELMo's capabilities across six different standard natural language processing jobs. In every job analyzed, just adding ELMo gives the newest top-of-the-line result, with relative error reductions of 6% to 20% over robust baseline models. This is a very widespread finding across various model structures and language understanding tasks. The rest of this section provides high-level outlines of the specific task results; the supplement has the full experimental details.  ","See the extra material for specifics. Table 1 displays ELMo's abilities on six different benchmark natural language processing activities. For each activity tested, simply incorporating ELMo produces the newest state-of-the-art outcome, with relative error decreases of 6% to 20% compared to strong existing models. This is a very general result across diverse model architectures and language comprehension tasks. The remainder of this section provides brief summaries of the individual task results; the extra material has the full experimental information.",A,1
Deep contextualized word representations,"The Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016) contains 100K+ crowd sourced questionanswer pairs where the answer is a span in a given Wikipedia paragraph. Our baseline model (Clark and Gardner, 2017) is an improved version of the Bidirectional Attention Flow model in Seo et al. (BiDAF; 2017). It adds a self-attention layer after the bidirectional attention component, simplifies some of the pooling operations and substitutes the LSTMs for gated recurrent units (GRUs; Cho et al., 2014). After adding ELMo to the baseline model, test set F1 improved by 4.7% from 81.1% to 85.8%, a 24.9% relative error reduction over the baseline, and improving the overall single model state-of-the-art by 1.4%. ","The Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016) has over 100,000 crowdsourced question-answer pairs where the answer is a span in a given Wikipedia paragraph. Our baseline model (Clark and Gardner, 2017) is an enhanced version of the Bidirectional Attention Flow model in Seo et al. (BiDAF; 2017). It incorporates a self-attention layer after the bidirectional attention component, streamlines some of the pooling operations and replaces the LSTMs with gated recurrent units (GRUs; Cho et al., 2014). After incorporating ELMo into the baseline model, test set F1 increased by 4.7% from 81.1% to 85.8%, a 24.9% relative reduction in error over the baseline, and improving the overall single model state-of-the-art by 1.4%.","The Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016) has over 100,000 question-answer pairs sourced from crowdsourcing where the answer is a span in a given Wikipedia paragraph. Our baseline model (Clark and Gardner, 2017) is an enhanced version of the Bidirectional Attention Flow model by Seo et al. (BiDAF; 2017). It adds a self-attention layer after the bidirectional attention component, simplifies some of the pooling operations and uses gated recurrent units (GRUs; Cho et al., 2014) instead of LSTMs. After incorporating ELMo into the baseline model, test set F1 increased by 4.7% from 81.1% to 85.8%, a 24.9% relative reduction in error compared to the baseline, and improving the overall single model state-of-the-art by 1.4%.","The Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016) contains over 100,000 question-answer pairs sourced by crowdsourcing where the answer is a span in a given Wikipedia paragraph. Our baseline model (Clark and Gardner, 2017) is an improved version of the Bidirectional Attention Flow model by Seo et al. (BiDAF; 2017). It incorporates a self-attention layer after bidirectional attention, streamlines some pooling operations and uses gated recurrent units (GRUs; Cho et al., 2014) instead of LSTMs. After adding ELMo to the baseline model, test set F1 score increased by 4.7% from 81.1% to 85.8%, a 24.9% relative error reduction versus the baseline, improving the overall single model state-of-the-art by 1.4%.",A,1
Deep contextualized word representations,"The performance metric varies across tasks – accuracy for SNLI and SST-5; F1 for SQuAD, SRL and NER; average F1 for Coref. Due to the small test sizes for NER and SST-5, we report the mean and standard deviation across five runs with different random seeds. The “increase” column lists both the absolute and relative improvements over our baseline. Textual entailment Textual entailment is the task of determining whether a “hypothesis” is true, given a “premise”. The Stanford Natural Language Inference (SNLI) corpus (Bowman et al., 2015) provides approximately 550K hypothesis/premise pairs. ","The measurement of performance is different for each task - accuracy for SNLI and SST-5; F1 for SQuAD, SRL and NER; mean F1 across five runs for Coref. Because of the small test sets for NER and SST-5, we present the average and standard deviation over five attempts with varying random seeds. The ""increase"" column provides the absolute and relative enhancements over our baseline. Textual entailment is determining if a ""hypothesis"" is true, given a ""premise"". The Stanford Natural Language Inference (SNLI) corpus (Bowman et al., 2015) has around 550K hypothesis/premise examples.","The way we measure how well a model performs varies between tasks - for SNLI and SST-5 it's accuracy; for SQuAD, SRL and NER it's F1 score; for Coref it's the average F1 across multiple runs. Since the test sets for NER and SST-5 are small, we show the mean and standard deviation over 5 runs with different random seeds. The ""increase"" column has the absolute and relative gains compared to our baseline. Textual entailment means deciding if a ""hypothesis"" is true, assuming a ""premise"" is true. The Stanford Natural Language Inference (SNLI) dataset (Bowman et al., 2015) provides about 550K hypothesis/premise pairs.  ","The performance metrics are not the same across all tasks - SNLI and SST-5 use accuracy; SQuAD, SRL and NER use F1 score; Coref uses the average F1 score from multiple runs. Because of the small test sets for NER and SST-5, we report the mean and standard deviation from 5 runs with different random number seeds. The ""increase"" column shows the absolute and relative improvements over our baseline. Textual entailment is determining if a ""hypothesis"" is true, given that a ""premise"" is true. The Stanford Natural Language Inference (SNLI) corpus (Bowman et al., 2015) contains around 550K hypothesis/premise examples.",A,1
Deep contextualized word representations,"Our baseline, the ESIM sequence model from Chen et al. (2017), uses a biLSTM to encode the premise and hypothesis, followed by a matrix attention layer, a local inference layer, another biLSTM inference composition layer, and finally a pooling operation before the output layer. Overall, adding ELMo to the ESIM model improves accuracy by an average of 0.7% across five random seeds. A five member ensemble pushes the overall accuracy to 89.3%, exceeding the previous ensemble best of 88.9% (Gong et al., 2018). ","Our starting point was the ESIM sequence model created by Chen and colleagues in 2017. This model utilizes a biLSTM to encode the premise and hypothesis. After that, there is a matrix attention layer, a local inference layer, another biLSTM layer for inference composition, and finally pooling before the output layer. On average, integrating ELMo into the ESIM model increased accuracy by 0.7% over 5 random seeds. A 5 member ensemble pushed the total accuracy to 89.3%, surpassing the previous best ensemble accuracy of 88.9% from Gong et al., 2018.","Our baseline was the ESIM sequence model developed by Chen's research team in 2017. It uses a biLSTM to encode the premise and hypothesis, then has a matrix attention layer, local inference layer, an additional biLSTM inference composition layer, and pooling before the output. Overall, adding ELMo to the ESIM model improved accuracy by 0.7% on average across 5 random seeds. A 5 member ensemble increased the total accuracy to 89.3%, beating the prior best ensemble accuracy of 88.9% from Gong and colleagues' 2018 paper.  ","Our starting model was the ESIM sequence model published by Chen and coauthors in 2017. It utilizes a biLSTM to encode the premise and hypothesis, followed by a matrix attention layer, local inference layer, another biLSTM inference composition layer, and pooling before output. Incorporating ELMo into the ESIM model increased accuracy by 0.7% on average over 5 random seeds. A 5 member ensemble pushed the total accuracy to 89.3%, surpassing the previous top ensemble accuracy of 88.9% from Gong et al.'s 2018 paper.",A,1
Deep contextualized word representations,"Semantic role labeling A semantic role labeling (SRL) system models the predicate-argument structure of a sentence, and is often described as answering “Who did what to whom”. He et al. (2017) modeled SRL as a BIO tagging problem and used an 8-layer deep biLSTM with forward and backward directions interleaved, following Zhou and Xu (2015). As shown in Table 1, when adding ELMo to a re-implementation of He et al. (2017) the single model test set F1 jumped 3.2% from 81.4% to 84.6% – a new state-of-the-art on the OntoNotes benchmark (Pradhan et al., 2013), even improving over the previous best ensemble result by 1.2%. Coreference resolution Coreference resolution is the task of clustering mentions in text that refer to the same underlying real world entities. ","Semantic role labeling A semantic role labeling (SRL) framework analyzes the predicate-argument arrangement of a sentence, and is frequently portrayed as determining ""Who did what to whom"". He et al. (2017) modeled SRL as a BIO tagging challenge and utilized an 8-layer profound biLSTM with forward and backward directions interwoven, following Zhou and Xu (2015). As exhibited in Table 1, when adding ELMo to a re-execution of He et al. (2017) the single model test set F1 bounced 3.2% from 81.4% to 84.6% – another best in class on the OntoNotes benchmark (Pradhan et al., 2013), even improving over the past best ensemble result by 1.2%. Coreference resolution Coreference resolution is the assignment of gathering references in content that allude to a similar fundamental real world elements.","Semantic role labeling A semantic role labeling (SRL) framework dissects the predicate-contention structure of a sentence, and is often portrayed as deciding ""Who did what to whom"". He et al. (2017) modeled SRL as a BIO labeling issue and utilized a 8-layer profound biLSTM with forward and in reverse bearings interwoven, following Zhou and Xu (2015). As shown in Table 1, when adding ELMo to a re-execution of He et al. (2017) the single model test set F1 bounced 3.2% from 81.4% to 84.6% – another top tier on the OntoNotes benchmark (Pradhan et al., 2013), even improving over the past best ensemble result by 1.2%. Coreference resolution Coreference resolution is the assignment of gathering references in content that refer to similar fundamental real world elements.","Semantic role labeling A semantic role labeling (SRL) system investigates the predicate-contention structure of a sentence, and is regularly portrayed as deciding ""Who did what to whom"". He et al. (2017) modeled SRL as a BIO marking issue and utilized a 8-layer profound biLSTM with forward and backward bearings interwoven, following Zhou and Xu (2015). As exhibited in Table 1, when adding ELMo to a re-execution of He et al. (2017) the single model test set F1 bounced 3.2% from 81.4% to 84.6% – another top level on the OntoNotes benchmark (Pradhan et al., 2013), even improving over the past best ensemble result by 1.2%. Coreference resolution Coreference resolution is the assignment of gathering references in content that allude to similar fundamental real world elements.",A,1
Deep contextualized word representations,"Our baseline model is the end-to-end span-based neural model of Lee et al. (2017). It uses a biLSTM and attention mechanism to first compute span representations and then applies a softmax mention ranking model to find coreference chains. In our experiments with the OntoNotes coreference annotations from the CoNLL 2012 shared task (Pradhan et al., 2012), adding ELMo improved the average F1 by 3.2% from 67.2 to 70.4, establishing a new state of the art, again improving over the previous best ensemble result by 1.6% F1. ","Our starting neural network model follows the span-based approach of Lee and colleagues (2017). It utilizes a biLSTM and attention to first create span embeddings, then applies a softmax mention ranking to identify coreference chains. When tested on the OntoNotes annotations from the CoNLL 2012 competition (Pradhan et al., 2012), integrating ELMo boosted the average F1 by 3.2% from 67.2 to 70.4, surpassing prior benchmarks. This even topped the best previous ensemble outcome by 1.6% F1.","Our baseline system uses the end-to-end span-focused neural architecture from Lee et al. (2017). It harnesses a biLSTM and attention to initially produce span representations, then employs a softmax mention ranking algorithm to detect coreference clusters. In experiments using the OntoNotes coreference labels from the CoNLL 2012 shared task (Pradhan et al., 2012), including ELMo increased the mean F1 score by 3.2% from 67.2 to 70.4, setting a new state of the art, again exceeding the top prior ensemble score by 1.6% F1.","Our foundational model follows the full span-oriented neural design of Lee and coauthors (2017). It leverages a biLSTM and attention to first construct span embeddings, then applies a softmax mention ranking method to identify coreference groups. When evaluated on the OntoNotes annotations from the CoNLL 2012 challenge (Pradhan et al., 2012), adding ELMo lifted the average F1 by 3.2% from 67.2 to 70.4, establishing a new best performance, once more surpassing the leading previous ensemble outcome by 1.6% F1.",A,1
Deep contextualized word representations,"The CoNLL 2003 NER task (Sang and Meulder, 2003) consists of newswire from the Reuters RCV1 corpus tagged with four different entity types (PER, LOC, ORG, MISC). Following recent state-of-the-art systems (Lample et al., 2016; Peters et al., 2017), the baseline model uses pre-trained word embeddings, a character-based CNN representation, two biLSTM layers and a conditional random field (CRF) loss (Lafferty et al., 2001), similar to Collobert et al. (2011). As shown in Table 1, our ELMo enhanced biLSTM-CRF achieves 92.22% F1 averaged over five runs. The key difference between our system and the previous state of the art from Peters et al. (2017) is that we allowed the task model to learn a weighted average of all biLM layers, whereas Peters et al. (2017) only use the top biLM layer. ","The CoNLL 2003 named entity recognition challenge (Sang and Meulder, 2003) uses news articles from the Reuters RCV1 collection labeled with four kinds of entities (PER, LOC, ORG, MISC). Following current best systems (Lample et al., 2016; Peters et al., 2017), our baseline model utilizes pre-trained word vectors, a character-based CNN representation, two biLSTM layers and a conditional random field (CRF) loss function (Lafferty et al., 2001), similar to Collobert et al. (2011). As shown in Table 1, our ELMo enhanced biLSTM-CRF achieves 92.22% F1 score averaged over five runs. The main difference between our system and the previous state of the art from Peters et al. (2017) is that we permitted the task model to learn a weighted mean of all biLM layers, whereas Peters et al. (2017) only utilize the top biLM layer.","The CoNLL 2003 named entity recognition challenge (Sang and Meulder, 2003) utilizes news stories from the Reuters RCV1 collection labeled with four entity types (PER, LOC, ORG, MISC). Following current top systems (Lample et al., 2016; Peters et al., 2017), our baseline utilizes pre-trained word embeddings, a character-based CNN representation, two biLSTM layers and a conditional random field (CRF) loss function (Lafferty et al., 2001), similar to Collobert et al. (2011). As shown in Table 1, our ELMo enhanced biLSTM-CRF achieves 92.22% F1 score averaged over five executions. The primary difference between our system and the previous state of the art from Peters et al. (2017) is that we allowed the task model to learn a weighted average of all biLM layers, whereas Peters et al. (2017) only utilize the top biLM layer.","The CoNLL 2003 named entity recognition challenge (Sang and Meulder, 2003) uses news articles from the Reuters RCV1 collection labeled with four entity types (PER, LOC, ORG, MISC). Following current top systems (Lample et al., 2016; Peters et al., 2017), our baseline uses pre-trained word embeddings, a character-based CNN representation, two biLSTM layers and a conditional random field (CRF) loss function (Lafferty et al., 2001), similar to Collobert et al. (2011). As shown in Table 1, our ELMo enhanced biLSTM-CRF achieves 92.22% F1 score averaged over five runs. The main difference between our system and the previous state of the art from Peters et al. (2017) is that we allowed the task model to learn a weighted average of all biLM layers, whereas Peters et al. (2017) only use the top biLM layer.",A,1
Deep contextualized word representations,"As shown in Sec. 5.1, using all layers instead of just the last layer improves performance across multiple tasks. Sentiment analysis The fine-grained sentiment classification task in the Stanford Sentiment Treebank (SST-5; Socher et al., 2013) involves selecting one of five labels (from very negative to very positive) to describe a sentence from a movie review. The sentences contain diverse linguistic phenomena such as idioms and complex syntac- Task Baseline Last Only All layers λ=1 λ=0.001 SQuAD 80.8 84.7 85.0 85.2 SNLI 88.1 89.1 89.3 89.5 SRL 81.6 84.1 84.6 84.8 Table 2: Development set performance for SQuAD, SNLI and SRL comparing using all layers of the biLM (with different choices of regularization strength λ) to just the top layer. ","The results shown in Section 5.1 demonstrate that utilizing all layers instead of only the final layer improves performance on multiple tasks. Sentiment analysis The intricate sentiment classification task in the Stanford Sentiment Treebank (SST-5; Socher et al., 2013) requires choosing one of five labels (from very negative to very positive) to characterize a sentence from a movie review. The sentences include diverse linguistic phenomena like idioms and complex syntax. Task Baseline Last Layer Only All Layers λ=1 λ=0.001 SQuAD 80.8 84.7 85.0 85.2 SNLI 88.1 89.1 89.3 89.5 SRL 81.6 84.1 84.6 84.8 Table 2: Development set results for SQuAD, SNLI and SRL comparing usage of all layers of the biLM (with different λ values) to just the top layer.","The data presented in Section 5.1 shows that leveraging all layers instead of only the final layer enhances performance across various tasks. Sentiment classification The nuanced sentiment classification challenge in the Stanford Sentiment Treebank (SST-5; Socher et al., 2013) requires selecting one of five labels (from very negative to very positive) to summarize a sentence from a movie review. The sentences contain varied linguistic phenomena including idioms and intricate syntax. Task Baseline Final Layer Only All Layers λ=1 λ=0.001 SQuAD 80.8 84.7 85.0 85.2 SNLI 88.1 89.1 89.3 89.5 SRL 81.6 84.1 84.6 84.8 Table 2: Development set accuracy for SQuAD, SNLI and SRL comparing utilization of all layers of the biLM (with different λ values) to just the top layer.","As evidenced in Section 5.1, leveraging all layers instead of solely the final layer boosts performance on several tasks. Sentiment analysis The nuanced sentiment classification challenge in the Stanford Sentiment Treebank (SST-5; Socher et al., 2013) entails choosing one of five labels (from very negative to very positive) to summarize a sentence from a movie review. The sentences feature varied linguistic phenomena like idioms and complex syntax. Task Baseline Final Layer Only All Layers λ=1 λ=0.001 SQuAD 80.8 84.7 85.0 85.2 SNLI 88.1 89.1 89.3 89.5 SRL 81.6 84.1 84.6 84.8 Table 2: Development set results for SQuAD, SNLI and SRL comparing use of all layers of the biLM (with different λ values) to just the top layer.",A,1
Deep contextualized word representations,"Our baseline model is the biattentive classification network (BCN) from McCann et al. (2017), which also held the prior state-of-the-art result when augmented with CoVe embeddings. Replacing CoVe with ELMo in the BCN model results in a 1.0% absolute accuracy improvement over the state of the art. This section provides an ablation analysis to validate our chief claims and to elucidate some interesting aspects of ELMo representations. Sec. 5.1 shows that using deep contextual representations in downstream tasks improves performance over previous work that uses just the top layer, regardless of whether they are produced from a biLM or MT encoder, and that ELMo representations provide the best overall performance. ","The initial model we used is the biattentive classification network (BCN) described in McCann et al. (2017), which previously achieved the best performance when combined with CoVe embeddings. Substituting ELMo for CoVe in the BCN model leads to a 1.0% absolute improvement in accuracy over the previous best result. This part provides an analysis removing different components to validate our main claims and illuminate some interesting properties of ELMo representations. Section 5.1 demonstrates that utilizing deep contextual representations in downstream tasks enhances performance compared to prior work using only the top layer, regardless of whether they come from a biLM or MT encoder, and that ELMo representations give the best overall performance.","Our starting model is the biattentive classification network (BCN) from the McCann et al. (2017) paper, which held the previous best result when used with CoVe embeddings. Putting ELMo in place of CoVe in the BCN model produces a 1.0% absolute gain in accuracy over the previous state of the art. This section does an analysis removing pieces to support our main claims and reveal some interesting aspects of ELMo representations. Part 5.1 shows that using deep contextual representations in later tasks improves performance over past work using just the top layer, whether they are from a biLM or MT encoder, and that ELMo representations provide the best performance overall.  ","The baseline model we used is the biattentive classification network (BCN) described in the McCann et al. (2017) paper, which had the previous best result when combined with CoVe embeddings. Using ELMo instead of CoVe in the BCN model leads to a 1.0% absolute increase in accuracy compared to the previous state of the art. This section does an ablation analysis to validate our main claims and illuminate some interesting properties of ELMo representations. Section 5.1 demonstrates that using deep contextual representations in downstream tasks improves performance over prior work using only the top layer, regardless of whether they come from a biLM or MT encoder, and that ELMo representations provide the best overall performance.",A,1
Deep contextualized word representations,"Sec. 5.3 explores the different types of contextual information captured in biLMs and uses two intrinsic evaluations to show that syntactic information is better represented at lower layers while semantic information is captured a higher layers, consistent with MT encoders. It also shows that our biLM consistently provides richer representations then CoVe. Additionally, we analyze the sensitivity to where ELMo is included in the task model (Sec. 5.2), training set size (Sec. 5.4), and visualize the ELMo learned weights across the tasks (Sec. 5.5). ","Section 5.3 investigates the various kinds of context data encoded in bidirectional language models. It utilizes two intrinsic assessments to demonstrate that syntactic data is represented better in lower layers while semantic data is captured in higher layers, aligning with machine translation encoders. It also indicates that our bidirectional language model consistently furnishes richer representations compared to CoVe. Furthermore, we analyze how sensitive ELMo is to where it is incorporated into the task model (Section 5.2), training set proportions (Section 5.4), and visualize the learned ELMo weights across the tasks (Section 5.5).","Section 5.3 looks at the different contextual information stored in bidirectional language models. It uses two internal evaluations to show that syntactic information is represented better in the lower layers while semantic information is captured in the higher layers, which is consistent with machine translation encoders. It also demonstrates that our bidirectional language model consistently provides more informative representations compared to CoVe. In addition, we study how sensitive ELMo is to where it is included in the task model (Section 5.2), the size of the training set (Section 5.4), and we visualize the learned ELMo weights across the tasks (Section 5.5).  ","Section 5.3 investigates the various types of context data encoded in bidirectional language models. It employs two intrinsic appraisals to exhibit that syntactic data is depicted superior in lower layers while semantic data is seized in higher layers, aligning with machine translation encoders. It also indicates that our bidirectional language model persistently furnishes richer representations compared to CoVe. Furthermore, we dissect how sensitive ELMo is to where it is incorporated into the task model (Section 5.2), training set proportions (Section 5.4), and visualize the learned ELMo weights across the tasks (Section 5.5).",A,1
Deep Residual Learning for Image Recognition,"Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers—8× deeper than VGG nets [41] but still having lower complexity.","Training deeper neural networks poses more challenges. We put forth a residual learning system to simplify training networks much deeper than before. We explicitly reframe the layers as acquiring residual functions relative to the layer inputs, rather than acquiring unreferenced functions. We offer extensive empirical proof that these residual networks are easier to optimize, and can gain accuracy from much greater depth. On the ImageNet dataset we assess residual nets up to 152 layers deep—8 times deeper than VGG nets [41] but still less complex.","More layers make neural networks harder to train. We provide a residual learning approach to make training networks with far more layers easier. We directly redefine the layers to learn residual functions compared to the inputs, instead of learning functions without reference. We have comprehensive evidence that these residual networks are more optimizable, and gain precision from substantially increased depth. We test residual nets up to 152 layers on ImageNet—8 times deeper than VGG nets [41] but still less complicated.  ","Neural networks with more layers pose greater training difficulties. We put forward a residual learning framework to facilitate training networks much deeper than before. We explicitly reconfigure the layers to acquire residual functions relative to the inputs, rather than acquiring functions lacking reference. We supply extensive proof that these residual networks are more readily optimized, and obtain accuracy from much increased depth. We evaluate residual nets up to 152 layers deep on ImageNet—8 times deeper than VGG nets [41] but still less complex.",A,1
Deep Residual Learning for Image Recognition,"An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1 , where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.","A group of these remaining networks gets 3.57% error on the ImageNet evaluation set. This outcome was the winner of 1st place in the ILSVRC 2015 categorization challenge. We also provide an examination on CIFAR-10 with 100 and 1000 layers. The depth of representations is critically important for numerous visual identification tasks. Purely because of our very deep representations, we achieve a 28% comparative enhancement on the COCO object detection dataset. Deep residual nets are the basis of our entries to ILSVRC & COCO 2015 contests1, where we also won the 1st spots on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.","Multiple of these left over networks accomplishes 3.57% mistake on the ImageNet test collection. This conclusion was victorious for 1st rank in the ILSVRC 2015 classification competition. We also present an analysis of CIFAR-10 with 100 and 1000 tiers. The profundity of depictions is centrally significant for many visual recognition errands. Exclusively on account of our profoundly deep portrayals, we acquire a 28% relative improvement on the COCO object identification dataset. Profound residual nets are the establishments of our entries to ILSVRC and COCO 2015 rivalries1, where we likewise won the 1st spots on the undertakings of ImageNet identification, ImageNet localization, COCO identification, and COCO division.  ","A group of these continuing networks achieves 3.57% error on the ImageNet evaluation set. This outcome was triumphant for 1st place in the ILSVRC 2015 categorization challenge. We also put forward an examination of CIFAR-10 with 100 and 1000 levels. The depth of representations is critically important for many visual recognition jobs. Purely due to our very deep representations, we obtain a 28% relative enhancement on the COCO object detection dataset. Deep residual nets are the bases of our submissions to ILSVRC & COCO 2015 competitions1, where we also were victorious for the 1st positions on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.",A,1
Deep Residual Learning for Image Recognition,"Deep convolutional neural networks [22, 21] have led to a series of breakthroughs for image classification [21, 50, 40]. Deep networks naturally integrate low/mid/highlevel features [50] and classifiers in an end-to-end multilayer fashion, and the “levels” of features can be enriched by the number of stacked layers (depth). Recent evidence [41, 44] reveals that network depth is of crucial importance, and the leading results [41, 44, 13, 16] on the challenging ImageNet dataset [36] all exploit “very deep” [41] models, with a depth of sixteen [41] to thirty [16].","Advanced deep convolutional neural networks [22, 21] have resulted in many groundbreaking achievements in image classification [21, 50, 40]. Deep networks seamlessly combine low/mid/high-level features [50] and classifiers in a unified end-to-end multilayer structure, and stacking more layers (increasing depth) enriches the ""levels"" of features. Recent studies [41, 44] show that depth is critical, and the top results [41, 44, 13, 16] on the difficult ImageNet dataset [36] all use ""very deep"" [41] models, with sixteen [41] to thirty [16] layers.","Sophisticated deep convolutional neural networks [22, 21] have led to many innovations in categorizing images [21, 50, 40]. These deep networks naturally fuse together low/mid/high-level patterns [50] and classifiers in a cohesive end-to-end multilayer architecture, and piling on more layers (increasing depth) improves the ""tiers"" of patterns. Latest evidence [41, 44] indicates depth is vital, and the best outcomes [41, 44, 13, 16] on the challenging ImageNet benchmark [36] all employ ""very deep"" [41] models, with sixteen [41] to thirty [16] tiers.  ","Advanced deep convolutional neural networks [22, 21] have resulted in numerous breakthroughs in identifying images [21, 50, 40]. These deep networks seamlessly integrate low/mid/high-level signatures [50] and classifiers in a unified end-to-end multilayer system, and stacking more layers (increasing depth) enriches the ""strata"" of signatures. Recent findings [41, 44] demonstrate depth is critical, and the leading results [41, 44, 13, 16] on the demanding ImageNet standard [36] all leverage ""very deep"" [41] models, with sixteen [41] to thirty [16] strata.",A,1
Deep Residual Learning for Image Recognition,"Driven by the significance of depth, a question arises: Is learning better networks as easy as stacking more layers? An obstacle to answering this question was the notorious problem of vanishing/exploding gradients [1, 9], which hamper convergence from the beginning. This problem, however, has been largely addressed by normalized initialization [23, 9, 37, 13] and intermediate normalization layers [16], which enable networks with tens of layers to start converging for stochastic gradient descent (SGD) with backpropagation [22].","Motivated by the importance of depth, a question comes up: Is constructing superior neural networks as simple as accumulating more layers? A barrier to responding to this question was the notorious issue of vanishing/exploding gradients [1, 9], which impede convergence from the start. However, this problem has been extensively solved by normalized initialization [23, 9, 37, 13] and intermediate normalization layers [16], which let networks with many layers begin converging for stochastic gradient descent (SGD) with backpropagation [22].","Driven by the meaning of depth, an inquiry emerges: Is developing more capable networks as straightforward as piling on extra layers? A hurdle to addressing this question was the well-known dilemma of vanishing/exploding gradients [1, 9], which obstruct convergence initially. Nevertheless, this dilemma has been widely tackled by normalized initialization [23, 9, 37, 13] and intermediate normalization layers [16], which enable networks with numerous layers to commence converging for stochastic gradient descent (SGD) with backpropagation [22].","Prompted by the consequence of depth, a question materializes: Is constructing more proficient networks as simple as adding more layers? An impediment to responding to this question was the notorious predicament of vanishing/exploding gradients [1, 9], which hamper convergence at the start. However, this predicament has been extensively solved by normalized initialization [23, 9, 37, 13] and intermediate normalization layers [16], which permit networks with many layers to begin converging for stochastic gradient descent (SGD) with backpropagation [22].",A,1
Deep Residual Learning for Image Recognition,"When deeper networks are able to start converging, a degradation problem has been exposed: with the network depth increasing, accuracy gets saturated (which might be unsurprising) and then degrades rapidly. Unexpectedly, such degradation is not caused by overfitting, and adding more layers to a suitably deep model leads to higher training error, as reported in [11, 42] and thoroughly verified by our experiments. Fig. 1 shows a typical example. The degradation (of training accuracy) indicates that not all systems are similarly easy to optimize.","As more complex neural networks start to converge, an issue emerges: as network depth increases, performance plateaus (which is predictable) and then worsens quickly. Surprisingly, this decline is not due to overfitting, and appending more layers to an adequately deep model raises training error, as described in [11, 42] and confirmed by our tests. Fig. 1 displays a typical case. The degradation (of training accuracy) implies that not all systems are equally simple to enhance.","When more sophisticated neural networks begin to converge, a problem surfaces: as the network gets deeper, accuracy stabilizes (which is foreseeable) then rapidly deteriorates. Counterintuitively, this decrease is not from overfitting, and attaching extra layers to a sufficiently deep model increases training mistake, as noted in [11, 42] and thoroughly corroborated by our trials. Fig. 1 exhibits a typical illustration. The degradation (of training precision) signifies that not all systems are similarly straightforward to optimize.","As more elaborate neural networks start converging, a drawback materializes: as network depth expands, performance saturates (which is expected) then quickly worsens. Strangely, this decline is not due to overfitting, and adding more layers to an adequately deep model raises training error, as documented in [11, 42] and thoroughly confirmed by our experiments. Fig. 1 displays a typical case. The degradation (of training accuracy) indicates that not all systems are equally easy to enhance.",A,1
Deep Residual Learning for Image Recognition,"Let us consider a shallower architecture and its deeper counterpart that adds more layers onto it. There exists a solution by construction to the deeper model: the added layers are identity mapping, and the other layers are copied from the learned shallower model. The existence of this constructed solution indicates that a deeper model should produce no higher training error than its shallower counterpart. In this paper, we address the degradation problem by introducing a deep residual learning framework. Instead of hoping each few stacked layers directly fit a desired underlying mapping, we explicitly let these layers fit a residual mapping.","We will think about a neural network with fewer layers compared to a similar network with additional layers stacked on top of it. We can construct a solution for the deeper network where the extra layers do nothing and the original layers are the same as the shallower network. The fact that we can build this solution means the deeper network should not have higher training error compared to the shallower one. In this paper, we tackle the degradation issue by presenting a deep residual learning framework. Rather than expecting a few stacked layers to directly approximate a desired mapping, we explicitly have these layers model the residual mapping.","Let's examine a neural network architecture with limited layers versus a comparable architecture with more layers added on top. There is a way to construct a solution for the deeper model: the supplementary layers are identity mappings, and the rest of the layers are identical to the learned shallower model. The existence of this constructed solution signifies that a deeper model should yield no greater training error than its shallower counterpart. In this paper, we address the degradation problem through introducing a deep residual learning framework. Instead of anticipating that a few stacked layers will directly fit a preferred underlying mapping, we explicitly require these layers to fit a residual mapping.","We will analyze a neural network with a small number of layers and a similar network with additional layers stacked on it. We can create a solution for the deeper network where the extra layers are pass-through and the original layers match the shallower network. Being able to construct this solution means the deeper network should not have higher training error compared to the shallower one. In this paper, we tackle the degradation issue by presenting a deep residual learning framework. Rather than expecting a few stacked layers to directly model a desired mapping, we explicitly require these layers to model the residual mapping.",A,1
Deep Residual Learning for Image Recognition,"We hypothesize that it is easier to optimize the residual mapping than to optimize the original, unreferenced mapping. To the extreme, if an identity mapping were optimal, it would be easier to push the residual to zero than to fit an identity mapping by a stack of nonlinear layers. The formulation of F(x) +x can be realized by feedforward neural networks with “shortcut connections” (Fig. 2). Shortcut connections [2, 34, 49] are those skipping one or more layers. In our case, the shortcut connections simply perform identity mapping, and their outputs are added to the outputs of the stacked layers (Fig. 2).","Our theory is that optimizing the residual function is simpler than optimizing the original unmodified function. In an extreme case, if the identity function was optimal, it would be easier to make the residual zero than to fit an identity function using nonlinear layers. The F(x)+x formulation can be implemented in neural networks by adding ""shortcut connections"" that skip one or more layers. Here, the shortcuts just do an identity mapping, and their outputs are summed with the outputs of the stacked layers.","We propose that tuning the residual mapping is less difficult than tuning the original unreferenced mapping. As an extreme example, if the identity map was ideal, it would be simpler to reduce the residual to nil rather than fit an identity map using a stack of nonlinear layers. The F(x)+x formulation can be created with feedforward neural networks containing ""shortcut links"" that miss one or more layers. In our case, the shortcuts just execute identity mapping, and their outputs are combined with the outputs of the stacked layers.  ","Our conjecture is that adjusting the residual function is easier than adjusting the original raw function. In the extreme scenario where the identity function is optimal, it would be less complex to make the residual zero than to model an identity function with nonlinear layers. The F(x)+x structure can be implemented in neural networks through ""shortcut connections"" that bypass one or more layers. Here, the shortcuts simply perform identity mapping, and their outputs are added to the outputs of the stacked layers.",A,1
Deep Residual Learning for Image Recognition,"Identity shortcut connections add neither extra parameter nor computational complexity. The entire network can still be trained end-to-end by SGD with backpropagation, and can be easily implemented using common libraries (e.g., Caffe [19]) without modifying the solvers. We present comprehensive experiments on ImageNet [36] to show the degradation problem and evaluate our method. We show that: 1) Our extremely deep residual nets are easy to optimize, but the counterpart “plain” nets (that simply stack layers) exhibit higher training error when the depth increases; 2) Our deep residual nets can easily enjoy accuracy gains from greatly increased depth, producing results substantially better than previous networks.","Identity shortcut links do not add any extra parameters or complexity to the computation. The full network can still be trained from end to end using SGD with backpropagation, and can be simply implemented with common libraries (like Caffe [19]) without changing the solvers. We provide extensive experiments on ImageNet [36] to demonstrate the degradation issue and assess our approach. We show that: 1) Our very deep residual networks are easy to optimize, but the corresponding ""plain"" networks (that just stack layers) have higher training error as depth rises; 2) Our deep residual networks can easily benefit from greatly increased depth, generating much better results than previous networks.","Identity shortcut connections do not bring additional parameters or computational intricacy. The whole network is still trainable end-to-end by stochastic gradient descent with backpropagation, and can be readily implemented utilizing common libraries (such as Caffe [19]) without altering the solvers. We present comprehensive experiments on ImageNet [36] to exhibit the degradation problem and evaluate our method. We demonstrate that: 1) Our extremely deep residual networks are easy to optimize, but the matching ""plain"" networks (that simply pile layers) display higher training error as depth increases; 2) Our deep residual networks can readily gain accuracy from greatly increased depth, producing substantially superior results to previous networks.  ","Identity shortcut links contribute neither extra parameters nor computational complexity. The full network remains trainable end-to-end by stochastic gradient descent with backpropagation, and can be easily implemented exploiting common libraries (like Caffe [19]) without modifying the solvers. We provide extensive experiments on ImageNet [36] to reveal the degradation issue and assess our approach. We establish that: 1) Our very deep residual networks are easy to optimize, but the related ""plain"" networks (that simply stack layers) show higher training error as depth rises; 2) Our deep residual networks can readily benefit from greatly increased depth, generating markedly better results than previous networks.",A,1
Deep Residual Learning for Image Recognition,"Similar phenomena are also shown on the CIFAR-10 set [20], suggesting that the optimization difficulties and the effects of our method are not just akin to a particular dataset. We present successfully trained models on this dataset with over 100 layers, and explore models with over 1000 layers. On the ImageNet classification dataset [36], we obtain excellent results by extremely deep residual nets. Our 152- layer residual net is the deepest network ever presented on ImageNet, while still having lower complexity than VGG nets [41].","Comparable results are demonstrated with the CIFAR-10 dataset [20] as well, implying that the challenges with optimization and the impacts of our approach are not restricted to a single data collection. We exhibit successfully optimized models on this set with over 100 layers, and investigate models with over 1000 layers. For ImageNet classification [36], we achieve outstanding performance through very deep residual networks. Our 152-layer residual network is the deepest ever applied to ImageNet, yet still has lower complexity versus VGG nets [41].","The same patterns emerge using the CIFAR-10 dataset [20], hinting that the difficulties with training and the benefits of our technique are not unique to one dataset. We present models that converge well on this collection with over 100 layers, and explore architectures with over 1000 layers. For ImageNet classification [36], we get excellent accuracy with very deep residual networks. Our 152-layer residual net is the deepest ever on ImageNet, but still less complex than VGG nets [41].  ","Identical trends are visible with CIFAR-10 [20], implying the optimization challenges and advantages of our approach generalize beyond one dataset. We achieve well-trained models on this data with over 100 layers, and analyze over 1000 layers. For ImageNet [36], extremely deep residual networks produce superb accuracy. Our 152-layer residual net is the deepest on ImageNet yet simpler than VGG [41].",A,1
Deep Residual Learning for Image Recognition,"Our ensemble has 3.57% top-5 error on the ImageNet test set, and won the 1st place in the ILSVRC 2015 classification competition. The extremely deep representations also have excellent generalization performance on other recognition tasks, and lead us to further win the 1st places on: ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation in ILSVRC & COCO 2015 competitions. This strong evidence shows that the residual learning principle is generic, and we expect that it is applicable in other vision and non-vision problems.","Our group achieved a top-5 error rate of 3.57% on the ImageNet test set, and was ranked first place in the ILSVRC 2015 image categorization contest. The very deep feature representations also exhibited great capability to generalize to other recognition tasks, enabling us to additionally secure first place in the ILSVRC & COCO 2015 challenges for: ImageNet object detection, ImageNet localization, COCO detection, and COCO segmentation. This robust evidence demonstrates that the residual learning approach is widely applicable, and we anticipate it can be successfully utilized in other vision and non-vision domains.","Our team attained a 3.57% top-5 mistake rate on the ImageNet evaluation set, and won the top position in the ILSVRC 2015 image classification tournament. The extremely deep depictions also displayed splendid generalization ability on further identification jobs, empowering us to furthermore seize the top spots on: ImageNet object detection, ImageNet localization, COCO detection, and COCO segmentation in the ILSVRC & COCO 2015 competitions. This sturdy proof exhibits that the residual learning tenet is far-reaching, and we expect it can be pertinent in additional vision and non-vision difficulties.  ","Our ensemble achieved a 3.57% top-5 error percentage on the ImageNet test collection, and was ranked first in the ILSVRC 2015 image categorization contest. The very profound representations also showcased outstanding generalization aptitude on other recognition tasks, enabling us to additionally take first prize in the ILSVRC & COCO 2015 challenges for: ImageNet object detection, ImageNet localization, COCO detection, and COCO segmentation. This robust evidence reveals that the residual learning principle is widely relevant, and we anticipate its applicability in other vision and non-vision problems.",A,1
Deep Residual Learning for Image Recognition,"In image recognition, VLAD [18] is a representation that encodes by the residual vectors with respect to a dictionary, and Fisher Vector [30] can be formulated as a probabilistic version [18] of VLAD. Both of them are powerful shallow representations for image retrieval and classification [4, 48]. For vector quantization, encoding residual vectors [17] is shown to be more effective than encoding original vectors. In low-level vision and computer graphics, for solving Partial Differential Equations (PDEs), the widely used Multigrid method [3] reformulates the system as subproblems at multiple scales, where each subproblem is responsible for the residual solution between a coarser and a finer scale.","In image identification, VLAD [18] is a depiction that encodes using the remaining vectors compared to a dictionary, and Fisher Vector [30] can be explained as a probabilistic form [18] of VLAD. Both are influential superficial depictions for image retrieval and sorting [4, 48]. For vector quantification, encoding remaining vectors [17] is demonstrated to be more effective than encoding original vectors. In low-level vision and computer graphics, for solving Partial Differential Equations (PDEs), the extensively utilized Multigrid approach [3] reformulates the system as subproblems at various scales, where each subproblem is accountable for the residual solution between a coarser and a finer scale.","In image recognition, VLAD [18] is a representation that encodes by the residual vectors in relation to a dictionary, and Fisher Vector [30] can be formulated as a probabilistic version [18] of VLAD. Both are powerful superficial representations for image retrieval and classification [4, 48]. For vector quantization, encoding residual vectors [17] is shown to be more effective than encoding original vectors. In low-level vision and computer graphics, for solving Partial Differential Equations (PDEs), the commonly used Multigrid method [3] reformulates the system as subproblems at multiple scales, where each subproblem is responsible for the residual solution between a coarser and a finer scale.","In image recognition, VLAD [18] is a representation that encodes utilizing the residual vectors compared to a dictionary, and Fisher Vector [30] can be explained as a probabilistic form [18] of VLAD. Both are influential shallow representations for image retrieval and categorization [4, 48]. For vector quantization, encoding residual vectors [17] is exhibited to be more effective than encoding original vectors. In low-level vision and computer graphics, for solving Partial Differential Equations (PDEs), the widely utilized Multigrid approach [3] reformulates the system as subproblems at multiple scales, where each subproblem is accountable for the residual solution between a coarser and a finer scale.",A,1
Deep Residual Learning for Image Recognition,"An alternative to Multigrid is hierarchical basis preconditioning [45, 46], which relies on variables that represent residual vectors between two scales. It has been shown [3, 45, 46] that these solvers converge much faster than standard solvers that are unaware of the residual nature of the solutions. These methods suggest that a good reformulation or preconditioning can simplify the optimization. Practices and theories that lead to shortcut connections [2, 34, 49] have been studied for a long time. An early practice of training multi-layer perceptrons (MLPs) is to add a linear layer connected from the network input to the output [34, 49].","Instead of Multigrid, hierarchical basis preconditioning [45, 46] can be used, which depends on variables that symbolize residual vectors between two scales. Studies [3, 45, 46] have demonstrated that these solvers converge much quicker than regular solvers that are oblivious to the residual essence of the solutions. These procedures imply that an effective reformulation or preconditioning can streamline the optimization. Methods and hypotheses that result in shortcut links [2, 34, 49] have been explored for a while. An early practice of training multi-layer perceptrons (MLPs) is to append a linear layer connected from the network input to the output [34, 49].","Hierarchical basis preconditioning [45, 46] is an alternative to Multigrid that uses variables representing residual vectors between two scales. It has been proven [3, 45, 46] that these solvers converge far faster than typical solvers unaware of the residual nature of the solutions. This suggests that good reformulation or preconditioning can simplify optimization. Practices and theories leading to shortcut connections [2, 34, 49] have long been studied. An early technique for training multi-layer perceptrons (MLPs) is adding a linear layer linking the network input to output [34, 49].  ","Instead of Multigrid, hierarchical basis preconditioning [45, 46] can be utilized, relying on variables symbolizing residual vectors between two scales. Studies [3, 45, 46] have shown these solvers converge much more quickly than standard solvers oblivious to the residual essence of the solutions. This implies effective reformulation or preconditioning can streamline optimization. Methods and theories resulting in shortcut connections [2, 34, 49] have been explored for some time. An early MLP (multi-layer perceptron) training practice is appending a linear layer linking network input to output [34, 49].",A,1
Deep Residual Learning for Image Recognition,"In [44, 24], a few intermediate layers are directly connected to auxiliary classifiers for addressing vanishing/exploding gradients. The papers of [39, 38, 31, 47] propose methods for centering layer responses, gradients, and propagated errors, implemented by shortcut connections. In [44], an “inception” layer is composed of a shortcut branch and a few deeper branches. Concurrent with our work, “highway networks” [42, 43] present shortcut connections with gating functions [15]. These gates are data-dependent and have parameters, in contrast to our identity shortcuts that are parameter-free.","The papers [44, 24] connect some middle layers straight to helper classifiers to fix vanishing/exploding gradients. The works [39, 38, 31, 47] suggest techniques for centering layer outputs, gradients, and forwarded mistakes, done via shortcut links. Paper [44] makes an ""inception"" layer out of a shortcut branch and some deeper branches. At the same time as our work, ""highway networks"" [42, 43] use shortcut connections with gating operations [15]. These gates rely on data and have parameters, unlike our identity shortcuts which have no parameters.","In [44, 24], intermediate layers are wired directly to supplementary classifiers to address vanishing/exploding gradients. The articles [39, 38, 31, 47] present approaches for centering layer reactions, gradients, and propagated errors, implemented through shortcut connections. In [44], an ""inception"" layer contains a shortcut branch and several deeper branches. Concurrent with our efforts, ""highway networks"" [42, 43] utilize shortcut connections with gating functions [15]. These gates are data-dependent and parametrized, in contrast to our identity shortcuts which are parameter-free.","The papers [44, 24] link some middle layers straight to auxiliary classifiers to fix vanishing/exploding gradients. The works [39, 38, 31, 47] propose techniques for centering layer outputs, gradients, and forwarded errors, done through shortcut connections. Paper [44] constructs an ""inception"" layer out of a shortcut branch and some deeper branches. Simultaneously with our work, ""highway networks"" [42, 43] employ shortcut connections with gating operations [15]. These gates depend on data and are parameterized, unlike our identity shortcuts which are parameterless.",A,1
Deep Residual Learning for Image Recognition,"Let us consider H(x) as an underlying mapping to be fit by a few stacked layers (not necessarily the entire net), with x denoting the inputs to the first of these layers. If one hypothesizes that multiple nonlinear layers can asymptotically approximate complicated functions2 , then it is equivalent to hypothesize that they can asymptotically approximate the residual functions, i.e., H(x) − x (assuming that the input and output are of the same dimensions). So rather than expect stacked layers to approximate H(x), we explicitly let these layers approximate a residual function F(x) := H(x) − x. The original function thus becomes F(x)+x.","We can think of H(x) as a fundamental mapping that a few stacked layers (not necessarily the whole network) are trying to model, where x represents the inputs to the first layer. If we assume that multiple nonlinear layers can asymptotically approximate complex functions, it's equivalent to assume they can asymptotically approximate the residual functions, meaning H(x) - x (assuming input and output dimensions match). So instead of expecting the stacked layers to approximate H(x) directly, we explicitly have them approximate a residual function F(x) = H(x) - x. The original function then becomes F(x) + x.","Consider H(x) as a base mapping that a subset of layers (not the full network) are fitting, with x as inputs to the first layer. If multiple nonlinear layers can asymptotically model intricate functions, they can asymptotically model the leftover functions, H(x) - x (granted input and output sizes are equal). Rather than directly approximate H(x) with the layers, explicitly have them approximate a residual function F(x) = H(x) - x. The original function is then F(x) + x.","Regard H(x) as a fundamental mapping that a few stacked layers (not the whole net) are approximating, where x denotes inputs to the first layer. If we posit that multiple nonlinear layers can asymptotically model complex functions, it's tantamount to positing they can asymptotically model the residual functions, namely H(x) - x (given input and output dimensions match). Instead of stacked layers directly approximating H(x), explicitly have them approximate a residual function F(x) = H(x) - x. The original function becomes F(x) + x.",A,1
Deep Residual Learning for Image Recognition,"Although both forms should be able to asymptotically approximate the desired functions (as hypothesized), the ease of learning might be different. This reformulation is motivated by the counterintuitive phenomena about the degradation problem (Fig. 1, left). As we discussed in the introduction, if the added layers can be constructed as identity mappings, a deeper model should have training error no greater than its shallower counterpart. The degradation problem suggests that the solvers might have difficulties in approximating identity mappings by multiple nonlinear layers.","While the two structures ought to be capable of closely estimating the target functions over time (as theorized), the simplicity of acquiring the skill could differ. This rethinking is driven by the unanticipated happenings regarding the deterioration issue (Fig. 1, left). As talked about earlier, if the extra tiers can be made to act as unchanged mappings, a more profound model should train with error less than or equal to its more shallow version. The degradation problem hints that the solvers may struggle to model identity mappings using multiple nonlinear tiers.","Although both forms should eventually be able to closely predict the desired functions (as proposed), the ease of picking up the skill might vary. This re-conception stems from the unexpected phenomena around the degradation problem (Fig. 1, left). As discussed previously, if the added layers can be constructed to act as identity functions, a deeper model ought to learn with error no more than its shallower equivalent. The degradation issue suggests that the solvers may have difficulty approximating identity functions using multiple nonlinear layers.","While both architectures should be capable of approximating the target functions over time (as theorized), the difficulty of acquiring the skill could differ. This rethinking is prompted by the counterintuitive happenings regarding the degradation issue (Fig. 1, left). As mentioned earlier, if the extra layers can be made to behave as identity functions, a deeper model should train with error less than or equal to its shallower version. The degradation problem indicates that the solvers may struggle to approximate identity functions using multiple nonlinear layers.",A,1
Deep Residual Learning for Image Recognition,"With the residual learning reformulation, if identity mappings are optimal, the solvers may simply drive the weights of the multiple nonlinear layers toward zero to approach identity mappings. In real cases, it is unlikely that identity mappings are optimal, but our reformulation may help to precondition the problem. If the optimal function is closer to an identity mapping than to a zero mapping, it should be easier for the solver to find the perturbations with reference to an identity mapping, than to learn the function as a new one. We show by experiments (Fig. 7) that the learned residual functions in general have small responses, suggesting that identity mappings provide reasonable preconditioning.","The residual learning rethinking means that if identical transformations work best, the solvers could just make the weights of the various nonlinear layers trend toward zero to get close to identical changes. In real situations, it's not likely that identical transformations are ideal, but our rethinking could help precondition the issue. If the best function is nearer to an identical transformation than a zero transformation, it should be simpler for the solver to find the adjustments compared to an identical transformation, than to learn the function as a new one. We demonstrate by trials (Fig. 7) that the learned residual functions usually have small reactions, implying that identical transformations give reasonable preconditioning.","With the residual learning reformulation, if mappings that do not change anything are optimal, the solvers may just reduce the weights of the multiple nonlinear layers to zero to get mappings that do not change anything. In real cases, it is probably not true that mappings that change nothing are best, but our reformulation may help prepare the problem. If the best function is closer to a mapping that changes nothing than a mapping that makes everything zero, it should be easier for the solver to find the small changes compared to a mapping that changes nothing, than to learn the function completely from scratch. We show through experiments (Fig. 7) that the learned residual functions tend to have small outputs, suggesting that mappings that change nothing provide reasonable preparation.","With the way we rethought residual learning, if keeping everything the same is best, the solvers could make the weights of the multiple nonlinear layers become zero to keep everything the same. In real situations, keeping everything the same is probably not ideal, but our rethinking could help get the problem ready. If the optimal function is more like keeping everything the same than making everything zero, it should be simpler for the solver to find the small tweaks compared to keeping everything the same, than to learn the function from nothing. We show through tests (Fig. 7) that the learned residual functions usually have small outputs, suggesting that keeping everything the same provides reasonable preparation.",A,1
Deep Residual Learning for Image Recognition,"We adopt residual learning to every few stacked layers. A building block is shown in Fig. 2. Formally, in this paper we consider a building block defined as: y = F(x, {Wi}) + x. (1) Here x and y are the input and output vectors of the layers considered. The function F(x, {Wi}) represents the residual mapping to be learned. For the example in Fig. 2 that has two layers, F = W2σ(W1x) in which σ denotes 2This hypothesis, however, is still an open question. See [28]. ReLU [29] and the biases are omitted for simplifying notations.","We apply the technique of residual learning to groups of a small number of stacked layers. A single component is depicted in Fig. 2. Specifically, in this document we examine a component defined by: y = F(x, {Wi}) + x. (1) Here x and y are the input and output vectors for the layers of interest. The function F(x, {Wi}) denotes the residual mapping that must be determined. For the instance in Fig. 2 containing two layers, F = W2σ(W1x) where σ represents the ReLU [29] activation function and the bias terms are left out to simplify the expressions.","We use residual learning for every few adjacent layers that are stacked together. One building block is illustrated in Fig. 2. Formally, we analyze a building block characterized as: y = F(x, {Wi}) + x. (1) In this equation, x and y are the input and output vectors for the selected layers. The function F(x, {Wi}) characterizes the residual mapping that needs to be learned. For the two layer example in Fig. 2, F = W2σ(W1x) where σ is the ReLU [29] activation function and the bias terms are omitted to simplify the notation. ","We apply residual learning to groups of a small number of adjoining stacked layers. A single building block is pictured in Fig. 2. Specifically, we study a building block defined by the equation: y = F(x, {Wi}) + x. (1) Here, x and y represent the input and output vectors for the layers of interest. The function F(x, {Wi}) denotes the residual mapping to be determined through training. For the two-layer instance in Fig. 2, F = W2σ(W1x) where σ is the ReLU [29] activation and the bias terms are excluded to simplify the expressions.",A,1
Deep Residual Learning for Image Recognition,"The operation F + x is performed by a shortcut connection and element-wise addition. We adopt the second nonlinearity after the addition (i.e., σ(y), see Fig. 2). The shortcut connections in Eqn.(1) introduce neither extra parameter nor computation complexity. This is not only attractive in practice but also important in our comparisons between plain and residual networks. We can fairly compare plain/residual networks that simultaneously have the same number of parameters, depth, width, and computational cost (except for the negligible element-wise addition).","The process of F + x is done through a shortcut link and adding the elements together. We use the second nonlinear function after the addition (meaning σ(y), see Fig. 2). The shortcut connections in Eqn.(1) bring in neither extra parameters nor complexity of computation. This is not just useful in practice but also key for our comparisons of plain and residual networks. We can fairly contrast plain/residual networks that concurrently have the same quantity of parameters, depth, width, and computational expense (except for the negligible element-wise addition).","The operation of F + x is carried out via a shortcut connection and element-by-element summation. We employ the second nonlinearity after the summation (specifically σ(y), refer to Fig. 2). The shortcut connections in Eqn.(1) introduce no extra parameters or computational complexity. This is attractive not just in practice but also vital for our comparisons of plain and residual networks. We can equitably compare plain/residual networks that simultaneously possess the same number of parameters, depth, width, and computational cost (aside from the negligible element-wise summation).  ","The process F + x is implemented through a shortcut link and adding the elements together. We utilize the second nonlinear function after the addition (that is σ(y), see Fig. 2). The shortcut connections in Eqn.(1) bring in neither supplementary parameters nor complexity of computation. This is appealing not merely in practice but also crucial for our comparisons of plain and residual networks. We can fairly contrast plain/residual networks that concurrently have identical quantities of parameters, depth, width, and computational expense (barring the negligible element-wise addition).",A,1
Deep Residual Learning for Image Recognition,"The dimensions of x and F must be equal in Eqn.(1). If this is not the case (e.g., when changing the input/output channels), we can perform a linear projection Ws by the shortcut connections to match the dimensions: y = F(x, {Wi}) + Wsx. (2) We can also use a square matrix Ws in Eqn.(1). But we will show by experiments that the identity mapping is sufficient for addressing the degradation problem and is economical, and thus Ws is only used when matching dimensions.","The sizes of x and F need to be the same in Equation 1. If they are not the same (for example, when altering the input/output channels), we can carry out a linear projection Ws using the shortcut connections to make the sizes match: y = F(x, {Wi}) + Wsx. (2) We can also utilize a square matrix Ws in Equation 1. However, we will demonstrate through experiments that the identity mapping is adequate for handling the degradation issue and is cost-effective, so Ws is only utilized when matching sizes.","The dimensions of x and F have to be identical in Formula 1. If that is not the situation (like when tweaking the input/output channels), we can implement a linear projection Ws via the shortcut links to align the dimensions: y = F(x, {Wi}) + Wsx. (2) We could also employ a square matrix Ws in Formula 1. But we will exhibit through tests that the identity mapping is sufficient for managing the degradation problem and is economical, thus Ws is only employed when matching dimensions.  ","The magnitudes of x and F need to coincide in Expression 1. If that's not the case (for instance, when altering the input/output channels), we can actualize a linear projection Ws by the shortcut connections to harmonize the magnitudes: y = F(x, {Wi}) + Wsx. (2) We can also utilize a square matrix Ws in Expression 1. However, we will demonstrate via experiments that the identity mapping is adequate for addressing the degradation issue and is cost-effective, so Ws is only utilized when matching magnitudes.",A,1
Deep Residual Learning for Image Recognition,"The form of the residual function F is flexible. Experiments in this paper involve a function F that has two or three layers (Fig. 5), while more layers are possible. But if F has only a single layer, Eqn.(1) is similar to a linear layer: y = W1x + x, for which we have not observed advantages. We also note that although the above notations are about fully-connected layers for simplicity, they are applicable to convolutional layers. The function F(x, {Wi}) can represent multiple convolutional layers. The element-wise addition is performed on two feature maps, channel by channel.","The structure of the residual function F can be adapted as needed. The experiments discussed in this paper use an F made up of either two or three layers (see Fig. 5), however more layers could be utilized. But if F only contains one layer, Eqn.(1) is comparable to a linear layer: y = W1x + x, which has not been shown to have benefits. It should also be noted that while the above notations refer to fully-connected layers for simplicity, they can be applied to convolutional layers as well. The function F(x, {Wi}) is able to represent multiple convolutional layers. The addition operation is carried out between two feature maps, channel-wise.","The residual function F is flexible in its form. The experiments covered in this paper make use of an F with either two or three layers (refer to Fig. 5), though more layers are feasible. However, if F contains just one layer, Eqn.(1) resembles a linear layer: y = W1x + x, for which no advantages have been observed. It is also worth noting that although the above notations refer to fully-connected layers for ease, they can be used for convolutional layers too. The function F(x, {Wi}) is capable of representing multiple convolutional layers. The element-wise addition happens between two feature maps, channel by channel.  ","The structure of the residual function F is adaptable. The experiments outlined in this paper utilize an F composed of two or three layers (see Fig. 5), however additional layers are possible. But if F comprises only a single layer, Eqn.(1) is similar to a linear layer: y = W1x + x, for which no benefits have been noticed. It should also be mentioned that while the above notations pertain to fully-connected layers for simplicity, they can apply to convolutional layers too. The function F(x, {Wi}) can depict multiple convolutional layers. The addition takes place between two feature maps, channel-wise.",A,1
Deep Residual Learning for Image Recognition,"We have tested various plain/residual nets, and have observed consistent phenomena. To provide instances for discussion, we describe two models for ImageNet as follows. Plain Network. Our plain baselines (Fig. 3, middle) are mainly inspired by the philosophy of VGG nets [41] (Fig. 3, left). The convolutional layers mostly have 3×3 filters and follow two simple design rules: (i) for the same output feature map size, the layers have the same number of filters; and (ii) if the feature map size is halved, the number of filters is doubled so as to preserve the time complexity per layer.","We have evaluated multiple basic/non-residual networks, and have seen the same patterns repeatedly. To give examples to talk about, we will describe two models for ImageNet like this. Simple Network. Our simple baseline models (Fig. 3, middle) are largely based on the ideas of VGG networks [41] (Fig. 3, left). The convolutional layers mostly use 3×3 filters and follow two straightforward design principles: (i) for the same output feature map dimensions, the layers have the same quantity of filters; and (ii) if the feature map size is reduced by half, the number of filters is doubled to keep the time complexity per layer the same.","We have tested various plain/non-residual networks, and have noticed consistent behaviors. To provide concrete instances for discussion, we will describe two models for ImageNet in this way. Basic Network. Our basic baseline models (Fig. 3, middle) are primarily inspired by the philosophy of VGG networks [41] (Fig. 3, left). The convolutional layers largely use 3×3 filters and adhere to two simple design guidelines: (i) for the same output feature map size, the layers utilize the same number of filters; and (ii) if the feature map size is halved, the number of filters is doubled in order to maintain the time complexity per layer.  ","We have evaluated multiple plain/non-residual networks, and have observed consistent phenomena. To give examples for conversation, we will describe two models for ImageNet like this. Plain Network. Our plain baseline models (Fig. 3, middle) are mostly inspired by the ideas of VGG networks [41] (Fig. 3, left). The convolutional layers primarily use 3×3 filters and follow two straightforward design principles: (i) for the same output feature map dimensions, the layers use the same quantity of filters; and (ii) if the feature map size is reduced by half, the number of filters is doubled to preserve the time complexity per layer.",A,1
Deep Residual Learning for Image Recognition,"We perform downsampling directly by convolutional layers that have a stride of 2. The network ends with a global average pooling layer and a 1000-way fully-connected layer with softmax. The total number of weighted layers is 34 in Fig. 3 (middle). It is worth noticing that our model has fewer filters and lower complexity than VGG nets [41] (Fig. 3, left). Our 34- layer baseline has 3.6 billion FLOPs (multiply-adds), which is only 18% of VGG-19 (19.6 billion FLOPs).","We implement downsampling through the use of convolutional layers that have a stride length of 2. The network concludes with a global average pooling layer along with a 1000-way fully-connected layer utilizing softmax. Fig. 3 (middle) demonstrates that the total quantity of weighted layers is 34. It merits observing that our model possesses fewer filters and is less complex versus VGG nets [41] (Fig. 3, left). Our 34-layer baseline contains 3.6 billion FLOPs (multiply-adds), which is merely 18% of VGG-19 (19.6 billion FLOPs).","We carry out downsampling directly via convolutional layers having a stride of 2. The network finishes with a global average pooling layer plus a 1000-way fully-connected layer having softmax. There are 34 weighted layers total in Fig. 3 (middle). It is noteworthy that our model has fewer filters and lower complexity compared to VGG nets [41] (Fig. 3, left). Our 34-layer baseline has only 3.6 billion FLOPs (multiply-adds), which is just 18% of VGG-19 (19.6 billion FLOPs).  ","We implement downsampling through convolutional layers possessing a stride of 2. The network terminates in a global average pooling layer and a 1000-way fully-connected layer utilizing softmax. Fig. 3 (middle) shows 34 total weighted layers. Our model has fewer filters and is less complex relative to VGG nets [41] (Fig. 3, left). Our 34-layer baseline possesses only 3.6 billion FLOPs (multiply-adds), merely 18% of VGG-19's 19.6 billion FLOPs.",A,1
Deep Residual Learning for Image Recognition,"Left: the VGG-19 model [41] (19.6 billion FLOPs) as a reference. Middle: a plain network with 34 parameter layers (3.6 billion FLOPs). Right: a residual network with 34 parameter layers (3.6 billion FLOPs). The dotted shortcuts increase dimensions. Table 1 shows more details and other variants. Based on the above plain network, we insert shortcut connections (Fig. 3, right) which turn the network into its counterpart residual version. The identity shortcuts (Eqn.(1)) can be directly used when the input and output are of the same dimensions (solid line shortcuts in Fig. 3).","On the left is the VGG-19 model [41] (19.6 billion FLOPs) as a benchmark. In the middle is an ordinary network with 34 parameter layers (3.6 billion FLOPs). On the right is a residual network with 34 parameter layers (3.6 billion FLOPs). The dotted shortcuts increase dimensions. Table 1 provides more specifics and other variants. Starting with the plain network above, we add shortcut connections (Fig. 3, right) which convert the network into its corresponding residual version. The identity shortcuts (Eqn.(1)) can be directly utilized when the input and output have the same dimensions (solid line shortcuts in Fig. 3).","The VGG-19 model [41] (19.6 billion FLOPs) is shown on the left side as a standard. A basic network with 34 parameter layers (3.6 billion FLOPs) is depicted in the middle. A residual network with 34 parameter layers (3.6 billion FLOPs) is illustrated on the right side. The dotted shortcuts increase dimensions. More information and other versions are provided in Table 1. Building on the plain network mentioned above, we insert shortcut connections (Fig. 3, right) which transform the network into its residual counterpart. The identity shortcuts (Eqn.(1)) can be used directly when the input and output have matching dimensions (solid line shortcuts in Fig. 3).","On the left is the VGG-19 model [41] (19.6 billion FLOPs) serving as a benchmark. In the middle is a simple network with 34 parameter layers (3.6 billion FLOPs). On the right is a residual network also with 34 parameter layers (3.6 billion FLOPs). The dotted shortcuts increase dimensions, with more details and variants shown in Table 1. Using the plain network as a base, we add shortcut connections (Fig. 3, right) to turn it into a residual version. The identity shortcuts (Eqn.(1)) can be applied directly when input and output are the same size (solid line shortcuts in Fig. 3).",A,1
Deep Residual Learning for Image Recognition,"When the dimensions increase (dotted line shortcuts in Fig. 3), we consider two options: (A) The shortcut still performs identity mapping, with extra zero entries padded for increasing dimensions. This option introduces no extra parameter; (B) The projection shortcut in Eqn.(2) is used to match dimensions (done by 1×1 convolutions). For both options, when the shortcuts go across feature maps of two sizes, they are performed with a stride of 2.","As the dimensions grow (dotted line shortcuts in Fig. 3), there are two choices: (A) The shortcut still does the same mapping, with extra zero entries added for larger dimensions. This choice does not add any new parameters; (B) The projection shortcut in Eqn.(2) is utilized to align dimensions (done by 1x1 convolutions). For both choices, when the shortcuts bridge feature maps of two sizes, they are done with a stride of 2.","When the size increases (dotted line shortcuts in Fig. 3), there are two possibilities: (A) The shortcut still carries out identical mapping, with extra zero values padded for bigger dimensions. This possibility introduces no extra variables; (B) The projection shortcut in Eqn.(2) is used to match sizes (done by 1x1 convolutions). For both possibilities, when the shortcuts span feature maps of two dimensions, they are executed with a stride of 2.","As the measurements grow (dotted line shortcuts in Fig. 3), there are two options: (A) The shortcut still performs the same mapping, with extra zero entries added for larger measurements. This option does not introduce any new variables; (B) The projection shortcut in Eqn.(2) is utilized to align measurements (done by 1x1 convolutions). For both options, when the shortcuts connect feature maps of two sizes, they are carried out with a stride of 2.",A,1
Deep Residual Learning for Image Recognition,"Our implementation for ImageNet follows the practice in [21, 41]. The image is resized with its shorter side randomly sampled in [256, 480] for scale augmentation [41]. A 224×224 crop is randomly sampled from an image or its horizontal flip, with the per-pixel mean subtracted [21]. The standard color augmentation in [21] is used. We adopt batch normalization (BN) [16] right after each convolution and before activation, following [16]. We initialize the weights as in [13] and train all plain/residual nets from scratch. We use SGD with a mini-batch size of 256. The learning rate starts from 0.1 and is divided by 10 when the error plateaus, and the models are trained for up to 60 × 104 iterations.","Our approach for ImageNet is consistent with prior work [21, 41]. The image's shorter side is randomly resized between [256, 480] pixels for scale variation [41]. A 224x224 crop is randomly extracted from the image or its horizontal reflection, with the mean color subtracted per pixel [21]. We use the standard color changes from [21]. We use batch normalization (BN) [16] directly after each convolution and before activation, as in [16]. We initialize the weights as in [13] and train all plain/residual networks from scratch. We use SGD with a batch size of 256. The learning rate begins at 0.1 and is reduced by 10x when error plateaus, training for up to 60 x 104 iterations.","Our ImageNet implementation adheres to common practice [21, 41]. The image has its shorter side randomly resized to between [256, 480] pixels for scale augmentation [41]. A 224x224 crop is randomly sampled from the image or its horizontal flip, subtracting the mean color per pixel [21]. We utilize the standard color augmentations from [21]. We use batch normalization (BN) [16] right after each convolution and before activation per [16]. We initialize weights as in [13] and train all plain/residual networks from scratch. We use SGD with a batch size of 256. The learning rate starts at 0.1 and drops by 10x at error plateaus, training up to 60 x 104 iterations.","Our ImageNet approach follows established conventions [21, 41]. The image's shorter side is randomly resized to between [256, 480] pixels for scale variation [41]. A 224x224 crop is randomly taken from the image or its horizontal flip, subtracting the mean color per pixel [21]. We employ the standard color augmentations from [21]. We apply batch normalization (BN) [16] directly after each convolution and before activation as in [16]. We initialize weights as in [13] and train all plain/residual networks from scratch. We use SGD with a batch size of 256. The learning rate begins at 0.1 and decreases by 10x at error plateaus, training up to 60 x 104 iterations.",A,1
Deep Residual Learning for Image Recognition,"We use a weight decay of 0.0001 and a momentum of 0.9. We do not use dropout [14], following the practice in [16]. In testing, for comparison studies we adopt the standard 10-crop testing [21]. For best results, we adopt the fullyconvolutional form as in [41, 13], and average the scores at multiple scales (images are resized such that the shorter side is in {224, 256, 384, 480, 640}).","We utilize a weight decay of 0.0001 and a momentum of 0.9. We do not employ dropout [14], as done in [16]. For comparison experiments, we use the standard 10-crop testing approach [21] during evaluation. To obtain optimal results, we use the fully convolutional architecture as in [41, 13], and take the mean of the predictions at multiple image scales (images are resized so the shorter side has lengths of {224, 256, 384, 480, 640}).","We make use of a weight decay parameter of 0.0001 and a momentum of 0.9. We avoid using dropout [14], following the methodology in [16]. For comparative assessments, we adopt the standard practice of 10-crop testing [21]. To achieve the best performance, we leverage the fully convolutional form as described in [41, 13], and average the outputs across multiple scales (images are resized such that the shorter dimension is {224, 256, 384, 480, 640}).","A weight decay of 0.0001 and momentum of 0.9 are utilized. Dropout [14] is not employed, per the approach in [16]. For comparison experiments, the standard 10-crop testing [21] is adopted. For optimal results, the fully convolutional architecture from [41, 13] is used, averaging predictions from multiple scales (images are resized to have shorter side lengths of {224, 256, 384, 480, 640}).",A,1
Deep Residual Learning for Image Recognition,"The models are trained on the 1.28 million training images, and evaluated on the 50k validation images. We also obtain a final result on the 100k test images, reported by the test server. We evaluate both top-1 and top-5 error rates. We first evaluate 18-layer and 34-layer plain nets. The 34-layer plain net is in Fig. 3 (middle). The 18-layer plain net is of a similar form. See Table 1 for detailed architectures. The results in Table 2 show that the deeper 34-layer plain net has higher validation error than the shallower 18-layer plain net. To reveal the reasons, in Fig. 4 (left) we compare their training/validation errors during the training procedure.","The models are educated on the 1.28 million preparation pictures, and appraised on the 50k validation images. We furthermore get a final outcome on the 100k test images, accounted for by the test server. We assess both top-1 and top-5 mistake percentages. We first appraise 18-layer and 34-layer ordinary nets. The 34-layer ordinary net is in Fig. 3 (center). The 18-layer ordinary net is of a comparable structure. Peruse Table 1 for point by point designs. The outcomes in Table 2 demonstrate that the more profound 34-layer ordinary net has higher approval mistake than the more shallow 18-layer ordinary net. To uncover the reasons, in Fig. 4 (left) we analyze their training/approval mistakes during the preparation system.","The models are educated on the 1.28 million preparation pictures, and evaluated on the 50k approval pictures. We likewise get a last result on the 100k test pictures, detailed by the test server. We survey both top-1 and top-5 slip-up rates. We initially assess 18-layer and 34-layer unadorned nets. The 34-layer unadorned net is in Fig. 3 (center). The 18-layer unadorned net is of a comparable structure. See Table 1 for nitty gritty designs. The outcomes in Table 2 show that the more profound 34-layer unadorned net has higher approval mistake than the more shallow 18-layer unadorned net. To uncover the reasons, in Fig. 4 (left) we analyze their training/approval botches during the preparation procedure.","The models are prepared on the 1.28 million training pictures, and surveyed on the 50k approval pictures. We also acquire a last outcome on the 100k test pictures, announced by the test server. We evaluate both top-1 and top-5 slip-up rates. We initially survey 18-layer and 34-layer plain nets. The 34-layer plain net is in Fig. 3 (focus). The 18-layer plain net is of a similar structure. Peruse Table 1 for point by point designs. The results in Table 2 demonstrate that the more profound 34-layer plain net has higher endorsement botch than the more shallow 18-layer plain net. To uncover the reasons, in Fig. 4 (left) we analyze their training/endorsement mistakes during the preparation procedure.",A,1
Deep Residual Learning for Image Recognition,"Building blocks are shown in brackets (see also Fig. 5), with the numbers of blocks stacked. Downsampling is performed by conv3 1, conv4 1, and conv5 1 with a stride of 2. 0 10 20 30 40 50 20 30 40 50 60 iter. (1e4) error (%) plain-18 plain-34 0 10 20 30 40 50 20 30 40 50 60 iter. (1e4) error (%) ResNet-18 ResNet-34 18-layer 34-layer 18-layer 34-layer Figure 4.","The fundamental components are displayed inside parentheses (refer to Figure 5 as well), with the quantities of components piled up. Subsampling is executed by conv3 1, conv4 1, and conv5 1 with a step of 2. 0 10 20 30 40 50 20 30 40 50 60 iteration (1e4) mistake (%) plain-18 plain-34 0 10 20 30 40 50 20 30 40 50 60 iteration (1e4) mistake (%) ResNet-18 ResNet-34 18-layer 34-layer 18-layer 34-layer Figure 4.","The building blocks are enclosed in brackets (also see Fig. 5), with the number of blocks stacked. Downsampling is done by conv3 1, conv4 1, and conv5 1 with a stride of 2. 0 10 20 30 40 50 20 30 40 50 60 iter. (1e4) error (%) plain-18 plain-34 0 10 20 30 40 50 20 30 40 50 60 iter. (1e4) error (%) ResNet-18 ResNet-34 18-layer 34-layer 18-layer 34-layer Figure 4.","The elementary components are shown inside parentheses (refer to Figure 5 too), with the amounts of components piled up. Subsampling is accomplished by conv3 1, conv4 1, and conv5 1 with a pace of 2. 0 10 20 30 40 50 20 30 40 50 60 iter. (1e4) error (%) plain-18 plain-34 0 10 20 30 40 50 20 30 40 50 60 iter. (1e4) error (%) ResNet-18 ResNet-34 18-layer 34-layer 18-layer 34-layer Figure 4.",A,1
Deep Residual Learning for Image Recognition,"Thin curves denote training error, and bold curves denote validation error of the center crops. Left: plain networks of 18 and 34 layers. Right: ResNets of 18 and 34 layers. In this plot, the residual networks have no extra parameter compared to their plain counterparts. plain ResNet 18 layers 27.94 27.88 34 layers 28.54 25.03 Table 2. Top-1 error (%, 10-crop testing) on ImageNet validation. Here the ResNets have no extra parameter compared to their plain counterparts.","Slim lines show the training error, and thick lines show the validation error of the center crops. On the left: basic networks with 18 and 34 layers. On the right: ResNets with 18 and 34 layers. In this graph, the residual networks don't have any extra parameters compared to their basic versions. Basic ResNet 18 layers 27.94 27.88 34 layers 28.54 25.03 Table 2. Top-1 error (%, 10-crop testing) on ImageNet validation set. Here the ResNets don't have any extra parameters compared to their basic versions.","Thin lines indicate training mistake, and bold lines indicate validation mistake of the center crops. On the left: plain networks of 18 and 34 tiers. On the right: ResNets of 18 and 34 tiers. In this chart, the residual networks contain no extra parameter compared to their plain counterparts. Plain ResNet 18 tiers 27.94 27.88 34 tiers 28.54 25.03 Table 2. Top-1 error (%, 10-crop testing) on ImageNet validation. Here the ResNets contain no extra parameter compared to their plain counterparts. ","Narrow lines show the training inaccuracies, and thick lines show the validation inaccuracies of the center crops. On the left: simple networks with 18 and 34 levels. On the right: ResNets with 18 and 34 levels. In this diagram, the residual networks have no additional parameters compared to their simple versions. Simple ResNet 18 levels 27.94 27.88 34 levels 28.54 25.03 Table 2. Top-1 inaccuracies (%, 10-crop testing) on ImageNet validation. Here the ResNets have no additional parameters compared to their simple versions.",A,1
Deep Residual Learning for Image Recognition,"Fig. 4 shows the training procedures. 34-layer plain net has higher training error throughout the whole training procedure, even though the solution space of the 18-layer plain network is a subspace of that of the 34-layer one. We argue that this optimization difficulty is unlikely to be caused by vanishing gradients. These plain networks are trained with BN [16], which ensures forward propagated signals to have non-zero variances. We also verify that the backward propagated gradients exhibit healthy norms with BN. So neither forward nor backward signals vanish.","The instructional diagram displays the preparation processes. The 34-layer elementary system has superior preparation inaccuracies over the entire preparation system, despite the clarification extent of the 18-layer elementary structure being a subarea of that of the 34-layer one. We debate that this enhancement challenge is improbable to be prompted by fading slopes. These elementary systems are prepared with BN [16], which guarantees forward spread signals to have non-zero contrasts. We additionally check that the in reverse spread slopes show solid standards with BN. So neither forward nor in reverse signals disappear.","Fig. 4 outlines the training procedures. The 34-layer basic net has higher training mistakes throughout the whole training process, even though the solution space of the 18-layer basic network is a subset of that of the 34-layer one. We argue that this optimization difficulty is unlikely to be caused by vanishing gradients. These basic networks are trained with BN [16], which ensures forwarded signals have non-zero variances. We also verify that the backward propagated gradients display strong norms with BN. So neither forwarded nor backward signals vanish.","The figure shows the training processes. The 34-layer plain network has more training errors during the whole training procedure, despite the solution space of the 18-layer plain network being a part of that of the 34-layer one. We contend that this optimization challenge is unlikely to be due to fading gradients. These plain networks are trained with BN [16], which guarantees forwarded signals have non-zero variances. We also check that the reversed propagated gradients exhibit robust standards with BN. So neither forwarded nor reversed signals disappear.",A,1
Deep Residual Learning for Image Recognition,"In fact, the 34-layer plain net is still able to achieve competitive accuracy (Table 3), suggesting that the solver works to some extent. We conjecture that the deep plain nets may have exponentially low convergence rates, which impact the reducing of the training error3 . The reason for such optimization difficulties will be studied in the future. Residual Networks. Next we evaluate 18-layer and 34- layer residual nets (ResNets). The baseline architectures are the same as the above plain nets, expect that a shortcut connection is added to each pair of 3×3 filters as in Fig. 3 (right).","Indeed, the 34-layer basic network can still attain good precision (Table 3), implying that the solver is somewhat effective. We hypothesize that the deep basic networks may have exponentially slow convergence rates, affecting the decrease of the training error3. The cause of such difficulty in optimization will be examined later. Networks with Residual Connections. Next we assess 18-layer and 34-layer networks with residual connections (ResNets). The baseline architectures are identical to the above basic nets, except that a shortcut connection is supplemented to each pair of 3×3 filters as in Fig. 3 (right).","In fact, the 34-layer unmodified net can still achieve competitive accuracy (Table 3), suggesting the solver works to a degree. We theorize the deep unmodified nets may have exponentially low rates of convergence, influencing the reduction of the training error3. The reason for such challenges with optimization will be analyzed in the future. Networks with Remaining Connections. We then evaluate 18-layer and 34-layer networks with remaining connections (ResNets). The foundational architectures are the same as the above unmodified nets, except a shortcut connection is appended to each pair of 3×3 filters as in Fig. 3 (right).","Indeed, the 34-layer plain network can still attain good precision (Table 3), implying the solver is somewhat successful. We conjecture the deep plain networks may have exponentially slow rates of convergence, impacting the decrease of the training error3. The cause of such difficulties with optimization will be examined later. Networks with Residual Links. Next we assess 18-layer and 34-layer networks with residual links (ResNets). The baseline architectures are identical to the above plain nets, except a shortcut connection is added to each pair of 3×3 filters as in Fig. 3 (right).",A,1
Deep Residual Learning for Image Recognition,"In the first comparison (Table 2 and Fig. 4 right), we use identity mapping for all shortcuts and zero-padding for increasing dimensions (option A). So they have no extra parameter compared to the plain counterparts. We have three major observations from Table 2 and Fig. 4. First, the situation is reversed with residual learning – the 34-layer ResNet is better than the 18-layer ResNet (by 2.8%). More importantly, the 34-layer ResNet exhibits considerably lower training error and is generalizable to the validation data.","In the initial contrast (Table 2 and Fig. 4 on the right), we utilize the same mapping for all shortcuts and zero-filling for expanding dimensions (choice A). Thus they have no supplementary parameters compared to the plain versions. We have three main notices from Table 2 and Fig. 4. First, the position is turned around with residual learning – the 34-layer ResNet is superior to the 18-layer ResNet (by 2.8%). More significantly, the 34-layer ResNet displays substantially lower training error and is generalizable to the validation information.","In the first juxtaposition (Table 2 and Fig. 4 on the right side), we apply identical projection for all shortcuts and zero-padding for increasing dimensions (option A). Hence they have no extra parameters relative to the plain counterparts. We have three major insights from Table 2 and Fig. 4. Initially, the situation is inverted with residual learning – the 34-layer ResNet is better than the 18-layer ResNet (by 2.8%). More crucially, the 34-layer ResNet exhibits considerably lower training mistake and is generalizable to the validation data.","In the initial comparison (Table 2 and Fig. 4 on the right), we use the same mapping for all shortcuts and zero-filling for expanding dimensions (choice A). Therefore they have no additional parameters compared to the plain versions. We have three major observations from Table 2 and Fig. 4. First, the position is reversed with residual learning – the 34-layer ResNet is superior to the 18-layer ResNet (by 2.8%). More importantly, the 34-layer ResNet displays substantially lower training error and is generalizable to the validation data.",A,1
Deep Residual Learning for Image Recognition,"This indicates that the degradation problem is well addressed in this setting and we manage to obtain accuracy gains from increased depth. Second, compared to its plain counterpart, the 34-layer 3We have experimented with more training iterations (3×) and still observed the degradation problem, suggesting that this problem cannot be feasibly addressed by simply using more iterations. This comparison verifies the effectiveness of residual learning on extremely deep systems. Last, we also note that the 18-layer plain/residual nets are comparably accurate (Table 2), but the 18-layer ResNet converges faster (Fig. 4 right vs. left).","This shows that the issue of performance decline is properly handled in this scenario and we succeed in getting accuracy improvements from more layers. Also, compared to the plain version, the 34-layer model demonstrates that residual learning is effective for very deep networks. Finally, we see that the 18-layer plain and residual networks have similar accuracy (Table 2), but the 18-layer ResNet learns faster (Fig. 4 right vs. left).","This implies that the problem of decreasing accuracy is adequately addressed here and we obtain gains in precision from increased depth. Secondly, in contrast to the basic model, the 34-layer one proves the usefulness of residual learning for extremely complex architectures. Additionally, the 18-layer standard and residual networks have comparable precision (Table 2), however the 18-layer ResNet converges more rapidly (Fig. 4 right vs. left).  ","This shows that the challenge of performance deterioration is properly managed in this case and we get improvements in correctness from more layers. Also, compared to the unmodified version, the 34-layer one verifies the value of residual learning on very intricate systems. Lastly, we observe that the 18-layer plain and residual networks have similar accuracy (Table 2), however the 18-layer ResNet learns quicker (Fig. 4 right vs. left).",A,1
Deep Residual Learning for Image Recognition,"When the net is “not overly deep” (18 layers here), the current SGD solver is still able to find good solutions to the plain net. In this case, the ResNet eases the optimization by providing faster convergence at the early stage. We have shown that 3x3, 64 1x1, 64 relu 1x1, 256 relu relu 3x3, 64 3x3, 64 relu relu 64-d 256-d Figure 5. A deeper residual function F for ImageNet. Left: a building block (on 56×56 feature maps) as in Fig. 3 for ResNet34. Right: a “bottleneck” building block for ResNet-50/101/152. parameter-free, identity shortcuts help with training.","Even when the neural network is relatively shallow (18 layers in this case), the existing stochastic gradient descent algorithm is still capable of finding good solutions for the plain network. Here, the ResNet facilitates optimization by enabling faster convergence early on. We have demonstrated that using identity shortcuts without extra parameters aids training.","When the neural net has limited depth (18 tiers in this instance), the current stochastic gradient descent solver still manages to identify effective solutions for the vanilla network. The ResNet simplifies optimization by providing quicker convergence initially. We've exhibited that parameter-free, identity shortcuts assist with training. ","When the neural network does not have great depth (18 layers here), the present stochastic gradient descent optimizer can still locate good solutions for the basic network. In this situation, the ResNet makes optimization easier by delivering faster convergence at first. We have revealed that identity shortcuts with no parameters help training.",A,1
Deep Residual Learning for Image Recognition,"Next we investigate projection shortcuts (Eqn.(2)). In Table 3 we compare three options: (A) zero-padding shortcuts are used for increasing dimensions, and all shortcuts are parameterfree (the same as Table 2 and Fig. 4 right); (B) projection shortcuts are used for increasing dimensions, and other shortcuts are identity; and (C) all shortcuts are projections. Table 3 shows that all three options are considerably better than the plain counterpart. B is slightly better than A.","We then examine projection shortcuts (Eqn.(2)). Table 3 contrasts three choices: (A) zero-padding shortcuts are utilized for expanding dimensions, and all shortcuts do not have parameters (identical to Table 2 and Fig. 4 on the right); (B) projection shortcuts are utilized for expanding dimensions, and other shortcuts are identity mappings; and (C) all shortcuts are projections. Table 3 indicates that all three choices are much better than the plain version. B is marginally superior to A.","Next we study projection shortcuts (Eqn.(2)). In Table 3 we juxtapose three alternatives: (A) zero-padding shortcuts are employed for increasing dimensions, and all shortcuts are parameter-free (the same as Table 2 and Fig. 4 on the right side); (B) projection shortcuts are employed for increasing dimensions, and other shortcuts are identity functions; and (C) all shortcuts are projections. Table 3 demonstrates that all three alternatives are substantially better than the plain counterpart. B is slightly superior to A.","Subsequently we analyze projection shortcuts (Eqn.(2)). In Table 3 we compare three possibilities: (A) zero-padding shortcuts are utilized for expanding dimensions, and all shortcuts do not contain parameters (identical to Table 2 and Fig. 4 on the right); (B) projection shortcuts are utilized for expanding dimensions, and other shortcuts are identity functions; and (C) all shortcuts are projections. Table 3 indicates that all three possibilities are considerably superior to the plain version. B is marginally better than A.",A,1
Deep Residual Learning for Image Recognition,"We argue that this is because the zero-padded dimensions in A indeed have no residual learning. C is marginally better than B, and we attribute this to the extra parameters introduced by many (thirteen) projection shortcuts. But the small differences among A/B/C indicate that projection shortcuts are not essential for addressing the degradation problem. So we do not use option C in the rest of this paper, to reduce memory/time complexity and model sizes. Identity shortcuts are particularly important for not increasing the complexity of the bottleneck architectures that are introduced below.","We contend that this is due to the zero-padded dimensions in A truly not having any residual learning. C is slightly superior to B, and we credit this to the extra parameters introduced by numerous (thirteen) projection shortcuts. However, the small variances between A/B/C signify that projection shortcuts are not imperative for tackling the degradation issue. Thus, we do not utilize option C for the remainder of this paper, to decrease memory/time intricacy and model magnitudes. Identity shortcuts are especially vital for not amplifying the complexity of the bottleneck architectures presented below.","Our position is that the reason for this is that the zero-padded dimensions in A genuinely have no leftover learning. C is marginally preferable to B, and we attribute this to the additional parameters created by many (thirteen) projection shortcuts. But the minor differences between A/B/C indicate that projection shortcuts are not essential for addressing the degradation dilemma. Therefore, we do not employ option C for the rest of this paper, to reduce memory/time complexity and model sizes. Identity shortcuts are particularly important for not increasing the intricacy of the bottleneck architectures introduced later.  ","Our stance is that the rationale for this is that the zero-padded dimensions in A truly possess no residual learning. C is slightly better than B, and we ascribe this to the extra parameters generated by numerous (thirteen) projection shortcuts. However, the small variances between A/B/C signify that projection shortcuts are not imperative for tackling the degradation predicament. As such, we do not use option C for the remainder of this paper, to decrease memory/time complexity and model magnitudes. Identity shortcuts are especially critical for not amplifying the complexity of the bottleneck architectures presented subsequently.",A,1
Deep Residual Learning for Image Recognition,"Next we describe our deeper nets for ImageNet. Because of concerns on the training time that we can afford, we modify the building block as a bottleneck design4 . For each residual function F, we use a stack of 3 layers instead of 2 (Fig. 5). The three layers are 1×1, 3×3, and 1×1 convolutions, where the 1×1 layers are responsible for reducing and then increasing (restoring) dimensions, leaving the 3×3 layer a bottleneck with smaller input/output dimensions. Fig. 5 shows an example, where both designs have similar time complexity.","In the following section we present the more complex neural networks we used for ImageNet. To address worries about the training time we could dedicate, we altered the architecture to a bottleneck design with 3 layers rather than 2 in each residual function F. The layers are 1x1, 3x3, and 1x1 convolutions, where the 1x1 layers decrease and then restore the dimensions, making the 3x3 layer a bottleneck with smaller input/output sizes. Figure 5 illustrates an example where both architectures have comparable time complexity.","Next we explain the more sophisticated neural networks we leveraged for ImageNet. Due to concerns about the training time available, we tweaked the building block to a bottleneck style with 3 layers instead of 2 in each residual function F. The layers are 1x1, 3x3, and 1x1 convolutions, where the 1x1 layers reduce and then expand the dimensions, causing the 3x3 layer to become a bottleneck with smaller input/output dimensions. Figure 5 provides an example, where both designs have similar time complexity.  ","In the following we describe the deeper neural networks we used for ImageNet. Because of worries about the training time we could spend, we altered the module to a bottleneck architecture with 3 layers rather than 2 in each residual function F. The layers are 1x1, 3x3, and 1x1 convolutions, where the 1x1 layers decrease and then increase the dimensions, making the 3x3 layer a bottleneck with smaller input/output sizes. Figure 5 shows an example, where both architectures have comparable time complexity.",A,1
Deep Residual Learning for Image Recognition,"The parameter-free identity shortcuts are particularly important for the bottleneck architectures. If the identity shortcut in Fig. 5 (right) is replaced with projection, one can show that the time complexity and model size are doubled, as the shortcut is connected to the two high-dimensional ends. So identity shortcuts lead to more efficient models for the bottleneck designs. So the usage of bottleneck designs is mainly due to practical considerations. We further note that the degradation problem of plain nets is also witnessed for the bottleneck designs. 6 34-layer net with this 3-layer bottleneck block, resulting in a 50-layer ResNet (Table 1).","The parameterless identity shortcuts have special importance for the bottleneck structures. If the identity shortcut in Fig. 5 (right) is substituted with projection, one could demonstrate that the time complexity and model dimensions are doubled, as the shortcut links to the two high-dimensional ends. Thus identity shortcuts result in more efficient models for the bottleneck designs. So the use of bottleneck architectures is primarily due to practical factors. We additionally observe that the degradation issue of plain networks is also seen for the bottleneck architectures. A 34-layer net with this 3-layer bottleneck block, yielding a 50-layer ResNet (Table 1).","The identity shortcuts without parameters are particularly vital for the bottleneck models. Replacing the identity shortcut in Fig. 5 (right) with projection would show that the time cost and model size are multiplied twofold, since the shortcut connects to the two high-dimensional extremities. Hence identity shortcuts produce more efficient models for the bottleneck blueprints. Thus the utilization of bottleneck plans is chiefly owing to practical considerations. We further notice that the degradation dilemma of plain networks is also evident for the bottleneck plans. A 34-layer network with this 3-layer bottleneck block, resulting in a 50-layer ResNet (Table 1).  ","The parameter-free identity shortcuts have special significance for the bottleneck architectures. Substituting the identity shortcut in Fig. 5 (right) with projection would demonstrate that the time expense and model dimensions are doubled, as the shortcut links the two high-dimensional ends. Therefore identity shortcuts yield more efficient models for the bottleneck designs. Thus the use of bottleneck structures is primarily for practical reasons. We also see that the degradation issue of plain networks is present for the bottleneck structures. A 34-layer network with this 3-layer bottleneck block, giving a 50-layer ResNet (Table 1).",A,1
Deep Residual Learning for Image Recognition,"We use option B for increasing dimensions. This model has 3.8 billion FLOPs. 101-layer and 152-layer ResNets: We construct 101- layer and 152-layer ResNets by using more 3-layer blocks (Table 1). Remarkably, although the depth is significantly increased, the 152-layer ResNet (11.3 billion FLOPs) still has lower complexity than VGG-16/19 nets (15.3/19.6 billion FLOPs). The 50/101/152-layer ResNets are more accurate than the 34-layer ones by considerable margins (Table 3 and 4). We do not observe the degradation problem and thus enjoy significant accuracy gains from considerably increased depth.","We utilize choice B for expanding proportions. This prototype has 3.8 billion FLOPs. 101-stratum and 152-stratum ResNets: We form 101- layer and 152-layer ResNets by applying more 3-layer blocks (Table 1). Remarkably, despite the fact that the profundity is essentially expanded, the 152-layer ResNet (11.3 billion FLOPs) actually has lower intricacy than VGG-16/19 nets (15.3/19.6 billion FLOPs). The 50/101/152-layer ResNets are more precise than the 34-layer ones by significant edges (Table 3 and 4). We don't watch the corruption issue thus appreciate huge precision gains from essentially expanded profundity.","We make use of approach B to increase measurements. This model contains 3.8 billion FLOPs. 101-level and 152-level ResNets: We build 101- layer and 152-layer ResNets by utilizing additional 3-layer blocks (Table 1). Astoundingly, despite the fact that the depth is fundamentally expanded, the 152-layer ResNet (11.3 billion FLOPs) still has lower complexity than VGG-16/19 nets (15.3/19.6 billion FLOPs). The 50/101/152-layer ResNets are more accurate than the 34-layer ones by significant margins (Table 3 and 4). We don't observe the degradation issue thus appreciate huge accuracy gains from fundamentally expanded depth.","We employ option B for enlarging sizes. This prototype possesses 3.8 billion FLOPs. 101-tier and 152-tier ResNets: We construct 101- layer and 152-layer ResNets by applying further 3-layer blocks (Table 1). Remarkably, despite the depth being considerably increased, the 152-layer ResNet (11.3 billion FLOPs) still retains lower intricacy than VGG-16/19 nets (15.3/19.6 billion FLOPs). The 50/101/152-layer ResNets are more precise than the 34-layer ones by substantial margins (Table 3 and 4). We do not witness the deterioration issue thus reap immense accuracy gains from substantially expanded depth.",A,1
Deep Residual Learning for Image Recognition,The benefits of depth are witnessed for all evaluation metrics (Table 3 and 4). Comparisons with State-of-the-art Methods. In Table 4 we compare with the previous best single-model results. Our baseline 34-layer ResNets have achieved very competitive accuracy. Our 152-layer ResNet has a single-model top-5 validation error of 4.49%. This single-model result outperforms all previous ensemble results (Table 5). We combine six models of different depth to form an ensemble (only with two 152-layer ones at the time of submitting). This leads to 3.57% top-5 error on the test set (Table 5). This entry won the 1st place in ILSVRC 2015.,The advantages of depth can be seen for all assessment metrics (Tables 3 and 4). Comparisons to Current Best Methods. In Table 4 we contrast with the preceding top single-model results. Our foundational 34-layer ResNets have accomplished very competitive precision. Our 152-layer ResNet has a single-model top-5 validation mistake of 4.49%. This single-model outcome surpasses all past ensemble outcomes (Table 5). We unite six models of differing depth to form a group (only with two 152-layer ones at the time of submitting). This leads to 3.57% top-5 error on the test set (Table 5). This entry won 1st place in ILSVRC 2015.,The positive impacts of depth are evident across all evaluation measures (Tables 3 and 4). Benchmarks Against Leading Techniques. In Table 4 we juxtapose with the prior best individual-model findings. Our baseline 34-layer ResNets have achieved very competitive accuracy. Our 152-layer ResNet has a single-model top-5 validation blunder rate of 4.49%. This single-model result beats all preceding ensemble results (Table 5). We combine six models of varying depth to create an ensemble (only with two 152-layer ones at the time of entry). This produces 3.57% top-5 error on the test set (Table 5). This submission won 1st position in ILSVRC 2015.  ,The merits of depth can be discerned across all assessment metrics (Tables 3 and 4). Comparisons to Current Top Performing Methods. In Table 4 we hold up against the former best stand-alone model outputs. Our foundational 34-layer ResNets have realized very competitive precision. Our 152-layer ResNet has a single-model top-5 validation misstep percentage of 4.49%. This single-model outcome exceeds all past ensemble outputs (Table 5). We coalesce six models of different depth to constitute an ensemble (only with two 152-layer ones at the time of submission). This yields 3.57% top-5 error on the test set (Table 5). This entry was victorious in 1st place in ILSVRC 2015.,A,1
Deep Residual Learning for Image Recognition,"We conducted more studies on the CIFAR-10 dataset [20], which consists of 50k training images and 10k testing images in 10 classes. We present experiments trained on the training set and evaluated on the test set. Our focus is on the behaviors of extremely deep networks, but not on pushing the state-of-the-art results, so we intentionally use simple architectures as follows. The plain/residual architectures follow the form in Fig. 3 (middle/right). The network inputs are 32×32 images, with the per-pixel mean subtracted. The first layer is 3×3 convolutions.","We performed additional experiments using the CIFAR-10 image dataset [20], which has 50,000 images for training and 10,000 for testing across 10 categories. We trained models on the training images and assessed performance on the test images. Our goal was to study the behaviors of very deep neural networks, not to achieve state-of-the-art results, so we used simple network architectures. The plain/residual networks have the structures shown in Fig. 3 (middle/right). The input images are 32x32 with per-pixel means removed. The first layer is 3x3 convolutions.","We conducted more analyses utilizing the CIFAR-10 dataset [20], comprising 50k images for training and 10k for evaluation across 10 classes. We trained on the training data and tested on the test data. We aimed to examine the behaviors of extremely deep models, rather than optimize for the best performance, so we used basic network designs. The plain/residual networks follow the architectures in Fig. 3 (middle/right). The inputs are 32x32 images with per-pixel means subtracted. The first layer is 3x3 convolutions.","Additional experiments were performed with the CIFAR-10 dataset [20], having 50,000 training images and 10,000 test images in 10 categories. Models were trained on the training data and evaluated on the test data. The focus was analyzing the behaviors of very deep networks, not achieving state-of-the-art accuracy, so simple network architectures were used. The plain/residual networks have the designs in Fig. 3 (middle/right). The inputs are 32x32 images with per-pixel means removed. The first layer is 3x3 convolutions.",A,1
Deep Residual Learning for Image Recognition,"Then we use a stack of 6n layers with 3×3 convolutions on the feature maps of sizes {32, 16, 8} respectively, with 2n layers for each feature map size. The numbers of filters are {16, 32, 64} respectively. The subsampling is performed by convolutions with a stride of 2. The network ends with a global average pooling, a 10-way fully-connected layer, and softmax. There are totally 6n+2 stacked weighted layers. The following table summarizes the architecture: output map size 32×32 16×16 8×8 # layers 1+2n 2n 2n # filters 16 32 64 When shortcut connections are used, they are connected to the pairs of 3×3 layers (totally 3n shortcuts).","We construct a stack of 6n layers containing 3x3 convolutions on feature maps of sizes {32, 16, 8} pixels, with 2n layers for each feature map size. The filter numbers are {16, 32, 64}. Subsampling happens via strided convolutions with stride 2. The network finishes with global average pooling, a 10-way fully-connected layer, and softmax. There are 6n+2 total weighted layers stacked. The architecture is summarized as: output size 32x32 16x16 8x8 # layers 1+2n 2n 2n # filters 16 32 64 Shortcuts connect pairs of 3x3 layers, making 3n shortcuts total.","We use a stack of 6n layers, with 3x3 convolutions on feature maps of sizes {32, 16, 8} pixels, using 2n layers for each map size. The filter numbers are {16, 32, 64}. We subsample by convolving with stride 2. The network has global average pooling, a 10-way fully-connected layer, and softmax. In total there are 6n+2 stacked weighted layers. The design is: output dimensions 32x32 16x16 8x8 layer count 1+2n 2n 2n filter count 16 32 64 Shortcuts connect 3x3 layer pairs, making 3n shortcuts. ","We build a stack of 6n layers, applying 3x3 convolutions to feature maps of sizes {32, 16, 8} pixels, utilizing 2n layers per map size. The filter counts are {16, 32, 64}. Downsampling is done by striding convolutions by 2. Finally there is global average pooling, a 10-way fully-connected layer, and softmax. Total stacked weighted layers is 6n+2. The layout is: output size 32x32 16x16 8x8 layer number 1+2n 2n 2n filter number 16 32 64 Shortcut connections link pairs of 3x3 layers, forming 3n shortcuts.",A,1
Deep Residual Learning for Image Recognition,"We use a weight decay of 0.0001 and momentum of 0.9, and adopt the weight initialization in [13] and BN [16] but with no dropout. These models are trained with a minibatch size of 128 on two GPUs. We start with a learning rate of 0.1, divide it by 10 at 32k and 48k iterations, and terminate training at 64k iterations, which is determined on a 45k/5k train/val split. We follow the simple data augmentation in [24] for training: 4 pixels are padded on each side, and a 32×32 crop is randomly sampled from the padded image or its horizontal flip.","We utilize a weight decay of 0.0001 and momentum of 0.9, and implement the weight initialization from [13] and batch normalization [16] but without dropout. These models are educated with a mini-batch size of 128 on two GPUs. We commence with a learning rate of 0.1, divide it by 10 at 32k and 48k iterations, and end training at 64k iterations, which is decided on a 45k/5k train/val split. We follow the straightforward data augmentation in [24] for training: 4 pixels are padded on each side, and a 32×32 crop is randomly sampled from the padded image or its horizontal reflection.","We make use of a weight decay of 0.0001 and momentum of 0.9, and adopt the weight initialization described in [13] and batch normalization [16] excluding dropout. These models are learned with a mini-batch dimension of 128 on two GPUs. We initiate with a learning rate of 0.1, reduce it by 10 at 32k and 48k iterations, and finish training at 64k iterations, which is established on a 45k/5k train/val split. We apply the simple data augmentation in [24] for training: 4 pixels are added on each side, and a 32×32 crop is randomly extracted from the padded image or its horizontal flip.  ","We employ a weight decay of 0.0001 and momentum of 0.9, and implement the weight initialization in [13] and batch normalization [16] without dropout. These models are educated with a mini-batch amount of 128 on two GPUs. We start off with a learning rate of 0.1, decrease it by 10 at 32k and 48k iterations, and stop training at 64k iterations, which is identified on a 45k/5k train/val split. We use the basic data augmentation in [24] for training: 4 pixels are appended on each side, and a 32×32 crop is randomly selected from the padded image or its horizontal reflection.",A,1
Deep Residual Learning for Image Recognition,"For testing, we only evaluate the single view of the original 32×32 image. We compare n = {3, 5, 7, 9}, leading to 20, 32, 44, and 56-layer networks. Fig. 6 (left) shows the behaviors of the plain nets. The deep plain nets suffer from increased depth, and exhibit higher training error when going deeper. This phenomenon is similar to that on ImageNet (Fig. 4, left) and on MNIST (see [42]), suggesting that such an optimization difficulty is a fundamental problem. Fig. 6 (middle) shows the behaviors of ResNets.","To evaluate, we only look at one view of the starting 32x32 picture. We try n = {3, 5, 7, 9}, making networks with 20, 32, 44, and 56 layers. Fig. 6 (left side) displays how the basic networks act. The deep basic networks have trouble with more depth, and show higher training mistakes when going deeper. This is similar to ImageNet (Fig. 4, left) and MNIST (see [42]), implying this optimization problem is fundamental. Fig. 6 (middle) displays how ResNets behave.","For analysis, we just use the single perspective of the initial 32x32 image. We test n = {3, 5, 7, 9}, resulting in networks with 20, 32, 44, and 56 tiers. Fig. 6 (left) illustrates the behaviors of the plain networks. The deep plain networks struggle with increased depth, and have higher training errors when made deeper. This is akin to ImageNet (Fig. 4, left) and MNIST (see [42]), hinting this optimization difficulty is inherent. Fig. 6 (middle) shows how ResNets act.","To assess, we only look at the single view of the starting 32x32 picture. We try n = {3, 5, 7, 9}, forming networks of 20, 32, 44, and 56 layers. Fig. 6 (left) shows the plain nets' behaviors. The deep plain nets have issues with more depth, and higher training mistakes when deeper. This mirrors ImageNet (Fig. 4, left) and MNIST (see [42]), implying this optimization problem is fundamental. Fig. 6 (middle) displays ResNets' behaviors.",A,1
Deep Residual Learning for Image Recognition,"Also similar to the ImageNet cases (Fig. 4, right), our ResNets manage to overcome the optimization difficulty and demonstrate accuracy gains when the depth increases. We further explore n = 18 that leads to a 110-layer ResNet. In this case, we find that the initial learning rate of 0.1 is slightly too large to start converging5 . So we use 0.01 to warm up the training until the training error is below 80% (about 400 iterations), and then go back to 0.1 and continue training. The rest of the learning schedule is as done previously. This 110-layer network converges well (Fig. 6, middle).","Likewise as with the ImageNet examples (Fig. 4, right), our ResNets are able to get over the optimization challenges and show precision improvements as the depth grows. We additionally investigate n = 18 which results in a 110-layer ResNet. Here, we find that the initial learning rate of 0.1 is slightly too big to start converging. So we utilize 0.01 to warm up the training until the training error is under 80% (about 400 iterations), then go back to 0.1 and keep training. The rest of the learning schedule is as before. This 110-layer network converges well (Fig. 6, middle).","Similarly to the ImageNet situations (Fig. 4, right), our ResNets can overcome the optimization problems and exhibit accuracy gains as the depth increases. We also explore n = 18 leading to a 110-layer ResNet. In this case, we determine that the initial learning rate of 0.1 is slightly too high to begin converging. Thus we use 0.01 to preheat the training until the training error is below 80% (around 400 iterations), then return to 0.1 and proceed training. The rest of the learning plan is as previously done. This 110-layer network converges successfully (Fig. 6, middle).  ","In the same way as the ImageNet examples (Fig. 4, right), our ResNets are able to conquer the optimization challenges and display precision improvements when the depth is increased. We further investigate n = 18 resulting in a 110-layer ResNet. Here, we find that the initial learning rate of 0.1 is slightly too large to start converging. Therefore, we utilize 0.01 to warm up the training until the training error is under 80% (approximately 400 iterations), then go back to 0.1 and continue training. The rest of the learning schedule is as before. This 110-layer network converges well (Fig. 6, middle).",A,1
Deep Residual Learning for Image Recognition,"It has fewer parameters than other deep and thin 5With an initial learning rate of 0.1, it starts converging (3% over VGG-16. This gain is solely because of the improved features learned by ResNet. MS COCO The MS COCO dataset [26] involves 80 object categories. We evaluate the PASCAL VOC metric (mAP @ IoU = 0.5) and the standard COCO metric (mAP @ IoU = .5:.05:.95). We use the 80k images on the train set for training and the 40k images on the val set for evaluation.","This model has less adjustable settings than other deep and narrow neural networks. With a beginning learning speed of 0.1, it starts to converge, achieving 3% better performance than VGG-16. This enhancement is only due to the more advanced features learned by ResNet. The MS COCO dataset involves 80 types of objects. We measure the PASCAL VOC metric (mAP at IoU = 0.5) and the normal COCO metric (mAP at IoU = .5:.05:.95). We utilize the 80k images in the train set for training and the 40k images in the val set for evaluation.","Compared to other deep and thin neural networks, this one has fewer tweakable parameters. With an initial learning rate of 0.1, convergence starts to occur, with 3% higher accuracy versus VGG-16. This boost comes solely from the more sophisticated features that ResNet is able to learn. The MS COCO dataset contains 80 categories of objects. We assess performance using the PASCAL VOC metric (mAP at an IoU of 0.5) and the standard COCO metric (mAP at IoU ranging from .5 to .95 in .05 increments). For training we use the 80k images in the training set, and for evaluation we use the 40k images in the validation set.  ","This neural network architecture has less tunable knobs relative to other deep, narrow models. With a starting learning rate of 0.1, it begins converging, achieving 3% superior performance over VGG-16. This improvement is entirely attributable to the more advanced representations learned by ResNet. The MS COCO dataset has 80 types of objects. We measure accuracy using the PASCAL VOC metric (mAP at an IoU of 0.5) and the conventional COCO metric (mAP at IoU from .5 to .95 in .05 steps). For training we utilize the 80k images in the training set, and for testing we use the 40k images in the validation set.",A,1
Deep Residual Learning for Image Recognition,"Our detection system for COCO is similar to that for PASCAL VOC. We train the COCO models with an 8-GPU implementation, and thus the RPN step has a mini-batch size of 8 images (i.e., 1 per GPU) and the Fast R-CNN step has a mini-batch size of 16 images. The RPN step and Fast RCNN step are both trained for 240k iterations with a learning rate of 0.001 and then for 80k iterations with 0.0001. Table 8 shows the results on the MS COCO validation set.","Our system for identifying objects in COCO images is comparable to our approach for PASCAL VOC images. We implement training on 8 GPUs, using mini-batches of 8 images for the region proposal network and 16 images for the Fast R-CNN classifier. Both networks are trained for 240k iterations at a learning rate of 0.001 followed by 80k iterations at 0.0001. Table 8 displays the performance on the COCO validation images.","Our COCO object detection model is similar in design to our PASCAL VOC detector. We utilize 8 GPUs for training, with mini-batches of 8 images for region proposal generation and 16 images for Fast R-CNN classification. The learning rate is 0.001 for the first 240k iterations and 0.0001 for the next 80k iterations. Validation results on COCO are shown in Table 8.  ","Our approach for detecting objects in the COCO dataset parallels our technique for PASCAL VOC. Training leverages 8 GPUs, using mini-batches of 8 images for proposing regions and 16 images for Fast R-CNN categorization. Both networks are trained at a learning rate of 0.001 for 240k iterations, then 0.0001 for 80k more. Performance on the COCO validation set is presented in Table 8.",A,1
Deep Residual Learning for Image Recognition,"ResNet-101 has a 6% increase of mAP@[.5, .95] over VGG-16, which is a 28% relative improvement, solely contributed by the features learned by the better network. Remarkably, the mAP@[.5, .95]’s absolute increase (6.0%) is nearly as big as mAP@.5’s (6.9%). This suggests that a deeper network can improve both recognition and localization. For completeness, we report the improvements made for the competitions. These improvements are based on deep features and thus should benefit from residual learning. MS COCO Box refinement. Our box refinement partially follows the iterative localization in [6]. In Faster R-CNN, the final output is a regressed box that is different from its proposal box.","ResNet-101 shows a 6% boost in mAP@[.5, .95] compared to VGG-16, which is a 28% relative enhancement, solely owing to the learned features of the superior network. Strikingly, the absolute increase in mAP@[.5, .95] (6.0%) is nearly as large as mAP@.5's (6.9%). This implies that a deeper network can improve both identification and localization. For completeness, we report the gains made for the competitions. These improvements are based on deep features and thus should benefit from residual learning. MS COCO Box refinement. Our box refinement somewhat follows the iterative localization in [6]. In Faster R-CNN, the final output is a regressed box distinct from its proposal box.","ResNet-101 demonstrates a 6% rise in mAP@[.5, .95] over VGG-16, which equals a 28% relative boost, solely thanks to the learned representations of the better network. Remarkably, the absolute improvement in mAP@[.5, .95] (6.0%) is nearly as large as mAP@.5's (6.9%). This hints that a deeper network can enhance both recognition and localization. For thoroughness, we document the advances made for the competitions. These gains build on deep features and thus should profit from residual learning. MS COCO Box refinement. Our box refinement somewhat adheres to the iterative localization in [6]. In Faster R-CNN, the final output is a regressed box different from its proposal box.","ResNet-101 shows a 6% increase in mAP@[.5, .95] compared to VGG-16, which equals a 28% relative improvement, solely due to the learned features of the superior network. Strikingly, the absolute gain in mAP@[.5, .95] (6.0%) is nearly as large as mAP@.5's (6.9%). This indicates that a deeper network can enhance both detection and localization. For completeness, we list the improvements made for the competitions. These gains utilize deep features and thus should benefit from residual learning. MS COCO Box refinement. Our box refinement partially follows the iterative localization in [6]. In Faster R-CNN, the final output box is regressed from its proposal box.",A,1
Deep Residual Learning for Image Recognition,"So for inference, we pool a new feature from the regressed box and obtain a new classification score and a new regressed box. We combine these 300 new predictions with the original 300 predictions. Non-maximum suppression (NMS) is applied on the union set of predicted boxes using an IoU threshold of 0.3 [8], followed by box voting [6]. Box refinement improves mAP by about 2 points (Table 9). We combine global context in the Fast R-CNN step. Given the full-image conv feature map, we pool a feature by global Spatial Pyramid Pooling [12] (with a “single-level” pyramid) which can be implemented as “RoI” pooling using the entire image’s bounding box as the RoI.","For making inferences, we take a new characteristic from the predicted box and get a new classification result and a new predicted box. We unite these 300 new forecasts with the original 300 forecasts. Non-max suppression (NMS) is used on the union set of expected boxes employing an IoU threshold of 0.3 [8], followed by box voting [6]. Box improvement boosts mAP by around 2 points (Table 9). We integrate global context in the Fast R-CNN step. Given the full-image conv feature map, we take a characteristic by global Spatial Pyramid Pooling [12] (with a ""single-level"" pyramid) which can be implemented as ""RoI"" pooling utilizing the entire image's bounding box as the RoI.","To make predictions, we extract a new feature from the regressed bounding box to obtain a new classification score and bounding box. We combine these 300 new predictions with the original 300. Non-maximum suppression (NMS) is applied to the union of predicted boxes using an IoU threshold of 0.3 [8], then box voting [6]. Refining the boxes improves mAP by about 2 points (Table 9). We incorporate global context in Fast R-CNN. Given the full image convolutional feature map, we extract a feature using global Spatial Pyramid Pooling [12] (with a single-level pyramid) implemented as RoI pooling using the whole image bounding box as the RoI.  ","For making inferences, we extract a new characteristic from the predicted box and get a new classification result and a new predicted box. We join these 300 new forecasts with the original 300. Non-max suppression (NMS) is utilized on the union of expected boxes employing an IoU threshold of 0.3 [8], then box voting [6]. Box refinement increases mAP by around 2 points (Table 9). We integrate global context in Fast R-CNN. Given the full image conv feature map, we extract a characteristic using global Spatial Pyramid Pooling [12] (with a single-level pyramid) implemented as RoI pooling using the entire image bounding box as the RoI.",A,1
Deep Residual Learning for Image Recognition,"This pooled feature is fed into the post-RoI layers to obtain a global context feature. This global feature is concatenated with the original per-region feature, followed by the sibling classification and box regression layers. This new structure is trained end-to-end. Global context improves mAP@.5 by about 1 point (Table 9). Multi-scale testing. In the above, all results are obtained by single-scale training/testing as in [32], where the image’s shorter side is s = 600 pixels. Multi-scale training/testing has been developed in [12, 7] by selecting a scale from a feature pyramid, and in [33] by using maxout layers.","This combined characteristic is provided to the post-RoI layers to acquire a universal context element. This universal component is joined with the primary per-region characteristic, followed by the sibling sorting and box regression layers. This new framework is educated from end to end. Universal context progresses mAP@.5 by around 1 point (Table 9). Multi-scale testing. As stated above, all outcomes are acquired by single-scale training/testing as in [32], where the image's shorter side is s = 600 pixels. Multi-scale training/testing has been formed in [12, 7] by choosing a scale from a feature pyramid, and in [33] by utilizing maxout layers.","This pooled feature is input into the after region of interest layers to get a global context feature. This global feature is concatenated with the original feature per region, followed by the sibling classification and box regression layers. This new architecture is trained from start to finish. Global context improves mAP@.5 by about 1 point (Table 9). Testing with multiple scales. As mentioned above, all results are obtained by training/testing with a single scale as in [32], where the shorter side of the image is s = 600 pixels. Training/testing with multiple scales has been developed in [12, 7] by selecting a scale from a feature pyramid, and in [33] by using maxout layers.","This combined feature is fed into the post region of interest layers to obtain a universal context feature. This global feature is joined with the original per-region feature, followed by the sibling categorization and box regression layers. This new design is educated completely. Universal context improves mAP@.5 by around 1 point (Table 9). Testing with multiple scales. As stated above, all outcomes are obtained by single-scale training/testing as in [32], where the image's shorter side is s = 600 pixels. Training/testing with multiple scales has been formed in [12, 7] by choosing a scale from a feature pyramid, and in [33] by using maxout layers.",A,1
Deep Residual Learning for Image Recognition,"RoI pooling and subsequent layers are performed on the feature maps of these two scales [33], which are merged by maxout as in [33]. Multi-scale testing improves the mAP by over 2 points (Table 9). Next we use the 80k+40k trainval set for training and the 20k test-dev set for evaluation. The testdev set has no publicly available ground truth and the result is reported by the evaluation server. Under this setting, the results are an mAP@.5 of 55.7% and an mAP@[.5, .95] of 34.9% (Table 9). This is our single-model result.","ROI pooling and later layers are implemented on the characteristic maps of these two scales [33], which are combined by maxout as described in [33]. Evaluating with multiple scales improves the mAP by more than 2 points (Table 9). We then utilize the 80k+40k trainval set for teaching and the 20k test-dev set for assessment. The testdev set has no publicly available factual information and the outcome is accounted for by the evaluation server. Under this configuration, the results are an mAP@.5 of 55.7% and an mAP@[.5, .95] of 34.9% (Table 9). This is our single-model result.","Region of Interest pooling and subsequent neural network layers are applied to the feature maps of these two image scales [33], which maxout merges as in [33]. Testing with multiple scales boosts the mean average precision by over 2 percentage points (Table 9). We then train on the 80k+40k trainval set and validate on the 20k test-dev set. The testdev set has no public ground truth and the evaluation server reports the result. With this setup, we achieve a mean average precision at 0.5 IoU of 55.7% and at 0.5 to 0.95 IoU of 34.9% (Table 9). This is our single model result.  ","ROI pooling and later layers work on the characteristic maps of these two image sizes [33], combined by maxout as described in [33]. Multi-scale validation improves mAP by more than 2 points (Table 9). We train on the 80k+40k trainval set and test on the 20k test-dev set. The testdev set has no public ground truth, so the evaluation server reports the result. With this, we get 55.7% mAP@0.5 IoU and 34.9% mAP@0.5-0.95 IoU (Table 9). This is our single model result.",A,1
Deep Residual Learning for Image Recognition,"In Faster R-CNN, the system is designed to learn region proposals and also object classifiers, so an ensemble can be used to boost both tasks. We use an ensemble for proposing regions, and the union set of proposals are processed by an ensemble of per-region classifiers. Table 9 shows our result based on an ensemble of 3 networks. The mAP is 59.0% and 37.4% on the test-dev set. This result won the 1st place in the detection task in COCO 2015.","The Faster R-CNN system is engineered to develop region recommendations and object classifiers. Therefore, an assembly can enhance both jobs. We utilize an assembly for suggesting areas, and the union collection of proposals are handled by an assembly of per-area categorizers. Table 9 displays our outcome founded on an assembly of 3 systems. The mAP is 59.0% and 37.4% on the test-dev collection. This outcome was victorious in the detection assignment in COCO 2015.","In Faster R-CNN, the framework is intended to learn region ideas and object classifiers too, so a group can boost both tasks. We employ a group for proposing zones, and the joined set of proposals are processed by a group of per-zone categorizers. Table 9 exhibits our result based on a group of 3 networks. The mAP is 59.0% and 37.4% on the test-dev set. This result won 1st place in the detection job in COCO 2015.","The Faster R-CNN framework is built to develop region suggestions and object classifiers as well. Therefore, a coalition can enhance both jobs. We use a coalition for proposing areas, and the combined set of proposals are handled by a coalition of per-area categorizers. Table 9 shows our outcome based on a coalition of 3 networks. The mAP is 59.0% and 37.4% on the test-dev set. This outcome was victorious in the detection task in COCO 2015.",A,1
Deep Residual Learning for Image Recognition,"The improvements of box refinement, context, and multi-scale testing are also adopted. Our results (mAP, %) on the ImageNet detection dataset. Our detection system is Faster R-CNN [32] with the improvements in Table 9, using ResNet-101. we achieve 85.6% mAP on PASCAL VOC 2007 (Table 10) and 83.8% on PASCAL VOC 2012 (Table 11) 6 . The result on PASCAL VOC 2012 is 10 points higher than the previous state-of-the-art result [6]. ImageNet Detection The ImageNet Detection (DET) task involves 200 object categories. The accuracy is evaluated by mAP@.5.","The enhancements of box adjustment, surrounding information, and testing across multiple scales are also utilized. Our outcomes (mAP, %) on the ImageNet object identification dataset. Our identification framework is Faster R-CNN [32] with the advancements in Table 9, utilizing ResNet-101. We accomplish 85.6% mAP on PASCAL VOC 2007 (Table 10) and 83.8% on PASCAL VOC 2012 (Table 11) 6. The outcome on PASCAL VOC 2012 is 10 points superior to the past top notch result [6]. ImageNet Detection The ImageNet Detection (DET) assignment includes 200 article classifications. The precision is assessed by mAP@.5.","The refinements of bounding box tuning, context, and evaluating across different scales are also employed. Our performance metrics (mAP, %) on the ImageNet object detection dataset. Our detection system is Faster R-CNN [32] with the improvements shown in Table 9, leveraging ResNet-101. We reach 85.6% mAP on PASCAL VOC 2007 (Table 10) and 83.8% on PASCAL VOC 2012 (Table 11) 6. The score on PASCAL VOC 2012 is 10 points better than the previous best result [6]. ImageNet Detection The ImageNet Detection (DET) job covers 200 object types. The accuracy is measured by mAP@.5.","The enhancements of bounding box adjustment, surrounding information, and testing at multiple scales are also used. Our results (mAP, %) on the ImageNet object detection dataset. Our detection framework is Faster R-CNN [32] with the advancements in Table 9, employing ResNet-101. We attain 85.6% mAP on PASCAL VOC 2007 (Table 10) and 83.8% on PASCAL VOC 2012 (Table 11) 6. The outcome on PASCAL VOC 2012 is 10 points above the prior state-of-the-art result [6]. ImageNet Detection The ImageNet Detection (DET) task includes 200 object categories. The precision is evaluated by mAP@.5.",A,1
Deep Residual Learning for Image Recognition,"Our object detection algorithm for ImageNet DET is the same as that for MS COCO in Table 9. The networks are pretrained on the 1000-class ImageNet classification set, and are fine-tuned on the DET data. We split the validation set into two parts (val1/val2) following [8]. We fine-tune the detection models using the DET training set and the val1 set. The val2 set is used for validation. We do not use other ILSVRC 2015 data. This result won the 1st place in the ImageNet detection task in ILSVRC 2015, surpassing the second place by 8.5 points (absolute).","Our image recognition program for ImageNet DET is identical to the one for MS COCO shown in Table 9. The neural networks are first trained on the 1000 ImageNet classification images, then optimized on the DET images. We separated the proof data into two groups (val1/val2) as described in [8]. We tuned the detection models using the DET training images and val1 group. The val2 group was used to validate. We did not utilize other ILSVRC 2015 data. This outcome was ranked 1st place in the ImageNet detection challenge in ILSVRC 2015, exceeding 2nd place by 8.5 points.","Our object spotting algorithm for ImageNet DET matches the one for MS COCO in Table 9. The networks are initially educated on the 1000-category ImageNet sorting set, then fine-adjusted on the DET information. We divided the confirmation set into two sections (val1/val2) as per [8]. We refined the identification models utilizing the DET preparing set and the val1 set. The val2 set was utilized for approval. We didn't utilize other ILSVRC 2015 information. This outcome won the first spot in the ImageNet identification task in ILSVRC 2015, beating the second spot by 8.5 points (outright).","Our article recognition program for ImageNet DET is the same as for MS COCO in Table 9. The networks are first taught on the 1000-type ImageNet grouping images, then finely tuned on the DET data. We separated the verification set into two segments (val1/val2) as indicated by [8]. We refined the recognition models using the DET preparing set and val1 set. The val2 set was used for confirmation. We didn't use other ILSVRC 2015 data. This result won first place in the ImageNet identification challenge in ILSVRC 2015, surpassing second place by 8.5 points (total).",A,1
Deep Residual Learning for Image Recognition,"The ImageNet Localization (LOC) task [36] requires to classify and localize the objects. Following [40, 41], we assume that the image-level classifiers are first adopted for predicting the class labels of an image, and the localization algorithm only accounts for predicting bounding boxes based on the predicted classes. We adopt the “per-class regression” (PCR) strategy [40, 41], learning a bounding box regressor for each class. We pre-train the networks for ImageNet classification and then fine-tune them for localization. We train networks on the provided 1000-class ImageNet training set. Our localization algorithm is based on the RPN framework of [32] with a few modifications.","The ImageNet Localization (LOC) challenge [36] necessitates categorizing and pinpointing the objects. As in [40, 41], we presume image-level classifiers are firstly utilized for forecasting the category labels of an image, and the localization algorithm only handles predicting bounding boxes founded on the anticipated classes. We take on the ""per-class regression"" (PCR) plan [40, 41], educating a bounding box regressor for each class. We pre-train the networks for ImageNet categorization then fine-tune them for localization. We coach networks on the given 1000-class ImageNet training set. Our localization algorithm depends on the RPN structure of [32] with a few tweaks.","The ImageNet Localization (LOC) job [36] entails sorting and situating the objects. As per [40, 41], we think image-level sorters are first adopted for predicting the type names of an image, and the localization formula only deals with foretelling bounding boxes based on the predicted types. We take up the ""per-class regression"" (PCR) policy [40, 41], teaching a bounding box regressor for each type. We pre-train the networks for ImageNet sorting then fine-tune them for localization. We tutor networks on the provided 1000-class ImageNet training set. Our localization formula relies on the RPN form of [32] with a few changes.  ","The ImageNet Localization (LOC) effort [36] necessitates categorizing and pinpointing the objects. As in [40, 41], we believe image-level categorizers are first used for anticipating the category labels of an image, and the localization method only manages predicting bounding boxes founded on the predicted categories. We adopt the ""per-class regression"" (PCR) strategy [40, 41], instructing a bounding box regressor for each category. We pre-train the networks for ImageNet categorization then fine-tune them for localization. We coach networks on the given 1000-class ImageNet training set. Our localization method hinges on the RPN framework of [32] with a few alterations.",A,1
Deep Residual Learning for Image Recognition,"Unlike the way in [32] that is category-agnostic, our RPN for localization is designed in a per-class form. This RPN ends with two sibling 1×1 convolutional layers for binary classification (cls) and box regression (reg), as in [32]. The cls and reg layers are both in a per-class from, in contrast to [32]. Specifically, the cls layer has a 1000-d output, and each dimension is binary logistic regression for predicting being or not being an object class; the reg layer has a 1000×4-d output consisting of box regressors for 1000 classes.","Our method for localization differs from [32] in that it is class-specific, not class-agnostic. Similar to [32], our region proposal network (RPN) concludes with two 1x1 convolutional layers for binary classification (cls) and bounding box regression (reg). However, the cls and reg layers are per-class, unlike [32]. In particular, the cls layer outputs 1000 dimensions, each being binary logistic regression to predict an object's presence or absence for each class. The reg layer outputs 1000x4 dimensions, comprising box regressors for the 1000 classes.","Our approach to localization is designed for individual classes rather than being general like [32]. Our region proposal network (RPN) ends with two 1x1 conv layers, one for binary classification (cls) and one for box regression (reg), following [32]. But our cls and reg layers are per-class rather than general, contrasting with [32]. Specifically, the cls layer outputs 1000 dimensions, with each dimension doing binary logistic regression to predict whether or not each class is present. The reg layer outputs 1000x4 dimensions, which are box regressors for the 1000 classes. ","Our localization method is tailored to each class, unlike the class-agnostic approach in [32]. Similar to [32], our region proposal network (RPN) terminates in two 1x1 convolutional layers, one for binary classification (cls) and one for box regression (reg). However, our cls and reg layers are per-class, differing from [32]. In particular, the cls layer outputs 1000 dimensions, with each dimension conducting binary logistic regression to predict object presence or absence for each class. The reg layer outputs 1000x4 dimensions, which are box regressors for the 1000 individual classes.",A,1
Deep Residual Learning for Image Recognition,"As in [32], our bounding box regression is with reference to multiple translation-invariant “anchor” boxes at each position. As in our ImageNet classification training (Sec. 3.4), we randomly sample 224×224 crops for data augmentation. We use a mini-batch size of 256 images for fine-tuning. To avoid negative samples being dominate, 8 anchors are randomly sampled for each image, where the sampled positive and negative anchors have a ratio of 1:1 [32]. For testing, the network is applied on the image fully-convolutionally.","Similar to [32], our bounding box regression relates to multiple anchor boxes at each location that are unaffected by translation. As with our ImageNet classification training (Sec. 3.4), we randomly extract 224x224 crops for data augmentation. We utilize a mini-batch amount of 256 images for fine-tuning. To prevent negative samples from dominating, 8 anchors are randomly chosen for each image, where the positive and negative sampled anchors have a 1:1 ratio [32]. For testing, the network is applied fully-convolutionally on the full image.","In line with [32], our bounding box regression refers to multiple anchor boxes at each spot that are translation-invariant. Like our ImageNet classification preparation (Sec. 3.4), we arbitrarily crop 224x224 sections for data enlargement. We employ a mini-batch quantity of 256 images for fine-tuning. To avoid negative examples prevailing, 8 anchors are randomly selected for each image, with the positive and negative sampled anchors having a 1:1 proportion [32]. For evaluation, the network is applied fully-convolutionally over the entire image.  ","Similar to [32], our bounding box regression is relative to multiple anchor boxes at every location that are unaffected by translation. As in our ImageNet classification teaching (Sec. 3.4), we randomly excerpt 224x224 segments for data expansion. We utilize a mini-batch amount of 256 images for fine-tuning. To prevent negative instances from dominating, 8 anchors are randomly chosen for each image, with the positive and negative sampled anchors having a 1:1 ratio [32]. For testing, the network is applied fully-convolutionally across the whole image.",A,1
Deep Residual Learning for Image Recognition,"Table 13 compares the localization results. Following [41], we first perform “oracle” testing using the ground truth class as the classification prediction. VGG’s paper [41] remethod top-5 localization err val test OverFeat [40] (ILSVRC’13) 30.0 29.9 GoogLeNet [44] (ILSVRC’14) - 26.7 VGG [41] (ILSVRC’14) 26.9 25.3 ours (ILSVRC’15) 8.9 9.0 Table 14. Comparisons of localization error (%) on the ImageNet dataset with state-of-the-art methods. ports a center-crop error of 33.1% (Table 13) using ground truth classes. Under the same setting, our RPN method using ResNet-101 net significantly reduces the center-crop error to 13.3%. This comparison demonstrates the excellent performance of our framework.","The table provides a comparison of the localization results. As described in [41], we first conduct ""oracle"" evaluation utilizing the actual class as the classification prediction. The paper by VGG [41] shows a top-5 localization error rate on the validation and test sets of 30.0% and 29.9% respectively. When using the ground truth classes, their center-crop error is 33.1% (Table 13). Under the identical configuration, our RPN approach leveraging a ResNet-101 network substantially decreases the center-crop error to 13.3%. This contrast exhibits the outstanding capabilities of our system.","The table presents a juxtaposition of the localization outputs. Per the methodology of [41], we initially carry out ""ideal"" assessment employing the genuine category as the classification result. VGG's publication [41] documents a top-5 localization mistake percentage on the validation and test sets of 30.0% and 29.9% correspondingly. Utilizing the real classes, their center-crop inaccuracy is 33.1% (Table 13). Under the same settings, our RPN methodology harnessing a ResNet-101 network markedly reduces the center-crop error to 13.3%. This comparison showcases the phenomenal performance of our framework.  ","The table provides a side-by-side analysis of the localization outputs. As outlined in [41], we first implement ""optimal"" testing leveraging the actual class as the classification prediction. The paper from VGG [41] presents a top-5 localization error rate on the validation and test sets of 30.0% and 29.9% respectively. When utilizing the genuine classes, their center-crop mistake is 33.1% (Table 13). Under identical conditions, our RPN approach employing a ResNet-101 network significantly decreases the center-crop error to 13.3%. This contrast demonstrates the outstanding capabilities of our system.",A,1
Deep Residual Learning for Image Recognition,"With dense (fully convolutional) and multi-scale testing, our ResNet-101 has an error of 11.7% using ground truth classes. Using ResNet-101 for predicting classes (4.6% top-5 classification error, Table 4), the top-5 localization error is 14.4%. The above results are only based on the proposal network (RPN) in Faster R-CNN [32]. One may use the detection network (Fast R-CNN [7]) in Faster R-CNN to improve the results. But we notice that on this dataset, one image usually contains a single dominate object, and the proposal regions highly overlap with each other and thus have very similar RoI-pooled features.","Utilizing dense (fully convolutional) and multi-scale testing, our ResNet-101 model achieves an error rate of 11.7% when using actual classes. With ResNet-101 to predict classes (4.6% top-5 classification error, Table 4), the top-5 localization error is 14.4%. The preceding outcomes depend solely on the proposal network (RPN) in Faster R-CNN [32]. One could employ the detection network (Fast R-CNN [7]) in Faster R-CNN to enhance the results. However, we notice that in this dataset, one image usually has a single main object, and the proposal regions are highly overlapping and thus have very similar RoI-pooled features.","When using dense (fully convolutional) and multi-scale testing, our ResNet-101 model produces an error rate of 11.7% with real classes. Utilizing ResNet-101 for class prediction (4.6% top-5 classification error, Table 4), the top-5 localization error is 14.4%. The previous numbers come only from the proposal network (RPN) in Faster R-CNN [32]. One option is to leverage the detection network (Fast R-CNN [7]) in Faster R-CNN to improve performance. However, we see that in this dataset, one image typically contains a single primary object, and the proposal regions have high overlap, leading to very similar RoI-pooled features.","Leveraging dense (fully convolutional) and multi-scale testing, our ResNet-101 achieves an error rate of 11.7% on actual classes. Applying ResNet-101 for class forecasting (4.6% top-5 classification error, Table 4), the top-5 localization error is 14.4%. The preceding statistics originate solely from the proposal network (RPN) in Faster R-CNN [32]. One possibility is to use the detection network (Fast R-CNN [7]) in Faster R-CNN to enhance results. However, we notice that in this dataset, one image usually has one main object, and the proposal areas have high overlap, producing very similar RoI-pooled features.",A,1
Deep Residual Learning for Image Recognition,"As a result, the image-centric training of Fast R-CNN [7] generates samples of small variations, which may not be desired for stochastic training. Motivated by this, in our current experiment we use the original RCNN [8] that is RoI-centric, in place of Fast R-CNN. Our R-CNN implementation is as follows. We apply the per-class RPN trained as above on the training images to predict bounding boxes for the ground truth class. These predicted boxes play a role of class-dependent proposals. For each training image, the highest scored 200 proposals are extracted as training samples to train an R-CNN classifier.","Consequently, the image-focused training process of Fast R-CNN [7] produces samples with minor differences, which may not be optimal for stochastic learning. Driven by this observation, in our present experiment we utilize the original RCNN [8] which is region of interest-focused, rather than Fast R-CNN. Our R-CNN implementation is as follows. We use the per-class RPN trained as described above on the training images to predict bounding boxes for the true class. These predicted boxes serve as class-specific proposals. For each training image, the top 200 highest scored proposals are extracted as training samples to train an R-CNN classifier.","For this reason, the image-oriented training of Fast R-CNN [7] generates examples with small variations, which might not be useful for random training. Prompted by this, in our current test we employ the original RCNN [8] which focuses on regions of interest, instead of Fast R-CNN. Our R-CNN implementation is like this. We apply the per-class RPN trained as mentioned above on the training images to forecast bounding boxes for the actual class. These predicted boxes act as class-specific suggestions. For every training image, the top 200 proposals with the highest scores are extracted as training samples to train an R-CNN classifier.  ","As a result, the image-driven training of Fast R-CNN [7] produces samples with minor differences, which may not be optimal for stochastic learning. Spurred by this finding, in our present analysis we utilize the original RCNN [8] which concentrates on regions of interest, rather than Fast R-CNN. Our R-CNN implementation is as follows. We use the per-class RPN trained as stated above on the training images to predict bounding boxes for the genuine class. These predicted boxes function as class-dependent proposals. For every training image, the 200 top-scoring proposals are extracted as training samples to train an R-CNN classifier.",A,1
Deep Residual Learning for Image Recognition,"The image region is cropped from a proposal, warped to 224×224 pixels, and fed into the classification network as in R-CNN [8]. The outputs of this network consist of two sibling fc layers for cls and reg, also in a per-class form. This R-CNN network is fine-tuned on the training set using a mini-batch size of 256 in the RoI-centric fashion. For testing, the RPN generates the highest scored 200 proposals for each predicted class, and the R-CNN network is used to update these proposals’ scores and box positions.","The image area is cut out from a proposed region, resized to 224x224 pixels, and input into the classification neural network as described in R-CNN [8]. The outputs of this network are two fully connected layers for classification and regression, also organized by class. This R-CNN network is tuned on the training data using mini-batches of 256 examples in a region of interest-focused way. For testing, the RPN produces the top 200 highest scoring proposals for each detected class, and the R-CNN network is utilized to refine these proposals' confidence scores and bounding box locations.","A section of the image is extracted from a proposed bounding box, scaled to 224x224 pixels, and fed into the classification model similar to R-CNN [8]. This model generates two sibling fully connected layers for predicting the class and bounding box, in a per-class layout. The R-CNN model is optimized on the training images with mini-batches of 256 centered on the region of interest. During testing, the RPN generates the 200 top-ranked proposals for every identified class, then the R-CNN model adjusts these proposals' probabilities and box coordinates.  ","An image patch is isolated from a proposed region, resized to 224x224, and input to the classification network as in R-CNN [8]. This network produces two parallel fully connected layers for classification and bounding box regression, organized by class. The R-CNN network is fine-tuned on the training images in batches of 256 focused on the region of interest. For testing, the RPN provides the 200 highest scoring proposals per detected class, then the R-CNN network refines these proposals' confidence estimates and bounding box locations.",A,1
Deep Residual Learning for Image Recognition,"This method reduces the top-5 localization error to 10.6% (Table 13). This is our single-model result on the validation set. Using an ensemble of networks for both classification and localization, we achieve a top-5 localization error of 9.0% on the test set. This number significantly outperforms the ILSVRC 14 results (Table 14), showing a 64% relative reduction of error. This result won the 1st place in the ImageNet localization task in ILSVRC 2015.","This technique decreases the top-5 localization mistake to 10.6% (Table 13). This is our single-model outcome on the validation set. Utilizing a group of networks for both categorization and localization, we accomplish a top-5 localization error of 9.0% on the test set. This number substantially surpasses the ILSVRC 14 results (Table 14), displaying a 64% relative decrease of error. This result was victorious in the 1st position in the ImageNet localization task in ILSVRC 2015.","This approach lowers the top-5 localization inaccuracy to 10.6% (Table 13). This is our single-model finding on the validation set. Employing an assembly of networks for both classification and localization, we achieve a top-5 localization mistake of 9.0% on the test set. This number significantly outdoes the ILSVRC 14 outcomes (Table 14), exhibiting a 64% relative reduction of error. This result won 1st place in the ImageNet localization challenge in ILSVRC 2015.  ","This technique reduces the top-5 localization error to 10.6% (Table 13). This is our single-model outcome on the validation set. Utilizing a collection of networks for both categorization and localization, we accomplish a top-5 localization mistake of 9.0% on the test set. This number substantially beats the ILSVRC 14 results (Table 14), displaying a 64% relative decrease of error. This result was first place in the ImageNet localization competition in ILSVRC 2015.",A,1
"DistilBERT, a distilled version of BERT","As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-theedge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller generalpurpose language representation model, called DistilBERT, which can then be finetuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40%, while retaining 97% of its language understanding capabilities and being 60% faster.","As the practice of Transfer Learning from large pre-trained models becomes more widespread in Natural Language Processing (NLP), running these massive models with limited computational resources for training or inference remains difficult. In this work, we introduce a technique to pre-train a smaller general-purpose language representation model, called DistilBERT, which can then be fine-tuned to perform well on a variety of tasks like its larger counterparts. While most prior work explored using distillation for building task-specific models, we leverage knowledge distillation during pre-training and show it's possible to decrease the size of a BERT model by 40%, while keeping 97% of its language understanding abilities and being 60% faster.","As Transfer Learning from huge pre-trained models is increasingly common in Natural Language Processing (NLP), operating these enormous models under constrained computational budgets for training or inference is still challenging. Here, we present a method to pre-train a smaller general-purpose language representation model, DistilBERT, which can then be fine-tuned to achieve good performance on many tasks like the larger models. Whereas most previous work studied distillation for building task-specific models, we use knowledge distillation during pre-training and show we can reduce a BERT model's size by 40%, while maintaining 97% of its language understanding capacity and being 60% faster.  ","As Transfer Learning from massive pre-trained models becomes more prevalent in Natural Language Processing (NLP), running these giant models within limited computational resources for training or inference remains difficult. In this work, we put forth a technique to pre-train a smaller general-purpose language representation model, DistilBERT, which can then be fine-tuned to perform well across a variety of tasks like its larger counterparts. Whereas most prior work investigated distillation for constructing task-specific models, we leverage knowledge distillation during pre-training and demonstrate we can decrease a BERT model's size by 40%, while retaining 97% of its language understanding abilities and being 60% faster.",A,1
"DistilBERT, a distilled version of BERT","While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40%, while retaining 97% of its language understanding capabilities and being 60% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study.","Though previous research examined using distillation for creating task-specific models, we apply knowledge distillation during pre-training and demonstrate that a BERT model can be reduced in size by 40% while keeping 97% of its language comprehension abilities and being 60% quicker. To take advantage of the inductive biases learned by larger models during pre-training, we present a triple loss joining together language modeling, distillation, and cosine-distance losses. Our smaller, faster, and lighter model costs less to pre-train and we exhibit its capabilities for on-device computations in a proof-of-concept test and a comparative on-device analysis.","Despite earlier work looking at utilizing distillation for constructing task-specific models, we leverage knowledge distillation during the pre-training phase and exhibit that it's feasible to decrease the size of a BERT model by 40%, while maintaining 97% of its language understanding capabilities and being 60% faster. To utilize the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation, and cosine-distance losses. Our smaller, quicker, and lighter model is less expensive to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study.  ","While previous studies focused on using distillation for developing task-specific models, we apply knowledge distillation during pre-training and show it's possible to shrink a BERT model by 40%, while keeping 97% of its natural language comprehension abilities and being 60% faster. To take advantage of the inductive biases learned by bigger models during pre-training, we present a triple loss fusing language modeling, distillation, and cosine-distance losses. Our smaller, faster, and lighter model costs less to pre-train and we display its capabilities for on-device computations in a proof-of-concept test and a comparative on-device analysis.",A,1
"DistilBERT, a distilled version of BERT","The last two years have seen the rise of Transfer Learning approaches in Natural Language Processing (NLP) with large-scale pre-trained language models becoming a basic tool in many NLP tasks [Devlin et al., 2018, Radford et al., 2019, Liu et al., 2019]. While these models lead to significant improvement, they often have several hundred million parameters and current research1 on pre-trained models indicates that training even larger models still leads to better performances on downstream tasks. The trend toward bigger models raises several concerns.","In the previous 24 months, there has been an increase in the use of Transfer Learning techniques in Natural Language Processing (NLP). Large pre-trained language models have become a fundamental instrument in numerous NLP tasks [Devlin et al., 2018, Radford et al., 2019, Liu et al., 2019]. Although these models result in considerable enhancement, they frequently contain hundreds of millions of parameters. Current research on pre-trained models shows that even larger models lead to superior performance on downstream assignments. The tendency to create bigger models raises multiple issues.","Over the past two years, Transfer Learning methods have become more popular in Natural Language Processing (NLP). Massive pre-trained language models are now a standard tool for many NLP tasks [Devlin et al., 2018, Radford et al., 2019, Liu et al., 2019]. While these models significantly improve performance, they often have hundreds of millions of parameters. Existing research on pre-trained models indicates that developing even larger models continues to improve performance on downstream tasks. The trend toward larger models raises several concerns.","In the previous 24 months, there has been growing usage of Transfer Learning techniques in Natural Language Processing (NLP). Gigantic pre-trained language models have turned into a fundamental tool in numerous NLP tasks [Devlin et al., 2018, Radford et al., 2019, Liu et al., 2019]. Despite the fact that these models result in major enhancement, they frequently contain hundreds of millions of parameters. Current research on pre-trained models demonstrates that bigger models still lead to superior performance on downstream assignments. The tendency toward larger models brings up several concerns.",A,1
"DistilBERT, a distilled version of BERT","In this paper, we show that it is possible to reach similar performances on many downstream-tasks using much smaller language models pre-trained with knowledge distillation, resulting in models that are lighter and faster at inference time, while also requiring a smaller computational training budget. Our general-purpose pre-trained models can be fine-tuned with good performances on several downstream tasks, keeping the flexibility of larger models. We also show that our compressed models are small enough to run on the edge, e.g. on mobile devices.","This article demonstrates that comparable capabilities on various downstream activities can be attained utilizing significantly smaller language models that are pre-trained with knowledge distillation. This produces models that are lighter and quicker during inference, while also necessitating less computational training resources. Our universally applicable pre-trained models can be fine-tuned to perform well on multiple downstream tasks, retaining the adaptability of larger models. We also exhibit that our compressed models are sufficiently small to operate on edge devices like mobile phones.","In this document, we establish that it's feasible to achieve analogous results on many subsequent tasks by using much more compact language models that are pre-trained using knowledge distillation. This gives models that are more lightweight and faster when making inferences, while also needing less computing power for training. Our widely useful pre-trained models can be fine-tuned to perform well on several downstream tasks, maintaining the flexibility of bigger models. We also demonstrate that our condensed models are small enough to run locally, for example on mobile gadgets.","This article shows that comparable performance on various downstream activities can be reached using significantly smaller language models that are pre-trained with knowledge distillation. This leads to models that are more lightweight and faster during inference, while also requiring less computational budget for training. Our generally applicable pre-trained models can be fine-tuned to achieve good results on multiple downstream tasks, retaining the adaptability of larger models. We also exhibit that our compressed models are small enough to operate locally, for instance on mobile platforms.",A,1
"DistilBERT, a distilled version of BERT","Using a triple loss, we show that a 40% smaller Transformer (Vaswani et al. [2017]) pre-trained through distillation via the supervision of a bigger Transformer language model can achieve similar performance on a variety of downstream tasks, while being 60% faster at inference time. Further ablation studies indicate that all the components of the triple loss are important for best performances. We have made the trained weights available along with the training code in the Transformers2 library from HuggingFace [Wolf et al., 2019].","By applying a three-part loss function, we demonstrate that a 40% smaller Transformer model (Vaswani et al. [2017]) that is pre-trained by mimicking a larger Transformer language model can attain comparable performance on various downstream tasks, while being 60% quicker during inference. Additional ablation studies show that all parts of the three-part loss are important for optimal performance. We have published the trained model parameters together with the training code in the Transformers2 library from HuggingFace [Wolf et al., 2019].","Using a loss function with three components, we exhibit that a 40% smaller Transformer (Vaswani et al. [2017]) pre-trained through imitation of a larger Transformer language model can reach similar results on many different downstream tasks, while being 60% faster during inference. More ablation experiments indicate that every piece of the three-part loss is vital for the best outcomes. We have released the trained weights and the training code in the Transformers2 library by HuggingFace [Wolf et al., 2019].","Applying a triple loss function, we prove a 40% smaller Transformer (Vaswani et al. [2017]) pre-trained by modeling a bigger Transformer language model can achieve comparable performance across various downstream tasks, while being 60% quicker at inference time. Additional ablation analyses show all pieces of the triple loss are critical for optimum performance. We have published the trained parameters and training code in HuggingFace's [Wolf et al., 2019] Transformers2 library.",A,1
"DistilBERT, a distilled version of BERT","Knowledge distillation [Bucila et al., 2006, Hinton et al., 2015] is a compression technique in which a compact model - the student - is trained to reproduce the behaviour of a larger model - the teacher - or an ensemble of models. In supervised learning, a classification model is generally trained to predict an instance class by maximizing the estimated probability of gold labels. A standard training objective thus involves minimizing the cross-entropy between the model’s predicted distribution and the one-hot empirical distribution of training labels. A model performing well on the training set will predict an output distribution with high probability on the correct class and with near-zero probabilities on other classes.","Knowledge distillation [Bucila et al., 2006, Hinton et al., 2015] is a compression method where a small model - the student - is educated to imitate the actions of a bigger model - the teacher - or a group of models. In supervised learning, a classification model is usually trained to guess an instance class by increasing the estimated chance of gold labels. A normal training goal thus requires decreasing the cross-entropy between the model's predicted distribution and the one-hot factual distribution of training labels. A model doing well on the training set will anticipate an output distribution with high probability on the correct class and with near-zero probabilities on other classes.","Knowledge distillation [Bucila et al., 2006, Hinton et al., 2015] is a compression technique where a compact model - the student - is taught to mimic the performance of a larger model - the teacher - or a collection of models. In supervised learning, a classification model is generally educated to predict an instance class by maximizing the calculated probability of gold labels. A standard training objective thus involves minimizing the divergence between the model's predicted distribution and the one-hot empirical distribution of training labels. A successful model on the training set will forecast an output distribution with high probability on the correct class and with near-zero probabilities on other classes.  ","Knowledge distillation [Bucila et al., 2006, Hinton et al., 2015] is a compression approach in which a small model - the student - is trained to reproduce the actions of a bigger model - the teacher - or a group of models. In supervised learning, a classification model is typically taught to estimate an instance class by increasing the computed likelihood of gold labels. A conventional training goal thus requires reducing the cross-entropy between the model's anticipated distribution and the one-hot actual distribution of training labels. A proficient model on the training set will predict an output distribution with high probability on the accurate class and with near-zero probabilities on other classes.",A,1
"DistilBERT, a distilled version of BERT","In the present work, the student - DistilBERT - has the same general architecture as BERT. The token-type embeddings and the pooler are removed while the number of layers is reduced by a factor of 2. Most of the operations used in the Transformer architecture (linear layer and layer normalisation) are highly optimized in modern linear algebra frameworks and our investigations showed that variations on the last dimension of the tensor (hidden size dimension) have a smaller impact on computation efficiency (for a fixed parameters budget) than variations on other factors like the number of layers. Thus we focus on reducing the number of layers.","This paper examines DistilBERT, which has a similar overall design to BERT. DistilBERT removes the token type embeddings and pooler, and halves the number of layers compared to BERT. The linear layers and layer normalization used in Transformer models are highly optimized in modern linear algebra software. Our tests showed that changing the last dimension (hidden size) has less impact on efficiency (for fixed parameters) than other factors like layer count. So we concentrated on reducing layers.","In this work, DistilBERT has the same general framework as BERT. DistilBERT takes out the token-type embeddings and pooler, and reduces the number of layers by half compared to BERT. Most of the operations in Transformer models (linear layers and layer normalization) are highly optimized in current linear algebra tools. Our experiments demonstrated that varying the final dimension (hidden dimension) has a smaller effect on computational efficiency (for a fixed parameter budget) than changing other factors such as number of layers. Therefore, we focus on decreasing the number of layers.","The student model examined here, DistilBERT, shares the overall design of BERT. DistilBERT removes the token-type embeddings and pooler present in BERT, and halves the number of layers. The linear layers and layer normalization central to Transformer models are highly optimized in modern linear algebra software. Our tests showed varying the last dimension (hidden size) impacts efficiency (for fixed parameters) less than other factors like layer count. So we prioritized reducing the number of layers.",A,1
"DistilBERT, a distilled version of BERT","We applied best practices for training BERT model recently proposed in Liu et al. [2019]. As such, DistilBERT is distilled on very large batches leveraging gradient accumulation (up to 4K examples per batch) using dynamic masking and without the next sentence prediction objective. Data and compute power We train DistilBERT on the same corpus as the original BERT model: a concatenation of English Wikipedia and Toronto Book Corpus [Zhu et al., 2015]. DistilBERT was trained on 8 16GB V100 GPUs for approximately 90 hours. For the sake of comparison, the RoBERTa model [Liu et al., 2019] required 1 day of training on 1024 32GB V100.","We implemented the optimal procedures for teaching BERT that were recently described in Liu et al. [2019]. As a result, DistilBERT is simplified on very large sets using accumulated slope (up to 4K samples per set) employing dynamic covering and excluding the next sentence forecast. Information and calculation capability We educate DistilBERT on the same collection of texts as the original BERT model: a blend of English Wikipedia and Toronto Book Corpus [Zhu et al., 2015]. DistilBERT was taught on 8 16GB V100 GPUs for about 90 hours. For comparison, the RoBERTa model [Liu et al., 2019] needed 1 day of teaching on 1024 32GB V100.","We put into practice the best methodologies for developing BERT recently documented in Liu et al. [2019]. Therefore, DistilBERT is condensed on very sizable batches applying gradient buildup (up to 4K instances per batch) utilizing dynamic obscuring and without the next sentence prediction goal. Data and processing power We develop DistilBERT on the same library as the original BERT model: a fusion of English Wikipedia and Toronto Book Corpus [Zhu et al., 2015]. DistilBERT was developed on 8 16GB V100 GPUs for about 90 hours. For reference, the RoBERTa model [Liu et al., 2019] called for 1 day of development on 1024 32GB V100.","We implemented the optimal procedures for training BERT recently outlined in Liu et al. [2019]. Consequently, DistilBERT is distilled on very large sets using accumulated gradient (up to 4K samples per set) applying dynamic masking and excluding the next sentence prediction aim. Information and computing capacity We educate DistilBERT on the same collection as the original BERT model: a combination of English Wikipedia and Toronto Book Corpus [Zhu et al., 2015]. DistilBERT was educated on 8 16GB V100 GPUs for about 90 hours. For comparison, the RoBERTa model [Liu et al., 2019] necessitated 1 day of education on 1024 32GB V100.",A,1
"DistilBERT, a distilled version of BERT","We assess the language understanding and generalization capabilities of DistilBERT on the General Language Understanding Evaluation (GLUE) benchmark [Wang et al., 2018], a collection of 9 datasets for evaluating natural language understanding systems. We report scores on the development sets for each task by fine-tuning DistilBERT without the use of ensembling or multi-tasking scheme for fine-tuning (which are mostly orthogonal to the present work). We compare the results to the baseline provided by the authors of GLUE: an ELMo (Peters et al. [2018]) encoder followed by two BiLSTMs.","We evaluate the natural language comprehension and generalizability of DistilBERT on the General Language Understanding Evaluation (GLUE) benchmark [Wang et al., 2018], which contains 9 different datasets for testing natural language processing systems. We present results on the development sets for each task by tuning DistilBERT individually on each task without using ensembling or multi-task tuning schemes (which are largely separate from our current work). We then compare these results to the baseline provided by the authors of GLUE: an ELMo (Peters et al. [2018]) encoder followed by two bidirectional LSTM layers.","We measure the language understanding and extrapolation abilities of DistilBERT on the General Language Understanding Evaluation (GLUE) benchmark [Wang et al., 2018], a collection of 9 datasets for assessing natural language processing systems. We show scores on the development sets for each task by adapting DistilBERT on each task separately without utilizing ensembling or multi-task adaptation techniques (which are mostly independent of our current work). We then contrast these scores to the baseline provided by the creators of GLUE: an ELMo (Peters et al. [2018]) encoder followed by two bidirectional LSTM layers.","We test the language comprehension and generalization capabilities of DistilBERT on the General Language Understanding Evaluation (GLUE) benchmark [Wang et al., 2018], a set of 9 datasets for evaluating natural language systems. We present results on the development sets for each task by fine-tuning DistilBERT on each task in isolation without using ensembling or multi-task fine-tuning schemes (which are largely separate from our present work). We then compare these results against the baseline provided by the authors of GLUE: an ELMo (Peters et al. [2018]) encoder followed by two BiLSTM layers.",A,1
"DistilBERT, a distilled version of BERT","The results on each of the 9 tasks are showed on Table 1 along with the macro-score (average of individual scores). Among the 9 tasks, DistilBERT is always on par or improving over the ELMo baseline (up to 19 points of accuracy on STS-B). DistilBERT also compares surprisingly well to BERT, retaining 97% of the performance with 40% fewer parameters. 4.1 Downstream task benchmark Downstream tasks We further study the performances of DistilBERT on several downstream tasks under efficient inference constraints: a classification task (IMDb sentiment classification - Maas et al. [2011]) and a question answering task (SQuAD v1.1 - Rajpurkar et al. [2016]).","The outcomes on all 9 jobs are displayed in Table 1 along with the macro-score (mean of separate totals). Of the 9 jobs, DistilBERT is always equal to or better than the ELMo baseline (up to 19 accuracy points higher on STS-B). DistilBERT also holds up surprisingly well compared to BERT, keeping 97% of the performance with 40% less parameters. 4.1 Downstream task analysis Downstream tasks We additionally examine DistilBERT's performance on several downstream tasks under efficient inference limits: a classification job (IMDb sentiment classification - Maas et al. [2011]) and a question answering job (SQuAD v1.1 - Rajpurkar et al. [2016]).","The figures for each of the 9 assignments are shown in Table 1 with the macro-score (average of individual marks). Among the 9 assignments, DistilBERT is always on par with or outperforming the ELMo baseline (up to 19 percentage points higher accuracy on STS-B). DistilBERT also compares astonishingly well to BERT, maintaining 97% of the performance with 40% fewer parameters. 4.1 Downstream task evaluation Downstream tasks We further investigate the performance of DistilBERT on several downstream tasks under efficient inference restrictions: a categorization assignment (IMDb sentiment categorization - Maas et al. [2011]) and a question answering assignment (SQuAD v1.1 - Rajpurkar et al. [2016]).","The data for every one of the 9 tasks are displayed in Table 1 along with the macro-score (mean of separate grades). Of the 9 tasks, DistilBERT is always equal to or surpassing the ELMo baseline (up to 19 points higher precision on STS-B). DistilBERT also measures up shockingly well compared to BERT, keeping 97% of the capability with 40% less parameters. 4.1 Downstream task analysis Downstream tasks We additionally study DistilBERT's capability on several downstream tasks under efficient inference limits: a classification task (IMDb sentiment classification - Maas et al. [2011]) and a question answering task (SQuAD v1.1 - Rajpurkar et al. [2016]).",A,1
"DistilBERT, a distilled version of BERT","As shown in Table 2, DistilBERT is only 0.6% point behind BERT in test accuracy on the IMDb benchmark while being 40% smaller. On SQuAD, DistilBERT is within 3.9 points of the full BERT. We also studied whether we could add another step of distillation during the adaptation phase by fine-tuning DistilBERT on SQuAD using a BERT model previously fine-tuned on SQuAD as a 4We use jiant [Wang et al., 2019] to compute the baseline.","The results in Table 2 demonstrate that DistilBERT achieves test accuracy on the IMDb benchmark that is only 0.6 percentage points lower than BERT, even though DistilBERT is 40% smaller in size. On the SQuAD dataset, DistilBERT's performance is within 3.9 points of the full BERT model. We also investigated if an additional distillation step during fine-tuning could improve DistilBERT's performance on SQuAD by using a BERT model previously fine-tuned on SQuAD as the teacher.","As shown by the findings in Table 2, DistilBERT comes within 0.6 percent of BERT's test accuracy on the IMDb benchmark while having 40% fewer parameters. On the SQuAD task, DistilBERT is within 3.9 points of the complete BERT model. We also explored whether adding another round of distillation while adapting DistilBERT on SQuAD, by fine-tuning it with a BERT model previously fine-tuned on SQuAD as the teacher, could further improve performance. ","The results presented in Table 2 indicate that DistilBERT achieves test accuracy on IMDb that is only 0.6 percentage points lower than BERT, even though DistilBERT's size is 40% smaller. On SQuAD, DistilBERT's performance is within 3.9 points of the full BERT model. We also tested whether an extra distillation step during fine-tuning on SQuAD, by training DistilBERT using a BERT model previously fine-tuned on SQuAD as the teacher, could further improve DistilBERT's performance.",A,1
"DistilBERT, a distilled version of BERT","To further investigate the speed-up/size trade-off of DistilBERT, we compare (in Table 3) the number of parameters of each model along with the inference time needed to do a full pass on the STSB development set on CPU (Intel Xeon E5-2690 v3 Haswell @2.9GHz) using a batch size of 1. DistilBERT has 40% fewer parameters than BERT and is 60% faster than BERT.","To further examine the relationship between speed and model size for DistilBERT, we contrast in Table 3 the quantity of parameters for each model and the time required on CPU (Intel Xeon E5-2690 v3 Haswell @2.9GHz) to run inference on the entire STSB development set using a batch size of 1. DistilBERT has 40% less parameters than BERT and runs 60% faster than BERT.","To additionally investigate the speed/size compromise of DistilBERT, we compare in Table 3 the number of weights of each model along with the inference time necessary to fully process the STSB development set on CPU (Intel Xeon E5-2690 v3 Haswell @2.9GHz) utilizing a batch size of 1. DistilBERT possesses 40% fewer weights than BERT and executes 60% quicker than BERT. ","To further analyze the speed versus size tradeoff of DistilBERT, we juxtapose in Table 3 the parameter tally for each model plus the inference duration essential to complete a full cycle on the STSB development set on CPU (Intel Xeon E5-2690 v3 Haswell @2.9GHz) applying a batch dimension of 1. DistilBERT contains 40% less parameters than BERT and operates 60% faster than BERT.",A,1
"DistilBERT, a distilled version of BERT","We studied whether DistilBERT could be used for on-the-edge applications by building a mobile application for question answering. We compare the average inference time on a recent smartphone (iPhone 7 Plus) against our previously trained question answering model based on BERT-base. Excluding the tokenization step, DistilBERT is 71% faster than BERT, and the whole model weighs 207 MB (which could be further reduced with quantization).","We investigated if DistilBERT was suitable for mobile apps by creating a question answering app for smartphones. We looked at the typical inference time on a current phone (iPhone 7 Plus) versus our past QA model using BERT-base. Leaving out tokenization, DistilBERT was 71% quicker than BERT, and the full model was 207 MB (which could be further reduced through quantization).","We studied whether DistilBERT could be utilized for apps on phones by making a question answering mobile app. We compared the mean inference speed on a new smartphone (iPhone 7 Plus) compared to our earlier trained QA model using BERT-base. Not including tokenization, DistilBERT was 71% faster than BERT, and the whole model was 207 MB (which could be additionally decreased with quantization).  ","We examined if DistilBERT was suitable for on-device apps by building a question answering mobile application. We benchmarked the average inference latency on a current smartphone (iPhone 7 Plus) against our previous QA model built on BERT-base. Omitting tokenization, DistilBERT was 71% faster than BERT, and the full model weighed 207 MB (which could be further shrunk with quantization).",A,1
"DistilBERT, a distilled version of BERT","Ablation study In this section, we investigate the influence of various components of the triple loss and the student initialization on the performances of the distilled model. We report the macro-score on GLUE. Table 4 presents the deltas with the full triple loss: removing the Masked Language Modeling loss has little impact while the two distillation losses account for a large portion of the performance.","Examination of model components Here, we look at how the different parts of the triple loss function and the initial student model settings impact the performance of the distilled model. We show the macro-score changes on GLUE. Table 4 displays the score differences compared to using the complete triple loss: taking away the Masked Language Modeling loss has minimal effect while the two distillation losses are responsible for a large improvement in performance.","Analyzing model design In this section, we study how the various parts of the triple loss and how the student model is initialized affect the distilled model's capabilities. We present the macro-score on GLUE. Table 4 gives the score changes relative to having the full triple loss: removing the Masked Language Modeling loss does not change much while the two distillation losses account for a large portion of the gains in performance. ","Model component investigation Here, we look at how different pieces of the triple loss function and how the student model is first set up impact the distilled model's performance. We show the macro-score on GLUE. Table 4 has the score differences compared to the complete triple loss: taking out the Masked Language Modeling loss has little effect while the two distillation losses are responsible for much of the improvement in performance.",A,1
"DistilBERT, a distilled version of BERT","Most of the prior works focus on building task-specific distillation setups. Tang et al. [2019] transfer fine-tune classification model BERT to an LSTM-based classifier. Chatterjee [2019] distill BERT model fine-tuned on SQuAD in a smaller Transformer model previously initialized from BERT. In the present work, we found it beneficial to use a general-purpose pre-training distillation rather than a task-specific distillation. Turc et al. [2019] use the original pretraining objective to train smaller student, then fine-tuned via distillation.","The majority of previous research concentrates on constructing task-specific teacher-student frameworks for knowledge distillation. Tang and colleagues [2019] transfer a fine-tuned BERT classification model into an LSTM-based classifier. Chatterjee [2019] distills a BERT model fine-tuned on SQuAD into a smaller Transformer initialized from BERT. In this work, we determined that utilizing general-purpose pre-training distillation is more beneficial than task-specific distillation. Turc et al. [2019] employ the original pretraining goal to train a smaller student, then fine-tune it via distillation.","Most prior work focuses on developing specialized distillation methods for specific tasks. Tang et al. [2019] transfer a fine-tuned BERT classifier to an LSTM classifier. Chatterjee [2019] compresses a BERT model fine-tuned on SQuAD into a smaller Transformer pretrained on BERT. Here, we found it useful to leverage general pre-training distillation rather than task-specific distillation. Turc et al. [2019] pretrain a smaller student model using the original pretraining objective, then fine-tune it via distillation.","The majority of previous studies concentrate on constructing task-dependent teacher-student setups for distillation. Tang and coauthors [2019] transfer a fine-tuned classification BERT model to an LSTM classifier. Chatterjee [2019] condenses a BERT model fine-tuned on SQuAD into a smaller Transformer initialized from BERT. In this paper, we determined that employing general pre-training distillation is more effective than task-specific distillation. Turc et al. [2019] use the original pretraining goal to train a smaller student model, then fine-tune it through distillation.",A,1
"DistilBERT, a distilled version of BERT","As shown in the ablation study, we found it beneficial to leverage the teacher’s knowledge to pre-train with additional distillation signal. Multi-distillation Yang et al. [2019] combine the knowledge of an ensemble of teachers using multi-task learning to regularize the distillation. The authors apply Multi-Task Knowledge Distillation to learn a compact question answering model from a set of large question answering models. An application of multi-distillation is multi-linguality: Tsai et al. [2019] adopts a similar approach to us by pre-training a multilingual model from scratch solely through distillation.",The ablation analysis demonstrated that utilizing the teacher's expertise to pre-train with extra distillation cues was advantageous. Yang et al. [2019] combined the insights of an ensemble of instructors through multi-task learning to regulate distillation. They employed Multi-Task Knowledge Distillation to develop a compact question answering system from a collection of large question answering models. One use of multi-distillation is multi-linguality: Tsai et al. [2019] took a similar approach to us by pre-training a multilingual model from the ground up solely via distillation.,"As evidenced in the ablation research, harnessing the teacher's knowledge to pre-train with supplementary distillation signals was found to be beneficial. Yang et al. [2019] amalgamated the wisdom of a group of teachers through multi-task learning to control distillation. They utilized Multi-Task Knowledge Distillation to construct a compact question answering model from multiple large question answering models. One application of multi-distillation is multi-lingual: Tsai et al. [2019] adopted a similar tactic to us by pre-training a multilingual model from baseline only through distillation.  ",The ablation study showed that using the teacher's expertise to pre-train with extra distillation clues was advantageous. Yang et al. [2019] combined the insights of a collection of teachers via multi-task learning to regulate distillation. They used Multi-Task Knowledge Distillation to develop a compact question answering system from several large question answering models. One use of multi-distillation is multi-lingual: Tsai et al. [2019] took a similar approach to us by pre-training a multilingual model from the ground up only through distillation.,A,1
"DistilBERT, a distilled version of BERT","However, as shown in the ablation study, leveraging the teacher’s knowledge with initialization and additional losses leads to substantial gains. Other compression techniques have been studied to compress large models. Recent developments in weights pruning reveal that it is possible to remove some heads in the self-attention at test time without significantly degrading the performance Michel et al. [2019]. Some layers can be reduced to one head.","Nevertheless, the ablation analysis demonstrated that taking advantage of the teacher's expertise through initialization and supplementary losses results in considerable improvements. Other methods of compression have been explored to shrink large models. Recent advancements in weights pruning indicate that removing certain heads in the self-attention during testing does not notably worsen performance, according to Michel et al. [2019]. Some layers can be decreased to a single head.","However, as exhibited in the ablation research, leveraging the teacher's knowledge through initialization and extra losses leads to major gains. Additional compression techniques have been analyzed to compress substantial models. Current developments in weights pruning show that it is feasible to eliminate some heads in the self-attention when testing without significantly diminishing the performance, as per Michel et al. [2019]. Certain layers can be reduced to one head. ","Though, as evidenced in the ablation study, taking advantage of the teacher's expertise via initialization and supplementary losses yields significant improvements. Other compression methods have been investigated to shrink large models. The latest advancements in weights pruning indicate that removing select heads in the self-attention during evaluation does not markedly degrade performance, as stated by Michel et al. [2019]. Specific layers can be decreased to a single head.",A,1
"DistilBERT, a distilled version of BERT","We introduced DistilBERT, a general-purpose pre-trained version of BERT, 40% smaller, 60% faster, that retains 97% of the language understanding capabilities. We showed that a general-purpose language model can be successfully trained with distillation and analyzed the various components with an ablation study. We further demonstrated that DistilBERT is a compelling option for edge applications.","We presented DistilBERT, a widely useful pre-trained adaptation of BERT, 40% more compact, 60% quicker, that keeps 97% of the language comprehension abilities. We displayed that a widely useful language model can be fruitfully trained with distillation and investigated the different parts with an ablation analysis. We additionally showed that DistilBERT is an appealing choice for edge applications.","We brought in DistilBERT, a generally useful pre-conditioned form of BERT, 40% smaller sized, 60% faster, that holds on to 97% of the language understanding capabilities. We evidenced that a generally useful language model can be successfully coached with distillation and examined the different pieces with an ablation review. We further illustrated that DistilBERT is a convincing selection for edge uses. ","We introduced DistilBERT, a commonly applicable pre-trained iteration of BERT, 40% more diminutive, 60% swifter, that retains 97% of the language comprehension skills. We exhibited that a commonly applicable language model can be fruitfully disciplined with distillation and analyzed the various constituents with an ablation survey. We additionally demonstrated that DistilBERT is an appealing preference for edge employments.",A,1
Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,"Transfer learning, where a model is first pre-trained on a data-rich task before being finetuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. ","The technique of first teaching a model on a large dataset before customizing it for a specific task, known as transfer learning, has become very effective in natural language processing (NLP). The success of transfer learning has led to many different techniques, methods, and ways of applying it. In this paper, we take a broad look at transfer learning techniques for NLP by proposing a single framework that turns all language tasks into a text-to-text format. Our thorough study compares different pre-training goals, model architectures, unlabeled datasets, transfer methods, and other factors across many language understanding tasks.","Transfer learning, pre-training a model on abundant data then fine-tuning for a particular task, has proven very powerful in natural language processing (NLP). This has spawned diverse transfer learning approaches, practices, and methodology. Here, we survey the landscape of transfer learning techniques for NLP by introducing a unified text-to-text framework that reformulates all language problems. Our systematic analysis compares pre-training objectives, model architectures, unlabeled corpora, transfer techniques, and other elements across dozens of language understanding tasks.  ","The technique of first pre-training models on data-rich tasks before fine-tuning them on downstream tasks, known as transfer learning, has become a very effective approach in natural language processing (NLP). The success of transfer learning has led to a proliferation of techniques, methods, and practices. In this paper, we explore the landscape of transfer learning techniques for NLP by proposing a single text-to-text framework that converts all language tasks into the same format. Our comprehensive study compares pre-training goals, model architectures, unlabeled data, transfer approaches, and other factors across many language understanding tasks.",A,1
Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,"By combining the insights from our exploration with scale and our new “Colossal Clean Crawled Corpus”, we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.1 Keywords: transfer learning, natural language processing, multi-task learning, attentionbased models, deep learning 1. Training a machine learning model to perform natural language processing (NLP) tasks often requires that the model can process text in a way that is amenable to downstream learning. ","Through integrating the knowledge gained from probing at scale along with our new ""Enormous Sanitized Web-Scraped Collection"", we attain unmatched performance across many evaluations covering summarization, QA, text classification, and more. To assist future work on transfer learning for NLP, we publish our information, pre-trained models, and code. Keywords: transfer learning, natural language processing, multi-task learning, attention-based models, deep learning","By bringing together the insights from exploring on a large scale and our new ""Gigantic Clean Web-Crawled Dataset"", we reach best-in-class results on numerous benchmarks involving summarization, question answering, text classification, and more. To help future research on transfer learning for NLP, we make our dataset, pre-trained models, and code available. Keywords: transfer learning, natural language processing, multi-task learning, attention-based models, deep learning  ","Through combining the understanding gained from investigation at scale along with our new ""Massive Sanitized Web-Extracted Collection"", we achieve unmatched performance across many evaluations involving summarization, QA, text classification, and more. To facilitate future studies on transfer learning for NLP, we provide our data, pre-trained models, and code. Keywords: transfer learning, natural language processing, multi-task learning, attention-based models, deep learning",A,1
Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,"This can be loosely viewed as developing general-purpose knowledge that allows the model to “understand” text. Attribution requirements are provided at http://jmlr.org/papers/v21/20-074.html. In modern machine learning practice, providing this knowledge is rarely done explicitly; instead, it is often learned as part of an auxiliary task. For example, a historically common approach is to use word vectors (Mikolov et al., 2013b,a; Pennington et al., 2014) to map word identities to a continuous representation where, ideally, similar words map to similar vectors. ","This can be seen as building general knowledge that lets the model ""comprehend"" text. Credit info is at http://jmlr.org/papers/v21/20-074.html. Nowadays in machine learning, this knowledge is rarely given explicitly; rather, it's frequently learned as part of a secondary task. For instance, a common historical method is to use word vectors (Mikolov et al., 2013b,a; Pennington et al., 2014) to map words to a continuous representation where, optimally, analogous words map to analogous vectors.","This can be viewed as developing broad understanding that enables the model to ""grasp"" text. Attribution requirements are available at http://jmlr.org/papers/v21/20-074.html. In modern machine learning, providing this understanding is rarely done directly; instead, it is often learned as part of a supplementary task. As an example, a standard past approach is to use word embeddings (Mikolov et al., 2013b,a; Pennington et al., 2014) to map word identities to a continuous representation where, ideally, similar words map to similar embeddings.","This can be seen as constructing general knowledge that allows the model to ""comprehend"" text. Credit information is provided at http://jmlr.org/papers/v21/20-074.html. In current machine learning, giving this knowledge directly is rare; rather, it is often learned as part of an additional task. For instance, a common old method is using word vectors (Mikolov et al., 2013b,a; Pennington et al., 2014) to map words to a continuous representation where, optimally, equivalent words map to equivalent vectors.",A,1
Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,"These vectors are often learned through an objective that, for example, encourages co-occurring words to be positioned nearby in the continuous space (Mikolov et al., 2013b). Recently, it has become increasingly common to pre-train the entire model on a data-rich task. Ideally, this pre-training causes the model to develop general-purpose abilities and knowledge that can then be transferred to downstream tasks. In applications of transfer learning to computer vision (Oquab et al., 2014; Jia et al., 2014; Huh et al., 2016; Yosinski et al., 2014), pre-training is typically done via supervised learning on a large labeled data set like ImageNet (Russakovsky et al., 2015; Deng et al., 2009). ","These vectors are frequently obtained by optimizing a goal that, for instance, motivates words that often appear together to be situated in close proximity in the continuous space (Mikolov et al., 2013b). In recent times, it has grown more and more prevalent to pretrain the whole model on a data-abundant task. Optimally, this pretraining enables the model to build general capabilities and knowledge that can then be utilized for downstream tasks. In applications of transfer learning to computer vision (Oquab et al., 2014; Jia et al., 2014; Huh et al., 2016; Yosinski et al., 2014), pretraining is usually accomplished via supervised learning on a large labeled dataset like ImageNet (Russakovsky et al., 2015; Deng et al., 2009).","These vectors are regularly derived using an objective function that, for instance, promotes words that co-occur to be mapped near each other in the continuous space (Mikolov et al., 2013b). Recently, pre-training the entire model on a data-rich task has become more and more commonplace. Ideally, this pre-training allows the model to develop general-purpose capabilities and knowledge that can then be applied to downstream tasks. In applications of transfer learning to computer vision (Oquab et al., 2014; Jia et al., 2014; Huh et al., 2016; Yosinski et al., 2014), pre-training is typically implemented via supervised learning on a large labeled dataset like ImageNet (Russakovsky et al., 2015; Deng et al., 2009).  ","These vectors are often produced by an objective function that, for example, encourages words that appear together to be positioned close by in the continuous space (Mikolov et al., 2013b). In recent times, pre-training the whole model on a data-abundant task has become increasingly prevalent. Optimally, this pre-training enables the model to develop general abilities and knowledge that can then be transferred to downstream tasks. In applications of transfer learning to computer vision (Oquab et al., 2014; Jia et al., 2014; Huh et al., 2016; Yosinski et al., 2014), pre-training is usually done via supervised learning on a large labeled data set like ImageNet (Russakovsky et al., 2015; Deng et al., 2009).",A,1
Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,"In contrast, modern techniques for transfer learning in NLP often pre-train using unsupervised learning on unlabeled data. This approach has recently been used to obtain state-of-the-art results in many of the most common NLP benchmarks (Devlin et al., 2018; Yang et al., 2019; Dong et al., 2019; Liu et al., 2019c; Lan et al., 2019). Beyond its empirical strength, unsupervised pre-training for NLP is particularly attractive because unlabeled text data is available en masse thanks to the Internet—for example, the Common Crawl project2 produces about 20TB of text data extracted from web pages each month. ","Conversely, current methods for transfer learning in natural language processing frequently pretrain using unsupervised learning on data without labels. This strategy has recently been utilized to achieve state-of-the-art performance on many of the most common NLP benchmarks (Devlin et al., 2018; Yang et al., 2019; Dong et al., 2019; Liu et al., 2019c; Lan et al., 2019). In addition to its empirical effectiveness, unsupervised pretraining for NLP is especially appealing because unlabeled text data is abundantly available on the internet - for instance, the Common Crawl project produces around 20TB of text extracted from webpages every month.","In contrast, modern techniques for transfer learning in NLP regularly pre-train using unsupervised learning on unlabeled information. This approach has recently been leveraged to obtain best-in-class results on many of the most common NLP benchmarks (Devlin et al., 2018; Yang et al., 2019; Dong et al., 2019; Liu et al., 2019c; Lan et al., 2019). Beyond its empirical strength, unsupervised pre-training for NLP is particularly attractive because unlabeled text information is widely available thanks to the Internet - for example, the Common Crawl project generates about 20TB of text extracted from webpages each month.","Conversely, current methods for transfer learning in natural language processing often pre-train using unsupervised learning on data without labels. This strategy has recently been used to achieve state-of-the-art performance on many of the most common NLP benchmarks (Devlin et al., 2018; Yang et al., 2019; Dong et al., 2019; Liu et al., 2019c; Lan et al., 2019). In addition to its empirical effectiveness, unsupervised pretraining for NLP is especially appealing because unlabeled text data is plentifully available on the internet – for instance, the Common Crawl project produces around 20TB of text taken from webpages every month.",A,1
Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,"This is a natural fit for neural networks, which have been shown to exhibit remarkable scalability, i.e. it is often possible to achieve better performance simply by training a larger model on a larger data set (Hestness et al., 2017; Shazeer et al., 2017; Jozefowicz et al., 2016; Mahajan et al., 2018; Radford et al., 2019; Shazeer et al., 2018; Huang et al., 2018b; Keskar et al., 2019a). This synergy has resulted in a great deal of recent work developing transfer learning methodology for NLP, which has produced a wide landscape of pre-training objectives (Howard and Ruder, 2018; Devlin et al., 2018; Yang et al., 2019; Dong et al., 2019), unlabeled data sets (Yang et al., 2019; Liu et al., 2019c; Zellers et al., 2019), benchmarks (Wang et al., 2019b, 2018; Conneau and Kiela, 2018), fine-tuning methods (Howard and Ruder, 2018; Houlsby et al., 2019; Peters et al., 2019), and more. ","This is a natural alignment for neural networks, which have demonstrated remarkable expandability, meaning it is frequently possible to attain superior performance just by educating a larger model on a more substantial data set. This synergy has resulted in much recent work developing transfer learning approaches for NLP, which has generated a broad landscape of pre-training goals, unlabeled data sets, benchmarks, fine-tuning methods, and more.","This is an organic match for neural networks, which have shown extraordinary scalability, that is, it is often feasible to achieve enhanced performance by just training a bigger model on a larger dataset. This synergy has led to abundant recent work developing transfer learning techniques for NLP, producing a wide landscape of pre-training objectives, unlabeled data collections, benchmarks, fine-tuning procedures, and more.  ","This is a natural fit for neural networks, which have exhibited notable scalability, specifically, it is frequently possible to obtain superior performance by just educating a larger model on a bigger dataset. This synergy has resulted in substantial recent work developing transfer learning approaches for NLP, generating a wide landscape of pre-training aims, unlabeled data assemblages, benchmarks, fine-tuning protocols, and more.",A,1
Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,"The rapid rate of progress and diversity of techniques in this burgeoning field can make it difficult to compare different algorithms, tease apart the effects of new contributions, and understand the space of existing methods for transfer learning. Motivated by a need for more rigorous understanding, we leverage a unified approach to transfer learning that allows us to systematically study different approaches and push the current limits of the field. The basic idea underlying our work is to treat every text processing problem as a “text-to-text” problem, i.e. taking text as input and producing new text as output. This approach is inspired by previous unifying frameworks for NLP tasks, including casting all text problems as question answering (McCann et al., 2018), language modeling (Radford et al., 2019), or span extraction Keskar et al. (2019b) tasks. ","The fast pace of improvement and variety of techniques in this rapidly growing field can make it tricky to compare different algorithms, untangle the effects of new contributions, and comprehend the space of existing methods for transfer learning. Driven by a need for more rigorous understanding, we use a unified approach to transfer learning that allows us to methodically study different approaches and push the current limits of the field. The fundamental concept underlying our work is to treat every text processing problem as a ""text-to-text"" problem, i.e. taking text as input and generating new text as output. This approach is inspired by previous unifying frameworks for NLP tasks, including casting all text problems as question answering, language modeling, or span extraction tasks.","The swift speed of advancements and diversity of techniques in this quickly expanding field can present challenges in contrasting different algorithms, isolating the impacts of new contributions, and grasping the space of existing methods for transfer learning. Prompted by a necessity for more rigorous comprehension, we leverage a unified approach to transfer learning that enables us to systematically analyze different approaches and push the current boundaries of the field. The core idea underpinning our work is to treat every text processing problem as a ""text-in, text-out"" problem, meaning taking text as input and producing new text as output. This approach is inspired by prior unifying frameworks for NLP tasks, including framing all text problems as question answering, language modeling, or span extraction tasks.","The rapid pace of improvements and variety of techniques in this rapidly growing field can make it tough to compare different algorithms, disentangle the effects of new contributions, and understand the landscape of existing methods for transfer learning. Driven by a need for more rigorous understanding, we employ a unified approach to transfer learning that allows us to methodically examine different approaches and push the current frontiers of the field. The fundamental notion behind our work is to treat every text processing problem as a ""text-to-text"" problem, meaning taking text as input and generating new text as output. This approach is inspired by earlier unifying frameworks for NLP tasks, including casting all text problems as question answering, language modeling, or span extraction tasks.",A,1
Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,"Crucially, the text-to-text framework allows us to directly apply the same model, objective, training procedure, and decoding process to every task we consider. We leverage this flexibility by evaluating performance on a wide variety of English-based NLP problems, including question answering, document 2. http://commoncrawl.org 2 Exploring the Limits of Transfer Learning ""translate English to German: That is good."" ""cola sentence: The course is jumping well."" ""summarize: state authorities dispatched emergency crews tuesday to survey the damage after an onslaught of severe weather in mississippi…"" ""stsb sentence1: The rhino grazed on the grass. sentence2: A rhino is grazing in a field."" T5 ""Das ist gut."" ""not acceptable"" ""six people hospitalized after a storm in attala county."" ""3.8"" Figure 1: A diagram of our text-to-text framework. ","Fundamentally, the text-to-text system enables us to directly apply the identical model, goal, training process, and decoding method to every assignment we examine. We leverage this adaptability by evaluating performance on a wide variety of English-based natural language processing problems, including question answering, document summarization, and translation.","At its core, the text-to-text framework gives us the ability to use the same model, purpose, training approach, and decoding technique for every task we look at. We take advantage of this flexibility by assessing how well it performs on many different natural language processing problems involving English, such as answering questions, summarizing documents, and translating text.","In essence, the text-to-text framework provides the capacity to straightforwardly utilize the same model, objective, training procedure, and decoding methodology for every task under consideration. This flexibility is leveraged by assessing performance across a diverse range of English natural language processing tasks, including question answering, document summarization, and translation.",A,1
Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,"Every task we consider—including translation, question answering, and classification—is cast as feeding our model text as input and training it to generate some target text. This allows us to use the same model, loss function, hyperparameters, etc. across our diverse set of tasks. It also provides a standard testbed for the methods included in our empirical survey. “T5” refers to our model, which we dub the “Text-to-Text Transfer Transformer”. With this unified approach, we can compare the effectiveness of different transfer learning objectives, unlabeled data sets, and other factors, while exploring the limits of transfer learning for NLP by scaling up models and data sets beyond what has previously been considered. ","All the jobs we think about - like translating, answering questions, and categorizing - are framed as feeding text into our system and teaching it to create some desired text. This lets us utilize the same system, error function, hyperparameters, etc. across our diverse set of jobs. It also gives a standard test environment for the techniques included in our empirical review. ""T5"" refers to our system, which we name the ""Text-to-Text Transfer Transformer"". With this unified approach, we can compare the effectiveness of different transfer learning goals, unlabeled data sets, and other factors, while exploring the limits of transfer learning for NLP by scaling up systems and data sets beyond what has been considered before.","Every task we examine—including translation, question answering, and classification—is formulated as providing our model text as input and training it to generate some target text. This enables us to use the same model, loss function, hyperparameters, etc. across our diverse set of tasks. It also provides a standard testbed for the methods included in our empirical survey. “T5” refers to our model, which we dub the “Text-to-Text Transfer Transformer”. With this unified approach, we can compare the effectiveness of different transfer learning objectives, unlabeled data sets, and other factors, while exploring the limits of transfer learning for NLP by scaling up models and data sets beyond what has previously been considered.","All the jobs we look at—like translating, answering questions, and categorizing—are set up as feeding text into our model and teaching it to produce some desired text. This allows us to use the same model, loss function, hyperparameters, etc. across our diverse set of jobs. It also gives a standard testing environment for the techniques included in our empirical review. ""T5"" refers to our model, which we call the ""Text-to-Text Transfer Transformer"". With this unified approach, we can compare the effectiveness of different transfer learning goals, unlabeled data sets, and other factors, while exploring the limits of transfer learning for NLP by scaling up models and data sets beyond what has been considered before.",A,1
Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,"We emphasize that our goal is not to propose new methods but instead to provide a comprehensive perspective on where the field stands. As such, our work primarily comprises a survey, exploration, and empirical comparison of existing techniques. We also explore the limits of current approaches by scaling up the insights from our systematic study (training models up to 11 billion parameters) to obtain state-of-the-art results in many of the tasks we consider. In order to perform experiments at this scale, we introduce the “Colossal Clean Crawled Corpus” (C4), a data set consisting of hundreds of gigabytes of clean English text scraped from the web. ","We want to stress that our intention is not to put forward novel techniques, but rather to give a thorough overview of the current state of the field. Therefore, our work is chiefly a review, investigation, and empirical juxtaposition of present methods. We also probe the boundaries of existing approaches by leveraging the insights from our systematic analysis (training models with up to 11 billion parameters) to achieve cutting-edge performance on many of the tasks we examine. To conduct experiments on this scale, we present the ""Colossal Clean Crawled Corpus"" (C4), a dataset containing hundreds of gigabytes of pristine English language text gathered from the internet.","We emphasize that our aim is not to introduce original approaches, but rather to provide a comprehensive perspective on the status quo of the field. As such, our work is primarily a survey, exploration, and comparative evaluation of current techniques. We also push the limits of existing methods by scaling up the insights from our systematic study (training models with up to 11 billion parameters) to achieve state-of-the-art results on many of the tasks we look at. To carry out experiments at this scale, we introduce the ""Colossal Clean Crawled Corpus"" (C4), a dataset comprising hundreds of gigabytes of uncontaminated English language text scraped from the web.  ","We want to make clear that our objective is not to put forward novel methods, but rather to give a thorough overview of where the field currently stands. Therefore, our work chiefly comprises a review, investigation, and empirical comparison of present techniques. We also extend the boundaries of current approaches by leveraging the insights from our systematic analysis (training models with up to 11 billion parameters) to obtain cutting-edge performance on many of the tasks we consider. In order to conduct experiments at this scale, we present the ""Colossal Clean Crawled Corpus"" (C4), a dataset containing hundreds of gigabytes of unpolluted English text gathered from the internet.",A,1
Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,"Recognizing that the main utility of transfer learning is the possibility of leveraging pre-trained models in data-scarce settings, we release our code, data sets, and pre-trained models.1 The remainder of the paper is structured as follows: In the following section, we discuss our base model and its implementation, our procedure for formulating every text processing problem as a text-to-text task, and the suite of tasks we consider. In Section 3, we present a large set of experiments that explore the field of transfer learning for NLP. At the end of the section (Section 3.7), we combine insights from our systematic study to obtain state-of-the-art results on a wide variety of benchmarks. Finally, we provide a summary of our results and wrap up with a look towards the future in Section 4. ","Knowing that the main usefulness of transfer learning is being able to use pre-trained models when data is scarce, we are releasing our code, datasets, and pre-trained models. The rest of the paper is organized as follows: In the next section, we talk about our base model and how it is implemented, our method for formulating every text processing problem as a text-to-text task, and the set of tasks we look at. In Section 3, we show a large number of experiments that explore transfer learning for NLP. At the end of the section (Section 3.7), we combine insights from our systematic study to get state-of-the-art results on a wide variety of benchmarks. Finally, we summarize our results and conclude with a look at the future in Section 4.","Understanding that the primary benefit of transfer learning is the ability to use pre-trained models when data is limited, we are making our code, datasets, and pre-trained models available. The paper continues as follows: In the next part, we discuss our foundation model and implementation, our process for framing every text processing problem as a text-to-text task, and the group of tasks we consider. In Section 3, we present many experiments that investigate transfer learning for NLP. At the conclusion of the section (Section 3.7), we combine insights from our methodical study to achieve state-of-the-art results on a wide range of benchmarks. Lastly, we give a summary of our findings and finish with a look at what's next in Section 4.  ","With the knowledge that the major advantage of transfer learning is the capacity to leverage pre-trained models when data is scarce, we publish our code, data sets, and pre-trained models. The rest of the paper is organized in this way: In the following portion, we describe our base model and its creation, our technique for formulating every text processing issue as a text-to-text task, and the collection of tasks we examine. In Section 3, we display a large set of experiments that explore transfer learning for NLP. At the conclusion of the section (Section 3.7), we combine insights from our systematic study to obtain state-of-the-art results on a wide variety of benchmarks. Finally, we give a recap of our results and wrap up with a look to the future in Section 4.",A,1
Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,"Before presenting the results from our large-scale empirical study, we review the necessary background topics required to understand our results, including the Transformer model architecture and the downstream tasks we evaluate on. We also introduce our approach for treating every problem as a text-to-text task and describe our “Colossal Clean Crawled Corpus” (C4), the Common Crawl-based data set we created as a source of unlabeled text data. We refer to our model and framework as the “Text-to-Text Transfer Transformer” (T5). Early results on transfer learning for NLP leveraged recurrent neural networks (Peters et al., 2018; Howard and Ruder, 2018), but it has recently become more common to use models based on the “Transformer” architecture (Vaswani et al., 2017). ","Before showing the findings from our large empirical study, we go over the necessary background information needed to comprehend our findings, including the Transformer model design and the downstream tasks we assess. We also present our approach for treating every problem as a text-to-text task and describe our ""Colossal Clean Crawled Corpus"" (C4), the Common Crawl-based dataset we created as a source of unlabeled text data. We refer to our model and framework as the ""Text-to-Text Transfer Transformer"" (T5). Early results on transfer learning for NLP used recurrent neural networks (Peters et al., 2018; Howard and Ruder, 2018), but recently it has become more common to use models based on the ""Transformer"" architecture (Vaswani et al., 2017).","Before revealing the outcomes from our large experimental study, we examine the essential preliminary topics required to grasp our outcomes, like the Transformer model structure and the downstream tasks we evaluate. We also introduce our method for treating every issue as a text-to-text task and depict our ""Colossal Clean Crawled Corpus"" (C4), the Common Crawl-based data collection we created as a source of unlabeled text information. We refer to our model and framework as the ""Text-to-Text Transfer Transformer"" (T5). Early outcomes on transfer learning for NLP used recurrent neural networks (Peters et al., 2018; Howard and Ruder, 2018), but lately it has become more prevalent to use models based on the ""Transformer"" architecture (Vaswani et al., 2017).  ","Before displaying the conclusions from our large investigative study, we review the necessary background knowledge needed to comprehend our conclusions, including the Transformer model blueprint and the downstream tasks we appraise. We also present our tactic for treating every problem as a text-to-text task and characterize our “Colossal Clean Crawled Corpus” (C4), the Common Crawl-based data bank we created as a source of unlabeled text information. We refer to our model and framework as the “Text-to-Text Transfer Transformer” (T5). Early conclusions on transfer learning for NLP employed recurrent neural networks (Peters et al., 2018; Howard and Ruder, 2018), but recently it has become more widespread to use models based on the “Transformer” architecture (Vaswani et al., 2017).",A,1
Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,"The Transformer was initially shown to be effective for machine translation, but it has subsequently been used in a wide variety of NLP settings (Radford et al., 2018; Devlin et al., 2018; McCann et al., 2018; Yu et al., 2018). Due to its increasing ubiquity, all of the models we study are based on the Transformer architecture. Apart from the details mentioned below and the variants we explore in Section 3.2, we do not deviate significantly from this architecture as originally proposed. Instead of providing a comprehensive definition of this model, we refer the interested reader to the original paper (Vaswani et al., 2017) or follow-up tutorials3,4 for a more detailed introduction. ","The Transformer was first demonstrated to be useful for automatic translation between languages, but it has since been utilized in many different natural language processing applications (Radford et al., 2018; Devlin et al., 2018; McCann et al., 2018; Yu et al., 2018). Because it is becoming so common, all the models we examine here are built on the Transformer design. Other than the specifics noted below and the variations we investigate in Section 3.2, we do not diverge substantially from the original Transformer architecture. Rather than giving a full definition of this model, we point interested readers to the original paper (Vaswani et al., 2017) or subsequent tutorials3,4 for a more in-depth introduction.","The Transformer was initially shown to be effective at machine translation, but subsequently it has been employed in many different NLP settings (Radford et al., 2018; Devlin et al., 2018; McCann et al., 2018; Yu et al., 2018). Owing to its expanding ubiquity, all the models we analyze are founded on the Transformer structure. Aside from the particulars mentioned below and the variants we explore in Section 3.2, we do not deviate considerably from this architecture as first proposed. Instead of providing an exhaustive definition of this model, we refer curious readers to the original paper (Vaswani et al., 2017) or follow-up tutorials3,4 for a more thorough introduction.","The Transformer was first demonstrated to be useful for automated translation between languages, and has since been applied in many different natural language processing tasks (Radford et al., 2018; Devlin et al., 2018; McCann et al., 2018; Yu et al., 2018). Because it is becoming very common, all the models we study here are constructed using the Transformer design. Except for the details noted below and the variations we investigate in Section 3.2, we do not diverge significantly from the original Transformer architecture. Rather than providing a comprehensive definition of this model, we point interested readers to the original paper (Vaswani et al., 2017) or subsequent tutorials3,4 for a more in-depth introduction.",A,1
Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,"The primary building block of the Transformer is self-attention (Cheng et al., 2016). Self-attention is a variant of attention (Graves, 2013; Bahdanau et al., 2015) that processes a sequence by replacing each element by a weighted average of the rest of the sequence. The original Transformer consisted of an encoder-decoder architecture and was intended for sequence-to-sequence (Sutskever et al., 2014; Kalchbrenner et al., 2014) tasks. It has recently also become common to use models consisting of a single Transformer layer stack, with varying forms of self-attention used to produce architectures appropriate for language modeling (Radford et al., 2018; Al-Rfou et al., 2019) or classification and span prediction tasks (Devlin et al., 2018; Yang et al., 2019). ","The fundamental component of the Transformer is self-attention (Cheng et al., 2016). Self-attention is a form of attention (Graves, 2013; Bahdanau et al., 2015) that processes a sequence by substituting each element with a weighted mean of the rest of the sequence. The original Transformer was made up of an encoder-decoder structure and was meant for sequence-to-sequence (Sutskever et al., 2014; Kalchbrenner et al., 2014) tasks. Recently it has also become common to use models made up of a single Transformer layer stack, with different forms of self-attention used to create architectures suitable for language modeling (Radford et al., 2018; Al-Rfou et al., 2019) or classification and span prediction tasks (Devlin et al., 2018; Yang et al., 2019).","The core building block of the Transformer is self-attention (Cheng et al., 2016). Self-attention is a type of attention (Graves, 2013; Bahdanau et al., 2015) that processes a sequence by swapping each element with a weighted average of the rest of the sequence. The original Transformer was composed of an encoder-decoder design and was intended for sequence-to-sequence (Sutskever et al., 2014; Kalchbrenner et al., 2014) tasks. Lately it has also become widespread to use models made up of a single Transformer layer stack, with various forms of self-attention used to construct architectures appropriate for language modeling (Radford et al., 2018; Al-Rfou et al., 2019) or classification and span prediction tasks (Devlin et al., 2018; Yang et al., 2019).  ","The key building block of the Transformer is self-attention (Cheng et al., 2016). Self-attention is a variation of attention (Graves, 2013; Bahdanau et al., 2015) that processes a sequence by substituting each element with a weighted mean of the rest of the sequence. The original Transformer was composed of an encoder-decoder design and was intended for sequence-to-sequence (Sutskever et al., 2014; Kalchbrenner et al., 2014) tasks. Recently it has also become widespread to use models consisting of a single Transformer layer stack, with different forms of self-attention used to create architectures suitable for language modeling (Radford et al., 2018; Al-Rfou et al., 2019) or classification and span prediction tasks (Devlin et al., 2018; Yang et al., 2019).",A,1
Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,"We empirically explore these architectural variants in Section 3.2. Overall, our encoder-decoder Transformer implementation closely follows its originallyproposed form (Vaswani et al., 2017). First, an input sequence of tokens is mapped to a sequence of embeddings, which is then passed into the encoder. The encoder consists of a stack of “blocks”, each of which comprises two subcomponents: a self-attention layer followed by a small feed-forward network. Layer normalization (Ba et al., 2016) is applied to the input of each subcomponent. ","We experimentally investigate these architectural variations in Section 3.2. In general, our encoder-decoder Transformer realization closely adheres to its originally suggested form (Vaswani et al., 2017). Initially, an input sequence of tokens is converted to a sequence of embeddings, which is then inputted into the encoder. The encoder is comprised of a pile of ""blocks"", each containing two subparts: a self-attention layer followed by a small feedforward network. Layer normalization (Ba et al., 2016) is applied to the input of each subpart.","We empirically analyze these architectural options in Section 3.2. On the whole, our encoder-decoder Transformer execution closely matches its originally described structure (Vaswani et al., 2017). To start, an input series of tokens is transformed into a series of embeddings, which is then fed into the encoder. The encoder consists of a stack of ""blocks"", each having two components: a self-attention layer followed by a small feedforward network. Layer normalization (Ba et al., 2016) is used on the input of each component.  ","We experimentally examine these architectural configurations in Section 3.2. In summary, our encoder-decoder Transformer implementation closely adheres to its originally presented form (Vaswani et al., 2017). First off, an input sequence of tokens is converted into a sequence of embeddings, which is then entered into the encoder. The encoder is made up of a pile of ""blocks"", each containing two pieces: a self-attention layer followed by a small feedforward network. Layer normalization (Ba et al., 2016) is applied to the input of each piece.",A,1
Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,"We use a simplified version of layer normalization where the activations are only rescaled and no additive bias is applied. After layer normalization, a residual skip connection (He et al., 2016) adds each subcomponent’s input to its output. Dropout (Srivastava et al., 2014) is applied within the feed-forward network, on the skip connection, on the attention weights, and at the input and output of the entire stack. The decoder is similar in structure to the encoder except that it includes a standard attention 3. ","We utilize a streamlined form of layer normalization that only rescales the activations without any additive predisposition. Post layer normalization, a residual bypass linkage (He et al., 2016) aggregates each subcomponent's input to its output. Dropout (Srivastava et al., 2014) is employed inside the feed-forward network, on the bypass linkage, on the attention weights, and at the input and output of the entire pile. The decoder has a comparable structure to the encoder excluding a standard attention mechanism.","We make use of a simplified adaptation of layer normalization which solely normalizes the activations minus any additive inclination. After layer normalization, a residual skip connection (He et al., 2016) combines each subcomponent's entrance to its exit. Dropout (Srivastava et al., 2014) is utilized inside the feed-forward network, on the skip connection, on the attention loads, and at the inlet and outlet of the entire stack. The decoder has a similar form to the encoder apart from comprising a standard attention procedure.  ","We utilize a streamlined variant of layer normalization that just rescales the activations without applying any additive predisposition. Following layer normalization, a residual bypass link (He et al., 2016) aggregates each subcomponent's entrance to its way out. Dropout (Srivastava et al., 2014) is employed within the feed-forward network, on the bypass link, on the attention burdens, and at the inlet and outlet of the entire pile. The decoder has a similar makeup to the encoder barring including a standard attention means.",A,1
Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,"The self-attention mechanism in the decoder also uses a form of autoregressive or causal selfattention, which only allows the model to attend to past outputs. The output of the final decoder block is fed into a dense layer with a softmax output, whose weights are shared with the input embedding matrix. All attention mechanisms in the Transformer are split up into independent “heads” whose outputs are concatenated before being further processed. Since self-attention is order-independent (i.e. it is an operation on sets), it is common to provide an explicit position signal to the Transformer. ","The decoder's self-attention system also utilizes a type of autoregressive or causal self-attention, which only permits the model to focus on previous outputs. The final decoder block's output is fed into a dense layer with a softmax output, whose weights are shared with the input embedding matrix. All attention systems in the Transformer are divided into separate ""heads"" whose outputs are joined before additional processing. As self-attention does not depend on order (meaning it operates on sets), providing an explicit position signal to the Transformer is common.","The decoder's self-attention tool likewise uses a form of autoregressive or causal self-attention, which exclusively enables the model to pay attention to past outputs. The output from the final decoder block enters a dense layer with a softmax output, sharing weights with the input embedding matrix. Every attention tool in the Transformer splits into independent ""heads"" whose outputs combine before further processing. Since self-attention does not depend on order (it works on sets), giving an explicit position signal to the Transformer is typical.  ","The decoder's self-attention mechanism also utilizes a type of autoregressive or causal self-attention, which only allows the model to focus on previous outputs. The output of the final decoder block feeds into a dense layer with a softmax output, sharing weights with the input embedding matrix. All attention mechanisms in the Transformer split into separate ""heads"" whose outputs join before additional processing. As self-attention does not rely on order (it operates on sets), providing an explicit position signal to the Transformer is commonplace.",A,1
Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,"While the original Transformer used a sinusoidal position signal or learned position embeddings, it has recently become more common to use relative position embeddings (Shaw et al., 2018; Huang et al., 2018a). Instead of using a fixed embedding for each position, relative position embeddings produce a different learned embedding according to the offset between the “key” and “query” being compared in the self-attention mechanism. We use a simplified form of position embeddings where each “embedding” is simply a scalar that is added to the corresponding logit used for computing the attention weights. For efficiency, we also share the position embedding parameters across all layers in our model, though within a given layer each attention head uses a different learned position embedding. ","The first Transformer utilized a sinusoidal position signal or learned position embeddings, but more recently it has become more popular to employ relative position embeddings (Shaw et al., 2018; Huang et al., 2018a). Rather than having a fixed embedding for every position, relative position embeddings generate a distinct learned embedding based on the offset between the ""key"" and ""query"" being compared in the self-attention mechanism. We utilize a simplified form of position embeddings where each ""embedding"" is just a scalar that is added to the corresponding logit used for computing the attention weights. For efficiency, we also share the position embedding parameters across all layers in our model, although within a given layer each attention head utilizes a different learned position embedding.","While the original Transformer used either a sinusoidal position signal or learned position embeddings, more modern versions tend to use relative position embeddings (Shaw et al., 2018; Huang et al., 2018a). With relative position embeddings, the embedding for each position is generated based on the offset between the ""key"" and ""query"" being compared in self-attention, rather than having a fixed embedding per position. Our model uses a simplified type of position embedding where each ""embedding"" is just a scalar added to the logit for computing attention weights. We also share position embedding parameters between all layers for efficiency, but each attention head in a layer learns its own position embedding.","Instead of the fixed position embeddings or sinusoidal position signals used in the original Transformer, more recent models typically employ relative position embeddings (Shaw et al., 2018; Huang et al., 2018a). These generate an embedding based on the difference between ""key"" and ""query"" positions compared in self-attention, rather than having a predefined embedding per position. Our model uses a simplified position embedding approach where each ""embedding"" is a scalar added to the logit for attention weight calculation. For efficiency we share position embedding parameters across layers, but each attention head within a layer learns its own position embedding.",A,1
Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,"Typically, a fixed number of embeddings are learned, each corresponding to a range of possible key-query offsets. In this work, we use 32 embeddings for all of our models with ranges that increase in size logarithmically up to an offset of 128 beyond which we assign all relative positions to the same embedding. Note that a given layer is insensitive to relative position beyond 128 tokens, but subsequent layers can build a sensitivity to larger offsets by combining local information from previous layers. To summarize, our model is roughly equivalent to the original Transformer proposed by Vaswani et al. (2017) with the exception of removing the Layer Norm bias, placing the layer normalization outside the residual path, and using a different position embedding scheme. ","Usually, a set number of embeddings are learned, with each matching a variety of potential key-query offsets. For all our models here, we utilize 32 embeddings with ranges growing logarithmically up to an offset of 128. After that, we assign all relative positions the same embedding. Note that one layer can't detect relative position past 128 tokens, but later layers can become sensitive to larger offsets by merging local info from prior layers. In summary, our model is essentially the original Transformer proposed by Vaswani et al. (2017), except we remove the Layer Norm bias, put the layer normalization outside the residual path, and employ a different position embedding system.","In general, a fixed quantity of embeddings are acquired, each relating to a span of potential key-query offsets. In this work, we employ 32 embeddings for all our models with spans that expand logarithmically up to an offset of 128 beyond which we designate all relative positions to the same embedding. Note that a given layer is blind to relative position past 128 tokens, but following layers can construct a sensitivity to larger offsets by combining local data from earlier layers. To recap, our model is roughly the same as the original Transformer proposed by Vaswani et al. (2017) except for erasing the Layer Norm bias, situating the layer normalization outside the residual path, and utilizing a distinct position embedding scheme.  ","Typically, a set number of embeddings are learned, each matching a variety of feasible key-query offsets. Here, we utilize 32 embeddings for all our models with ranges growing logarithmically up to an offset of 128. Afterward, we assign all relative positions the same embedding. Note that one layer can't sense relative position beyond 128 tokens, but subsequent layers can become sensitive to larger offsets by merging local information from prior layers. In summary, our model is essentially the original Transformer by Vaswani et al. (2017), excluding removing the Layer Norm bias, positioning the layer normalization outside the residual path, and employing a different position embedding system.",A,1
Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,"Since these architectural changes are orthogonal to the experimental factors we consider in our empirical survey of transfer learning, we leave the ablation of their impact for future work. As part of our study, we experiment with the scalability of these models, i.e. how their performance changes as they are made to have more parameters or layers. Training large models can be non-trivial since they might not fit on a single machine and require a great deal of computation. As a result, we use a combination of model and data parallelism and train models on “slices” of Cloud TPU Pods.5 TPU pods are are multi-rack ML supercomputers that contain 1,024 TPU v3 chips connected via a high-speed 2D mesh interconnect with supporting CPU host machines. ","Because these architectural adjustments do not relate to the experimental factors we examine in our empirical analysis of transfer learning, we postpone evaluating their effect for future work. As part of our research, we test the scalability of these models, meaning how their performance alters as they are made to have more parameters or layers. Training large models can be challenging since they may not fit on a single machine and need a lot of computation. Therefore, we utilize a combination of model and data parallelism and train models on ""slices"" of Cloud TPU Pods. TPU pods are multi-rack ML supercomputers containing 1,024 TPU v3 chips linked via a high-speed 2D mesh interconnect with supporting CPU host machines.","Since these architectural modifications are separate from the experimental factors we inspect in our empirical study of transfer learning, we defer analyzing their impact to future work. As part of our investigation, we examine the scalability of these models, that is how their performance shifts as they are configured to have more parameters or layers. Educating large models can be tricky since they may not suit a single machine and necessitate substantial computation. Consequently, we employ a blend of model and data parallelism and educate models on ""slices"" of Cloud TPU Pods. TPU pods are multi-rack ML supercomputers having 1,024 TPU v3 chips connected via a high-speed 2D mesh interconnect with assisting CPU host machines.  ","Because these architectural changes do not pertain to the experimental factors we evaluate in our empirical analysis of transfer learning, we put off assessing their effect for future work. As part of our study, we test the scalability of these models, meaning how their performance varies as they are made to have more parameters or layers. Training large models can be difficult since they may not fit on one machine and need a lot of computation. Thus, we use a combination of model and data parallelism and train models on ""slices"" of Cloud TPU Pods. TPU pods are multi-rack ML supercomputers containing 1,024 TPU v3 chips linked through a high-speed 2D mesh interconnect with supporting CPU host machines.",A,1
Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,"We leverage the Mesh TensorFlow library (Shazeer et al., 2018) for ease of implementation of both model parallelism and data parallelism (Krizhevsky, 2014). Much of the previous work on transfer learning for NLP makes use of large unlabeled data sets for unsupervised learning. In this paper, we are interested in measuring the effect of the quality, characteristics, and size of this unlabeled data. To generate data sets that satisfy our needs, we leverage Common Crawl as a source of text scraped from the web. Common Crawl is a publicly-available web archive that provides “web extracted text” by removing markup and other non-text content from the scraped HTML files. ","We use the Mesh TensorFlow code library (Shazeer et al., 2018) to easily implement both model parallelism and data parallelism (Krizhevsky, 2014). A lot of previous work on transfer learning for NLP utilizes large unlabeled data sets for unsupervised learning. In this paper, we want to measure the impact of the quality, traits, and size of this unlabeled data. To create data sets that meet our needs, we use Common Crawl as a source of text scraped from the web. Common Crawl is a public web archive that provides ""web extracted text"" by taking out markup and other non-text content from the scraped HTML files.","We take advantage of the Mesh TensorFlow library (Shazeer et al., 2018) to conveniently implement both model parallelism and data parallelism (Krizhevsky, 2014). Much previous research on transfer learning for NLP employs large unlabeled data sets for unsupervised learning. In this paper, we are interested in assessing the effect of the quality, features, and size of this unlabeled data. To generate data sets that fulfill our requirements, we use Common Crawl as a source of text scraped from the web. Common Crawl is a publicly available web archive that furnishes ""web extracted text"" by removing markup and other non-text content from the scraped HTML files.  ","We utilize the Mesh TensorFlow code library (Shazeer et al., 2018) for easy implementation of both model parallelism and data parallelism (Krizhevsky, 2014). A great deal of prior work on transfer learning for NLP makes use of large unlabeled data sets for unsupervised learning. In this paper, we want to gauge the impact of the quality, attributes, and size of this unlabeled data. To create data sets that meet our needs, we leverage Common Crawl as a source of text scraped from the web. Common Crawl is a public web archive that provides ""web extracted text"" by taking out markup and other non-text content from the scraped HTML files.",A,1
Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,"This process produces around 20TB of scraped text data each month. Unfortunately, the majority of the resulting text is not natural language. Instead, it largely comprises gibberish or boiler-plate text like menus, error messages, or duplicate text. Furthermore, a good deal of the scraped text contains content that is unlikely to be helpful for any of the tasks we consider (offensive language, placeholder text, source code, etc.). To address these issues, we used the following heuristics for cleaning up Common Crawl’s web extracted text: • We only retained lines that ended in a terminal punctuation mark (i.e. a period, exclamation mark, question mark, or end quotation mark). ","This procedure generates about 20 terabytes of scraped text information every month. However, most of the resulting text is not natural language. Rather, it largely includes nonsense or standard text like menus, error notifications, or duplicate content. Additionally, a significant amount of the scraped text has stuff that is probably not helpful for any of the tasks we think about (offensive words, placeholder text, source code, etc.). To tackle these problems, we utilized the following rules of thumb for tidying up Common Crawl's web extracted text: • We only kept lines that finished with a terminal punctuation symbol (meaning a period, exclamation point, question mark, or end quote mark).","This process produces around 20 TB of scraped text material on a monthly basis. Unfortunately, most of the text is not real language. Instead, it is mostly gibberish or generic text such as menus, error messages, or repeated text. Also, a good amount of the scraped text has content that is unlikely to be useful for any of the tasks we are considering (offensive content, placeholder text, source code, etc.). To address these issues, we used the following heuristics to clean up the web extracted text from Common Crawl: • We only kept lines that ended with punctuation that signals the end of a sentence (periods, exclamation points, question marks, or end quotation marks).","This procedure generates approximately 20 terabytes of scraped text data every month. However, the bulk of the resulting text is not natural language. Rather, it is mostly nonsense or standard text such as menus, error messages, or duplicate content. Furthermore, a significant portion of the scraped text contains material that is probably not helpful for any of the tasks we are looking at (offensive content, placeholder text, source code, etc.). To tackle these problems, we utilized the following rules of thumb to clean up the web extracted text from Common Crawl: • We only retained lines that concluded with punctuation denoting the end of a sentence (periods, exclamation points, question marks, or end quote marks).",A,1
Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,"We discarded any page with fewer than 3 sentences and only retained lines that contained at least 5 words. We removed any page that contained any word on the “List of Dirty, Naughty, Obscene or Otherwise Bad Words”.6 • Many of the scraped pages contained warnings stating that Javascript should be enabled so we removed any line with the word Javascript. Some pages had placeholder “lorem ipsum” text; we removed any page where the phrase “lorem ipsum” appeared. Some pages inadvertently contained code. ","We threw out any page that had less than 3 sentences and kept only the lines that had a minimum of 5 words. We got rid of any page that had any of the words from the ""Directory of Inappropriate, Vulgar, Obscene or Otherwise Unacceptable Words"". Many of the webpages we extracted had alerts saying Javascript should be activated so we deleted any line with the word Javascript. A few pages had placeholder ""lorem ipsum"" content; we removed any page where the phrase ""lorem ipsum"" was present. Some pages accidentally had code.","We removed any page with under 3 sentences and retained only lines with at least 5 words. We eliminated any page containing any term from the ""Index of Unseemly, Profane, Explicit or Else Unfit Words"". Numerous of the scraped pages had warnings that Javascript ought to be enabled so we took out any line containing the word Javascript. Certain pages contained placeholder ""lorem ipsum"" text; we deleted any page where the phrase ""lorem ipsum"" showed up. Some pages unintentionally included code.","We got rid of any page that contained less than 3 sentences and kept only the lines that had a minimum of 5 words. We discarded any page that had any of the words from the ""Register of Indecent, Vulgar, Lewd or Otherwise Unacceptable Words"". Many of the webpages we extracted contained alerts stating Javascript should be turned on so we removed any line that had the word Javascript. A few pages contained placeholder ""lorem ipsum"" text; we took out any page where the phrase ""lorem ipsum"" was present. Some pages mistakenly had code.",A,1
Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,"Since the curly bracket “{” appears in many programming languages (such as Javascript, widely used on the web) but not in natural text, we removed any pages that contained a curly bracket. Since some of the scraped pages were sourced from Wikipedia and had citation markers (e.g. [1], [citation needed], etc.), we removed any such markers. Many pages had boilerplate policy notices, so we removed any lines containing the strings “terms of use”, “privacy policy”, “cookie policy”, “uses cookies”, “use of cookies”, or “use cookies”. To deduplicate the data set, we discarded all but one of any three-sentence span occurring more than once in the data set. ","The curly brace character ""{"" is common in programming languages like Javascript for the web but not in everyday writing, so we deleted any pages with a ""{"". Some pages were from Wikipedia and had citation tags like ""[1]"" and ""[citation needed]"", which we removed. Lots of pages had cookie notices and terms of use, so we got rid of lines with phrases like ""privacy policy"", ""uses cookies"", or ""use of cookies"". To avoid repetition, if the same 3 sentences showed up more than once, we kept only 1 version.","Since the { curly bracket is used often in programming languages including Javascript for websites but not in natural language, we eliminated any pages containing a {. Certain pages were from Wikipedia and contained citation markers such as [1], [citation required], etc. which were removed. Numerous pages had boilerplate policy statements, so we deleted any lines with the strings ""terms of use"", ""privacy policy"", ""cookie policy"", ""uses cookies"", ""use of cookies"", or ""use cookies"". To deduplicate the data set, we threw out all but one instance of any three-sentence segment occurring more than once.","The { curly brace character appears frequently in programming languages like Javascript for webpages but not in normal text, so we removed any pages with a {. Some pages were sourced from Wikipedia and contained citation tags such as [1], [citation needed], etc. which we took out. Many pages contained boilerplate policy notices, so we eliminated any lines with the phrases ""terms of use"", ""privacy policy"", ""cookie policy"", ""uses cookies"", ""use of cookies"", or ""use cookies"". To remove duplicate data, we discarded all but one case of any three-sentence segment that showed up more than once.",A,1
Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,"Additionally, since most of our downstream tasks are focused on English-language text, we used langdetect7 to filter out any pages that were not classified as English with a probability of at least 0.99. Our heuristics are inspired by past work on using Common 6. For example, Grave et al. (2018) also filter text using an automatic language detector and discard short lines and Smith et al. (2013); Grave et al. (2018) both perform line-level deduplication. ","Moreover, as the majority of our future work involves analyzing English text, we utilized langdetect7 to remove any pages not identified as English with 99% certainty or higher. Our methods are based on previous research using Common Crawl. For instance, Grave et al. (2018) also filtered text using automated language recognition and ignored short lines, while Smith et al. (2013) and Grave et al. (2018) both carried out deduplication at the line level.","Furthermore, since most of our upcoming assignments focus on English language content, we used langdetect7 to eliminate any pages that were not classified as English with a minimum probability of 0.99. Our techniques are inspired by earlier work using Common Crawl. For example, Grave et al. (2018) also filtered out text using computerized language identification and skipped short lines, and Smith et al. (2013) along with Grave et al. (2018) both performed duplication removal on a per line basis.  ","In addition, as the bulk of our future undertakings center around English verbiage, we leveraged langdetect7 to dispense with any folios not pinpointed as English with a likelihood of no less than 0.99. Our methodologies are roused by past work utilizing Common Crawl. For instance, Grave et al. (2018) likewise sifted text utilizing mechanized language recognition and disregarded short lines, while Smith et al. (2013) and Grave et al. (2018) both did duplication end on a line-by-line premise.",A,1
Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,"However, we opted to create a new data set because prior data sets use a more limited set of filtering heuristics, are not publicly available, and/or are different in scope (e.g. are limited to News data (Zellers et al., 2019; Liu et al., 2019c), comprise only Creative Commons content (Habernal et al., 2016), or are focused on parallel training data for machine translation (Smith et al., 2013)). This produces a collection of text that is not only orders of magnitude larger than most data sets used for pre-training (about 750 GB) but also comprises reasonably clean and natural English text. We dub this data set the “Colossal Clean Crawled Corpus” (or C4 for short) and release it as part of TensorFlow Datasets.8 We consider the impact of using various alternative versions of this data set in Section 3.4. ","However, we decided to make a new data set because previous data sets use more limited filtering rules, are not publicly available, and/or have a different scope (for example, are restricted to News data (Zellers et al., 2019; Liu et al., 2019c), only contain Creative Commons content (Habernal et al., 2016), or focus on parallel training data for machine translation (Smith et al., 2013)). This results in a collection of text that is not just orders of magnitude larger than most data sets used for pre-training (about 750 GB) but also contains reasonably clean and natural English text. We call this data set the ""Enormous Clean Crawled Collection"" (or C4 for short) and release it as part of TensorFlow Datasets. We look at the impact of using different versions of this data set in Section 3.4.","However, we elected to construct a novel data set since earlier data sets utilize a more constrained set of filtering heuristics, are not publicly accessible, and/or have a different scope (for instance, are limited to News data (Zellers et al., 2019; Liu et al., 2019c), comprise only Creative Commons content (Habernal et al., 2016), or focus on parallel training data for machine translation (Smith et al., 2013)). This yields a collection of text that is not just orders of magnitude larger than most data sets used for pre-training (about 750 GB) but also contains reasonably clean and natural English text. We name this data set the ""Gigantic Clean Crawled Collection"" (or C4 for short) and release it as part of TensorFlow Datasets. We examine the impact of utilizing various alternative versions of this data set in Section 3.4.","However, we chose to generate a new data set since previous data sets employ a more constrained set of filtering rules, are not publicly available, and/or differ in scope (for example, are limited to News data (Zellers et al., 2019; Liu et al., 2019c), include only Creative Commons content (Habernal et al., 2016), or focus on parallel training data for machine translation (Smith et al., 2013)). This produces a collection of text that is not just orders of magnitude larger than most data sets used for pre-training (about 750 GB) but also comprises reasonably clean and natural English text. We entitle this data set the ""Massive Clean Crawled Collection"" (or C4 for short) and release it as part of TensorFlow Datasets. We analyze the impact of applying various alternative versions of this data set in Section 3.4.",A,1
Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,"Our goal in this paper is to measure general language learning abilities. As such, we study downstream performance on a diverse set of benchmarks, including machine translation, question answering, abstractive summarization, and text classification. Specifically, we measure performance on the GLUE and SuperGLUE text classification meta-benchmarks; CNN/Daily Mail abstractive summarization; SQuAD question answering; and WMT English to German, French, and Romanian translation. ","The objective of this report is to evaluate broad language learning skills. Therefore, we analyze subsequent performance on a wide variety of benchmarks, including machine translation, question answering, abstractive summarization, and text classification. In particular, we measure performance on the GLUE and SuperGLUE text classification meta-benchmarks; CNN/Daily Mail abstractive summarization; SQuAD question answering; and WMT English to German, French, and Romanian translation.","Our aim in this article is to quantify general language learning capabilities. To do so, we examine downstream results on a diverse collection of benchmarks, which include machine translation, question answering, abstractive summarization, and text classification. We specifically quantify performance on the GLUE and SuperGLUE text classification meta-benchmarks; CNN/Daily Mail abstractive summarization; SQuAD question answering; and WMT English to German, French, and Romanian translation.  ","The purpose of this paper is to evaluate broad language learning aptitude. Therefore, we review subsequent outcomes across a varied set of benchmarks, encompassing machine translation, question answering, abstractive summarization, and text classification. We specifically gauge performance on the GLUE and SuperGLUE text classification meta-benchmarks; CNN/Daily Mail abstractive summarization; SQuAD question answering; and WMT English to German, French, and Romanian translation.",A,1
Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,"For simplicity, when fine-tuning we treat all of the tasks in the GLUE benchmark (and similarly for SuperGLUE) as a single task by concatenating all of the constituent data sets. As suggested by Kocijan et al. (2019) we also include the Definite Pronoun Resolution (DPR) data set (Rahman and Ng, 2012) in the combined SuperGLUE task. The CNN/Daily Mail (Hermann et al., 2015) data set was introduced as a questionanswering task but was adapted for text summarization by Nallapati et al. (2016); we use the non-anonymized version from See et al. (2017) as an abstractive summarization task. SQuAD (Rajpurkar et al., 2016) is a common question-answering benchmark. ","For simplicity, when fine-tuning we treat all the tasks in the GLUE benchmark (and similarly for SuperGLUE) as one task by joining together all the different data sets. As proposed by Kocijan et al. (2019) we also add the Definite Pronoun Resolution (DPR) data set (Rahman and Ng, 2012) to the combined SuperGLUE task. The CNN/Daily Mail (Hermann et al., 2015) data set was originally introduced as a question-answering task but was adapted for text summarization by Nallapati et al. (2016); we use the non-anonymized version from See et al. (2017) as an abstractive summarization task. SQuAD (Rajpurkar et al., 2016) is a well-known question-answering benchmark.","To simplify, when fine-tuning we treat all the tasks in the GLUE benchmark (and similarly for SuperGLUE) as one task by concatenating all the different data sets together. As suggested by Kocijan et al. (2019) we also incorporate the Definite Pronoun Resolution (DPR) data set (Rahman and Ng, 2012) into the combined SuperGLUE task. The CNN/Daily Mail (Hermann et al., 2015) data set was first introduced as a question-answering task but was adapted for text summarization by Nallapati et al. (2016); we use the non-anonymized version from See et al. (2017) as an abstractive summarization task. SQuAD (Rajpurkar et al., 2016) is a widely used question-answering benchmark.  ","For simplicity, when fine-tuning we treat all the tasks in the GLUE benchmark (and similarly for SuperGLUE) as one task by merging all the different data sets together. As recommended by Kocijan et al. (2019) we also include the Definite Pronoun Resolution (DPR) data set (Rahman and Ng, 2012) in the combined SuperGLUE task. The CNN/Daily Mail (Hermann et al., 2015) data set was first presented as a question-answering task but was modified for text summarization by Nallapati et al. (2016); we use the non-anonymized version from See et al. (2017) as an abstractive summarization task. SQuAD (Rajpurkar et al., 2016) is a common question-answering benchmark dataset.",A,1
Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,"In our experiments, the model is fed the question and its context and asked to generate the answer token-by-token. For WMT English to German, we use the same training data as (Vaswani et al., 2017) (i.e. News Commentary v13, Common Crawl, Europarl v7) and newstest2013 as a validation set (Bojar et al., 2014). For English to French, we use the standard training data from 2015 and newstest2014 as a validation set (Bojar et al., 2015). For English to Romanian, which is a standard lower-resource machine translation benchmark, we use the train and validation sets from WMT 2016 (Bojar et al., 2016). ","In our tests, we provide the model with the question and context and have it generate the answer word-by-word. For translating English to German, we utilize the same training information as Vaswani et al. (2017), which includes News Commentary v13, Common Crawl, and Europarl v7, using newstest2013 as the validation set (Bojar et al., 2014). For English to French, we employ the standard 2015 training data with newstest2014 as the validation set (Bojar et al., 2015). For English to Romanian, a common low-resource machine translation benchmark, we use the training and validation sets from WMT 2016 (Bojar et al., 2016).","During our experiments, we feed the model the question and surrounding context then have it produce the answer one token at a time. For English-German translation, we employ the same training data as Vaswani et al. (2017), specifically News Commentary v13, Common Crawl, and Europarl v7, using newstest2013 for validation (Bojar et al., 2014). For English-French, we utilize the standard 2015 training information with newstest2014 as validation (Bojar et al., 2015). For English-Romanian, a typical low-resource machine translation test, we use the training and validation sets from WMT 2016 (Bojar et al., 2016).  ","In our tests, we input the question and context into the model and have it generate the answer word-by-word. For English to German translation, we use the same training data as Vaswani et al. (2017), which includes News Commentary v13, Common Crawl, and Europarl v7, with newstest2013 as validation (Bojar et al., 2014). For English to French, we use the standard 2015 training data with newstest2014 for validation (Bojar et al., 2015). For English to Romanian, a common low-resource machine translation benchmark, we utilize the training and validation sets from WMT 2016 (Bojar et al., 2016).",A,1
Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,"Note that we only pre-train on English data, so in order to learn to translate a given model will need to learn to generate text in a new language. In order to train a single model on the diverse set of tasks described above, we cast all of the tasks we consider into a “text-to-text” format—that is, a task where the model is fed some text for context or conditioning and is then asked to produce some output text. This framework provides a consistent training objective both for pre-training and fine-tuning. Specifically, the model is trained with a maximum likelihood objective (using “teacher forcing” (Williams and Zipser, 1989)) regardless of the task. ","Keep in mind that we only do preliminary training using English language data. Therefore, to be able to translate, a given model needs to learn how to generate text in a new tongue. To train just one model on the wide variety of tasks delineated above, we format all the tasks we're looking at into a ""text-to-text"" structure - that is, a task where the model gets some text for context or conditions and is then prompted to generate some output text. This structure gives a steady training goal for both pre-training and fine-tuning. Specifically, the model is trained using a maximum likelihood objective (utilizing ""teacher forcing"" (Williams and Zipser, 1989)) no matter the task.","It should be noted that we only do initial training with data in English. So for a particular model to learn translation, it will need to learn to produce text in a new language. To educate a single model on the diverse set of tasks outlined previously, we put all the tasks we're considering into a ""text-to-text"" form - meaning a task where the model gets some text for context or constraints and is then asked to generate some output text. This format provides a consistent training aim for both pre-training and fine-tuning. In particular, the model is trained using a maximum likelihood goal (employing ""teacher forcing"" (Williams and Zipser, 1989)) regardless of the task.  ","Be aware that we only conduct pre-training using English language data. Therefore, for a given model to learn translation, it will need to learn to create text in a new tongue. To train a single model on the wide variety of tasks described before, we structure all the tasks we're considering into a ""text-to-text"" format - meaning a task where the model receives some text for context or conditions and is then prompted to produce some output text. This structure provides a steady training objective for both pre-training and fine-tuning. Specifically, the model is educated using a maximum likelihood goal (utilizing ""teacher forcing"" (Williams and Zipser, 1989)) no matter the task.",A,1
Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,"To specify which task the model should perform, we add a task-specific (text) prefix to the original input sequence before feeding it to the model. As an example, to ask the model to translate the sentence “That is good.” from English to German, the model would be fed the sequence “translate English to German: That is good.” and would be trained to output “Das ist gut.” For text classification tasks, the model simply predicts a single word corresponding to the target label. For example, on the MNLI benchmark (Williams et al., 2017) the goal is to predict whether a premise implies (“entailment”), contradicts (“contradiction”), or neither (“neutral”) a hypothesis. With our preprocessing, the input sequence becomes “mnli premise: I hate pigeons. hypothesis: My feelings towards pigeons are filled with animosity.” with the corresponding target word “entailment”. ","To indicate the task the model should carry out, we append a task-specific (text) prefix to the original input sequence before providing it to the model. For illustration, to request the model to translate the sentence ""That is good."" from English to German, the model would receive the sequence ""translate English to German: That is good."" and would be trained to generate ""Das ist gut."". For text classification tasks, the model just predicts a single word matching the target label. For instance, on the MNLI benchmark (Williams et al., 2017) the goal is to determine if a premise implies (""entailment""), contradicts (""contradiction""), or neither (""neutral"") a hypothesis. With our preprocessing, the input sequence becomes ""mnli premise: I hate pigeons. hypothesis: My feelings towards pigeons are filled with animosity."" with the corresponding target word ""entailment"".","To indicate which task the model should execute, we add a task-specific (text) prefix to the original input sequence before feeding it into the model. As an illustration, to direct the model to translate the sentence ""That is good."" from English to German, the model would receive the sequence ""translate English to German: That is good."" and would be trained to produce ""Das ist gut."". For text classification tasks, the model simply predicts a single word matching the target label. For example, on the MNLI benchmark (Williams et al., 2017) the goal is to determine if a premise implies (""entailment""), contradicts (""contradiction""), or neither (""neutral"") a hypothesis. With our preprocessing, the input sequence becomes ""mnli premise: I hate pigeons. hypothesis: My feelings towards pigeons are filled with animosity."" with the corresponding target word ""entailment"".","To indicate the task the model should perform, we prepend a task-specific (text) prefix to the original input sequence before inputting it into the model. For example, to instruct the model to translate the sentence ""That is good."" from English to German, the model would receive the sequence ""translate English to German: That is good."" and would be trained to generate ""Das ist gut."". For text classification tasks, the model simply predicts a single word matching the target label. As an illustration, on the MNLI benchmark (Williams et al., 2017) the goal is to determine if a premise implies (""entailment""), contradicts (""contradiction""), or neither (""neutral"") a hypothesis. With our preprocessing, the input sequence becomes ""mnli premise: I hate pigeons. hypothesis: My feelings towards pigeons are filled with animosity."" with the corresponding target word ""entailment"".",A,1
Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,"Note that an issue arises if our model outputs text on a text classification task that does not correspond to any of the possible labels (for example if the model outputs “hamburger” when the only possible labels for a task were “entailment”, “neutral”, or “contradiction”). In this case, we always count the model’s output as wrong, though we never observed this behavior in any of our trained models. Note that the choice of text prefix used for a given task is essentially a hyperparameter; we found that changing the exact wording of the prefix had limited impact and so did not perform extensive experiments into different prefix choices. ","Be aware that a problem comes up if our model generates text for a text classification assignment that does not match any of the available labels (for instance, if the model outputs ""hamburger"" but the only possible labels for the task were ""entailment"", ""neutral"", or ""contradiction""). In this situation, we always consider the model's output incorrect, although we never saw this occur with any of our trained models. Recognize that the selection of text prefix utilized for a particular task is basically a hyperparameter; we determined that modifying the precise wording of the prefix had minimal effect and therefore did not conduct extensive experiments with different prefix options.","Note that a complication emerges if our model produces text on a text categorization job that does not align with any of the potential tags (for example, if the model generates ""hamburger"" but the only feasible tags for the task were ""entailment"", ""neutral"", or ""contradiction""). In such a case, we invariably deem the model's output fallacious, despite never witnessing this behavior in any of our conditioned models. Acknowledge that the choice of text prefix employed for a given task is fundamentally a hyperparameter; we ascertained that altering the exact verbiage of the prefix had negligible impact and thus did not perform comprehensive experiments on different prefix selections.  ","Understand that an issue materializes if our model formulates text on a text sorting assignment that does not match any of the viable labels (for instance, if the model composes ""hamburger"" however the sole viable labels for the task were ""entailment"", ""neutral"", or ""contradiction""). In said case, we always adjudicate the model's output erroneous, despite never espying this conduct in any of our cultivated models. Appreciate that the picking of text prefix harnessed for a particular task is intrinsically a hyperparameter; we discerned that modifying the precise diction of the prefix had trivial collision and ergo did not enact extensive experiments on discrete prefix choices.",A,1
Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,"A diagram of our text-to-text framework with a few input/output 8 Exploring the Limits of Transfer Learning examples is shown in Figure 1. We provide full examples of preprocessed inputs for every task we studied in Appendix D. Our text-to-text framework follows previous work that casts multiple NLP tasks into a common format: McCann et al. (2018) propose the “Natural Language Decathlon”, a benchmark that uses a consistent question-answering format for a suite of ten NLP tasks. ","An illustration of our text-to-text system with some input/output examples is displayed in Figure 1. We give complete preprocessed input examples for every task we analyzed in Appendix D. Our text-to-text system is based on previous work that puts multiple NLP tasks into a shared format: McCann et al. (2018) present the ""Natural Language Decathlon"", a benchmark that utilizes a steady question-answering format for a collection of ten NLP tasks.","A diagram of our text-to-text framework with a few instances of inputs and outputs is shown in Figure 1. We provide the full pre-processed inputs for each task we examined in Appendix D. Our text-to-text framework uses the same approach as previous work that frames multiple NLP tasks in a common way: McCann et al. (2018) propose the ""Natural Language Decathlon"", a benchmark that employs a consistent question-answering structure for a set of ten NLP tasks.","An illustration of our text-to-text system with some input and output examples is provided in Figure 1. We give the complete pre-processed inputs for every task we studied in Appendix D. Our text-to-text system follows previous work that represents multiple NLP tasks in a shared format: McCann et al. (2018) introduce the ""Natural Language Decathlon"", a benchmark that adopts a steady question-answering design for a collection of ten NLP tasks.",A,1
Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,"The Natural Language Decathlon also stipulates that all models must be multi-task, i.e. are able to simultaneously tackle all of the tasks at once. We instead allow for separately fine-tuning the model on each individual task and use short task prefixes instead of an explicit question-answer format. Radford et al. (2019) evaluate the zero-shot learning capabilities of language models by feeding some input to the model as a prefix and then autoregressively sampling an output. ","The Natural Language Decathlon also states that all models need to be multi-task, meaning they must be capable of handling all the tasks at the same time. However, we permit separately fine-tuning the model on each separate task and utilize short task prefixes rather than an explicit question-answer structure. Radford et al. (2019) assess the zero-shot learning abilities of language models by providing some input to the model as a prefix and then autoregressively generating an output.","The Natural Language Decathlon also necessitates that all models be multi-task, i.e. able to concurrently tackle all the tasks together. But we allow tuning the model independently on each discrete task and employ brief task prefixes instead of a clear question-answer format. Radford et al. (2019) evaluate the zero-shot learning capabilities of language models by feeding some input into the model as a prefix and then autoregressively producing an output.","The Natural Language Decathlon also mandates that all models be multi-task, meaning they must be able to simultaneously handle all the tasks at once. However, we permit separately optimizing the model on each single task and utilize short task prefixes rather than an unambiguous question-answer structure. Radford et al. (2019) assess the zero-shot learning abilities of language models by providing some input to the model as a prefix and then autoregressively generating an output.",A,1
Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,"For example, automatic summarization is done by feeding in a document followed by the text “TL;DR:” (short for “too long, didn’t read”, a common abbreviation) and then the summary is predicted via autoregressive decoding. We mainly consider models that explicitly process an input with an encoder before generating an output with a separate decoder and we focus on transfer learning rather than zero-shot learning. Finally, Keskar et al. (2019b) unify many NLP tasks as “span extraction”, where text corresponding to possible output choices are appended to the input and the model is trained to extract the input span corresponding to the correct choice. ","As an illustration, summarizing text automatically involves inputting a document and then the abbreviation ""TL;DR:"" (meaning ""too long, didn't read""), after which the summary is generated via autoregressive decoding. We primarily examine models that explicitly encode an input before generating an output with a distinct decoder, and we emphasize transfer learning over zero-shot learning. Furthermore, Keskar et al. (2019b) consolidate many NLP tasks into ""span extraction"", where text matching potential output options is added to the input, and the model is conditioned to extract the input span that matches the right choice.","To demonstrate, automated summarization works by feeding in a document followed by the shorthand ""TL;DR:"" (standing for ""too long, didn't read""), and then predicting the summary via autoregressive decoding. Our focus is on models that first encode an input explicitly before generating output with a separate decoder, and we prioritize transfer learning instead of zero-shot learning. Additionally, Keskar et al. (2019b) unify many NLP tasks into ""span extraction"", appending text corresponding to possible output choices to the input, and training the model to extract the input span that matches the correct choice.","As an example, automatic summarization involves inputting a document and the abbreviation ""TL;DR:"" (meaning ""too long, didn't read""), and then generating the summary through autoregressive decoding. We concentrate on models that first encode an input explicitly before producing output with a distinct decoder, and we emphasize transfer learning rather than zero-shot learning. Also, Keskar et al. (2019b) consolidate many NLP tasks into ""span extraction"", adding text corresponding to potential output options to the input, and conditioning the model to extract the input span matching the right choice.",A,1
Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,"In contrast, our framework also allows for generative tasks like machine translation and abstractive summarization where it is not possible to enumerate all possible output choices. We were able to straightforwardly cast all of the tasks we considered into a text-to-text format with the exception of STS-B, which is a regression task where the goal is to predict a similarity score between 1 and 5. We found that most of these scores were annotated in increments of 0.2, so we simply rounded any score to the nearest increment of 0.2 and converted the result to a literal string representation of the number (e.g. the floating-point value 2.57 would be mapped to the string “2.6”). ","Conversely, our system also enables generative tasks such as language translation and summarizing texts where one cannot list all potential outputs. We could easily reformat all the jobs we looked at into a text-to-text structure except for STS-B, which is a regression task aiming to predict a similarity score from 1 to 5. We noticed most scores were labeled in 0.2 increments, so we just rounded any score to the nearest 0.2 increment and turned that into an exact string version of the number (for instance, 2.57 becomes the string ""2.6"").","In contrast, our approach allows for creative tasks like translating languages and summarizing texts where you can't enumerate all feasible outputs. We were able to effortlessly transform all the tasks we examined into a text-to-text format except for STS-B, which is a regression task trying to predict a closeness score from 1 to 5. We saw most scores were annotated in steps of 0.2, so we simply rounded any score to the nearest 0.2 step and changed that to a literal string form of the number (for example, 2.57 becomes the string ""2.6"").","On the other hand, our system permits generative tasks such as machine translation and abstractive summarization where you cannot list all possible outputs. We could straightforwardly recast all of the tasks we looked at into a text-to-text format except for STS-B, which is a regression task attempting to predict a similarity score from 1 to 5. We noticed most scores were marked in increments of 0.2, so we simply rounded any score to the nearest 0.2 increment and transformed that into an exact string version of the number (so 2.57 becomes the string ""2.6"").",A,1
Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,"At test time, if the model outputs a string corresponding to a number between 1 and 5, we convert it to a floating-point value; otherwise, we treat the model’s prediction as incorrect. This effectively recasts the STS-B regression problem as a 21-class classification problem. Separately, we also convert the Winograd tasks (WNLI from GLUE, WSC from SuperGLUE, and the DPR data set we add to SuperGLUE) into a simpler format that is more amenable to the text-to-text framework. Examples from the Winograd tasks consist of a text passage containing an ambiguous pronoun that could refer to more than one of the noun phrases in the passage. ","During evaluation, if the model generates a string matching a number from 1 to 5, we turn it into a float; else, we see the prediction as wrong. This essentially changes the STS-B regression issue into a 21-class classification one. Also, we convert the Winograd tasks (WNLI from GLUE, WSC from SuperGLUE, and the DPR dataset we append to SuperGLUE) into a simpler form more suitable for the text-to-text framework. Instances from the Winograd tasks have a text excerpt with an ambiguous pronoun that could refer to multiple noun phrases in the excerpt.","When testing, if the model outputs a string representing a number between 1 and 5, we transform it into a floating-point value; otherwise, we consider the model's guess incorrect. This effectively alters the STS-B regression problem into a 21-class classification problem. Independently, we also change the Winograd tasks (WNLI from GLUE, WSC from SuperGLUE, and the DPR data set we add to SuperGLUE) into a more straightforward format more compatible with the text-to-text framework. Examples from the Winograd tasks contain a text passage with an ambiguous pronoun that could refer to more than one noun phrase in the passage.","During evaluation, if the model generates a string matching a number from 1 to 5, we convert it to a float; otherwise, we deem the prediction false. This effectively turns the STS-B regression issue into a 21-class classification one. Separately, we also modify the Winograd tasks (WNLI from GLUE, WSC from SuperGLUE, and the DPR dataset we append to SuperGLUE) into a simpler form more amenable to the text-to-text system. Instances from the Winograd tasks have a text excerpt containing an ambiguous pronoun that could refer to multiple noun phrases in the excerpt.",A,1
Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,"For example, the passage might be “The city councilmen refused the demonstrators a permit because they feared violence.”, which contains the ambiguous pronoun “they” that could refer to “city councilmen” or “demonstrators”. We cast the WNLI, WSC, and DPR tasks as text-to-text problems by highlighting the ambiguous pronoun in the text passage and asking the model to predict the noun that it refers to. The example mentioned above would be transformed to the input “The city councilmen refused the demonstrators a permit because *they* feared violence.” and the model would be trained to predict the target text “The city councilmen”. ","The city officials denied the protesters a license because the officials were scared of unrest. This sentence has the vague pronoun ""they"" which could refer to either ""city officials"" or ""protesters"". We framed the WNLI, WSC, and DPR tasks as text-to-text problems by underlining the unclear pronoun in the passage and prompting the model to foresee the noun that the pronoun refers to. The instance stated earlier would be changed to the input ""The city officials denied the protesters a license because *they* were scared of unrest."" and the model would be taught to generate the target text ""The city officials"".","The members of the city council refused to give the demonstrators a permit because the council members were afraid of violence. This contains the ambiguous pronoun ""they"" which could mean either ""members of the city council"" or ""demonstrators"". We presented the WNLI, WSC, and DPR tasks as text-to-text challenges by highlighting the vague pronoun in the passage and asking the model to predict the noun that it refers to. The example mentioned earlier would become the input ""The members of the city council refused to give the demonstrators a permit because *they* were afraid of violence."" and the model would be trained to produce the target text ""The members of the city council"".","The city council denied the protestors a license to demonstrate because the council feared violence would occur. This has the unclear pronoun ""they"" which could refer to the ""city council"" or the ""protestors"". We framed the WNLI, WSC, and DPR tasks as text-to-text problems by underlining the ambiguous pronoun in the passage and prompting the model to predict the noun that it refers to. The example stated before would become the input ""The city council denied the protestors a license to demonstrate because *they* feared violence would occur."" and the model would be trained to generate the target text ""The city council"".",A,1
Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,"For WSC, examples contain the passage, the ambiguous pronoun, a candidate noun, and a True/False label reflecting whether the candidate matches the pronoun (ignoring any articles). We only train on examples with a “True” label since we do not know the correct noun targets for examples with a “False” label. For evaluation, we assign a “True” label if 9 Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu the words in the model’s output are a subset of the words in the candidate noun phrase (or vice versa) and assign a “False” label otherwise. ","The WSC examples have the passage, the unclear pronoun, a possible noun, and a True/False tag indicating if the noun matches the pronoun (not considering any articles). We only teach with samples having a ""True"" tag since we don't know the right noun choices for samples with a ""False"" tag. For assessing, we assign a ""True"" tag if the words in the model's result are a subset of the words in the noun phrase candidate (or vice versa) and we assign a ""False"" tag otherwise.","For WSC, the examples include the passage, the ambiguous pronoun, a possible noun, and a True/False label showing if the noun matches the pronoun (disregarding any articles). We only train using examples with a ""True"" label because we don't know the correct noun options for examples with a ""False"" label. For evaluation, we give a ""True"" label if the words in the model's output are a subset of the words in the candidate noun phrase (or vice versa) and we give a ""False"" label otherwise.","In the WSC examples, there is the passage, the unclear pronoun, a candidate noun, and a True/False mark indicating if the noun matches the pronoun (ignoring any articles). We only train with samples that have a ""True"" mark since we don't know the accurate noun selections for samples with a ""False"" mark. For assessing, we give a ""True"" mark if the words in the model's output are a subset of the words in the candidate noun phrase (or vice versa) and we give a ""False"" mark otherwise.",A,1
Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,"This removes roughly half of the WSC training set, but the DPR data set adds about 1,000 pronoun resolution examples. Examples from DPR are annotated with the correct referent noun, making it easy to use this data set in the format listed above. The WNLI training and validation sets have a significant overlap with the WSC training set. To avoid leaking validation examples into our training data (a particular issue in the multi-task experiments of Section 3.5.2), we therefore never train on WNLI and never report results on the WNLI validation set. ","This takes away around half of the WSC training set, however the DPR data set provides about 1,000 pronoun resolution instances. Samples from DPR have the accurate referent noun annotated, making it straightforward to utilize this data set in the format described above. There is considerable overlap between the training and validation sets of WNLI and WSC. To prevent validation examples from entering our training data (a specific problem in the multi-task experiments of Section 3.5.2), we thus do not train on WNLI and do not document results on the WNLI validation set.","This eliminates roughly 50% of the examples in the WSC training set, but the DPR data set contributes around 1,000 pronoun resolution cases. DPR examples have the right referring noun marked, so it's easy to use this data set in the format shown above. The WNLI training and validation sets share a lot of overlap with the WSC training set. To avoid validation instances getting into our training data (an issue especially in the multi-task experiments of Section 3.5.2), we therefore do not train on WNLI and do not report on the WNLI validation set.  ","This takes out about half of the training examples from WSC, however the DPR data set provides approximately 1,000 instances of pronoun resolution. DPR samples have the correct reference noun labeled, which makes it simple to utilize this data set in the format described above. There is significant overlap between the training and validation sets of WNLI and those of WSC. In order to prevent validation examples from entering our training data (a particular problem in the multi-task experiments of Section 3.5.2), we thus do not use WNLI for training and do not show results on the WNLI validation set.",A,1
Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,"Omitting results on the WNLI validation set is standard practice (Devlin et al., 2018) due to the fact that it is “adversarial” with respect to the training set, i.e. validation examples are all slightly-perturbed versions of training examples with the opposite label. As such, we do not include WNLI in the average GLUE score whenever we report on the validation set (all sections except Section 3.7 where results are presented on the test sets). Converting examples from WNLI to the “referent noun prediction” variant described above is a little more involved; we describe this process in Appendix B. ","Leaving out results on the WNLI validation set is common practice (Devlin et al., 2018) because the validation set is intentionally made to be different from the training set, with validation examples being slightly changed versions of training examples but with the opposite label. That's why we don't include WNLI when calculating the average GLUE score whenever we report results on the validation set (except in Section 3.7 where we show test set results). Transforming examples from WNLI to the ""referent noun prediction"" version described above requires some extra work; we explain how we do this in Appendix B.","It is standard to exclude results on the WNLI validation set (Devlin et al., 2018) since the validation examples are purposefully ""adversarial"" compared to the training examples - they are minimally altered versions of training examples but with the opposite label. Therefore, we leave out WNLI when reporting the average GLUE score on validation sets (in all sections other than Section 3.7 where we show test set results). Modifying examples from WNLI to the ""referent noun prediction"" variant discussed above takes some extra effort; we elucidate this process in Appendix B.  ","Omitting outcomes on the WNLI validation collection is common convention (Devlin et al., 2018) because the validation entries are intentionally ""contrarian"" relative to the training entries - they are slightly tweaked versions of training entries except with the flipped label. Hence, we exclude WNLI when stating the average GLUE tally on validation collections (in all portions bar Section 3.7 where we exhibit test collection outcomes). Altering exemplars from WNLI to the ""referent noun prognostication"" variant covered above necessitates additional exertion; we expound this workflow in Appendix B.",A,1
Faster R-CNN Towards Real-Time Object Detection with Region Proposal Networks,"State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet [7] and Fast R-CNN [5] have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully-convolutional network that simultaneously predicts object bounds and objectness scores at each position.","The most advanced object recognition systems rely on region proposal methods to predict where objects are located. Improvements like SPPnet [7] and Fast R-CNN [5] have made these detection systems faster, revealing that generating region proposals is now the slowest part. Here we present a Region Proposal Network (RPN) that uses the same full-image convolutional features as the detection network, allowing region proposals that are nearly free computationally. An RPN is a fully convolutional neural network that concurrently predicts object locations and objectness scores at every location.","State-of-the-art object detection algorithms need region proposal techniques to hypothesize where objects are. Innovations such as SPPnet [7] and Fast R-CNN [5] have sped up these detection algorithms, showing that region proposal is now the bottleneck. We introduce a Region Proposal Network (RPN) that shares convolutional features across the whole image with the detection network, enabling region proposals that are almost free. An RPN is a fully convolutional network that simultaneously predicts bounding boxes and objectness likelihoods at each spot.","Cutting-edge object recognition models require region proposal methods to guess object positions. Enhancements including SPPnet [7] and Fast R-CNN [5] have accelerated these detection models, revealing region proposal as the current limitation. Here we present a Region Proposal Network (RPN) that leverages the same full-image convolutional features as the detection network, allowing nearly costless region proposals. An RPN is a fully convolutional neural net that concurrently forecasts object boxes and objectness scores everywhere.",A,1
Faster R-CNN Towards Real-Time Object Detection with Region Proposal Networks,"RPNs are trained end-to-end to generate high quality region proposals, which are used by Fast R-CNN for detection. With a simple alternating optimization, RPN and Fast R-CNN can be trained to share convolutional features. For the very deep VGG-16 model [19], our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007 (73.2% mAP) and 2012 (70.4% mAP) using 300 proposals per image.","RPNs are educated from start to finish to make high-caliber area recommendations, which Fast R-CNN utilizes for identification. Through basic back and forth enhancement, RPN and Fast R-CNN can be prepared to share convolutional highlights. For the exceptionally profound VGG-16 model [19], our identification framework has an edge rate of 5fps (including all means) on a GPU, while accomplishing first class question location precision on PASCAL VOC 2007 (73.2% mAP) and 2012 (70.4% mAP) utilizing 300 recommendations per picture.","RPNs are taught completely to produce top notch region proposals, which Fast R-CNN uses for detection. With straightforward reciprocal streamlining, RPN and Fast R-CNN can be prepared to share convolutional features. For the profoundly deep VGG-16 model [19], our identification framework has an edge rate of 5fps (incorporating all advances) on a GPU, while achieving best in class protest identification accuracy on PASCAL VOC 2007 (73.2% mAP) and 2012 (70.4% mAP) utilizing 300 proposals per image.  ","RPNs are developed start to finish to generate high-quality region suggestions, which Fast R-CNN employs for recognition. Through simple back-and-forth enhancement, RPN and Fast R-CNN can be trained to share convolutional characteristics. For the very deep VGG-16 model [19], our detection system has a frame rate of 5fps (covering all steps) on a GPU, while attaining state-of-the-art object recognition accuracy on PASCAL VOC 2007 (73.2% mAP) and 2012 (70.4% mAP) using 300 proposals per image.",A,1
Faster R-CNN Towards Real-Time Object Detection with Region Proposal Networks,"Recent advances in object detection are driven by the success of region proposal methods (e.g., [22]) and region-based convolutional neural networks (R-CNNs) [6]. Although region-based CNNs were computationally expensive as originally developed in [6], their cost has been drastically reduced thanks to sharing convolutions across proposals [7, 5]. The latest incarnation, Fast R-CNN [5], achieves near real-time rates using very deep networks [19], when ignoring the time spent on region proposals. Now, proposals are the computational bottleneck in state-of-the-art detection systems.","The latest progress in recognizing objects in images is enabled by the effectiveness of techniques that suggest image regions (for example, [22]) and neural networks that examine image regions (R-CNNs) [6]. While R-CNNs were originally very slow [6], their speed has been greatly improved by reusing computations across regions [7, 5]. The most recent version, Fast R-CNN [5], can operate nearly in real time using very deep neural networks [19], if you don't count the time taken to generate region proposals. So now, creating region proposals is the computational limitation in cutting-edge object detection systems.","Recent advancements in detecting objects in images have been driven by the triumph of methods that propose image regions (like [22]) and convolutional neural networks that analyze proposed regions (region-based CNNs or R-CNNs) [6]. Although R-CNNs were very computationally expensive originally [6], their cost has plummeted thanks to sharing neural network computations between proposed regions [7, 5]. The newest version, Fast R-CNN [5], can run almost in real time using very deep neural networks [19], excluding the time for generating region proposals. Therefore, generating region proposals is now the computational bottleneck in state-of-the-art object detection systems.  ","The latest improvements in recognizing objects in images are enabled by the success of techniques that suggest areas of the image to analyze (e.g. [22]) and convolutional neural networks that focus on proposed image regions (region-based CNNs or R-CNNs) [6]. While R-CNNs were initially extremely computationally expensive [6], their cost has been drastically cut by reusing neural network computations across proposed regions [7, 5]. The most modern version, Fast R-CNN [5], can operate nearly in real time using very deep neural networks [19], not counting the time for proposing regions. As a result, proposing regions is now the computational limitation in cutting-edge object detection systems.",A,1
Faster R-CNN Towards Real-Time Object Detection with Region Proposal Networks,"Region proposal methods typically rely on inexpensive features and economical inference schemes. Selective Search (SS) [22], one of the most popular methods, greedily merges superpixels based on engineered low-level features. Yet when compared to efficient detection networks [5], Selective Search is an order of magnitude slower, at 2s per image in a CPU implementation. EdgeBoxes [24] currently provides the best tradeoff between proposal quality and speed, at 0.2s per image. Nevertheless, the region proposal step still consumes as much running time as the detection network.","Region proposal techniques usually depend on cheap characteristics and affordable deduction plans. Selective Search (SS) [22], one of the most well-known techniques, avariciously combines superpixels based on engineered low-level characteristics. However, when compared to efficient detection networks [5], Selective Search is 10 times slower, at 2s per image in a CPU execution. EdgeBoxes [24] currently gives the best compromise between proposal quality and velocity, at 0.2s per image. Still, the region proposal step still uses up as much running time as the detection network.","Region proposal algorithms typically use basic features and simple inference methods. Selective Search (SS) [22], a very common algorithm, greedily merges superpixels using engineered low-level features. But compared to fast detection networks [5], Selective Search is 10 times slower, taking 2s per image on a CPU. EdgeBoxes [24] currently has the best balance between proposal quality and speed, at 0.2s per image. However, the region proposal step still takes up as much running time as the detection network.  ","Region proposal techniques usually utilize inexpensive characteristics and cost-effective deduction schemes. Selective Search (SS) [22], one of the most popular techniques, avariciously integrates superpixels based on engineered low-level characteristics. However, compared to efficient detection networks [5], Selective Search is an order of magnitude slower, at 2s per image in a CPU implementation. EdgeBoxes [24] currently provides the best compromise between proposal quality and rapidity, at 0.2s per image. Nevertheless, the region proposal step still consumes as much running time as the detection network.",A,1
Faster R-CNN Towards Real-Time Object Detection with Region Proposal Networks,"One may note that fast region-based CNNs take advantage of GPUs, while the region proposal methods used in research are implemented on the CPU, making such runtime comparisons inequitable. An obvious way to accelerate proposal computation is to re-implement it for the GPU. This may be an effective engineering solution, but re-implementation ignores the down-stream detection network and therefore misses important opportunities for sharing computation.","It can be observed that quick region-based CNNs utilize GPUs, however the region proposal techniques used in studies are executed on the CPU, resulting in unfair runtime comparisons. An apparent approach to speed up proposal calculation is to re-implement it for the GPU. This could be an effective engineering fix, but re-implementation overlooks the down-stream detection network and thus misses critical chances for sharing computation.","One can notice that fast region-based CNNs leverage GPUs, while the region proposal algorithms employed in research are built on the CPU, making such runtime benchmarks biased. A clear way to accelerate proposal processing is to re-code it for the GPU. This might be a good engineering solution, but re-coding disregards the down-stream detection model and thereby loses out on key opportunities for sharing computation.  ","It can be seen that quick region-based CNNs make use of GPUs, whereas the region proposal procedures utilized in experiments are carried out on the CPU, resulting in inequitable runtime analyses. An obvious approach to speed up proposal processing is to re-develop it for the GPU. This could be an effective engineering remedy, however re-developing overlooks the down-stream detection architecture and thus fails to capitalize on vital chances for sharing computation.",A,1
Faster R-CNN Towards Real-Time Object Detection with Region Proposal Networks,"Our observation is that the convolutional (conv) feature maps used by region-based detectors, like Fast R-CNN, can also be used for generating region proposals. On top of these conv features, we construct RPNs by adding two additional conv layers: one that encodes each conv map position into a short (e.g., 256-d) feature vector and a second that, at each conv map position, outputs an objectness score and regressed bounds for k region proposals relative to various scales and aspect ratios at that location (k = 9 is a typical value).","We noticed that the convolutional feature maps utilized by region-based detectors, such as Fast R-CNN, can be leveraged to produce region proposals as well. We build RPNs on top of these convolutional features by appending two extra convolutional layers: one layer that transforms each convolutional map position into a compact (for example, 256-dimensional) feature vector and another layer that, for each convolutional map position, generates an objectness score and predicted bounds for k region proposals compared to various scales and aspect ratios at that spot (k = 9 is a typical number).","Our observation is that the convolutional feature maps employed by region-based detectors, like Fast R-CNN, can also be harnessed to create region proposals. We construct RPNs on top of these convolutional features by introducing two more convolutional layers: one layer that encodes each convolutional map location into a short (for instance, 256-dimensional) feature vector and another layer that, for each convolutional map location, outputs an objectness score and bounded regions for k region proposals relative to various scales and aspect ratios at that point (k = 9 is a common value). ","We noticed that the convolutional feature maps used by region-based detectors, such as Fast R-CNN, can also be utilized to generate region proposals. On top of these convolutional features, we build RPNs by appending two extra convolutional layers: one layer that transforms each convolutional map position into a compact (for example, 256-dimensional) feature vector and a second layer that, at each convolutional map position, produces an objectness score and predicted bounds for k region proposals compared to various scales and aspect ratios at that location (k = 9 is a typical number).",A,1
Faster R-CNN Towards Real-Time Object Detection with Region Proposal Networks,"Our RPNs are thus a kind of fully-convolutional network (FCN) [14] and they can be trained end-to-end specifically for the task for generating detection proposals. To unify RPNs with Fast R-CNN [5] object detection networks, we propose a simple training scheme that alternates between fine-tuning for the region proposal task and then fine-tuning for object detection, while keeping the proposals fixed. This scheme converges quickly and produces a unified network with conv features that are shared between both tasks.","Our RPN models are a type of fully convolutional network (FCN) [14] and they can be trained from end to end particularly for generating object proposal regions. To combine RPNs with Fast R-CNN [5] object detection models, we suggest a simple training method that switches between fine-tuning for proposing regions and then fine-tuning for object detection, while keeping the proposals unchanged. This method converges rapidly and results in a unified network with convolutional features that are shared between both tasks.","Our RPN architectures are a form of fully convolutional networks (FCNs) [14] and they are trainable in an end-to-end manner designed specifically for producing object proposal boxes. To integrate RPNs with Fast R-CNN [5] object detection architectures, we put forth a straightforward training procedure that toggles between optimizing for the proposal generation objective and then optimizing for object detection, with the proposals fixed. This procedure quickly converges and yields a unified network with conv filters that are mutual across both objectives.","Our RPN designs are a variety of fully convolutional neural networks (FCNNs) [14] and they are able to be trained from start to finish particularly for generating object proposal boxes. To combine RPNs with Fast R-CNN [5] object detection designs, we present a simple training methodology that alternates between tuning for the proposal generation task and then tuning for object detection, while keeping the proposals stationary. This methodology rapidly converges and produces a unified network with convolutional filters that are shared between both tasks.",A,1
Faster R-CNN Towards Real-Time Object Detection with Region Proposal Networks,"We evaluate our method on the PASCAL VOC detection benchmarks [4], where RPNs with Fast R-CNNs produce detection accuracy better than the strong baseline of Selective Search with Fast R-CNNs. Meanwhile, our method waives nearly all computational burdens of SS at test-time—the effective running time for proposals is just 10 milliseconds. Using the expensive very deep models of [19], our detection method still has a frame rate of 5fps (including all steps) on a GPU, and thus is a practical object detection system in terms of both speed and accuracy (73.2% mAP on PASCAL VOC 2007 and 70.4% mAP on 2012).","We assess our approach using the PASCAL VOC detection benchmarks [4], where RPNs paired with Fast R-CNNs yield superior detection accuracy over the robust baseline of Selective Search with Fast R-CNNs. Furthermore, our approach removes nearly all computational expenses of SS during testing—the effective runtime for proposals is a mere 10 milliseconds. Utilizing the very computationally expensive models of [19], our detection approach still accomplishes a frame rate of 5fps (comprising all steps) on a GPU, thus constituting a viable object detection system regarding both velocity and precision (73.2% mAP on PASCAL VOC 2007 and 70.4% mAP on 2012).","We evaluate our technique on the PASCAL VOC detection benchmarks [4], where RPNs combined with Fast R-CNNs produce detection performance superior to the strong standard of Selective Search plus Fast R-CNNs. Meanwhile, our technique eliminates nearly all computational burdens of SS during testing—the real running time for proposals is only 10 milliseconds. Employing the very deep, expensive models of [19], our detection technique still accomplishes a frame rate of 5fps (including all procedures) on a GPU, and hence is a practical object detection system in terms of both quickness and accuracy (73.2% mAP on PASCAL VOC 2007 and 70.4% mAP on 2012).","We analyze our approach utilizing the PASCAL VOC detection benchmarks [4], where RPNs working with Fast R-CNNs generate detection results surpassing the robust baseline of Selective Search integrated with Fast R-CNNs. Furthermore, our approach removes nearly all computational loads of SS during testing—the true runtime for proposals is only 10 milliseconds. Leveraging the extremely deep, computationally costly models of [19], our detection approach still realizes a frame rate of 5fps (encompassing all steps) on a GPU, and thus constitutes a feasible object detection system regarding both speed and precision (73.2% mAP on PASCAL VOC 2007 and 70.4% mAP on 2012).",A,1
Faster R-CNN Towards Real-Time Object Detection with Region Proposal Networks,"Several recent papers have proposed ways of using deep networks for locating class-specific or class agnostic bounding boxes [21, 18, 3, 20]. In the OverFeat method [18], a fully-connected (fc) layer is trained to predict the box coordinates for the localization task that assumes a single object. The fc layer is then turned into a conv layer for detecting multiple class-specific objects. The MultiBox methods [3, 20] generate region proposals from a network whose last fc layer simultaneously predicts multiple (e.g., 800) boxes, which are used for R-CNN [6] object detection.","A number of latest research articles have put forward techniques for utilizing deep neural networks to find bounding boxes specific to a class or irrespective of the class [21, 18, 3, 20]. In OverFeat [18], a fully connected layer is educated to anticipate the coordinates of the box for localization, assuming just one object. This fully connected layer is then transformed into a convolutional layer to detect multiple objects of particular classes. The MultiBox approaches [3, 20] produce region proposals from a network whose final fully connected layer concurrently predicts many (for instance 800) boxes, which are utilized for R-CNN [6] object detection.","Several recent publications have suggested methods for leveraging deep learning models to identify bounding boxes for certain classes or all classes [21, 18, 3, 20]. OverFeat [18] trains a fully-connected layer to predict box coordinates for localization, presuming a single object. This fully-connected layer is converted to a convolutional layer for detecting multiple class-specific objects. MultiBox [3, 20] generates region proposals from a network whose final fully-connected layer simultaneously predicts many (e.g. 800) boxes, used for R-CNN [6] object detection.  ","A number of latest papers have put forward techniques to employ deep neural networks for finding bounding boxes for specific classes or all classes [21, 18, 3, 20]. In OverFeat [18], a fully connected layer is trained to forecast the coordinates of the box for localization, with the assumption of one object. This fully connected layer is then transformed into a convolutional layer for spotting multiple objects of certain classes. The MultiBox methods [3, 20] produce region proposals from a network whose final fully connected layer at the same time predicts numerous (for example 800) boxes, which are utilized for R-CNN [6] object detection.",A,1
Faster R-CNN Towards Real-Time Object Detection with Region Proposal Networks,"Their proposal network is applied on a single image or multiple large image crops (e.g., 224×224) [20]. We discuss OverFeat and MultiBox in more depth later in context with our method. Shared computation of convolutions [18, 7, 2, 5] has been attracting increasing attention for efficient, yet accurate, visual recognition. The OverFeat paper [18] computes conv features from an image pyramid for classification, localization, and detection. Adaptively-sized pooling (SPP) [7] on shared conv feature maps is proposed for efficient region-based object detection [7, 16] and semantic segmentation [2]. Fast R-CNN [5] enables end-to-end detector training on shared conv features and shows compelling accuracy and speed.","Their suggested design is implemented on either a sole picture or multiple large cropped images (for instance, 224x224) [20]. We will examine OverFeat and MultiBox more thoroughly later when comparing them to our approach. The technique of sharing computations for convolutions [18, 7, 2, 5] has been gaining interest due to its ability to enable efficient yet precise visual recognition. The OverFeat paper [18] produces convolution features from an image pyramid which are then used for classification, localization, and detection. Adaptive-sized pooling (SPP) [7] on shared convolution feature maps is presented for efficient region-based object detection [7, 16] and semantic segmentation [2]. Fast R-CNN [5] allows end-to-end detector training on shared convolution features and demonstrates compelling accuracy and speed.","Their proposed architecture is implemented on either a single image or multiple large cropped images (for example, 224x224) [20]. We will discuss OverFeat and MultiBox in more detail later when comparing them to our method. The technique of sharing computations of convolutions [18, 7, 2, 5] has been attracting growing attention due to its ability to enable efficient yet accurate visual recognition. The OverFeat paper [18] generates convolution features from an image pyramid which are then used for classification, localization, and detection. Adaptively-sized pooling (SPP) [7] on shared convolution feature maps is proposed for efficient region-based object detection [7, 16] and semantic segmentation [2]. Fast R-CNN [5] permits end-to-end detector training on shared convolution features and exhibits compelling accuracy and speed.","Their suggested system is deployed on either a single image or multiple large cropped images (such as 224x224) [20]. We will examine OverFeat and MultiBox more thoroughly later when contrasting them with our method. The technique of sharing computations of convolutions [18, 7, 2, 5] has been attracting increasing attention due to its ability to allow efficient yet precise visual recognition. The OverFeat paper [18] produces convolution features from an image pyramid which are then utilized for classification, localization, and detection. Adaptively-sized pooling (SPP) [7] on shared convolution feature maps is presented for efficient region-based object detection [7, 16] and semantic segmentation [2]. Fast R-CNN [5] enables end-to-end detector training on shared convolution features and displays compelling accuracy and speed.",A,1
Faster R-CNN Towards Real-Time Object Detection with Region Proposal Networks,"A Region Proposal Network (RPN) takes an image (of any size) as input and outputs a set of rectangular object proposals, each with an objectness score.1 We model this process with a fully convolutional network [14], which we describe in this section. Because our ultimate goal is to share computation with a Fast R-CNN object detection network [5], we assume that both nets share a common set of conv layers. In our experiments, we investigate the Zeiler and Fergus model [23] (ZF), which has 5 shareable conv layers and the Simonyan and Zisserman model [19] (VGG), which has 13 shareable conv layers.","A Region Proposal Network (RPN) accepts an image (of any dimension) as input and generates a collection of rectangular object proposals, each with an objectness score. We represent this process with a fully convolutional neural network [14], which we elucidate in this section. Because our final aim is to share computation with a Fast R-CNN object detection network [5], we presume that both networks share a common set of convolutional layers. In our experiments, we analyze the Zeiler and Fergus model [23] (ZF), which has 5 shareable convolutional layers and the Simonyan and Zisserman model [19] (VGG), which has 13 shareable convolutional layers.","A Region Proposal Network (RPN) takes an image (any size) and outputs rectangular region proposals, each with a score indicating if it contains an object. We implement this with a fully convolutional network [14], described here. Our goal is to share computation with Fast R-CNN [5], so we assume both networks share convolutional layers. We test the Zeiler and Fergus [23] model (ZF), with 5 shareable convolutional layers, and the Simonyan and Zisserman [19] model (VGG), with 13 shareable convolutional layers.  ","A Region Proposal Network (RPN) inputs an image (any dimensions) and generates rectangular object proposals, each scored for object presence. We model this with a fully convolutional network [14], detailed here. Our aim is computational sharing with Fast R-CNN [5], so both networks share convolutional layers. We evaluate the Zeiler and Fergus [23] model (ZF), with 5 shareable convolutional layers, and the Simonyan and Zisserman [19] model (VGG), with 13 shareable convolutional layers.",A,1
Faster R-CNN Towards Real-Time Object Detection with Region Proposal Networks,"To generate region proposals, we slide a small network over the conv feature map output by the last shared conv layer. This network is fully connected to an n × n spatial window of the input conv feature map. Each sliding window is mapped to a lower-dimensional vector (256-d for ZF and 512-d for VGG). This vector is fed into two sibling fully-connected layers—a box-regression layer (reg) and a box-classification layer (cls). We use n = 3 in this paper, noting that the effective receptive field on the input image is large (171 and 228 pixels for ZF and VGG, respectively).","In order to produce region proposals, we move a small neural network across the convolutional feature map that is output by the final shared convolutional layer. This network is fully connected to an n x n spatial window of the input convolutional feature map. Each sliding window is transformed into a lower-dimensional vector (256-d for ZF and 512-d for VGG). This vector is input into two fully-connected sibling layers - one for box regression (reg) and one for box classification (cls). We utilize n = 3 in this work, observing that the effective receptive field on the input image is large (171 and 228 pixels for ZF and VGG, respectively).","To create region proposals, we pass a small network across the conv feature map produced by the last shared conv layer. This network is fully linked to an n x n spatial window of the input conv feature map. Every sliding window is converted to a lower-dimensional vector (256-d for ZF and 512-d for VGG). This vector is provided to two related fully-connected layers - a box-regression layer (reg) and a box-classification layer (cls). We employ n = 3 here, noting the effective receptive field on the input image is substantial (171 and 228 pixels for ZF and VGG, respectively).  ","In order to generate region proposals, we move a small network over the convolutional feature map output by the final shared convolutional layer. This network is fully connected to an n x n spatial portion of the input convolutional feature map. Each sliding window is transformed into a lower-dimensional vector (256-d for ZF and 512-d for VGG). This vector is input into two sibling fully-connected layers - one for box regression (reg) and one for box classification (cls). We use n = 3 here, observing that the effective receptive field on the input image is large (171 and 228 pixels for ZF and VGG, respectively).",A,1
Faster R-CNN Towards Real-Time Object Detection with Region Proposal Networks,"An important property of our approach is that it is translation invariant, both in terms of the anchors and the functions that compute proposals relative to the anchors. As a comparison, the MultiBox method [20] uses k-means to generate 800 anchors, which are not translation invariant. If one translates an object in an image, the proposal should translate and the same function should be able to predict the proposal in either location. Moreover, because the MultiBox anchors are not translation invariant, it requires a (4+1)×800-dimensional output layer, whereas our method requires a (4+2)×9-dimensional output layer.","A key aspect of our method is that it does not change based on translations, both for the anchors and the functions that generate proposals related to the anchors. In contrast, the MultiBox approach [20] utilizes k-means to produce 800 anchors, which vary with translations. If an object in an image is moved, the proposal should also move and the same function ought to be capable of predicting the proposal in either place. Furthermore, as the MultiBox anchors are sensitive to translations, it necessitates a (4+1)×800-dimensional output layer, while our approach only requires a (4+2)×9-dimensional output layer.","An essential characteristic of our technique is that it remains unaltered under translations, for both the anchors and the functions that create suggestions relative to those anchors. On the other hand, the MultiBox system [20] makes use of k-means to construct 800 anchors, which change with translations. If an object in a picture is shifted, the proposal should be shifted as well, and the same function should be able to forecast the proposal in either spot. Additionally, since the MultiBox anchors vary with translations, it needs a (4+1)×800-dimensional output layer, while our method just needs a (4+2)×9-dimensional output layer.  ","A vital property of our methodology is that it does not vary with translations, for both the anchors and the functions that produce recommendations in relation to those anchors. By contrast, the MultiBox approach [20] employs k-means to build 800 anchors, which are sensitive to translations. If an object in an image is displaced, the proposal ought to be displaced too, and the identical function should be capable of predicting the proposal in either location. Moreover, as the MultiBox anchors change with translations, it necessitates a (4+1)×800-dimensional output layer, whereas our technique only calls for a (4+2)×9-dimensional output layer.",A,1
Faster R-CNN Towards Real-Time Object Detection with Region Proposal Networks,"For training RPNs, we assign a binary class label (of being an object or not) to each anchor. We assign a positive label to two kinds of anchors: (i) the anchor/anchors with the highest Intersection-over-Union (IoU) overlap with a ground-truth box, or (ii) an anchor that has an IoU overlap higher than 0.7 with any ground-truth box. Note that a single ground-truth box may assign positive labels to multiple anchors. We assign a negative label to a non-positive anchor if its IoU ratio is lower than 0.3 for all ground-truth boxes. Anchors that are neither positive nor negative do not contribute to the training objective.","When teaching RPNs, we give each anchor a binary class tag (of being an item or not). We give a positive tag to two types of anchors: (i) the anchor/anchors with the greatest Intersection-over-Union (IoU) overlap with a true box, or (ii) an anchor with an IoU overlap above 0.7 with any true box. Note that one true box can assign positive tags to multiple anchors. We give a negative tag to a non-positive anchor if its IoU ratio is under 0.3 for all true boxes. Anchors that are neither positive nor negative do not add to the training goal.","For instructing RPNs, we designate a binary class description (of being a thing or not) to every anchor. We assign a affirmative description to two varieties of anchors: (i) the anchor/anchors with the topmost Intersection-over-Union (IoU) overlap with a factual box, or (ii) an anchor that has an IoU overlap superior to 0.7 with any factual box. Note that a single factual box can assign affirmative descriptions to multiple anchors. We assign a negative description to a non-affirmative anchor if its IoU ratio is inferior to 0.3 for all factual boxes. Anchors that are neither affirmative nor negative do not play a role in the training objective.  ","When coaching RPNs, we attach a binary class characterization (of being an article or not) to each anchor. We fasten a approving characterization to two forms of anchors: (i) the anchor/anchors with the greatest Intersection-over-Union (IoU) overlap with a authentic box, or (ii) an anchor that has an IoU overlap higher than 0.7 with any authentic box. Note that a single authentic box can assign approving characterizations to multiple anchors. We fasten a disapproving characterization to a non-approving anchor if its IoU ratio is lower than 0.3 for all authentic boxes. Anchors that are neither approving nor disapproving do not add to the training aim.",A,1
Faster R-CNN Towards Real-Time Object Detection with Region Proposal Networks,"Nevertheless, our method achieves bounding-box regression by a different manner from previous feature-map-based methods [7, 5]. In [7, 5], bounding-box regression is performed on features pooled from arbitrarily sized regions, and the regression weights are shared by all region sizes. In our formulation, the features used for regression are of the same spatial size (n × n) on the feature maps. To account for varying sizes, a set of k bounding-box regressors are learned. Each regressor is responsible for one scale and one aspect ratio, and the k regressors do not share weights. As such, it is still possible to predict boxes of various sizes even though the features are of a fixed size/scale.","However, our approach accomplishes bounding-box regression in a different way than previous methods based on feature maps [7, 5]. In [7, 5], bounding-box regression is done on features pooled from regions of arbitrary sizes, and the regression weights are shared across all region sizes. In our formulation, the features used for regression have the same spatial dimensions (n × n) on the feature maps. To handle varying sizes, a set of k bounding-box regressors are learned. Each regressor is responsible for one scale and aspect ratio, and the k regressors do not share weights. Therefore, it is still feasible to predict boxes of different sizes even though the features have a fixed size/scale.","Nonetheless, our technique achieves bounding-box regression differently compared to prior feature-map-based approaches [7, 5]. In [7, 5], bounding-box regression is carried out on features aggregated from arbitrarily sized areas, and the regression weights are common across all area sizes. In our formulation, the features utilized for regression possess the same spatial size (n × n) on the feature maps. To account for varying dimensions, a set of k bounding-box regressors are learned. Each regressor is accountable for one scale and aspect ratio, and the k regressors do not share weights. As such, it is still viable to predict boxes of different sizes despite the features having a fixed size/scale.  ","However, our approach performs bounding-box regression in a distinct manner from previous feature-map-dependent techniques [7, 5]. In [7, 5], bounding-box regression is executed on features pooled from arbitrarily proportioned regions, and the regression coefficients are shared among all region proportions. In our formulation, the features leveraged for regression possess the same spatial dimensions (n × n) on the feature maps. To accommodate varying proportions, a set of k bounding-box regressors are learned. Each regressor is responsible for one scale and aspect ratio, and the k regressors do not share coefficients. Therefore, it is still feasible to predict boxes of varying sizes despite the features maintaining a fixed size/scale.",A,1
Faster R-CNN Towards Real-Time Object Detection with Region Proposal Networks,"The RPN, which is naturally implemented as a fully-convolutional network [14], can be trained end-to-end by back-propagation and stochastic gradient descent (SGD) [12]. We follow the “imagecentric” sampling strategy from [5] to train this network. Each mini-batch arises from a single image that contains many positive and negative anchors. It is possible to optimize for the loss functions of all anchors, but this will bias towards negative samples as they are dominate. Instead, we randomly sample 256 anchors in an image to compute the loss function of a mini-batch, where the sampled positive and negative anchors have a ratio of up to 1:1.","The RPN, which has a fully-convolutional structure [14], can learn end-to-end through backpropagation and stochastic gradient descent (SGD) [12]. We use the ""imagecentric"" sampling method from [5] to teach this network. Each mini-batch comes from one image with many positive and negative anchors. Although you could optimize the loss functions of all anchors, this would skew towards negative samples since they dominate. Instead, we randomly choose 256 anchors in an image to calculate the loss function of a mini-batch, where the sampled positive and negative anchors have a ratio of up to 1:1.","The RPN, implemented as a fully-convolutional net [14], is trainable end-to-end via backprop and stochastic gradient descent (SGD) [12]. We utilize the ""imagecentric"" sampling approach from [5] to train this net. Each mini-batch is from a single image containing numerous positive and negative anchors. You could optimize the loss functions for all anchors, but that would bias towards negative samples since they predominate. Rather, we randomly sample 256 anchors in an image to compute the mini-batch loss function, where the sampled positive and negative anchors have a max ratio of 1:1.  ","The RPN, built as a fully-convolutional network [14], can learn end-to-end through backpropagation and stochastic gradient descent (SGD) [12]. We employ the ""imagecentric"" sampling strategy from [5] to train this network. Each mini-batch originates from one image with many positive and negative anchors. Optimizing the loss functions of all anchors is possible but would skew towards negative samples since they are more common. Instead, we randomly select 256 anchors in an image to calculate the mini-batch loss function, where the sampled positive and negative anchors have a ratio up to 1:1.",A,1
Faster R-CNN Towards Real-Time Object Detection with Region Proposal Networks,"If there are fewer than 128 positive samples in an image, we pad the mini-batch with negative ones. We randomly initialize all new layers by drawing weights from a zero-mean Gaussian distribution with standard deviation 0.01. All other layers (i.e., the shared conv layers) are initialized by pretraining a model for ImageNet classification [17], as is standard practice [6]. We tune all layers of the ZF net, and conv3 1 and up for the VGG net to conserve memory [5]. We use a learning rate of 0.001 for 60k mini-batches, and 0.0001 for the next 20k mini-batches on the PASCAL dataset. We also use a momentum of 0.9 and a weight decay of 0.0005 [11]. Our implementation uses Caffe [10].","If an image has under 128 affirmative instances, we supplement the small-scale group with negative ones. We arbitrarily start all new tiers by obtaining weights from a zero-centered Gaussian circulation with a standard deviation of 0.01. We initialize all other layers (namely, the mutual convolution layers) by pre-teaching a prototype for ImageNet categorization [17], as is conventional practice [6]. We calibrate all layers of the ZF net, and conv3 1 and up for the VGG net to conserve reminiscence [5]. On the PASCAL dataset, we employ a learning percentage of 0.001 for 60k small-scale groups, and 0.0001 for the subsequent 20k small-scale groups. We also employ a momentum of 0.9 and a weight decay of 0.0005 [11]. Our execution employs Caffe [10].","If an image contains less than 128 favorable samples, we fill the mini-batch with unfavorable ones. We arbitrarily initialize all new tiers by extracting weights from a zero-centered Gaussian distribution with a standard deviation of 0.01. We initialize all other tiers (namely, the shared convolution tiers) by pre-training a model for ImageNet categorization [17], as is standard practice [6]. We calibrate all tiers of the ZF net, and conv3 1 and up for the VGG net to conserve memory [5]. On the PASCAL dataset, we use a learning rate of 0.001 for 60k mini-batches, and 0.0001 for the next 20k mini-batches. We also use a momentum of 0.9 and a weight decay of 0.0005 [11]. Our implementation utilizes Caffe [10].","If there are less than 128 positive examples in an image, we supplement the mini-batch with negative ones. We randomly initialize all new layers by extracting weights from a zero-centered Gaussian distribution with a standard deviation of 0.01. We initialize all other layers (specifically, the shared convolution layers) by pre-training a model for ImageNet classification [17], as is standard practice [6]. We tune all layers of the ZF net, and conv3 1 and up for the VGG net to conserve memory [5]. On the PASCAL dataset, we employ a learning rate of 0.001 for 60k mini-batches, and 0.0001 for the subsequent 20k mini-batches. We also use a momentum of 0.9 and a weight decay of 0.0005 [11]. Our implementation employs Caffe [10].",A,1
Faster R-CNN Towards Real-Time Object Detection with Region Proposal Networks,"Thus far we have described how to train a network for region proposal generation, without considering the region-based object detection CNN that will utilize these proposals. For the detection network, we adopt Fast R-CNN [5] and now describe an algorithm that learns conv layers that are shared between the RPN and Fast R-CNN. Both RPN and Fast R-CNN, trained independently, will modify their conv layers in different ways. We therefore need to develop a technique that allows for sharing conv layers between the two networks, rather than learning two separate networks. Note that this is not as easy as simply defining a single network that includes both RPN and Fast R-CNN, and then optimizing it jointly with backpropagation.","Up to this point, we have talked about how to teach a network to generate region proposals, without thinking about the region-based object detection CNN that will use these proposals. For the detection network, we take Fast R-CNN [5] and now describe an algorithm that learns conv layers shared between the RPN and Fast R-CNN. RPN and Fast R-CNN, if trained separately, will adjust their conv layers differently. So we need to make a technique that enables sharing conv layers between the two networks, rather than learning two distinct networks. Note that this is not as straightforward as just defining one network that has both RPN and Fast R-CNN, and then optimizing it together with backpropagation.","So far, we have explained how to train a network to produce region proposals, without considering the region-based object detection CNN that will employ these proposals. For the detection network, we use Fast R-CNN [5] and now describe an algorithm that learns conv layers that are common between the RPN and Fast R-CNN. Both RPN and Fast R-CNN, if trained independently, will modify their conv layers diversely. Therefore, we need to develop a technique that permits sharing conv layers between the two networks, rather than learning two separate networks. Note that this is not as simple as just defining one single network that contains both RPN and Fast R-CNN, and then enhancing it jointly with backpropagation. ","Up until now, we have elucidated how to train a network for region proposal generation, without contemplating the region-based object detection CNN that will leverage these proposals. For the detection network, we take on Fast R-CNN [5] and now elucidate an algorithm that learns conv layers that are mutual between the RPN and Fast R-CNN. Both RPN and Fast R-CNN, if trained autonomously, will modify their conv layers differently. Therefore, we need to conceive a technique that provides for sharing conv layers between the two networks, rather than learning two discrete networks. Note that this is not as straightforward as simply characterizing one network that embodies both RPN and Fast R-CNN, and then optimizing it collectively with backpropagation.",A,1
Faster R-CNN Towards Real-Time Object Detection with Region Proposal Networks," The reason is that Fast R-CNN training depends on fixed object proposals and it is not clear a priori if learning Fast R-CNN while simultaneously changing the proposal mechanism will converge. While this joint optimizing is an interesting question for future work, we develop a pragmatic 4-step training algorithm to learn shared features via alternating optimization. In the first step, we train the RPN as described above. This network is initialized with an ImageNetpre-trained model and fine-tuned end-to-end for the region proposal task. In the second step, we train a separate detection network by Fast R-CNN using the proposals generated by the step-1 RPN. This detection network is also initialized by the ImageNet-pre-trained model.","The explanation is that Fast R-CNN instruction depends on static object proposals, so it's unclear if learning Fast R-CNN while simultaneously modifying the proposal system will unite. Although this collective enhancement is an interesting issue for future work, we formulate a practical 4-step learning algorithm to acquire shared features through alternating optimization. First, we coach the RPN as identified above. This framework is commenced with an ImageNet-pre-trained archetype and fine-tuned end-to-end for the region proposal task. Second, we coach a distinct detection network by Fast R-CNN employing the proposals spawned by the step-1 RPN. This detection network is also commenced by the ImageNet-pre-trained archetype.","The justification is that Fast R-CNN schooling hinges on fixed object recommendations and it's ambiguous initially if comprehending Fast R-CNN while concurrently altering the recommendation means will merge. Despite the fact that this collective refining is an appealing inquiry for future work, we construct a pragmatic 4-step education algorithm to obtain shared traits through alternating enhancement. At first, we discipline the RPN as depicted above. This structure is started with an ImageNet-pre-trained model and fine-tuned end-to-end for the region proposal task. Second, we discipline a distinct detection network by Fast R-CNN operating the recommendations produced by the step-1 RPN. This detection network is also started by the ImageNet-pre-trained model.","The clarification is that Fast R-CNN preparation depends on static object proposals, so it's unclear initially if learning Fast R-CNN while simultaneously changing the proposal system will unite. Even though this collective streamlining is an interesting point for future work, we devise a practical 4-step training algorithm to obtain shared features through alternating improvement. Initially, we drill the RPN as portrayed above. This framework is initiated with an ImageNet-pre-trained archetype and fine-tuned end-to-end for the region proposal task. Secondly, we drill a distinct detection network by Fast R-CNN employing the proposals generated by the step-1 RPN. This detection network is also initiated by the ImageNet-pre-trained archetype.",A,1
Faster R-CNN Towards Real-Time Object Detection with Region Proposal Networks,"At this point the two networks do not share conv layers. In the third step, we use the detector network to initialize RPN training, but we fix the shared conv layers and only fine-tune the layers unique to RPN. Now the two networks share conv layers. Finally, keeping the shared conv layers fixed, we fine-tune the fc layers of the Fast R-CNN. As such, both networks share the same conv layers and form a unified network. We train and test both region proposal and object detection networks on single-scale images [7, 5]. We re-scale the images such that their shorter side is s = 600 pixels [5]. Multi-scale feature extraction may improve accuracy but does not exhibit a good speed-accuracy trade-off [5].","Currently the two networks do not share convolutional layers. In step three, we utilize the detector network to initialize RPN training, but we keep the shared convolutional layers fixed and only fine-tune the layers unique to RPN. Now the two networks share convolutional layers. Lastly, with the shared convolutional layers fixed, we fine-tune the fully connected layers of Fast R-CNN. Therefore, both networks share the same convolutional layers and form a unified network. We train and evaluate both region proposal and object detection networks on single-scale images [7, 5]. We resize the images so their shorter side is s = 600 pixels [5]. Multi-scale feature extraction may improve accuracy but does not show a good speed-accuracy trade-off [5].","At this point the two networks do not have any convolutional layers in common. In the third step, we leverage the detector network to initialize RPN training, but we stabilize the shared convolutional layers and only fine-tune the layers exclusive to RPN. Now the two networks share convolutional layers. Finally, keeping the shared convolutional layers stabilized, we fine-tune the fully connected layers of Fast R-CNN. Consequently, both networks share the same convolutional layers and form a combined network. We train and test both region proposal and object detection networks on single-scale images [7, 5]. We rescale the images so their shorter side is s = 600 pixels [5]. Multi-scale feature extraction may improve accuracy but does not demonstrate a good speed-accuracy trade-off [5].  ","Currently the two networks do not share any conv layers. In step three, we use the detector network to initialize RPN training, but we fix the shared conv layers and only fine-tune the layers unique to RPN. Now the two networks share conv layers. Lastly, keeping the shared conv layers fixed, we fine-tune the fc layers of Fast R-CNN. Therefore, both networks share the same conv layers and form a unified network. We train and evaluate both region proposal and object detection networks on single-scale images [7, 5]. We resize the images so their shorter side is s = 600 pixels [5]. Multi-scale feature extraction may improve accuracy but does not exhibit a good speed-accuracy compromise [5].",A,1
Faster R-CNN Towards Real-Time Object Detection with Region Proposal Networks,"The anchor boxes that cross image boundaries need to be handled with care. During training, we ignore all cross-boundary anchors so they do not contribute to the loss. For a typical 1000 × 600 image, there will be roughly 20k (≈ 60 × 40 × 9) anchors in total. With the cross-boundary anchors ignored, there are about 6k anchors per image for training. If the boundary-crossing outliers are not ignored in training, they introduce large, difficult to correct error terms in the objective, and training does not converge. During testing, however, we still apply the fully-convolutional RPN to the entire image.","The anchor boxes that go past the edges of the image must be cautiously managed. While training, we disregard all cross-border anchors so they do not add to the loss. For a standard 1000 × 600 image, there will be around 20k (≈ 60 × 40 × 9) anchors total. With the boundary-traversing anomalies disregarded, there are about 6k anchors per image for training. If the outliers that cross borders are not overlooked in training, they introduce big, hard to fix error quantities in the goal, and training does not converge. However, during testing, we still implement the completely convolutional RPN on the whole image.","The anchor boxes extending outside the image need special handling. We exclude all anchors crossing image borders during training so they don't contribute to the loss function. A typical 1000 × 600 image has about 20k (≈ 60 × 40 × 9) anchors in total. Excluding boundary-crossing anchors leaves around 6k anchors per image for training. Including these outlier anchors in training introduces large errors in the loss that are difficult to correct, preventing training convergence. But we still apply the fully convolutional RPN across the entire image during testing.  ","Anchor boxes that go past image edges require careful management. We ignore anchors that cross image boundaries during training so they don't add to the loss. A normal 1000 × 600 image has roughly 20k (≈ 60 × 40 × 9) anchors total. Disregarding anchors that cross borders leaves around 6k anchors per image for training. Including boundary-crossing outliers in training introduces big errors in the loss that are hard to fix, stopping training convergence. However, we still use the fully convolutional RPN on the whole image during testing.",A,1
Faster R-CNN Towards Real-Time Object Detection with Region Proposal Networks,"This may generate cross-boundary proposal boxes, which we clip to the image boundary. Some RPN proposals highly overlap with each other. To reduce redundancy, we adopt nonmaximum suppression (NMS) on the proposal regions based on their cls scores. We fix the IoU threshold for NMS at 0.7, which leaves us about 2k proposal regions per image. As we will show, NMS does not harm the ultimate detection accuracy, but substantially reduces the number of proposals. After NMS, we use the top-N ranked proposal regions for detection. In the following, we train Fast R-CNN using 2k RPN proposals, but evaluate different numbers of proposals at test-time.","This process can produce proposed regions that extend past the edges of the image. We modify any such regions so they stop at the image perimeter. There is significant overlap between some of the proposed regions. To minimize repetitive proposals, we utilize non-maximum suppression to remove proposed regions that have high overlap with each other based on their confidence scores. We set the overlap threshold for this process to 0.7, which leaves around 2,000 proposed regions per image. As we will demonstrate, non-maximum suppression does not negatively impact final detection accuracy, but substantially decreases the number of proposals. After non-maximum suppression, we utilize the top N highest ranked proposed regions for detection. In the following experiments, we train Fast R-CNN using 2,000 RPN proposals, but test varying numbers of proposals.","This technique can suggest regions that reach beyond the image borders. We adjust any such regions to end at the image edge. Some of the recommended regions have considerable overlap. To reduce redundant suggestions, we use non-maximum suppression to eliminate proposed regions with high overlap according to their confidence values. We set the overlap limit for this at 0.7, leaving around 2,000 proposed regions per image. As we will show, non-maximum suppression does not harm final detection precision, but significantly reduces the number of proposals. After non-maximum suppression, we take the top N ranked proposed regions for detection. In the following, we train Fast R-CNN with 2,000 RPN proposals, but evaluate varying proposal counts during testing.","This approach can produce proposed boxes that extend past the image boundaries. We clip any such boxes so they are limited to the image border. Some of the RPN proposals have high overlap with each other. To decrease redundancy, we use non-maximum suppression on the proposals based on their confidence scores. We set the overlap threshold for this at 0.7, which leaves about 2,000 proposals per image. As we will demonstrate, non-maximum suppression does not negatively affect final detection performance, but greatly reduces the number of proposals. After non-maximum suppression, we utilize the top N ranked proposals for detection. In the experiments below, we train Fast R-CNN with 2,000 RPN proposals, but test with different proposal counts.",A,1
Faster R-CNN Towards Real-Time Object Detection with Region Proposal Networks,"To investigate the behavior of RPNs as a proposal method, we conducted several ablation studies. First, we show the effect of sharing conv layers between the RPN and Fast R-CNN detection network. To do this, we stop after the second step in the 4-step training process. Using separate networks reduces the result slightly to 58.7% (RPN+ZF, unshared, Table 1). We observe that this is because in the third step when the detector-tuned features are used to fine-tune the RPN, the proposal quality is improved. Next, we disentangle the RPN’s influence on training the Fast R-CNN detection network.","To examine how RPNs work as a proposal technique, we did some experiments removing components. First, we demonstrate the impact of using the same conv layers for both the RPN and Fast R-CNN detection network. We stop after step 2 of the 4-step training process to test this. Using separate networks somewhat reduces the result to 58.7% (RPN+ZF, unshared, Table 1). We see this is because in step 3 when the detector-tuned features fine-tune the RPN, the proposal quality improves. Next, we separate out the RPN's effect on training the Fast R-CNN detection network.","To study the performance of RPNs as a proposal approach, we conducted some ablation experiments. Initially, we show the consequence of sharing conv layers between the RPN and Fast R-CNN detection model. To do this, we halt after the second phase in the 4-phase training workflow. Employing distinct models slightly decreases the outcome to 58.7% (RPN+ZF, unshared, Table 1). We notice this is because in the third phase when the detector-tuned features are utilized to fine-tune the RPN, the proposal quality is enhanced. Subsequently, we isolate the RPN's influence on training the Fast R-CNN detection model.","To analyze the functioning of RPNs as a proposal technique, we performed some ablation analyses. Firstly, we demonstrate the impact of utilizing the same conv layers for the RPN and Fast R-CNN detection network. We stop after step 2 of the 4-step training process for this. Using separate networks slightly reduces the result to 58.7% (RPN+ZF, unshared, Table 1). We observe this is because in step 3 when the detector-tuned features fine-tune the RPN, the proposal quality improves. Afterward, we detach the RPN's effect on training the Fast R-CNN detection network.",A,1
Faster R-CNN Towards Real-Time Object Detection with Region Proposal Networks,"For this purpose, we train a Fast R-CNN model by using the 2k SS proposals and ZF net. We fix this detector and evaluate the detection mAP by changing the proposal regions used at test-time. In these ablation experiments, the RPN does not share features with the detector. Replacing SS with 300 RPN proposals at test-time leads to an mAP of 56.8%. The loss in mAP is because of the inconsistency between the training/testing proposals. This result serves as the baseline for the following comparisons. On the other extreme, using the top-ranked 6k RPN proposals (without NMS) has a comparable mAP (55.2%), suggesting NMS does not harm the detection mAP and may reduce false alarms.","To do this, we develop a Fast R-CNN system by utilizing the 2k SS proposals and ZF net. We keep this detector constant and measure the detection mAP by modifying the proposal areas used during testing. In these analysis tests, the RPN does not share features with the detector. Substituting SS with 300 RPN proposals at test time leads to an mAP of 56.8%. The mAP loss is due to the inconsistency between the training/testing proposals. This outcome provides the baseline for the following comparisons. On the other extreme, utilizing the top-ranked 6k RPN proposals (without NMS) has a comparable mAP (55.2%), implying NMS does not damage the detection mAP and may decrease false alarms.","For this goal, we build a Fast R-CNN system utilizing the 2k SS proposals and ZF net. We stabilize this detector and evaluate the detection mAP by altering the proposal regions utilized at test time. In these examination experiments, the RPN does not share features with the detector. Swapping SS with 300 RPN proposals at test time results in an mAP of 56.8%. The mAP loss is because of the inconsistency between the training/testing proposals. This outcome serves as the baseline for the subsequent comparisons. On the other extreme, employing the top-ranked 6k RPN proposals (without NMS) has a similar mAP (55.2%), indicating NMS does not impair the detection mAP and may reduce false alarms.  ","To accomplish this, we develop a Fast R-CNN model using the 2k SS proposals and ZF net. We fix this detector and assess the detection mAP by modifying the proposal areas used during testing. In these analysis experiments, the RPN does not share features with the detector. Exchanging SS with 300 RPN proposals at test time leads to an mAP of 56.8%. The mAP loss is due to the inconsistency between the training/testing proposals. This result acts as the baseline for the following comparisons. On the other extreme, utilizing the top-ranked 6k RPN proposals (without NMS) has a comparable mAP (55.2%), suggesting NMS does not damage the detection mAP and may decrease false alarms.",A,1
Faster R-CNN Towards Real-Time Object Detection with Region Proposal Networks,"Next, we separately investigate the roles of RPN’s cls and reg outputs by turning off either of them at test-time. When the cls layer is removed at test-time (thus no NMS/ranking is used), we randomly sample N proposals from the unscored regions. The mAP is nearly unchanged with N = 1k (55.8%), but degrades considerably to 44.6% when N = 100. This shows that the cls scores account for the accuracy of the highest ranked proposals. On the other hand, when the reg layer is removed at test-time (so the proposals become anchor boxes), the mAP drops to 52.1%.","Subsequently, we independently examine the functions of RPN's cls and reg outputs by disabling one or the other during testing. When the cls layer is eliminated at test time (so no NMS/ranking is utilized), we arbitrarily choose N proposals from the unscored areas. The mAP stays nearly the same with N = 1k (55.8%), but declines significantly to 44.6% when N = 100. This demonstrates that the cls scores are responsible for the accuracy of the top ranked proposals. Conversely, when the reg layer is removed at test time (so the proposals transform into anchor boxes), the mAP drops to 52.1%.","In the next step, we separately analyze the roles of the classification (cls) and regression (reg) outputs of the region proposal network (RPN) by turning off one or the other during testing. Removing the cls layer at test time (eliminating NMS and ranking) and randomly sampling N proposals from unscored regions leads to a nearly unchanged mAP with N=1k (55.8%) but a considerable degradation to 44.6% with N=100. This indicates the cls scores determine the accuracy of the highest ranked proposals. On the other hand, removing the reg layer at test time (converting proposals to anchor boxes) reduces the mAP to 52.1%.","Subsequently, we investigate the individual contributions of the RPN's classification (cls) and bounding box regression (reg) outputs by disabling one or the other during testing. Eliminating the cls layer at test time (removing NMS and ranking) and arbitrarily choosing N proposals from unscored areas keeps mAP almost constant at 55.8% with N=1k, but significantly reduces it to 44.6% with N=100. This demonstrates the cls scores control the precision of the top ranked proposals. In contrast, removing the reg layer at test time (turning proposals into anchor boxes) decreases mAP to 52.1%.",A,1
Faster R-CNN Towards Real-Time Object Detection with Region Proposal Networks,"We also evaluate the effects of more powerful networks on the proposal quality of RPN alone. We use VGG-16 to train the RPN, and still use the above detector of SS+ZF. The mAP improves from 56.8% (using RPN+ZF) to 59.2% (using RPN+VGG). This is a promising result, because it suggests that the proposal quality of RPN+VGG is better than that of RPN+ZF. Because proposals of RPN+ZF are competitive with SS (both are 58.7% when consistently used for training and testing), we may expect RPN+VGG to be better than SS. The following experiments justify this hypothesis.","We also assess the impacts of more capable networks on just the proposal quality of RPN. We utilize VGG-16 to train RPN, and still employ the above detector of SS+ZF. The mAP increases from 56.8% (utilizing RPN+ZF) to 59.2% (utilizing RPN+VGG). This is an encouraging outcome, since it implies that the proposal quality of RPN+VGG is superior to that of RPN+ZF. Because proposals of RPN+ZF are competitive with SS (both are 58.7% when consistently utilized for training and testing), we may anticipate RPN+VGG to surpass SS. The ensuing experiments justify this hypothesis.","In addition, we evaluate how more powerful neural networks affect the proposal quality of RPN on its own. We use VGG-16 to train RPN, while still employing the previous detector of SS+ZF. The mAP rises from 56.8% (with RPN+ZF) to 59.2% (with RPN+VGG). This is a promising finding, as it indicates the proposal quality of RPN+VGG is better than RPN+ZF. Since proposals from RPN+ZF are on par with SS (both at 58.7% when consistently used for training and evaluation), we can expect RPN+VGG to exceed SS. The next experiments confirm this hypothesis.","Furthermore, we assess the impacts of more capable networks solely on the proposal quality of RPN. We make use of VGG-16 to train RPN, while still applying the earlier detector of SS+ZF. The mAP increases from 56.8% (using RPN+ZF) to 59.2% (using RPN+VGG). This is an encouraging result, because it hints that the proposal quality of RPN+VGG is superior to RPN+ZF. As proposals from RPN+ZF are competitive with SS (both at 58.7% when steadily used for training and testing), we can anticipate RPN+VGG to outperform SS. The following experiments validate this hypothesis.",A,1
Faster R-CNN Towards Real-Time Object Detection with Region Proposal Networks,"Table 2 shows the results of VGG-16 for both proposal and detection. Using RPN+VGG, the Fast R-CNN result is 68.5% for unshared features, slightly higher than the SS baseline. As shown above, this is because the proposals generated by RPN+VGG are more accurate than SS. Unlike SS that is pre-defined, the RPN is actively trained and benefits from better networks. For the feature-shared variant, the result is 69.9%—better than the strong SS baseline, yet with nearly cost-free proposals.","The data in Table 2 displays the outcomes of utilizing VGG-16 for proposal generation and object detection. Implementing RPN with VGG yields a Fast R-CNN accuracy of 68.5% without weight sharing, somewhat surpassing the Selective Search baseline. This minor improvement arises because the region proposals created by RPN+VGG are more precise than those from Selective Search. Unlike Selective Search's predefined regions, RPN is trained end-to-end and gains from more capable networks. With weight sharing, the accuracy is 69.9%, outperforming Selective Search while requiring almost no extra computation for proposals.","The numbers in Table 2 exhibit the performance of VGG-16 on proposing regions and recognizing objects. Combining RPN and VGG provides 68.5% detection accuracy with Fast R-CNN without sharing weights, slightly better than Selective Search. This small increase is because the proposed regions from RPN+VGG are more accurate than those from Selective Search. RPN is actively optimized unlike the predefined Selective Search, thus benefiting from more powerful networks. Weight sharing leads to 69.9% accuracy, surpassing Selective Search while generating proposals at nearly zero cost.  ","The statistics in Table 2 show how VGG-16 does on recommending image patches and identifying objects. Using RPN and VGG together results in 68.5% precision with Fast R-CNN when weights are separate, a bit higher than Selective Search. This minor boost stems from RPN+VGG's proposed regions being more precise than Selective Search's. Unlike the predefined Selective Search, RPN is trained end-to-end and gains from stronger networks. With weight sharing, the precision rises to 69.9%, beating Selective Search while proposal generation has almost no overhead.",A,1
Faster R-CNN Towards Real-Time Object Detection with Region Proposal Networks,"We further train the RPN and detection network on the union set of PASCAL VOC 2007 trainval and 2012 trainval, following [5]. The mAP is 73.2%. On the PASCAL VOC 2012 test set (Table 3), our method has an mAP of 70.4% trained on the union set of VOC 2007 trainval+test and VOC 2012 trainval, following [5]. In Table 4 we summarize the running time of the entire object detection system. SS takes 1-2 seconds depending on content (on average 1.51s), and Fast R-CNN with VGG-16 takes 320ms on 2k SS proposals (or 223ms if using SVD on fc layers [5]).","We additionally educate the RPN and detection network on the combined set of PASCAL VOC 2007 trainval and 2012 trainval, as described in [5]. The mAP is 73.2%. On the PASCAL VOC 2012 evaluation set (Table 3), our approach has an mAP of 70.4% instructed on the combined set of VOC 2007 trainval+test and VOC 2012 trainval, as stated in [5]. In Table 4 we summarize the execution time of the whole object detection framework. SS takes 1-2 seconds based on content (typically 1.51s), and Fast R-CNN with VGG-16 expends 320ms on 2k SS suggestions (or 223ms if applying SVD on fc layers [5]).","We further develop the RPN and identification model on the union of PASCAL VOC 2007 trainval and 2012 trainval, as per [5]. The mAP is 73.2%. On the PASCAL VOC 2012 validation dataset (Table 3), our technique has an mAP of 70.4% learned on the union of VOC 2007 trainval+test and VOC 2012 trainval, as mentioned in [5]. In Table 4 we outline the running period of the full object recognition structure. SS takes 1-2 seconds contingent on content (on median 1.51s), and Fast R-CNN with VGG-16 uses 320ms on 2k SS nominations (or 223ms if leveraging SVD on fc layers [5]).  ","We additionally coach the RPN and detection algorithm on the combined set of PASCAL VOC 2007 trainval and 2012 trainval, as stated in [5]. The mAP is 73.2%. On the PASCAL VOC 2012 verification set (Table 3), our process has an mAP of 70.4% educated on the combined set of VOC 2007 trainval+test and VOC 2012 trainval, as noted in [5]. In Table 4 we summarize the execution duration of the entire object identification system. SS takes 1-2 seconds based on content (typically 1.51s), and Fast R-CNN with VGG-16 consumes 320ms on 2k SS candidates (or 223ms if applying SVD on fc layers [5]).",A,1
Faster R-CNN Towards Real-Time Object Detection with Region Proposal Networks,"Our system with VGG-16 takes in total 198ms for both proposal and detection. With the conv features shared, the RPN alone only takes 10ms computing the additional layers. Our region-wise computation is also low, thanks to fewer proposals (300). Our system has a frame-rate of 17 fps with the ZF net. Next we compute the recall of proposals at different IoU ratios with ground-truth boxes. It is noteworthy that the Recall-to-IoU metric is just loosely [9, 8, 1] related to the ultimate detection accuracy. It is more appropriate to use this metric to diagnose the proposal method than to evaluate it.","Our framework utilizing VGG-16 requires 198ms in full for both the proposal and detection stages. By sharing convolutional features, the region proposal network alone necessitates just 10ms to calculate the extra layers. Our region-wise computation is also low, owing to the smaller number of proposals (300). Our framework accomplishes a frame rate of 17 fps with the ZF network. We then determine the recall of proposals at various IoU ratios against ground-truth boxes. It merits noting that the Recall-to-IoU measure is only loosely [9, 8, 1] linked to the final detection precision. This metric is more fitting for diagnosing the proposal technique rather than assessing it.","Our architecture with VGG-16 needs a total of 198ms for the proposal and detection phases together. Through sharing conv features, the RPN by itself only spends 10ms processing the extra layers. Our region-wise computation is also low, thanks to having fewer proposals (300). Our architecture achieves 17 fps frame rate with the ZF net. We then compute proposal recall at different IoU ratios compared to ground-truth boxes. It's important to note the Recall-to-IoU metric is just loosely [9, 8, 1] correlated to ultimate detection accuracy. This metric is more suitable for diagnosing the proposal method rather than evaluating it.  ","Our model utilizing VGG-16 takes 198ms in total for proposal generation and object detection. By sharing convolutional features, the region proposal network alone requires just 10ms to compute the additional layers. Our region-wise computation is also low, due to fewer proposals (300). Our model achieves 17 fps frame rate with the ZF network. We then calculate proposal recall at various IoU thresholds compared to ground-truth boxes. Notably, the Recall-to-IoU measure is only loosely [9, 8, 1] linked to final detection performance. This metric is better suited for analyzing the proposal method rather than assessing it.",A,1
Generative Adversarial Nets,"We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game.","We present a new system for calculating generative models through an adversarial process. In this system, we concurrently develop two models: a generative model G that represents the data distribution, and a discriminative model D that calculates the likelihood that a sample originated from the training data rather than G. The training process for G is to maximize the probability of D making an error. This framework is equivalent to a minimax two-player game.","We put forward a novel approach for determining generative models using an adversarial technique. Here, we simultaneously construct two models: a generative model G capturing the data distribution, and a discriminative model D evaluating the chance a sample was from the training information not G. The training technique for G is increasing the likelihood of D misjudging. This framework matches a minimax two-player match. ","We present a new method for assessing generative models through an adversarial procedure. We concurrently build two models: a generative model G grasping the data distribution, and a discriminative model D gauging the probability a sample was from the training data instead of G. The training process for G is maximizing the chance of D erring. This framework is tantamount to a minimax two-player contest.",A,1
Generative Adversarial Nets,"In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.","For any functions G and D, there is one solution where G reproduces the training data distribution and D is 1/2 everywhere. When G and D are multilayer perceptrons, the whole system can learn via backpropagation. No Markov chains or approximate inference networks are needed during training or when creating samples. Tests show the potential of the method through qualitative and quantitative assessment of the generated samples.","With arbitrary functions G and D, a single solution exists such that G recovers the training data distribution and D is 0.5 universally. When G and D are multilayer perceptrons, backpropagation can train the full system. Neither Markov chains nor approximate inference networks are required during training or sample generation. Experiments highlight the framework's potential through qualitative and quantitative evaluation of the produced samples.  ","For any G and D functions, there is a sole solution where G replicates the training data distribution and D equals 0.5 everywhere. When G and D are multilayer perceptrons, backpropagation can learn the complete system. Markov chains and approximate inference networks are unnecessary during training and sample creation. Tests exhibit the framework's potential via qualitative and quantitative appraisal of the created samples.",A,1
Generative Adversarial Nets,"The promise of deep learning is to discover rich, hierarchical models [2] that represent probability distributions over the kinds of data encountered in artificial intelligence applications, such as natural images, audio waveforms containing speech, and symbols in natural language corpora. So far, the most striking successes in deep learning have involved discriminative models, usually those that map a high-dimensional, rich sensory input to a class label [14, 20]. These striking successes have primarily been based on the backpropagation and dropout algorithms, using piecewise linear units [17, 8, 9] which have a particularly well-behaved gradient .","The potential of deep learning is to find complex, layered models [2] that characterize probability spreads over the types of information seen in artificial intelligence uses, like natural pictures, audio waveforms with speech, and symbols in collections of natural language. Up to this point, the most remarkable victories in deep learning have concerned discriminative models, usually those that map a high-dimensional, information-rich sensory input to a class tag [14, 20]. These remarkable victories have mostly depended on the backpropagation and dropout algorithms, utilizing piecewise linear units [17, 8, 9] which have an especially well-behaved slope.","The promise of deep learning is to uncover intricate, hierarchical models [2] that describe probability distributions over the kinds of information found in AI applications, including natural images, audio waveforms with vocalizations, and symbols in bodies of natural language. Thus far, the most striking triumphs in deep learning have been about discriminative models, often ones that map a high-dimensional, rich sensory input to a class label [14, 20]. These striking triumphs have largely relied on the backpropagation and dropout algorithms, employing piecewise linear units [17, 8, 9] which have a gradient that is particularly well-behaved.","The potential of deep learning is to develop complex, layered models [2] that characterize probability spreads over the types of data seen in artificial intelligence uses, such as natural images, audio waveforms containing speech, and symbols in collections of natural language. To this point, the most remarkable successes in deep learning have been about discriminative models, usually ones that map a high-dimensional, rich sensory input to a class tag [14, 20]. These remarkable successes have primarily depended on the backpropagation and dropout algorithms, using piecewise linear units [17, 8, 9] which have an especially well-behaved slope.",A,1
Generative Adversarial Nets,"Deep generative models have had less of an impact, due to the difficulty of approximating many intractable probabilistic computations that arise in maximum likelihood estimation and related strategies, and due to difficulty of leveraging the benefits of piecewise linear units in the generative context. We propose a new generative model estimation procedure that sidesteps these difficulties. 1 In the proposed adversarial nets framework, the generative model is pitted against an adversary: a discriminative model that learns to determine whether a sample is from the model distribution or the data distribution.","Advanced generative models have not made as much of an impact, because approximating the many challenging probabilistic calculations that come up with maximum likelihood estimation and related techniques is difficult. Also, it's hard to take advantage of the benefits of piecewise linear units in generative models. We suggest a new way to estimate generative models that avoids these problems. With adversarial networks, the generative model competes against a discriminator: a model that learns to tell whether a sample is from the model or the real data.","Sophisticated generative models have not had as much influence, since approximating numerous complex probabilistic computations from maximum likelihood estimation and related methods is challenging. Furthermore, capitalizing on the strengths of piecewise linear units in generative settings is difficult. We put forth a novel generative model estimation approach that circumvents these challenges. In adversarial networks, the generative model is opposed by a discriminator: a discriminative model that learns to discern whether a sample is from the model distribution or actual data distribution.","Advanced generative models have not made as big an impact, because it's difficult to approximate the many intractable probabilistic calculations from maximum likelihood estimation and related techniques. Also, it's hard to take full advantage of the benefits of piecewise linear units in generative contexts. We present a new way to estimate generative models that avoids these issues. With adversarial nets, the generative model competes against a discriminator: a discriminative model that learns to identify whether a sample comes from the model distribution or real data distribution.",A,1
Generative Adversarial Nets,"The generative model can be thought of as analogous to a team of counterfeiters, trying to produce fake currency and use it without detection, while the discriminative model is analogous to the police, trying to detect the counterfeit currency. Competition in this game drives both teams to improve their methods until the counterfeits are indistinguishable from the genuine articles.","The generative model is like a group of forgers attempting to create false money and use it without being caught. The discriminative model is like the police, working to identify the counterfeit bills. The competition between the two teams motivates both sides to refine their techniques until the forgeries are indistinguishable from real currency.",The generative model is similar to a band of counterfeiters trying to make fake money and pass it off unnoticed. The discriminative model is like the authorities seeking to detect the bogus bills. The contest between them pushes both sides to hone their methods until the fakes are identical to the real thing. ,Think of the generative model as a gang of fraudsters striving to manufacture counterfeit cash and circulate it without being spotted. The discriminative model is comparable to the law enforcement attempting to identify the phony money. The rivalry between the two groups spurs each to improve their tactics until the forgeries are indiscernible from genuine bills.,A,1
Generative Adversarial Nets,"This framework can yield specific training algorithms for many kinds of model and optimization algorithm. In this article, we explore the special case when the generative model generates samples by passing random noise through a multilayer perceptron, and the discriminative model is also a multilayer perceptron. We refer to this special case as adversarial nets. In this case, we can train both models using only the highly successful backpropagation and dropout algorithms [16] and sample from the generative model using only forward propagation. No approximate inference or Markov chains are necessary.","This system can produce particular training methods for many types of models and optimization algorithms. In this paper, we look at the special case where the generative model makes samples by sending random noise through a multilayer perceptron, and the discriminative model is also a multilayer perceptron. We call this specific case adversarial networks. Here, we can train both models using only the very successful backpropagation and dropout algorithms [16] and sample from the generative model using only forward propagation. No approximate inference or Markov chains are needed.","This framework is able to generate tailored training procedures for many kinds of models and optimization techniques. In this article, we examine the special scenario where the generative model produces samples by feeding random noise into a multilayer perceptron, and the discriminative model is also a multilayer perceptron. We refer to this particular case as adversarial networks. In this situation, we can educate both models utilizing only the highly effective backpropagation and dropout algorithms [16] and sample from the generative model employing only forward propagation. No approximate inference or Markov chains are required.  ","This structure can produce specialized training methods for many types of models and optimization methods. In this paper, we investigate the specific case where the generative model creates samples by inputting random noise into a multilayer perceptron, and the discriminative model is also a multilayer perceptron. We call this particular case adversarial networks. Here, we can train both models using only the very successful backpropagation and dropout algorithms [16] and take samples from the generative model using only forward propagation. No approximate inference or Markov chains are needed.",A,1
Generative Adversarial Nets,"Until recently, most work on deep generative models focused on models that provided a parametric specification of a probability distribution function. The model can then be trained by maximizing the log likelihood. In this family of model, perhaps the most succesful is the deep Boltzmann machine [25]. Such models generally have intractable likelihood functions and therefore require numerous approximations to the likelihood gradient. These difficulties motivated the development of “generative machines”–models that do not explicitly represent the likelihood, yet are able to generate samples from the desired distribution.","In the past, most research on deep generative models concentrated on models that gave a parametric definition of a probability distribution function. These models could then be trained by maximizing the log likelihood. In this type of model, perhaps the most successful is the deep Boltzmann machine [25]. Such models usually have intractable likelihood functions and thus need many approximations to the likelihood gradient. These problems led to the creation of ""generative machines"" - models that do not explicitly show the likelihood, but can still generate samples from the desired distribution.","Until recently, a lot of work on deep generative models was focused on models that provided a parametric specification of a probability distribution function. The model could then be trained by maximizing the log likelihood. In this family of models, maybe the most succesful was the deep Boltzmann machine [25]. Such models tend to have likelihood functions that are intractable and therefore need many approximations to the likelihood gradient. These difficulties motivated the development of “generative machines”– models that do not explicitly represent the likelihood, yet are able to generate samples from the desired distribution.","In the past, most research on deep generative models concentrated on models that provided a parametric definition of a probability distribution function. The model could then be trained by maximizing the log likelihood. In this type of model, perhaps the most successful was the deep Boltzmann machine [25]. Such models generally have likelihood functions that cannot be solved and therefore require many approximations to the likelihood gradient. These problems led to the creation of ""generative machines"" - models that do not explicitly show the likelihood, but can still generate samples from the desired distribution.",A,1
Generative Adversarial Nets," Generative stochastic networks [4] are an example of a generative machine that can be trained with exact backpropagation rather than the numerous approximations required for Boltzmann machines. This work extends the idea of a generative machine by eliminating the Markov chains used in generative stochastic networks We were unaware at the time we developed this work that Kingma and Welling [18] and Rezende et al. [23] had developed more general stochastic backpropagation rules, allowing one to backpropagate through Gaussian distributions with finite variance, and to backpropagate to the covariance parameter as well as the mean.","Recently developed generative stochastic networks [4] exemplify a type of generative machine that permits precise backpropagation rather than the many estimations mandatory for Boltzmann machines. This research expands upon the concept of a generative machine by dispensing with the Markov chains employed in generative stochastic networks. At the time we advanced this work, we were uninformed that Kingma and Welling [18] and Rezende et al. [23] had formulated more universal stochastic backpropagation principles, enabling backpropagation via Gaussian distributions possessing limited variance, and backpropagation to the covariance parameter along with the mean.","A current instance of a generative system that can be trained with exact backpropagation instead of the numerous approximations needed for Boltzmann machines is generative stochastic networks [4]. Our work here expands on the idea of a generative system by removing the Markov chains present in generative stochastic networks. When we developed this work, we were unaware that Kingma and Welling [18] and Rezende et al. [23] had created more general stochastic backpropagation techniques, which allow backpropagating through Gaussian distributions with finite variance, and backpropagating to the covariance parameter as well as the mean.","Generative stochastic networks [4] exemplify a type of generative machine that can be trained with precise backpropagation rather than the many approximations mandatory for Boltzmann machines. This research builds on the concept of a generative machine by eliminating the Markov chains employed in generative stochastic networks. At the time we put forth this work, we were oblivious that Kingma and Welling [18] and Rezende et al. [23] had formulated more broad stochastic backpropagation principles, permitting backpropagation via Gaussian distributions having limited variance, and backpropagation to the covariance parameter in addition to the mean.",A,1
Generative Adversarial Nets,"These backpropagation rules could allow one to learn the conditional variance of the generator, which we treated as a hyperparameter in this work. Kingma and Welling [18] and Rezende et al. [23] use stochastic backpropagation to train variational autoencoders (VAEs). Like generative adversarial networks, variational autoencoders pair a differentiable generator network with a second neural network. Unlike generative adversarial networks, the second network in a VAE is a recognition model that performs approximate inference. GANs require differentiation through the visible units, and thus cannot model discrete data, while VAEs require differentiation through the hidden units, and thus cannot have discrete latent variables.","These reverse propagation principles could enable one to learn the conditional variability of the generator, which we dealt with as a hyperparameter in this work. Kingma and Welling [18] and Rezende et al. [23] utilize stochastic reverse propagation to instruct variational autoencoders (VAEs). Like generative adversarial networks, variational autoencoders couple a differentiable generator network with a second neural network. Dissimilar to generative adversarial networks, the second network in a VAE is a recognition model that executes approximate inference. GANs necessitate differentiation through the visible units, and therefore cannot model discrete data, while VAEs necessitate differentiation through the hidden units, and thus cannot have discrete latent variables.","These backpropagation guidelines could let someone acquire the conditional variance of the generator, which we handled as a hyperparameter here. Kingma and Welling [18] and Rezende et al. [23] employ stochastic backpropagation to educate variational autoencoders (VAEs). As with generative adversarial networks, variational autoencoders match a differentiable generator network with a second neural network. Not like generative adversarial networks, the second network in a VAE is a recognition model that conducts approximate inference. GANs want differentiation through the visible units, and hence cannot model discrete data, while VAEs want differentiation through the hidden units, and thus cannot have discrete latent variables.  ","These backward propagation rules could enable one to learn the conditional changeability of the generator, which we treated as a hyperparameter in this work. Kingma and Welling [18] and Rezende et al. [23] utilize stochastic backward propagation to train variational autoencoders (VAEs). Similar to generative adversarial networks, variational autoencoders pair a differentiable generator network with a second neural network. In contrast to generative adversarial networks, the second network in a VAE is a recognition model that performs estimated inference. GANs require differentiation through the visible units, and thus cannot model discrete data, while VAEs require differentiation through the hidden units, and therefore cannot have discrete latent variables.",A,1
Generative Adversarial Nets,"Other VAElike approaches exist [12, 22] but are less closely related to our method. Previous work has also taken the approach of using a discriminative criterion to train a generative model [29, 13]. These approaches use criteria that are intractable for deep generative models. These methods are difficult even to approximate for deep models because they involve ratios of probabilities which cannot be approximated using variational approximations that lower bound the probability. Noise-contrastive estimation (NCE) [13] involves training a generative model by learning the weights that make the model useful for discriminating data from a fixed noise distribution.","There are other techniques similar to VAEs [12, 22] but they are not as directly related to our approach. Prior work has also tried using a discriminative objective to learn a generative model [29, 13]. However, those methods rely on intractable calculations for deep generative models. They are challenging to approximate for deep models since they need ratios of probabilities, which cannot be bounded using variational methods. Noise-contrastive estimation (NCE) [13] trains a generative model by learning weights that help distinguish data from a static noise distribution.","Some other VAE-style methods exist [12, 22] but are less connected to our technique. Earlier research has also utilized a discriminative goal to develop a generative system [29, 13]. Those approaches require computations that are infeasible for complex generative models. They are problematic to estimate for deep models because they include proportions of probabilities that cannot be bounded using variational techniques that lower bound the probability. Noise-contrastive estimation (NCE) [13] trains a generative model by determining weights that enable the model to differentiate data from a fixed noise distribution.","There are other VAE-like approaches [12, 22] but they are less directly linked to our method. Past work has also used a discriminative objective to learn a generative model [29, 13]. However, those approaches rely on calculations that cannot be done for deep generative models. They are hard to approximate for deep models since they need ratios of probabilities, which cannot be bounded variationaly. Noise-contrastive estimation (NCE) [13] learns a generative model by finding weights that allow the model to distinguish data from a static noise distribution.",A,1
Generative Adversarial Nets,"Using a previously trained model as the noise distribution allows training a sequence of models of increasing quality. This can be seen as an informal competition mechanism similar in spirit to the formal competition used in the adversarial networks game. The key limitation of NCE is that its “discriminator” is defined by the ratio of the probability densities of the noise distribution and the model distribution, and thus requires the ability to evaluate and backpropagate through both densities. Some previous work has used the general concept of having two neural networks compete.","Leveraging a pre-trained model as the noise distribution enables training a sequence of progressively better models. This can be viewed as an informal competition mechanism analogous in essence to the formal competition utilized in adversarial networks. A primary constraint of NCE is its ""discriminator"" being characterized by the proportion of the probability densities of the noise distribution and model distribution, hence necessitating the capacity to compute and backpropagate through both densities. Prior work has applied the general notion of having two neural networks contend.","Using a previously learned model as the noise distribution facilitates instructing a succession of increasingly superior models. This could be interpreted as an informal rivalry mechanism similar in spirit to the formal rivalry employed in adversarial networks. The major limitation of NCE is its ""discriminator"" being defined by the ratio of the probability densities of the noise distribution and model distribution, thus needing the ability to evaluate and backpropagate through both densities. Some past work has utilized the general concept of having two neural networks compete against each other.  ","Leveraging a formerly trained model as the noise distribution enables teaching a sequence of progressively better models. This can be viewed as an informal competition mechanism analogous in essence to the formal competition used in adversarial networks. A key constraint of NCE is its ""discriminator"" being characterized by the proportion of the probability densities of the noise distribution and model distribution, therefore necessitating the ability to compute and propagate gradients through both densities. Prior work has applied the general notion of having two neural networks engage in rivalry.",A,1
Generative Adversarial Nets,"The most relevant work is predictability minimization [26]. In predictability minimization, each hidden unit in a neural network is trained to be different from the output of a second network, which predicts the value of that hidden unit given the value of all of the other hidden units. This work differs from predictability minimization in three important ways: 1) in this work, the competition between the networks is the sole training criterion, and is sufficient on its own to train the network. Predictability minimization is only a regularizer that encourages the hidden units of a neural network to be statistically independent while they accomplish some other task; it is not a primary training criterion. 2) The nature of the competition is different.","The most pertinent prior work is minimizing predictability [26]. In minimizing predictability, every concealed unit in a neural network is conditioned to diverge from the production of a second network, which estimates the value of that concealed unit given the values of all the other concealed units. This work is distinct from minimizing predictability in three major aspects: 1) here, the rivalry between the networks is the only training standard, and is adequate by itself to train the network. Minimizing predictability is only a regularizer that promotes the concealed units of a neural network to be statistically autonomous while they accomplish some other task; it is not a primary training standard. 2) The essence of the rivalry is different.","The most relevant previous research is predictability reduction [26]. In predictability reduction, each hidden node in a neural network is trained to differ from the output of a second network, which predicts the value of that hidden node given the value of all the other hidden nodes. This work is different from predictability reduction in three key ways: 1) here, the competition between the networks is the sole training objective, and is sufficient by itself to train the network. Predictability reduction is only a regularizer that encourages the hidden nodes of a neural network to be statistically independent while accomplishing some other task; it is not a primary training objective. 2) The nature of the competition is different.","The most applicable prior effort is unpredictability maximization [26]. In unpredictability maximization, every concealed unit in a neural network is conditioned to diverge from the production of a second network, which approximates the value of that concealed unit given the values of all the other concealed units. This effort differs from unpredictability maximization in three crucial aspects: 1) here, the contention between the networks is the sole training criterion, and is adequate on its own to train the network. Unpredictability maximization is only a regularizer that promotes the concealed units of a neural network to be statistically autonomous while they accomplish some other task; it is not a primary training criterion. 2) The essence of the contention is different.",A,1
Generative Adversarial Nets,"In predictability minimization, two networks’ outputs are compared, with one network trying to make the outputs similar and the other trying to make the outputs different. The output in question is a single scalar. In GANs, one network produces a rich, high dimensional vector that is used as the input to another network, and attempts to choose an input that the other network does not know how to process. 3) The specification of the learning process is different. Predictability minimization is described as an optimization problem with an objective function to be minimized, and learning approaches the minimum of the objective function.","In predictability minimization, two networks' outputs are analyzed, with one network attempting to make the outputs alike and the other trying to make the outputs distinct. The output being examined is a single scalar value. In GANs, one network generates a complex, high dimension vector that is utilized as the input to another network, and tries to select an input that the other network cannot handle. The definition of the learning procedure is different. Predictability minimization is depicted as an optimization issue with a goal function to be reduced, and learning moves toward the minimum of the goal function.","In predictability minimization, two networks have their outputs compared, with one network striving to make the outputs similar and the other attempting to make the outputs different. The output being looked at is a single scalar. In GANs, one network produces a rich, high dimensional vector which is fed into another network, and tries to pick an input that the other network is unable to process. The explanation of the learning algorithm is different. Predictability minimization is described as an optimization problem with an objective function to be decreased, and learning approaches the minimum of the objective function.","In predictability minimization, two networks have their outputs contrasted, with one network working to make the outputs alike and the other working to make the outputs distinct. The output in question is a single scalar value. In GANs, one network generates a complex, high dimension vector which is utilized as the input to another network, and attempts to select an input that the other network does not comprehend how to process. The characterization of the learning procedure is different. Predictability minimization is portrayed as an optimization issue with a goal function to be reduced, and learning moves toward the minimum of the goal function.",A,1
Generative Adversarial Nets,"GANs are based on a minimax game rather than an optimization problem, and have a value function that one agent seeks to maximize and the other seeks to minimize. The game terminates at a saddle point that is a minimum with respect to one player’s strategy and a maximum with respect to the other player’s strategy. Generative adversarial networks has been sometimes confused with the related concept of “adversarial examples” [28]. Adversarial examples are examples found by using gradient-based optimization directly on the input to a classification network, in order to find examples that are similar to the data yet misclassified. This is different from the present work because adversarial examples are not a mechanism for training a generative model.","GANs rely on a minimax game instead of an optimization problem. They have a value function where one agent tries to maximize it and the other tries to minimize it. The game ends when a saddle point is reached, which is the minimum for one player's plan and the maximum for the other player's plan. GANs have sometimes been mixed up with the related idea of ""adversarial examples"". Adversarial examples are found by using gradient-based optimization right on the input to a classification network, to find examples that are like the data but misclassified. This is not the same as the present work because adversarial examples are not a way to train a generative model.","GANs are built on a minimax game rather than an optimization challenge, and have a value function where one agent attempts to maximize it and the other attempts to minimize it. The game finishes at a saddle point which is a minimum regarding one player's strategy and a maximum regarding the other player's strategy. Generative adversarial networks have sometimes been confused with the related concept of ""adversarial examples"". Adversarial examples are examples discovered by utilizing gradient-based enhancement directly on the input to a classification network, in order to discover examples that are comparable to the data yet misclassified. This is distinct from the present work because adversarial examples are not an instrument for preparing a generative model.","GANs are founded on a minimax match rather than an optimization dilemma, and possess a value function where one agent tries to maximize it and the other tries to minimize it. The match concludes at a saddle point which is a minimum with respect to one player's plan and a maximum with respect to the other player's plan. Generative adversarial networks have occasionally been muddled with the related notion of ""adversarial instances"". Adversarial instances are instances uncovered by operating gradient-based improvement directly on the input to a classification network, in order to uncover instances that are similar to the data yet misclassified. This is dissimilar from the present work because adversarial instances are not an apparatus for educating a generative model.",A,1
Generative Adversarial Nets," Instead, adversarial examples are primarily an analysis tool for showing that neural networks behave in intriguing ways, often confidently classifying two images differently with high confidence even though the difference between them is imperceptible to a human observer. The existence of such adversarial examples does suggest that generative adversarial network training could be inefficient, because they show that it is possible to make modern discriminative networks confidently recognize a class without emulating any of the human-perceptible attributes of that class.","Adversarial examples are not a security concern, but rather a way to analyze neural networks. They show neural networks can be very confident in classifying two extremely similar images differently. This implies generative adversarial network training may be inefficient, since it's possible to make networks certain of a classification without matching any human-noticeable features of that class.","Instead of a security issue, adversarial examples chiefly demonstrate intriguing neural network behavior. They can categorize two nearly identical images differently with high confidence, even if humans can't see a difference. This suggests generative adversarial network learning could be ineffective, since it's feasible to make modern discriminative networks sure of a class without imitating any human-visible traits of that class. ","Rather than a threat, adversarial examples mostly reveal fascinating aspects of how neural networks function. They can separate two imperceptibly different images into distinct classes with high certainty. This hints that generative adversarial network education may not be optimal, given it's doable to make current discriminative networks steadfastly detect a class without replicating any human-discernible qualities of that class.",A,1
Generative Adversarial Nets,"In the next section, we present a theoretical analysis of adversarial nets, essentially showing that the training criterion allows one to recover the data generating distribution as G and D are given enough capacity, i.e., in the non-parametric limit. See Figure 1 for a less formal, more pedagogical explanation of the approach. In practice, we must implement the game using an iterative, numerical approach. Optimizing D to completion in the inner loop of training is computationally prohibitive, and on finite datasets would result in overfitting.","The following part puts forth a theoretical examination of adversarial networks, fundamentally demonstrating that the training standard enables one to regain the data producing distribution as G and D have adequate ability, meaning in the non-parametric limit. Refer to Figure 1 for a less formal, more educational clarification of the method. In application, we need to carry out the game utilizing an iterative, numerical approach. Enhancing D to fulfillment in the internal cycle of preparing is computationally impractical, and on limited datasets would bring about overfitting.","In the next portion, we introduce a hypothetical investigation of adversarial networks, basically showing that the preparation benchmark permits one to recover the information producing dissemination as G and D are given enough limit, i.e., in the non-parametric limit. See Figure 1 for a less formal, more instructive clarification of the methodology. In practice, we should execute the game utilizing an iterative, numerical methodology. Streamlining D to fulfillment in the internal loop of preparing is computationally prohibitive, and on limited datasets would bring about overfitting. ","The accompanying area presents a hypothetical examination of adversarial networks, fundamentally showing that the preparation standard empowers one to reclaim the information creating dissemination as G and D have satisfactory ability, meaning in the non-parametric limit. Refer to Figure 1 for a less formal, more enlightening clarification of the strategy. In application, we need to play out the game using an iterative, numerical methodology. Improving D to satisfaction in the internal cycle of preparing is computationally impractical, and on restricted datasets would bring about overfitting.",A,1
Generative Adversarial Nets,"Instead, we alternate between k steps of optimizing D and one step of optimizing G. This results in D being maintained near its optimal solution, so long as G changes slowly enough. The procedure is formally presented in Algorithm 1. In practice, equation 1 may not provide sufficient gradient for G to learn well. Early in learning, when G is poor, D can reject samples with high confidence because they are clearly different from the training data. In this case, log(1 − D(G(z))) saturates. Rather than training G to minimize log(1 − D(G(z))) we can train G to maximize log D(G(z)). This objective function results in the same fixed point of the dynamics of G and D but provides much stronger gradients early in learning.","Instead, we take turns optimizing D for k steps and optimizing G for one step. This keeps D near its best solution, provided G changes gradually enough. The process is formally shown in Algorithm 1. In practice, equation 1 may not sufficiently train G. Early on, when G is poor, D can confidently reject samples since they clearly differ from the training data. Here, log(1 − D(G(z))) levels off. Rather than training G to minimize log(1 − D(G(z))), we can train G to maximize log D(G(z)). This objective gives the same fixed point for G and D dynamics but much better gradients early in learning.","Rather, we alternate between k iterations of enhancing D and one iteration of enhancing G. This maintains D near its optimal form, assuming G alters slowly. The technique is formally depicted in Algorithm 1. In reality, equation 1 may not adequately gradient G to learn well. Initially, when G is inadequate, D can dismiss samples with certainty since they visibly differ from the training information. Here, log(1 − D(G(z))) saturates. Instead of coaching G to minimize log(1 − D(G(z))), we can coach G to maximize log D(G(z)). This goal yields the same fixed point of the G and D dynamics but provides much stronger gradients early in learning.","Instead, we switch between k phases of refining D and one phase of refining G. This keeps D near its best arrangement, provided G evolves gradually. The workflow is formally presented in Algorithm 1. In practice, equation 1 may not sufficiently slope G to learn effectively. At the start, when G is poor, D can reject samples with assurance since they are clearly distinct from the training data. Here, log(1 − D(G(z))) plateaus. Rather than directing G to minimize log(1 − D(G(z))), we can direct G to maximize log D(G(z)). This aim produces the same fixed point of the G and D kinetics but gives much steeper gradients early in learning.",A,1
Generative Adversarial Nets,"Parzen window-based log-likelihood estimates. The reported numbers on MNIST are the mean loglikelihood of samples on test set, with the standard error of the mean computed across examples. On TFD, we computed the standard error across folds of the dataset, with a different σ chosen using the validation set of each fold. On TFD, σ was cross validated on each fold and mean log-likelihood on each fold were computed. For MNIST we compare against other models of the real-valued (rather than binary) version of dataset.","Log-likelihood approximations using Parzen window method. The MNIST results show the average log-likelihood for test set samples, with standard error of mean calculated across samples. For TFD, we calculated standard error over dataset folds, optimizing σ on each fold's validation set. For TFD, σ was validated per fold and mean log-likelihood was calculated per fold. On MNIST we compare with other models for the real-valued MNIST dataset version.","Estimating log-likelihoods via Parzen window approach. The MNIST numbers are mean log-likelihood on test samples, standard error of mean over test cases. For TFD, standard error was computed over folds, picking σ per fold's validation set. σ was chosen per fold for TFD, mean log-likelihood per fold. MNIST compares to other real-valued MNIST models.  ","Log-likelihood estimates using Parzen windows. MNIST results are average log-likelihood on test samples, standard error of mean over test samples. On TFD, standard error was computed across folds, selecting σ using each fold's validation set. σ was cross-validated per fold on TFD, mean log-likelihood per fold calculated. MNIST compares with other models on real-valued version of dataset.",A,1
Generative Adversarial Nets,"We trained adversarial nets on a range of datasets including MNIST[21], the Toronto Face Database (TFD) [27], and CIFAR-10 [19]. The generator nets used a mixture of rectifier linear activations [17, 8] and sigmoid activations, while the discriminator net used maxout [9] activations. Dropout [16] was applied in training the discriminator net. While our theoretical framework permits the use of dropout and other noise at intermediate layers of the generator, we used noise as the input to only the bottommost layer of the generator network.","We educated hostile networks on various data collections containing MNIST[21], the Toronto Face Database (TFD) [27], and CIFAR-10 [19]. The generator networks utilized a combination of rectifier linear activations [17, 8] and sigmoid activations, whereas the discriminator network utilized maxout [9] activations. Dropout [16] was applied when teaching the discriminator network. Although our theoretical structure enables using dropout and other noise at middle layers of the generator, we utilized noise as the input to only the last layer of the generator network.","We trained antagonistic nets on a variety of datasets encompassing MNIST[21], the Toronto Face Database (TFD) [27], and CIFAR-10 [19]. The generator nets harnessed a mixture of rectifier linear activations [17, 8] and sigmoid activations, while the discriminator net leveraged maxout [9] activations. Dropout [16] was employed in educating the discriminator net. Despite our theoretical framework allowing the use of dropout and other noise at intermediate layers of the generator, we employed noise as the input to only the bottom layer of the generator network.  ","We educated adversarial networks on several datasets containing MNIST[21], the Toronto Face Database (TFD) [27], and CIFAR-10 [19]. The generator networks used a blend of rectifier linear activations [17, 8] and sigmoid activations, while the discriminator network used maxout [9] activations. Dropout [16] was utilized when training the discriminator network. Although our theoretical framework permits using dropout and other noise at middle layers of the generator, we utilized noise as the input to only the base layer of the generator network.",A,1
Generative Adversarial Nets,"We estimate probability of the test set data under pg by fitting a Gaussian Parzen window to the samples generated with G and reporting the log-likelihood under this distribution. The σ parameter of the Gaussians was obtained by cross validation on the validation set. This procedure was introduced in Breuleux et al. [7] and used for various generative models for which the exact likelihood is not tractable [24, 3, 4]. Results are reported in Table 1. This method of estimating the likelihood has somewhat high variance and does not perform well in high dimensional spaces but it is the best method available to our knowledge.","We calculate the likelihood of the test data based on pg by adapting a Gaussian Parzen window to the examples created by G and documenting the log-probability under this distribution. The σ parameter of the Gaussians was found through cross approval on the validation set. This process was presented in Breuleux et al. [7] and utilized for various generative models where the precise likelihood is not solvable [24, 3, 4]. Outcomes are shown in Table 1. This technique of estimating the likelihood has fairly high changeability and does not work well in high dimensional spaces but it is the best approach available to our understanding.","We evaluate the chance of the test set information under pg by installing a Gaussian Parzen window to the specimens produced with G and revealing the log-likelihood under this dissemination. The σ parameter of the Gaussians was acquired by cross endorsement on the approval set. This system was acquainted in Breuleux et al. [7] and utilized for different generative models where the careful probability is not explainable [24, 3, 4]. Results are accounted for in Table 1. This strategy for assessing the likelihood has somewhat high difference and does not perform well in high dimensional spaces however it is the most ideal approach accessible to our insight.","We gauge the probability of the test set information under pg by fitting a Gaussian Parzen window to the examples created with G and uncovering the log-probability under this dispersion. The σ parameter of the Gaussians was gotten by cross support on the approval set. This procedure was presented in Breuleux et al. [7] and utilized for different generative models where the exact likelihood is not comprehensible [24, 3, 4]. Results are accounted for in Table 1. This technique for evaluating the likelihood has somewhat high changeability and does not perform well in high dimensional spaces yet it is the best technique accessible to our comprehension.",A,1
Generative Adversarial Nets,"Advances in generative models that can sample but not estimate likelihood directly motivate further research into how to evaluate such models. In Figures 2 and 3 we show samples drawn from the generator net after training. While we make no claim that these samples are better than samples generated by existing methods, we believe that these samples are at least competitive with the better generative models in the literature and highlight the potential of the adversarial framework.","Improvements in generative models that can produce samples but not directly calculate likelihood encourage more research into how to assess such models. In Figures 2 and 3 we display examples produced by the generator network after training. Although we do not claim these samples are superior to samples created by current techniques, we think these samples are at minimum on par with the best generative models available and demonstrate the promise of the adversarial framework.","Progress in generative models that can generate samples but cannot directly compute likelihood prompts further investigation into how to evaluate these models. In Figures 2 and 3 we present samples created by the generator network after training. While we do not state that these samples are better than samples made by existing methods, we believe these samples are competitive with the top generative models in research and highlight the possibilities of the adversarial approach.  ","Advancements in generative models that can produce samples but cannot directly determine likelihood motivate more study into how to appraise such models. In Figures 2 and 3 we display examples produced by the generator network after training. Although we do not assert that these samples are superior to samples produced by current techniques, we consider these samples to be comparable to the best generative models available and exhibit the potential of the adversarial system.",A,1
Generative Adversarial Nets,"This new framework comes with advantages and disadvantages relative to previous modeling frameworks. The disadvantages are primarily that there is no explicit representation of pg(x), and that D must be synchronized well with G during training (in particular, G must not be trained too much without updating D, in order to avoid “the Helvetica scenario” in which G collapses too many values of z to the same value of x to have enough diversity to model pdata), much as the negative chains of a Boltzmann machine must be kept up to date between learning steps.","This recently introduced structure has both positives and negatives compared to earlier modeling systems. The negatives are mostly that pg(x) is not directly depicted, and D needs to be well coordinated with G during learning (specifically, G can't be trained excessively without refreshing D, to avoid ""the Helvetica case"" where G maps too many z values to the same x value to have sufficient diversity to represent pdata), similar to how the negative chains of a Boltzmann machine must be kept current between learning phases.","This newly presented framework has some advantages and drawbacks relative to prior modeling approaches. The drawbacks are primarily the lack of explicit representation for pg(x), and the need to synchronize D closely with G during training (in particular, avoiding overtraining G without updating D, which can lead to ""the Helvetica scenario"" where G maps too many z values to the same x value to adequately capture diversity for modeling pdata), analogous to keeping the negative chains updated in a Boltzmann machine. ","This newly introduced architecture has some pros and cons compared to previous modeling frameworks. The cons are mostly the absence of direct depiction of pg(x), and the requirement to coordinate D tightly with G during learning (specifically, avoiding overtraining G without refreshing D, which can result in ""the Helvetica case"" where G compresses too many z values into the same x value to have enough diversity to represent pdata), similar to maintaining up-to-date negative chains in a Boltzmann machine.",A,1
Generative Adversarial Nets,"The advantages are that Markov chains are never needed, only backprop is used to obtain gradients, no inference is needed during learning, and a wide variety of functions can be incorporated into the model. Table 2 summarizes the comparison of generative adversarial nets with other generative modeling approaches. The aforementioned advantages are primarily computational. Adversarial models may also gain some statistical advantage from the generator network not being updated directly with data examples, but only with gradients flowing through the discriminator.","The benefits are that Markov chains are not required, backpropagation alone is utilized to get gradients, no inference is necessary during training, and many functions can be included in the model. Table 2 outlines the comparison of generative adversarial networks with other generative modeling techniques. The previously mentioned benefits are mostly computational. Adversarial models may also gain some statistical benefit from the generator network not being directly updated with data samples, but only with gradients going through the discriminator.","The positives are that Markov chains are not needed, only backpropagation is used to get gradients, no inference is required during learning, and many functions can be built into the model. Table 2 summarizes the contrast of generative adversarial networks with other generative modeling methods. The aforesaid positives are primarily computational. Adversarial models may also gain some statistical advantage from the generator network not being directly updated with data examples, but only with gradients passing through the discriminator.  ","The pros are that Markov chains are unnecessary, only backprop is utilized to derive gradients, no inference is necessitated during training, and numerous functions can be incorporated into the model. Table 2 outlines the comparison of generative adversarial nets with other generative modeling approaches. The aforementioned pros are mostly computational. Adversarial models may also gain some statistical benefit from the generator network not being directly updated with data instances, but only with gradients flowing through the discriminator.",A,1
Generative Adversarial Nets,"Visualization of samples from the model. Rightmost column shows the nearest training example of the neighboring sample, in order to demonstrate that the model has not memorized the training set. Samples are fair random draws, not cherry-picked. Unlike most other visualizations of deep generative models, these images show actual samples from the model distributions, not conditional means given samples of hidden units. Moreover, these samples are uncorrelated because the sampling process does not depend on Markov chain mixing. a) MNIST b) TFD c) CIFAR-10 (fully connected model) d) CIFAR-10 (convolutional discriminator and “deconvolutional” generator).","Display of randomly chosen outputs from the model. The column on the far right displays the closest training instance to each generated sample, proving the model has not just memorized the training data. The samples shown are fair draws, not hand-picked. Dissimilar to most visualizations of deep generative models, these images are real samples from the model's distributions, not conditional averages based on samples of hidden units. Also, these samples are independent as the sampling process does not rely on Markov chain mixing. a) MNIST b) TFD c) CIFAR-10 (fully connected model) d) CIFAR-10 (convolutional discriminator and ""deconvolutional"" generator).","Randomly selected model outputs pictured. Final column exhibits closest training example to each output, verifying model did not just memorize training information. Exhibited samples are fair, not cherry-picked. Contrary to most visualizations of deep generative models, these images are genuine samples from model distributions, not conditional means given hidden unit samples. Furthermore, these samples are uncorrelated since sampling process does not depend on Markov chain mixing. a) MNIST b) TFD c) CIFAR-10 (fully connected model) d) CIFAR-10 (convolutional discriminator and “deconvolutional” generator).","Visual examples of randomly chosen model outputs. The last column shows the most similar training example to each output, proving the model did not simply memorize the training data. The samples shown are fair draws, not selectively chosen. Unlike most visualizations of deep generative models, these images display actual samples from the model distributions, not conditional means based on hidden unit samples. Also, these samples are independent because the sampling process does not use Markov chain mixing. a) MNIST b) TFD c) CIFAR-10 (fully connected model) d) CIFAR-10 (convolutional discriminator and ""deconvolutional"" generator).",A,1
Generative Adversarial Nets,"Generative adversarial nets are trained by simultaneously updating the discriminative distribution (D, blue, dashed line) so that it discriminates between samples from the data generating distribution (black, dotted line) px from those of the generative distribution pg (G) (green, solid line). The lower horizontal line is the domain from which z is sampled, in this case uniformly. The horizontal line above is part of the domain of x. The upward arrows show how the mapping x = G(z) imposes the non-uniform distribution pg on transformed samples. G contracts in regions of high density and expands in regions of low density of pg. (a) Consider an adversarial pair near convergence: pg is similar to pdata and D is a partially accurate classifier.","Generative adversarial networks learn by concurrently changing the discriminative model (D, blue, dashed line) so it can tell apart examples from the real data distribution (black, dotted line) px versus the fake data distribution pg (G) (green, solid line). The bottom horizontal line shows the domain z is sampled from, uniformly here. The top horizontal line is part of the domain of x. The up arrows demonstrate how the mapping x = G(z) forces the non-uniform distribution pg on the transformed samples. G squeezes in high density areas and stretches in low density areas of pg. (a) Look at an adversarial pair near convergence: pg is close to pdata and D is a partially correct classifier.","Generative adversarial networks are trained simultaneously by modifying the discriminator model (D, blue, dashed line) to distinguish between instances from the true data distribution (black, dotted line) px and the generated data distribution pg (G) (green, solid line). The lower horizontal line represents the domain z is randomly sampled from. The upper horizontal line is part of the domain of x. The upward arrows exhibit how the function x = G(z) induces the non-uniform distribution pg on the transformed samples. G compresses in high density regions and expands in low density regions of pg. (a) Consider a trained adversarial pair: pg resembles pdata closely and D is a partially accurate classifier.","Generative adversarial networks learn concurrently by adapting the discriminator (D, blue, dashed line) to tell real data samples from the actual data distribution (black, dotted line) px apart from fake samples from the generated distribution pg (G) (green, solid line). The bottom horizontal line is the domain z is randomly drawn from. The top horizontal line is part of the domain of x. The up arrows display how the mapping x = G(z) enforces the non-uniform distribution pg on the transformed samples. G shrinks in high density areas and enlarges in low density areas of pg. (a) Examine a trained adversarial pair: pg is similar to pdata and D is a partially correct classifier.",A,1
Generative Adversarial Nets,"We would like to acknowledge Patrice Marcotte, Olivier Delalleau, Kyunghyun Cho, Guillaume Alain and Jason Yosinski for helpful discussions. Yann Dauphin shared his Parzen window evaluation code with us. We would like to thank the developers of Pylearn2 [11] and Theano [6, 1], particularly Fred´ eric Bastien who rushed a Theano feature specifically to benefit this project. Arnaud Bergeron provided much-needed support with LATEX typesetting. We would also like to thank CIFAR, and Canada Research Chairs for funding, and Compute Canada, and Calcul Quebec for ´ providing computational resources. Ian Goodfellow is supported by the 2013 Google Fellowship in Deep Learning. Finally, we would like to thank Les Trois Brasseurs for stimulating our creativity.","We express gratitude to Patrice Marcotte, Olivier Delalleau, Kyunghyun Cho, Guillaume Alain and Jason Yosinski for their helpful conversations. Yann Dauphin generously shared his Parzen window assessment code with us. We appreciate the developers of Pylearn2 [11] and Theano [6, 1], especially Fred ́er ic Bastien who swiftly added a Theano capability specifically for this project. Arnaud Bergeron gave much-needed help with LATEX formatting. We also thank CIFAR, Canada Research Chairs for funding, Compute Canada, and Calcul Quebec for providing computing resources. Ian Goodfellow receives support from the 2013 Google Fellowship in Deep Learning. Finally, we are grateful to Les Trois Brasseurs for stimulating our creativity.","We would like to thank Patrice Marcotte, Olivier Delalleau, Kyunghyun Cho, Guillaume Alain and Jason Yosinski for their insightful discussions. Yann Dauphin kindly shared his Parzen window evaluation code with our team. We are appreciative of the developers of Pylearn2 [11] and Theano [6, 1], especially Fred ́eric Bastien who quickly added a Theano feature specifically for this project. Arnaud Bergeron provided much-needed assistance with LATEX formatting. We also acknowledge CIFAR, Canada Research Chairs for funding, Compute Canada, and Calcul Quebec for providing computing resources. Ian Goodfellow is funded by the 2013 Google Fellowship in Deep Learning. Finally, we thank Les Trois Brasseurs for stimulating our creative thinking.","We extend our thanks to Patrice Marcotte, Olivier Delalleau, Kyunghyun Cho, Guillaume Alain and Jason Yosinski for their valuable discussions. Yann Dauphin generously provided us with his Parzen window evaluation code. We are grateful to the developers of Pylearn2 [11] and Theano [6, 1], particularly Fred ́eric Bastien who swiftly implemented a Theano feature specifically for this project. Arnaud Bergeron gave much-needed support with LATEX typesetting. We also acknowledge CIFAR, Canada Research Chairs for funding, Compute Canada, and Calcul Quebec for providing computing resources. Ian Goodfellow receives funding from the 2013 Google Fellowship in Deep Learning. Finally, we thank Les Trois Brasseurs for stimulating our creativity.",A,1
GloVe_Global Vectors for Word Representation,"Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods.","The latest techniques for creating vector representations of words have been successful in capturing subtle semantic and syntactic patterns using vector math. However, the source of these patterns has not been clear. We examine and clarify the model attributes required for such patterns to appear in word vectors. This results in a new global logbilinear regression model that unites the benefits of the two main model families in the literature: global matrix factorization and local context window approaches.",Recently developed methods for generating vector space representations of words have managed to capture fine semantic and syntactic regularities by using vector arithmetic. But where these regularities come from has been unclear. We inspect and explicate the model features needed for such regularities to emerge in word vectors. This produces a new global logbilinear regression model that brings together the strengths of the two predominant model types described in the literature: global matrix factorization and local context window techniques.,"The latest procedures for learning vector space embeddings of words have been able to capture subtle semantic and syntactic patterns through vector math operations. However, the source of these patterns has remained mysterious. We analyze and make explicit the model characteristics necessary for such patterns to materialize in word vectors. The end result is a new global logbilinear regression model that combines the upsides of the two major model families discussed in the literature: global matrix factorization and local context window methods.",A,1
GloVe_Global Vectors for Word Representation,"Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.","Our system effectively uses statistical data by learning only from the non-zero components in a word-word co-occurrence table, instead of the entire sparse table or separate context windows in a large dataset. The system generates a vector space with meaningful substructure, as shown by its 75% accuracy on a recent word analogy task. It also exceeds the performance of related models on similarity tasks and named entity recognition.","Our algorithm efficiently harnesses statistical patterns by being trained exclusively on the non-null elements in a word-word co-occurrence matrix, rather than the full sparse matrix or individual context windows in an extensive corpus. The algorithm produces a vector space with meaningful latent structure, demonstrated by its 75% score on a recent word analogy evaluation. It also surpasses related algorithms on similarity evaluations and named entity recognition.","Our program successfully leverages statistical information by learning solely from the filled components in a word-word co-occurrence array, rather than the whole sparse array or isolated context windows in a massive dataset. The program generates a vector space with meaningful underlying structure, validated by its 75% result on a recent word analogy assessment. It also outperforms analogous programs on similarity assessments and named entity recognition.",A,1
GloVe_Global Vectors for Word Representation,"Semantic vector space models of language represent each word with a real-valued vector. These vectors can be used as features in a variety of applications, such as information retrieval (Manning et al., 2008), document classification (Sebastiani, 2002), question answering (Tellex et al., 2003), named entity recognition (Turian et al., 2010), and parsing (Socher et al., 2013). Most word vector methods rely on the distance or angle between pairs of word vectors as the primary method for evaluating the intrinsic quality of such a set of word representations.","Semantic vector area models of language signify every term with a real-valued vector. These vectors can be utilized as characteristics in an assortment of uses, for example, data retrieval (Manning et al., 2008), report characterization (Sebastiani, 2002), question replying (Tellex et al., 2003), named element acknowledgment (Turian et al., 2010), and parsing (Socher et al., 2013). Most word vector techniques depend on the distance or point between sets of word vectors as the essential technique for assessing the natural nature of such a lot of word portrayals.","Semantic vector space prototypes of language emblematize each lexicon with a genuine-esteemed vector. These vectors can be harnessed as lineaments in a miscellany of exertions, savor information salvaging (Manning et al., 2008), scroll categorization (Sebastiani, 2002), question rejoining (Tellex et al., 2003), designated entity admission (Turian et al., 2010), and parsing (Socher et al., 2013). Most word vector customs rest on the aloofness or corner between braces of word vectors as the cardinal routine for evaluating the congenital calibre of such a collect of word depictions. ","Semantic vector latitude patterns of language betoken every term with a real-valued vector. These vectors can be utilized as attributes in a variety of endeavors, for instance, data retrieval (Manning et al., 2008), report characterization (Sebastiani, 2002), question replying (Tellex et al., 2003), named element acknowledgment (Turian et al., 2010), and parsing (Socher et al., 2013). Most word vector techniques depend on the remoteness or point between sets of word vectors as the key technique for surveying the natural nature of such a gathering of word portrayals.",A,1
GloVe_Global Vectors for Word Representation,"Recently, Mikolov et al. (2013c) introduced a new evaluation scheme based on word analogies that probes the finer structure of the word vector space by examining not the scalar distance between word vectors, but rather their various dimensions of difference. For example, the analogy “king is to queen as man is to woman” should be encoded in the vector space by the vector equation king − queen = man − woman. This evaluation scheme favors models that produce dimensions of meaning, thereby capturing the multi-clustering idea of distributed representations (Bengio, 2009).","Lately, Mikolov and colleagues (2013c) presented a new system for evaluating word embeddings based on word analogies that investigates the more detailed structure of the vector space by looking not at the scalar distance between vectors, but rather their different dimensions of contrast. For instance, the analogy ""king is to queen as man is to woman"" should be represented in the vector space by the vector equation king - queen = man - woman. This evaluation approach prefers models that generate dimensions of meaning, thereby capturing the concept of distributed representations (Bengio, 2009).","Recently, Mikolov and co-authors (2013c) brought forward a novel assessment methodology for word embeddings founded on word analogies that examines the finer-grained arrangement of the vector space by considering not the scalar separation between word vectors, but rather their multiple facets of difference. As an example, the analogy ""king is to queen as man is to woman"" ought to be encoded in the vector space by the vector equation king - queen = man - woman. This evaluation methodology favors models that produce dimensions of meaning, thereby seizing the idea of distributed representations (Bengio, 2009). ","Not long ago, Mikolov and colleagues (2013c) presented a new evaluation framework based on word analogies that investigates the more intricate structure of the vector space by analyzing not the scalar distance between word vectors, but rather their various dimensions of contrast. For instance, the analogy ""king is to queen as man is to woman"" should be represented in the vector space by the vector equation king - queen = man - woman. This evaluation framework advantages models that generate dimensions of meaning, thereby capturing the concept of distributed representations (Bengio, 2009).",A,1
GloVe_Global Vectors for Word Representation,"The two main model families for learning word vectors are: 1) global matrix factorization methods, such as latent semantic analysis (LSA) (Deerwester et al., 1990) and 2) local context window methods, such as the skip-gram model of Mikolov et al. (2013c). Currently, both families suffer significant drawbacks. While methods like LSA efficiently leverage statistical information, they do relatively poorly on the word analogy task, indicating a sub-optimal vector space structure. Methods like skip-gram may do better on the analogy task, but they poorly utilize the statistics of the corpus since they train on separate local context windows instead of on global co-occurrence counts.","The two primary groups of techniques for obtaining word vectors are: 1) techniques that factorize a global matrix, like latent semantic analysis (LSA) (Deerwester et al., 1990) and 2) techniques that use local context windows, like the skip-gram model of Mikolov et al. (2013c). At present, both groups have major flaws. Although methods like LSA make good use of statistical data, they are not very good at word analogy tasks, suggesting the vector space structure is suboptimal. Methods like skip-gram may perform better on analogies, but they do not make full use of corpus statistics since they train on separate local context windows rather than global co-occurrence counts.","The two main families of models for learning word embeddings are: 1) approaches that do matrix factorization on the full corpus, such as latent semantic analysis (LSA) (Deerwester et al., 1990) and 2) approaches that look at local context windows, like the skip-gram model of Mikolov et al. (2013c). Currently, both families have significant limitations. While methods like LSA effectively leverage statistical patterns, they perform relatively poorly on word analogy tasks, indicating the vector space structure could be better. Methods like skip-gram may perform well on analogies, but they do not fully utilize corpus statistics since they learn from individual local context windows instead of global co-occurrence counts.","The two primary categories of models for obtaining word vectors are: 1) global techniques that factorize a matrix for the whole corpus, including latent semantic analysis (LSA) (Deerwester et al., 1990) and 2) local techniques that use context windows, such as the skip-gram model of Mikolov et al. (2013c). Both categories currently have major shortcomings. Although global methods like LSA make effective use of statistical patterns, they struggle on word analogy tasks, suggesting suboptimal vector space structure. Local methods like skip-gram may succeed better on analogies, but they fail to fully exploit corpus statistics since they learn from separate context windows rather than global co-occurrences.",A,1
GloVe_Global Vectors for Word Representation,"In this work, we analyze the model properties necessary to produce linear directions of meaning and argue that global log-bilinear regression models are appropriate for doing so. We propose a specific weighted least squares model that trains on global word-word co-occurrence counts and thus makes efficient use of statistics. The model produces a word vector space with meaningful substructure, as evidenced by its state-of-the-art performance of 75% accuracy on the word analogy dataset. We also demonstrate that our methods outperform other current methods on several word similarity tasks, and also on a common named entity recognition (NER) benchmark.","This research examines the model features needed to generate linear shifts in meaning. We show global log-bilinear regression models can achieve this goal. We introduce a weighted least squares model trained on overall word-word co-occurrence statistics for computational efficiency. This model generates a word vector space with meaningful structure, demonstrated by achieving top accuracy of 75% on a word analogy test set. Our methods also surpass other current techniques on several word similarity tasks and a standard named entity recognition (NER) benchmark.","In this paper, we study the model attributes required to create linear trajectories of semantic meaning and posit that global log-bilinear regression models suit this purpose. We put forth a particular weighted least squares model trained on comprehensive word-word co-occurrence frequencies to capitalize on statistical information. This model produces a word vector space exhibiting meaningful latent structure, verified by setting the state-of-the-art at 75% accuracy on a word analogy evaluation set. Additionally, our methods eclipse other existing approaches on multiple word similarity jobs and a prevalent named entity recognition (NER) benchmark.  ","Here we investigate the model properties needed to yield linear shifts in semantic meaning and contend global log-bilinear regression models can deliver this. We introduce a weighted least squares model leveraging global word-word co-occurrence statistics for computational thrift. This model derives a word vector space with interpretable latent structure, proven by achieving premier accuracy of 75% on a word analogy test collection. Our methods also best other current techniques on several word similarity tasks and a widespread named entity recognition (NER) benchmark.",A,1
GloVe_Global Vectors for Word Representation,"Matrix factorization methods for generating low-dimensional word representations have roots stretching as far back as LSA. These methods utilize low-rank approximations to decompose large matrices that capture statistical information about a corpus. The particular type of information captured by such matrices varies by application. In LSA, the matrices are of “term-document” type, i.e., the rows correspond to words or terms, and the columns correspond to different documents in the corpus. In contrast, the Hyperspace Analogue to Language (HAL) (Lund and Burgess, 1996), for example, utilizes matrices of “term-term” type, i.e., the rows and columns correspond to words and the entries correspond to the number of times a given word occurs in the context of another given word.","Methods of generating compact semantic representations of words by factorizing large matrices have origins dating back to latent semantic analysis (LSA). These techniques use low-rank approximations to decompose sizable matrices that encode statistical patterns from a text corpus. The precise kind of data encoded in the matrices varies across applications. In LSA, the matrices have a ""term-document"" structure, meaning the rows represent words or terms, and the columns represent individual documents in the corpus. By contrast, Hyperspace Analogue to Language (HAL) (Lund and Burgess, 1996) employs ""term-term"" matrices, where both rows and columns correspond to words, and entries record how often a given word appears near another given word.","Techniques for producing low-dimensional semantic word vectors by decomposing large matrices trace their history to latent semantic analysis (LSA). These approaches leverage low-rank approximations to factor large matrices capturing statistical information from a text corpus. The specific type of data captured in the matrices changes based on the application. With LSA, the matrices follow a ""term-document"" format, with rows as words or terms, and columns as documents from the corpus. Conversely, Hyperspace Analogue to Language (HAL) (Lund and Burgess, 1996) uses ""term-term"" matrices, where rows and columns are words, and entries signify how often one word occurs near another. ","Methods of constructing compact semantic word embeddings via matrix factorization have their origins in latent semantic analysis (LSA). Such techniques employ low-rank approximations to decompose substantial matrices encoding statistical patterns from a text corpus. The particular kind of information encoded in these matrices varies by use case. In LSA, a ""term-document"" matrix structure is used, with word/terms as rows and individual corpus documents as columns. In contrast, Hyperspace Analogue to Language (HAL) (Lund and Burgess, 1996) adopts a ""term-term"" matrix format, where both rows and columns represent words, and entries capture co-occurrence statistics of word pairs.",A,1
GloVe_Global Vectors for Word Representation,"A main problem with HAL and related methods is that the most frequent words contribute a disproportionate amount to the similarity measure: the number of times two words co-occur with the or and, for example, will have a large effect on their similarity despite conveying relatively little about their semantic relatedness. A number of techniques exist that addresses this shortcoming of HAL, such as the COALS method (Rohde et al., 2006), in which the co-occurrence matrix is first transformed by an entropy- or correlation-based normalization.","A major issue with HAL and similar approaches is that very common words like ""the"" and ""and"" have an outsized influence on the similarity score, even though they don't reveal much about the actual semantic relatedness of words. There are techniques like COALS (Rohde et al., 2006) that address this weakness of HAL by first normalizing the co-occurrence matrix using entropy or correlation.","One significant problem with HAL and analogous methods is that high frequency words contribute excessively to the similarity even though they provide little insight into semantic relatedness. For instance, co-occurrences with ""the"" or ""and"" greatly impact similarity despite conveying minimal semantic meaning. Some existing techniques like COALS (Rohde et al., 2006) mitigate this HAL shortcoming by first transforming the co-occurrence matrix using entropy or correlation normalization.","A major flaw with HAL and similar approaches is that ubiquitous words like ""the"" and ""and"" account for a disproportionate amount of the similarity score, even though they reveal little about true semantic relatedness between words. Some techniques like COALS (Rohde et al., 2006) address this issue in HAL by first normalizing the co-occurrence matrix using entropy or correlation to reduce the influence of these frequent but semantically weak words.",A,1
GloVe_Global Vectors for Word Representation,"An advantage of this type of transformation is that the raw co-occurrence counts, which for a reasonably sized corpus might span 8 or 9 orders of magnitude, are compressed so as to be distributed more evenly in a smaller interval. A variety of newer models also pursue this approach, including a study (Bullinaria and Levy, 2007) that indicates that positive pointwise mutual information (PPMI) is a good transformation. More recently, a square root type transformation in the form of Hellinger PCA (HPCA) (Lebret and Collobert, 2014) has been suggested as an effective way of learning word representations.","One benefit of this kind of change is that the unmodified co-occurrence totals, which for a fairly large body of text might extend over 8 or 9 orders of size, are condensed so they are more evenly spread out in a smaller range. A number of more recent models also use this tactic, including research (Bullinaria and Levy, 2007) showing that positive pointwise mutual information (PPMI) works well as an alteration. Most recently, a square root style change called Hellinger PCA (HPCA) (Lebret and Collobert, 2014) has been recommended as an effective approach for learning word representations.","An advantage of this type of adjustment is that the raw co-occurrence figures, which for a reasonably large collection of text could cover 8 or 9 orders of magnitude, are compressed so they are distributed more uniformly over a smaller interval. Various more modern models also employ this strategy, including a study (Bullinaria and Levy, 2007) indicating that positive pointwise mutual information (PPMI) is a good adjustment. More recently, a square root style adjustment called Hellinger PCA (HPCA) (Lebret and Collobert, 2014) has been suggested as an effective way of learning word representations.  ","One benefit of this kind of transformation is that the unprocessed co-occurrence totals, which for a fairly sizable body of text might span 8 or 9 orders of size, are condensed so they are more evenly dispersed over a smaller range. A number of newer models also utilize this approach, including research (Bullinaria and Levy, 2007) showing that positive pointwise mutual information (PPMI) is an effective transformation. Most recently, a square root form of change called Hellinger PCA (HPCA) (Lebret and Collobert, 2014) has been proposed as an effective method for learning word representations.",A,1
GloVe_Global Vectors for Word Representation,"Another approach is to learn word representations that aid in making predictions within local context windows. For example, Bengio et al. (2003) introduced a model that learns word vector representations as part of a simple neural network architecture for language modeling. Collobert and Weston (2008) decoupled the word vector training from the downstream training objectives, which paved the way for Collobert et al. (2011) to use the full context of a word for learning the word representations, rather than just the preceding context as is the case with language models.","A different tactic is to develop word depictions that help in making forecasts within small surrounding windows. For instance, Bengio et al. (2003) presented a model that learns word vector representations as part of a basic neural network design for language modeling. Collobert and Weston (2008) separated the word vector training from the downstream training goals, which cleared the path for Collobert et al. (2011) to utilize the full context of a word for learning the word representations, rather than just the preceding context as is the case with language models.","An alternative approach is to acquire word characterizations that assist in making predictions within localized contextual windows. As an illustration, Bengio et al. (2003) brought in a model that acquires word vector representations as part of an elementary neural network blueprint for language modeling. Collobert and Weston (2008) disconnected the word vector preparation from the downstream preparation objectives, which paved the way for Collobert et al. (2011) to employ the complete context of a word for learning the word depictions, rather than just the preceding context as is the case with language models.  ","One more strategy is to obtain word portrayals that help in making forecasts inside small surrounding windows. For example, Bengio et al. (2003) presented a model that learns word vector representations as part of a simple neural network design for language modeling. Collobert and Weston (2008) unlinked the word vector training from the downstream training goals, which opened the way for Collobert et al. (2011) to utilize the full context of a word for acquiring the word portrayals, rather than just the preceding context as is the case with language models.",A,1
GloVe_Global Vectors for Word Representation,"Recently, the importance of the full neural network structure for learning useful word representations has been called into question. The skip-gram and continuous bag-of-words (CBOW) models of Mikolov et al. (2013a) propose a simple single-layer architecture based on the inner product between two word vectors. Mnih and Kavukcuoglu (2013) also proposed closely-related vector log-bilinear models, vLBL and ivLBL, and Levy et al. (2014) proposed explicit word embeddings based on a PPMI metric. In the skip-gram and ivLBL models, the objective is to predict a word’s context given the word itself, whereas the objective in the CBOW and vLBL models is to predict a word given its context.","The significance of the complete neural network design for acquiring beneficial word representations was recently questioned. The skip-gram and continuous bag-of-words (CBOW) architectures from Mikolov et al. (2013a) put forth a basic single-layer structure founded on the inner product of two word vectors. Mnih and Kavukcuoglu (2013) also presented strongly linked vector log-bilinear models, vLBL and ivLBL, and Levy et al. (2014) proposed explicit word embeddings based on a PPMI metric. In the skip-gram and ivLBL models, the goal is to predict a word's context given the word itself, while the goal in the CBOW and vLBL models is to predict a word given its context.","Recently, doubts have emerged about how important the full neural network architecture is for learning useful word representations. The skip-gram and continuous bag-of-words (CBOW) models proposed by Mikolov et al. (2013a) have a simple single-layer design based on the inner product of two word vectors. Related vector log-bilinear models called vLBL and ivLBL were also proposed by Mnih and Kavukcuoglu (2013), and Levy et al. (2014) proposed explicit word embeddings using a PPMI metric. The objective in the skip-gram and ivLBL models is to predict a word's context from the word itself, while the objective in the CBOW and vLBL models is to predict a word from its context.","The necessity of the complete neural network structure for acquiring beneficial word representations has recently come into question. The skip-gram and continuous bag-of-words (CBOW) models from Mikolov et al. (2013a) employ a basic single-layer architecture utilizing the inner product of two word vectors. Highly related vector log-bilinear models called vLBL and ivLBL were also put forth by Mnih and Kavukcuoglu (2013), and explicit word embeddings based on a PPMI metric were proposed by Levy et al. (2014). The skip-gram and ivLBL models aim to predict a word's context from the word itself, while the CBOW and vLBL models aim to predict a word from its context.",A,1
GloVe_Global Vectors for Word Representation,"Through evaluation on a word analogy task, these models demonstrated the capacity to learn linguistic patterns as linear relationships between the word vectors. Unlike the matrix factorization methods, the shallow window-based methods suffer from the disadvantage that they do not operate directly on the co-occurrence statistics of the corpus. Instead, these models scan context windows across the entire corpus, which fails to take advantage of the vast amount of repetition in the data.","By testing on a word analogy challenge, these models showed they can learn language patterns as linear associations between the word vectors. In contrast to the matrix factorization techniques, the shallow window-based methods have the downside that they do not work directly with the co-occurrence data of the corpus. Rather, these models look through context windows across the whole corpus, which does not utilize the huge amount of repetition in the data.","Through assessment on a word analogy exercise, these models exhibited the ability to acquire linguistic patterns as linear links between the word vectors. Dissimilar to the matrix factorization approaches, the superficial window-based methods suffer from the weakness that they do not leverage the co-occurrence statistics of the corpus directly. Instead, these models scan context windows over the full corpus, which fails to capitalize on the vast repetition present in the data.  ","By evaluating on a word analogy evaluation, these models proved their capacity to learn language patterns as linear connections between the word vectors. Contrary to the matrix factorization techniques, the shallow window-based methods have the disadvantage that they do not harness the co-occurrence frequencies of the corpus straightaway. Rather, these models traverse context windows throughout the complete corpus, which neglects to exploit the tremendous repetition existent in the data.",A,1
GloVe_Global Vectors for Word Representation,"The statistics of word occurrences in a corpus is the primary source of information available to all unsupervised methods for learning word representations, and although many such methods now exist, the question still remains as to how meaning is generated from these statistics, and how the resulting word vectors might represent that meaning. In this section, we shed some light on this question. We use our insights to construct a new model for word representation which we call GloVe, for Global Vectors, because the global corpus statistics are captured directly by the model.","The frequency of words in a large collection of text is the main data available to any unsupervised technique for learning vector representations of words. Many such methods exist now, but it's still unclear exactly how meaning arises from these frequencies and how the resulting word vectors encode that meaning. Here we provide some insight into this question. We use these ideas to build a new model called GloVe, which stands for Global Vectors, because it directly incorporates global corpus statistics.","The count of how often each word appears in a body of text is the primary information accessible to all unguided ways of learning word embeddings, and while there are now many such approaches, how meaning emerges from these counts and how the resulting word vectors represent meaning remains unclear. In this section, we shed light on this issue. We utilize our understanding to construct a novel model for word embeddings called GloVe, which stands for Global Vectors, since it directly captures global corpus statistics.  ","The tally of word occurrences within a large text dataset is the main data available to any unsupervised learning method for generating vector representations of words. Many such techniques now exist, but how meaning arises from these tallies and how the resulting word vectors encode meaning is still an open question. Here we provide insight into this question. We leverage these insights to build a new model called GloVe, short for Global Vectors, because it incorporates global corpus statistics directly.",A,1
GloVe_Global Vectors for Word Representation,"Co-occurrence probabilities for target words ice and steam with selected context words from a 6 billion token corpus. Only in the ratio does noise from non-discriminative words like water and fashion cancel out, so that large values (much greater than 1) correlate well with properties specific to ice, and small values (much less than 1) correlate well with properties specific of steam.","The likelihood of the words ice and steam appearing together with chosen surrounding words in a 6 billion word text collection. Just in the proportion does randomness from non-descriptive words such as water and fashion disappear, so that big values (far above 1) associate strongly with qualities particular to ice, and small values (far below 1) associate strongly with qualities distinctive of steam.","The chance of the terms ice and steam occurring with selected nearby words in a 6 billion token text database. Only in the rate does variability from non-selective words like water and fashion vanish, so that large ratios (much bigger than 1) connect well with attributes unique to ice, and small ratios (much smaller than 1) connect well with attributes unique to steam.  ","The probability of the words ice and steam co-happening with chosen context words in a 6 billion expression corpus. Just in the quotient does noise from non-discriminating words such as water and fashion fade away, so that high values (much greater than 1) correlate strongly with traits specific to ice, and low values (much less than 1) correlate strongly with traits specific to steam.",A,1
GloVe_Global Vectors for Word Representation,"For words k like water or fashion, that are either related to both ice and steam, or to neither, the ratio should be close to one. Table 1 shows these probabilities and their ratios for a large corpus, and the numbers confirm these expectations. Compared to the raw probabilities, the ratio is better able to distinguish relevant words (solid and gas) from irrelevant words (water and fashion) and it is also better able to discriminate between the two relevant words. The above argument suggests that the appropriate starting point for word vector learning should be with ratios of co-occurrence probabilities rather than the probabilities themselves.","Words such as water or fashion, which are related to both ice and steam or neither, should have a ratio close to one. Table 1 displays these probabilities and ratios for a large text collection, and the figures support these predictions. In contrast to the unmodified probabilities, the ratio is superior at differentiating applicable words (solid and gas) from non-applicable words (water and fashion) and it is also better at telling apart the two applicable words. The preceding reasoning indicates that ratios of co-occurrence probabilities, rather than the raw probabilities, should be the basis for learning word vectors.","Certain words like water or fashion, that are linked to ice and steam or not related to either, will probably have a ratio near one. Table 1 provides these likelihoods and their ratios for a big set of texts, and the values align with these expectations. Compared with the unmodified probabilities, the ratio is more capable of singling out relevant words (solid and gas) from irrelevant words (water and fashion) and it is also better at distinguishing between the two relevant words. The above thinking hints that ratios of co-occurrence probabilities rather than the plain probabilities themselves should be the starting point for learning word vectors.  ","Specific words such as water or fashion, which are associated with both ice and steam or with neither, are expected to have a ratio close to one. Table 1 exhibits these probabilities and ratios for a large collection of texts, and the figures bear out these predictions. In contrast with the raw probabilities, the ratio is more adept at isolating pertinent words (solid and gas) from impertinent words (water and fashion) and it is also better at differentiating between the two pertinent words. The preceding analysis intimates that ratios of co-occurrence probabilities rather than the unmodified probabilities should form the basis for acquiring word vectors.",A,1
GloVe_Global Vectors for Word Representation,"In this equation, the right-hand side is extracted from the corpus, and F may depend on some as-of-yet unspecified parameters. The number of possibilities for F is vast, but by enforcing a few desiderata we can select a unique choice. First, we would like F to encode the information present the ratio Pik /Pjk in the word vector space. Since vector spaces are inherently linear structures, the most natural way to do this is with vector differences. With this aim, we can restrict our consideration to those functions F that depend only on the difference of the two target words.","This formula has the right section derived from the data source, and F might rely on some yet-to-be-defined values. There are many possibilities for F, but enforcing a few preferences allows picking one option. Primarily, we want F to capture the data in the proportion Pik/Pjk within the word vector area. Because vector spaces have an inherent linear design, the most intuitive approach is to use vector differences. With this goal, we can focus only on functions F that use the distinction between the two target words.","In this mathematical expression, the right half comes from the corpus, and F can depend on unspecified parameters. While there are numerous choices for F, imposing certain criteria lets us choose one. We desire F to represent the information in the ratio Pik/Pjk in the word vector field. Since vector spaces have an inherent linear structure, the most natural method is to use vector subtractions. Given this purpose, we can limit ourselves to functions F that rely solely on the difference between the two words of interest.","This formula's right portion is extracted from the data set, and F may utilize unspecified values. The possibilities for F are vast, but enforcing preferences allows selecting one function. Primarily, we want F to convey the data in the proportion Pik/Pjk within the word vector domain. As vector spaces have an inherent linear design, the most intuitive technique is vector differences. With this objective, we can restrict focus to functions F that depend only on the distinction between the two target words.",A,1
GloVe_Global Vectors for Word Representation,"Next, we note that the arguments of F in Eqn. (2) are vectors while the right-hand side is a scalar. While F could be taken to be a complicated function parameterized by, e.g., a neural network, doing so would obfuscate the linear structure we are trying to capture. Next, note that for word-word co-occurrence matrices, the distinction between a word and a context word is arbitrary and that we are free to exchange the two roles. Our final model should be invariant under this relabeling, but Eqn. (3) is not. However, the symmetry can be restored in two steps.","Furthermore, we observe that the inputs to F in Equation 2 are vectors whereas the right side is a scalar. Although F could be represented as a complex function like a neural network, that would conceal the linear structure we want to model. Also, for word-word co-occurrence matrices, the differentiation between a word and a context word is arbitrary and we can swap the two roles. Our final model should be unchanged under this relabeling, but Equation 3 does not have this property. However, we can restore the symmetry in two steps.","In addition, we see that the arguments of F in Formula 2 are vectors while the right hand side is a scalar. Even though F could be a complicated function like a neural network, that would hide the linear structure we want to capture. Moreover, for word-word co-occurrence matrices, the distinction between a word and a context word is random and we can switch the two positions. Our final model should be the same after this relabeling, but Formula 3 does not have this characteristic. Still, we can reinstate the symmetry in two steps.  ","Moreover, we find that the inputs to F in Expression 2 are vectors while the right hand side is a scalar. Despite F could be a complex function like a neural network, that would conceal the linear structure we aim to model. Furthermore, for word-word co-occurrence matrices, the differentiation between a word and a context word is arbitrary and we can exchange the two roles. Our final model should be unaltered under this relabeling, but Expression 3 does not possess this property. However, we can restore the symmetry in two steps.",A,1
GloVe_Global Vectors for Word Representation,"Next, we note that Eqn. (6) would exhibit the exchange symmetry if not for the log(Xi) on the right-hand side. However, this term is independent of k so it can be absorbed into a bias bi for wi. Eqn. (7) is a drastic simplification over Eqn. (1), but it is actually ill-defined since the logarithm diverges whenever its argument is zero. One resolution to this issue is to include an additive shift in the logarithm, log(Xik ) → log(1 + Xik ), which maintains the sparsity of X while avoiding the divergences. The idea of factorizing the log of the co-occurrence matrix is closely related to LSA and we will use the resulting model as a baseline in our experiments.","Subsequently, we observe that Equation 6 would display the exchange symmetry if not for the log(Xi) on the right side. However, this term does not depend on k so it can be combined into a bias bi for wi. Equation 7 is a major simplification over Equation 1, but it is actually not well-defined since the logarithm diverges whenever its argument is zero. One solution to this issue is to include an additive shift in the logarithm, log(Xik) → log(1 + Xik), which maintains the sparsity of X while avoiding the divergences. The concept of factorizing the log of the co-occurrence matrix is strongly connected to LSA and we will utilize the resulting model as a baseline in our experiments.","Next, we point out that Formula 6 would show the exchange symmetry except for the log(Xi) on the right side. But this term does not vary with k so it can be merged into a bias bi for wi. Formula 7 is a huge simplification of Formula 1, but it is actually not properly defined since the logarithm blows up whenever its input is zero. One fix for this problem is to put in an additive shift in the logarithm, log(Xik) → log(1 + Xik), which keeps the sparsity of X while avoiding the divergences. The idea of factorizing the log of the co-occurrence matrix is very similar to LSA and we will use the resulting model as a baseline in our tests.  ","Subsequently, we indicate that Equation 6 would demonstrate the exchange symmetry were it not for the log(Xi) on the right-hand side. However, this term does not depend on k so it can be consolidated into a bias bi for wi. Equation 7 is a major simplification of Equation 1, but it is ill-defined since the logarithm diverges when its argument is zero. One solution is to include an additive shift in the logarithm, log(Xik) → log(1 + Xik), retaining the sparsity of X while avoiding the divergences. Factorizing the log of the co-occurrence matrix is akin to LSA and we will utilize the resulting model as a baseline in our experiments.",A,1
GloVe_Global Vectors for Word Representation,"A main drawback to this model is that it weighs all co-occurrences equally, even those that happen rarely or never. Such rare cooccurrences are noisy and carry less information than the more frequent ones — yet even just the zero entries account for 75–95% of the data in X, depending on the vocabulary size and corpus. We propose a new weighted least squares regression model that addresses these problems.","A major weakness of this method is that it provides equal importance to all co-occurrences, including those that seldom or never happen. These rare co-occurrences are imprecise and convey less useful information compared to more common ones - however, even the zero entries make up 75-95% of the data in X, based on the vocabulary size and corpus. We put forward a new weighted least squares regression framework that tackles these issues.","One significant drawback of this approach is that it assigns the same weight to all co-occurrences, even those that materialize rarely or not at all. Such infrequent co-occurrences are noisy and communicate less insight versus more prevalent ones - nonetheless, just the zero values constitute 75-95% of the data in X, contingent on the vocabulary extent and corpus. We present a new weighted least squares regression system that resolves these challenges. ","A major shortcoming of this technique is that it treats all co-occurrences as equally important, including those that happen only occasionally or never. These rare co-occurrences are imprecise and convey less meaningful information than more frequent ones - however, even just the zero values make up 75-95% of the data in X, based on the vocabulary size and corpus used. We put forward a new weighted least squares regression approach to address these limitations.",A,1
GloVe_Global Vectors for Word Representation,"The performance of the model depends weakly on the cutoff, which we fix to xmax = 100 for all our experiments. We found that α = 3/4 gives a modest improvement over a linear version with α = 1. Although we offer only empirical motivation for choosing the value 3/4, it is interesting that a similar fractional power scaling was found to give the best performance in (Mikolov et al., 2013a).","The effectiveness of the model is only slightly influenced by the maximum value, which we set to 100 for all of our tests. We determined that using α = 3/4 provides a small boost over using α = 1, which would be a linear model. While we only have experimental results to justify selecting 3/4, it is notable that a similar nonlinear scaling with an exponent was optimal in (Mikolov et al., 2013a).","The model's success depends minimally on the cutoff point, which we establish as 100 for our experiments. We found that setting α = 3/4 gives a modest improvement compared to a linear version with α = 1. Although our rationale for picking 3/4 is empirical, it is interesting that a comparable nonlinear power scaling worked best in (Mikolov et al., 2013a).  ","How well the model works is not very sensitive to the maximum cutoff value, which we use 100 for all tests. Using α = 3/4 provides a small enhancement over a linear model with α = 1. While our motivation for 3/4 is experimental, it is notable that a similar fractional exponent scaling performed optimally in (Mikolov et al., 2013a).",A,1
GloVe_Global Vectors for Word Representation,"Because all unsupervised methods for learning word vectors are ultimately based on the occurrence statistics of a corpus, there should be commonalities between the models. Nevertheless, certain models remain somewhat opaque in this regard, particularly the recent window-based methods like skip-gram and ivLBL. Therefore, in this subsection we show how these models are related to our proposed model, as defined in Eqn. (8). The starting point for the skip-gram or ivLBL methods is a model Qi j for the probability that word j appears in the context of word i.","Since all unsupervised techniques for learning word vectors stem from the occurrence data of a text collection, there should be similarities between the models. However, some models stay fairly unclear about this relationship, especially the latest window-based approaches such as skip-gram and ivLBL. Thus, in this part we illustrate how these models connect to our suggested model, as characterized in Eqn. (8). The basis for the skip-gram or ivLBL approaches is a model Qi j for the likelihood that word j shows up near word i.","Given that all unsupervised procedures for acquiring word vectors originate from the statistical patterns in a dataset, commonalities between the models should exist. But certain models continue to be quite vague about this connection, notably the recent context window methods such as skip-gram and ivLBL. Therefore, here we elucidate how these models relate to our proposed model, as formulated in Eqn. (8). The starting point for the skip-gram or ivLBL techniques is a model Qi j for the probability of word j appearing close to word i.","Since all unsupervised learning algorithms for word vectors stem from the occurrence statistics present in a text corpus, similarities between the models should arise. However, some models remain somewhat unclear about this relationship, especially the latest context window approaches like skip-gram and ivLBL. As such, we demonstrate in this section how these models link to our suggested model, as defined in Eqn. (8). The basis for the skip-gram or ivLBL methods is a model Qi j representing the probability of word j occurring in proximity to word i.",A,1
GloVe_Global Vectors for Word Representation,"Most of the details of these models are irrelevant for our purposes, aside from the the fact that they attempt to maximize the log probability as a context window scans over the corpus. Evaluating the normalization factor of the softmax for each term in this sum is costly. To allow for efficient training, the skip-gram and ivLBL models introduce approximations to Qi j. As a weighted sum of cross-entropy error, this objective bears some formal resemblance to the weighted least squares objective of Eqn. (8). In fact, it is possible to optimize Eqn. (13) directly as opposed to the on-line training methods used in the skip-gram and ivLBL models.","The specifics of these models are not important for our goals, other than that they try to maximize the log probability as a context window looks over the corpus. Calculating the normalization factor of the softmax for each term in this sum takes a lot of computing power. To allow for fast training, the skip-gram and ivLBL models bring in approximations to Qi j. As a weighted sum of cross-entropy error, this goal looks somewhat like the weighted least squares goal of Equation 8. Indeed, it is possible to optimize Equation 13 straight away instead of the online training methods used in the skip-gram and ivLBL models.","Most details of these models don't matter for what we want to do, except that they attempt to maximize the log probability as a context window analyzes the corpus. Working out the normalization factor of the softmax for every term in this sum requires a lot of computation. To enable efficient training, the skip-gram and ivLBL models introduce approximations to Qi j. As a weighted sum of cross-entropy error, this aim has some formal similarity to the weighted least squares aim of Equation 8. In fact, it's possible to optimize Equation 13 directly rather than the online training methods used in the skip-gram and ivLBL models. ","Most specifics of these models are not relevant for our purposes, other than the fact that they try to maximize the log probability as a context window examines the corpus. Evaluating the normalization factor of the softmax for each term in this sum takes a lot of computing resources. To allow for quick training, the skip-gram and ivLBL models bring in approximations to Qi j. As a weighted sum of cross-entropy error, this objective has some formal resemblance to the weighted least squares objective of Equation 8. Indeed, it's possible to optimize Equation 13 straight away instead of the online training methods used in the skip-gram and ivLBL models.",A,1
GloVe_Global Vectors for Word Representation,"One could interpret this objective as a “global skip-gram” model, and it might be interesting to investigate further. On the other hand, Eqn. (13) exhibits a number of undesirable properties that ought to be addressed before adopting it as a model for learning word vectors. To begin, cross entropy error is just one among many possible distance measures between probability distributions, and it has the unfortunate property that distributions with long tails are often modeled poorly with too much weight given to the unlikely events. Furthermore, for the measure to be bounded it requires that the model distribution Q be properly normalized.","This goal could be seen as a ""universal skip-gram"" model, and it may be worthwhile to explore it more. However, Equation 13 shows several problematic characteristics that should be dealt with before using it to learn word vectors. To start, cross entropy error is only one of many potential distance metrics between probability distributions, and it unfortunately tends to model distributions with long tails poorly by giving too much importance to rare events. Also, for the measure to have an upper bound, it necessitates that the model distribution Q be correctly normalized.","One interpretation of this aim is a ""broad skip-gram"" model, and further investigation into it could be interesting. But Equation 13 has a number of unfavorable properties that need addressing before adopting it to learn word vectors. For one, cross entropy error is just one of multiple possible distance measures between probability distributions, and it has the bad tendency of poorly modeling distributions with long tails by assigning too much weight to unlikely events. In addition, for the measure to have a finite bound, it requires the model distribution Q to be properly normalized.","You could view this goal as a ""universal skip-gram"" model, and exploring it further might be valuable. However, Equation 13 displays several undesirable characteristics that should be resolved before using it as a model to learn word vectors. To start, cross entropy error is only one of many potential distance metrics between probability distributions, and unfortunately it tends to poorly model distributions with long tails by assigning too much significance to rare events. Also, for the measure to have a finite upper limit, it necessitates that the model distribution Q be appropriately normalized.",A,1
GloVe_Global Vectors for Word Representation,"Finally, we observe that while the weighting factor Xi is preordained by the on-line training method inherent to the skip-gram and ivLBL models, it is by no means guaranteed to be optimal. In fact, Mikolov et al. (2013a) observe that performance can be increased by filtering the data so as to reduce the effective value of the weighting factor for frequent words. With this in mind, we introduce a more general weighting function, which we are free to take to depend on the context word as well.","In conclusion, we see that although the weighting component Xi is predetermined by the online training technique built into the skip-gram and ivLBL models, there is no guarantee that it is ideal. Mikolov et al. (2013a) note that performance can be boosted by filtering the information to decrease the effective value of the weighting element for common words. Considering this, we present a more general weighting function, which we can choose to rely on the context word too.","To summarize, we find that while the weighting variable Xi is pre-set by the real-time training process inherent in the skip-gram and ivLBL approaches, it is not necessarily optimal. In reality, Mikolov et al. (2013a) show that results can be improved by processing the data to reduce the practical value of the weighting variable for frequent terms. With this in mind, we introduce a more flexible weighting function, which we have the freedom to make dependent on the context word also.","In closing, we see that even though the weighting coefficient Xi is pre-defined by the live training technique built into the skip-gram and ivLBL frameworks, there is no assurance that it is the best. Indeed, Mikolov et al. (2013a) demonstrate that performance can be increased by filtering the information to decrease the effective value of the weighting coefficient for common words. Considering this, we present a more general weighting function, which we have the option to make reliant on the context word too.",A,1
GloVe_Global Vectors for Word Representation,"While the analogy task is our primary focus since it tests for interesting vector space substructures, we also evaluate our model on a variety of word similarity tasks in Table 3. These include WordSim-353 (Finkelstein et al., 2001), MC (Miller and Charles, 1991), RG (Rubenstein and Goodenough, 1965), SCWS (Huang et al., 2012), and RW (Luong et al., 2013). The CoNLL-2003 English benchmark dataset for NER is a collection of documents from Reuters newswire articles, annotated with four entity types: person, location, organization, and miscellaneous.","The comparison assignment is our main concentration as it examines for fascinating vector space substructures, but we also assess our model on an assortment of word similarity assignments in Table 3. These encompass WordSim-353 (Finkelstein et al., 2001), MC (Miller and Charles, 1991), RG (Rubenstein and Goodenough, 1965), SCWS (Huang et al., 2012), and RW (Luong et al., 2013). The CoNLL-2003 English benchmark dataset for NER is a gathering of records from Reuters newswire articles, annotated with four element types: individual, area, association, and different.","While the likeness errand is our essential center since it tests for intriguing vector space substructures, we additionally assess our model on an assortment of word likeness assignments in Table 3. These incorporate WordSim-353 (Finkelstein et al., 2001), MC (Miller and Charles, 1991), RG (Rubenstein and Goodenough, 1965), SCWS (Huang et al., 2012), and RW (Luong et al., 2013). The CoNLL-2003 English benchmark dataset for NER is an accumulation of reports from Reuters newswire articles, commented on with four substance types: individual, area, association, and different. ","Despite the fact that the examination task is our essential concentration since it tests for fascinating vector space substructures, we likewise assess our model on an assortment of word likeness errands in Table 3. These incorporate WordSim-353 (Finkelstein et al., 2001), MC (Miller and Charles, 1991), RG (Rubenstein and Goodenough, 1965), SCWS (Huang et al., 2012), and RW (Luong et al., 2013). The CoNLL-2003 English benchmark dataset for NER is an assortment of archives from Reuters newswire articles, explained with four element types: individual, area, association, and different.",A,1
GloVe_Global Vectors for Word Representation,"We train models on CoNLL-03 training data on test on three datasets: 1) ConLL-03 testing data, 2) ACE Phase 2 (2001-02) and ACE-2003 data, and 3) MUC7 Formal Run test set. We adopt the BIO2 annotation standard, as well as all the preprocessing steps described in (Wang and Manning, 2013). We use a comprehensive set of discrete features that comes with the standard distribution of the Stanford NER model (Finkel et al., 2005). A total of 437,905 discrete features were generated for the CoNLL2003 training dataset. In addition, 50-dimensional vectors for each word of a five-word context are added and used as continuous features. With these features as input, we trained a conditional random field (CRF) with exactly the same setup as the CRFjoin model of (Wang and Manning, 2013).","We educate algorithms using the CoNLL-03 preparation information and evaluate on three collections: 1) ConLL-03 testing data, 2) ACE Phase 2 (2001-02) and ACE-2003 data, and 3) MUC7 Formal Run test set. We take on the BIO2 annotation standard, as well as all the pre-processing steps illustrated in (Wang and Manning, 2013). We utilize a wide-ranging set of discrete features that accompanies the standard distribution of the Stanford NER model (Finkel et al., 2005). A total of 437,905 discrete features were generated for the CoNLL2003 training dataset. Additionally, 50-dimensional vectors for each word of a five-word context are appended and used as continuous features. With these features as inputs, we educated a conditional random field (CRF) with precisely the same configuration as the CRFjoin model of (Wang and Manning, 2013).","We develop algorithms utilizing the CoNLL-03 training information and assess on three groups: 1) ConLL-03 testing data, 2) ACE Phase 2 (2001-02) and ACE-2003 data, and 3) MUC7 Formal Run test set. We embrace the BIO2 annotation standard, as well as all the pre-processing steps illustrated in (Wang and Manning, 2013). We employ a comprehensive set of discrete features that accompanies the standard distribution of the Stanford NER model (Finkel et al., 2005). A total of 437,905 discrete features were generated for the CoNLL2003 training dataset. Additionally, 50-dimensional vectors for each word of a five-word context are appended and utilized as continuous features. With these features as inputs, we developed a conditional random field (CRF) with exactly the same settings as the CRFjoin model of (Wang and Manning, 2013).  ","We educate models utilizing the CoNLL-03 training data and evaluate on three groups: 1) ConLL-03 testing data, 2) ACE Phase 2 (2001-02) and ACE-2003 data, and 3) MUC7 Formal Run test set. We take on the BIO2 annotation standard, as well as all the pre-processing steps described in (Wang and Manning, 2013). We use a comprehensive set of discrete features that accompanies the standard distribution of the Stanford NER model (Finkel et al., 2005). A total of 437,905 discrete features were generated for the CoNLL2003 training dataset. Additionally, 50-dimensional vectors for each word of a five-word context are added and utilized as continuous features. With these features as inputs, we trained a conditional random field (CRF) with precisely the same configuration as the CRFjoin model of (Wang and Manning, 2013).",A,1
GloVe_Global Vectors for Word Representation,"We trained our model on five corpora of varying sizes: a 2010 Wikipedia dump with 1 billion tokens; a 2014 Wikipedia dump with 1.6 billion tokens; Gigaword 5 which has 4.3 billion tokens; the combination Gigaword5 + Wikipedia2014, which has 6 billion tokens; and on 42 billion tokens of web data, from Common Crawl . We tokenize and lowercase each corpus with the Stanford tokenizer, build a vocabulary of the 400,000 most frequent words , and then construct a matrix of cooccurrence counts X. In constructing X, we must choose how large the context window should be and whether to distinguish left context from right context.","We educated our system using 5 groups of texts of different sizes: a 2010 Wikipedia copy containing 1 billion words; a 2014 Wikipedia copy with 1.6 billion words; Gigaword 5 having 4.3 billion words; the mix of Gigaword5 + Wikipedia2014, containing 6 billion words; and 42 billion words of web data, from Common Crawl. We separate and convert to lowercase each group of texts using the Stanford tokenizer, build a vocabulary of the 400,000 most common words, and then make a matrix of co-occurrence counts X. In making X, we need to decide how big the context window should be and whether to differentiate left context from right context.","We trained our model using 5 collections of text of varying length: a 2010 Wikipedia extract with 1 billion terms; a 2014 Wikipedia extract with 1.6 billion terms; Gigaword 5 which has 4.3 billion terms; the blend of Gigaword5 + Wikipedia2014, containing 6 billion terms; and 42 billion terms of web content, from Common Crawl. We break down and change to lowercase each collection using the Stanford tokenizer, construct a lexicon of the 400,000 most frequent words, and then assemble a matrix of co-occurrence tallies X. In constructing X, we have to determine how large the context window should be and whether to separate left context from right context.","We developed our model utilizing 5 groups of texts of different sizes: a 2010 Wikipedia excerpt with 1 billion words; a 2014 Wikipedia excerpt with 1.6 billion words; Gigaword 5 containing 4.3 billion words; the fusion of Gigaword5 + Wikipedia2014, having 6 billion words; and 42 billion words of web material, from Common Crawl. We split and convert to lowercase each group using the Stanford tokenizer, create a vocabulary of the 400,000 most common words, and then generate a matrix of co-occurrence counts X. In creating X, we need to choose how big the context window should be and whether to differentiate left context from right context.",A,1
GloVe_Global Vectors for Word Representation,"We explore the effect of these choices below. In all cases we use a decreasing weighting function, so that word pairs that are d words apart contribute 1/d to the total count. This is one way to account for the fact that very distant word pairs are expected to contain less relevant information about the words’ relationship to one another. For all our experiments, we set xmax = 100, α = 3/4, and train the model using AdaGrad (Duchi et al., 2011), stochastically sampling nonzero elements from X, with initial learning rate of 0.05. We run 50 iterations for vectors smaller than 300 dimensions, and 100 iterations otherwise (see Section 4.6 for more details about the convergence rate).","We analyze the impact of these selections in the sections below. In every case we utilize a declining weighting function, so word pairs that are d words separated add 1/d to the total tally. This is one technique to take into account that very far apart word pairs are predicted to hold less useful data about the words' connection. For all our trials, we fix xmax = 100, α = 3/4, and train the model applying AdaGrad (Duchi et al., 2011), randomly sampling non-zero elements from X, with starting learning rate of 0.05. We execute 50 cycles for vectors smaller than 300 dimensions, and 100 cycles otherwise (refer to Section 4.6 for more specifics on the convergence pace).","We inspect the consequence of these choices in the following. Always we employ a decreasing weighting function, so word pairs that are d words distant provide 1/d to the full count. This is one approach to consider that very separated word pairs are assumed to have less relevant knowledge about the words' association to one another. For all our tests, we set xmax = 100, α = 3/4, and train the model utilizing AdaGrad (Duchi et al., 2011), stochastically drawing nonzero elements from X, with initial learning rate of 0.05. We run 50 iterations for vectors smaller than 300 dimensions, and 100 iterations otherwise (see Section 4.6 for more information about the convergence speed).  ","We examine the effect of these selections below. In every case we use a declining weighting function, so word pairs that are d words apart add 1/d to the total tally. This is one technique to account for the fact that very far word pairs are expected to contain less useful details about the words' linkage. For all our experiments, we fix xmax = 100, α = 3/4, and train the model applying AdaGrad (Duchi et al., 2011), randomly sampling nonzero elements from X, with initial learning rate of 0.05. We execute 50 cycles for vectors smaller than 300 dimensions, and 100 cycles otherwise (refer to Section 4.6 for more specifics about the convergence rate).",A,1
GloVe_Global Vectors for Word Representation,"Unless otherwise noted, we use a context of ten words to the left and ten words to the right. The model generates two sets of word vectors, W and W˜ . When X is symmetric, W and W˜ are equivalent and differ only as a result of their random initializations; the two sets of vectors should perform equivalently. On the other hand, there is evidence that for certain types of neural networks, training multiple instances of the network and then combining the results can help reduce overfitting and noise and generally improve results (Ciresan et al., 2012). With this in mind, we choose to use the sum W +W˜ as our word vectors.","If not stated differently, we utilize a setting of ten terms to the left and ten terms to the right. The model makes two groups of word vectors, W and W ̃. When X is symmetrical, W and W ̃ are the same and only differ because of their arbitrary early settings; the two vector sets should act comparably. However, there is proof that for some neural network kinds, teaching multiple examples of the network then merging the outputs can assist with decreasing overfitting and noise and generally bettering results (Ciresan et al., 2012). With this in mind, we decide to utilize the sum W + W ̃ as our word vectors.","Unless mentioned otherwise, our context consists of ten words before and after. The system generates two sets of word vectors called W and W~. When X is balanced, W and W~ are equivalent and only differ due to random initialization; the two vector sets ought to perform similarly. However, research shows that for some neural networks, training multiple instances then combining their outputs can reduce overfitting, noise, and improve overall performance (Ciresan et al., 2012). Therefore, we use the sum W + W~ for our word vectors.  ","If not noted differently, our setting is ten words left and right. The model makes two word vector sets, W and W~. If X is even, W and W~ are the same except for random early values; the vector sets should function equally. But evidence shows that for some neural networks, training multiple versions then merging outputs can decrease overfitting and noise and improve results overall (Ciresan et al., 2012). So we use the sum W + W~ as our word vectors.",A,1
GloVe_Global Vectors for Word Representation,"We present results on the word analogy task in Table 2. The GloVe model performs significantly better than the other baselines, often with smaller vector sizes and smaller corpora. Our results using the word2vec tool are somewhat better than most of the previously published results. This is due to a number of factors, including our choice to use negative sampling (which typically works better than the hierarchical softmax), the number of negative samples, and the choice of the corpus. We demonstrate that the model can easily be trained on a large 42 billion token corpus, with a substantial corresponding performance boost. We note that increasing the corpus size does not guarantee improved results for other models, as can be seen by the decreased performance of the SVD.","The outcomes of the word analogy task are shown in Table 2. The GloVe model carries out significantly superior to the other baseline models, frequently with smaller vector dimensions and smaller text collections. Our outcomes utilizing the word2vec tool are somewhat enhanced compared to most of the previously released results. This is due to numerous factors, including our decision to employ negative sampling (which usually works better than the hierarchical softmax), the quantity of negative samples, and the choice of the corpus. We prove that the model can be easily trained on a large 42 billion token corpus, with a significant corresponding performance improvement. We observe that expanding the corpus size does not ensure enhanced outcomes for other models, as can be seen by the decreased performance of the SVD.","We display the findings on the word analogy assignment in Table 2. The GloVe model executes noticeably better than the other reference models, often with smaller vector sizes and smaller datasets. Our outputs using the word2vec apparatus are somewhat superior to most of the previously published outputs. This is owing to various elements, comprising our preference to utilize negative sampling (which characteristically executes superior to the hierarchical softmax), the amount of negative exemplars, and the choice of the dataset. We demonstrate that the model can be effortlessly educated on a substantial 42 billion token dataset, with a significant related performance boost. We take note that expanding the dataset size does not assure enhanced outputs for other models, as can be perceived by the decreased execution of the SVD.  ","The conclusions on the word analogy exercise are exhibited in Table 2. The GloVe model acts essentially better compared to the other foundational models, frequently with more modest vector sizes and more modest corpora. Our consequences utilizing the word2vec instrument are fairly improved contrasted with a large portion of the recently distributed results. This is because of various components, including our decision to utilize negative sampling (which typically performs better compared to the progressive softmax), the quantity of negative tests, and the decision of the corpus. We show that the model can be effectively prepared on an enormous 42 billion token corpus, with a huge relating execution support. We take note of that expanding the corpus size doesn't ensure improved outcomes for other models, as can be found in the diminished execution of the SVD.",A,1
GloVe_Global Vectors for Word Representation,"We also investigated several other weighting schemes for transforming X; what we report here performed best. Many weighting schemes like PPMI destroy the sparsity of X and therefore cannot feasibly be used with large vocabularies. With smaller vocabularies, these information-theoretic transformations do indeed work well on word similarity measures, but they perform very poorly on the word analogy task.","Additionally, we looked into other weighting systems for changing X. The method described here was superior. Numerous weighting systems such as PPMI eliminate the sparseness of X, so they can't be utilized with large vocabularies. For smaller vocabularies, these information-theoretic transformations are effective for word similarity metrics, but they are very ineffective for the word analogy task.","Furthermore, we investigated other weighting schemes to modify X. The one illustrated here was the best. Many weighting schemes including PPMI remove the sparsity of X, so they are not feasible to use with big vocabularies. With smaller vocabularies, these transformations based on information theory do work well on word similarity measures, however they are very bad on the word analogy task. ","We also explored several alternative weighting plans for altering X. The one outlined here was optimal. Numerous weighting plans such as PPMI eliminate the sparseness of X, so they cannot be used with large vocabularies. For smaller vocabularies, these transformations founded on information theory are indeed effective on word similarity metrics, but they are very poor on the word analogy task.",A,1
Going deeper with convolutions,"We propose a deep convolutional neural network architecture codenamed Inception, which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing.","We put forward a deep convolutional neural network design called Inception, which set a new benchmark for categorization and identification in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The primary characteristic of this design is the enhanced use of the computing capabilities inside the network. This was realized through a meticulously developed architecture that enables increasing the depth and breadth of the network while maintaining a constant computational budget. To enhance quality, the architectural choices were grounded in the Hebbian theory and the concept of multi-scale processing.","We present a deep convolutional neural network model named Inception, which established the new state-of-the-art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge in 2014 (ILSVRC14). The principal hallmark of this model is the improved harnessing of the computing resources within the network. This was enabled by a thoughtfully engineered architecture that permits expanding the depth and width of the network while keeping the computational cost fixed. To optimize performance, the architectural decisions were based on the Hebbian principle and the notion of multi-scale processing.  ","We introduce a deep convolutional neural network design called Inception, which set the new benchmark for categorization and identification in the ImageNet Large-Scale Visual Recognition Challenge in 2014 (ILSVRC14). The major characteristic of this design is the enhanced utilization of the computing capabilities internal to the network. This was realized through a carefully conceived architecture that provides for increasing the depth and breadth of the network while maintaining the same computational budget. To enhance results, the architectural choices were founded on the Hebbian theory and the concept of multi-scale processing.",A,1
Going deeper with convolutions,"In the last three years, mainly due to the advances of deep learning, more concretely convolutional networks [10], the quality of image recognition and object detection has been progressing at a dramatic pace. One encouraging news is that most of this progress is not just the result of more powerful hardware, larger datasets and bigger models, but mainly a consequence of new ideas, algorithms and improved network architectures. No new data sources were used, for example, by the top entries in the ILSVRC 2014 competition besides the classification dataset of the same competition for detection purposes.","Over the past three years, the quality of image recognition and object detection has been rapidly improving, largely thanks to innovations in deep learning and convolutional neural networks. This progress is not just from having more powerful computers, bigger datasets, and larger models - it's mostly due to new algorithms, network architectures, and ideas. For instance, the top performers in the 2014 ILSVRC competition used the same classification dataset for detection, without any new data sources.","In the last three years, advances in deep learning, especially convolutional networks, have led to dramatic improvements in image recognition and object detection. Encouragingly, this progress stems primarily from new approaches, architectures, and concepts rather than just stronger hardware, larger datasets, or bigger models. The top finishers in the 2014 ILSVRC contest, for example, relied on the same classification data for detection without using any new data sources. ","Over the past three years, breakthroughs in deep learning and convolutional neural networks have rapidly accelerated progress in image recognition and object detection. This dramatic improvement is attributable mainly to novel methods, architectures, and ideas rather than simply more powerful computers, larger datasets, or bigger models. Notably, the top performers in the 2014 ILSVRC competition utilized the existing classification dataset for detection purposes without introducing any new data sources.",A,1
Going deeper with convolutions,"Our GoogLeNet submission to ILSVRC 2014 actually uses 12× fewer parameters than the winning architecture of Krizhevsky et al [9] from two years ago, while being significantly more accurate. The biggest gains in object-detection have not come from the utilization of deep networks alone or bigger models, but from the synergy of deep architectures and classical computer vision, like the R-CNN algorithm by Girshick et al [6]. Another notable factor is that with the ongoing traction of mobile and embedded computing, the efficiency of our algorithms – especially their power and memory use – gains importance.","Our architecture for the ILSVRC 2014 competition requires far fewer parameters than the victorious model from 2012 by Krizhevsky et al. [9], yet achieves substantially better accuracy. The largest improvements in object detection have resulted not solely from deep networks or larger models, but rather from combining deep learning with traditional computer vision methods like R-CNN by Girshick et al. [6]. Furthermore, as mobile and embedded devices become more widespread, the efficiency of algorithms - particularly their power consumption and memory requirements - becomes increasingly important.","Our GoogLeNet model for the 2014 ILSVRC contest uses only 12 times fewer parameters than the winning 2012 model by Krizhevsky et al. [9], but is markedly more precise. The biggest advancements in object recognition have come not just from deep learning alone or larger models, but rather from integrating deep neural networks with established computer vision techniques like R-CNN by Girshick et al. [6]. Additionally, as mobile and embedded computing grows, the efficiency of our algorithms - especially their energy use and memory footprint - becomes more vital.  ","Our GoogLeNet entry for the ILSVRC 2014 challenge has 12 times fewer parameters than the victorious architecture from 2012 by Krizhevsky et al. [9], yet achieves substantially higher accuracy. The most significant improvements in object detection have resulted not solely from applying deep networks or bigger models, but rather from combining deep learning with time-tested computer vision methods such as R-CNN by Girshick et al. [6]. Moreover, as mobile and embedded systems become more pervasive, the efficiency of our algorithms - particularly their power consumption and memory requirements - increases in importance.",A,1
Going deeper with convolutions,"It is noteworthy that the considerations leading to the design of the deep architecture presented in this paper included this factor rather than having a sheer fixation on accuracy numbers. For most of the experiments, the models were designed to keep a computational budget of 1.5 billion multiply-adds at inference time, so that they do not end up to be a purely academic curiosity, but could be put to real world use, even on large datasets, at a reasonable cost.","It is important to point out that the thoughts that led to creating the deep learning model described here focused on this element rather than just aiming for high accuracy. For most of the tests, the models were made to keep the number of computations needed low (1.5 billion), so that they could actually be used in the real world on big datasets without being too expensive.","It should be emphasized that the reasoning behind developing the deep neural network architecture presented in this report considered this factor more than just maximizing accuracy. For the majority of experiments, the models were designed to require a reasonable computational cost during use (1.5 billion operations), so that they would not just be theoretical, but could actually be utilized in practice, even for large datasets, without being prohibitively expensive.  ","Notably, the motivations for engineering the deep learning architecture outlined in this document revolved around this aspect rather than purely chasing high accuracy metrics. For most experiments, the models were constructed to have an affordable computational budget at prediction time (1.5 billion multiplications), so that they would not merely be academic, but could be deployed in real applications, even on massive datasets, at a viable cost.",A,1
Going deeper with convolutions,"In this paper, we will focus on an efficient deep neural network architecture for computer vision, codenamed Inception, which derives its name from the Network in network paper by Lin et al [12] in conjunction with the famous “we need to go deeper” internet meme [1]. In our case, the word “deep” is used in two different meanings: first of all, in the sense that we introduce a new level of organization in the form of the “Inception module” and also in the more direct sense of increased network depth. In general, one can view the Inception model as a logical culmination of [12] while taking inspiration and guidance from the theoretical work by Arora et al [2].","This article examines an effective deep neural network design for computer vision, referred to as Inception, which gets its name from the Network in network paper by Lin et al [12] as well as the popular ""we need to go deeper"" meme [1]. For us, ""deep"" has two meanings: first, we present a new level of structure with the ""Inception module"", and second, we increase the depth of the network. Overall, one can see the Inception model as a logical extension of [12], while also drawing inspiration and direction from the theoretical work by Arora et al [2].","In this report, we focus on an efficient deep learning network architecture for computer vision called Inception, named after the Network in network paper by Lin et al [12] and the well-known ""we need to go deeper"" meme [1]. Here, ""deep"" has two senses: firstly, we introduce a new organizational layer called the ""Inception module"", and secondly, we increase the depth of the network. Generally, one can view the Inception model as a natural progression of [12], while also taking inspiration and guidance from the theoretical research of Arora et al [2].","This paper examines an effective deep neural network design for computer vision nicknamed Inception. Its name comes from the Network in network paper by Lin et al [12] and the popular ""we need to go deeper"" internet meme [1]. For our purposes, ""deep"" has two connotations: first, we present a new structural level called the ""Inception module"", and second, we increase the depth of the network. In general, one can see the Inception model as a logical extension of the work in [12], while also drawing inspiration and direction from the theoretical research of Arora et al [2].",A,1
Going deeper with convolutions,"For larger datasets such as Imagenet, the recent trend has been to increase the number of layers [12] and layer size [21, 14], while using dropout [7] to address the problem of overfitting. Despite concerns that max-pooling layers result in loss of accurate spatial information, the same convolutional network architecture as [9] has also been successfully employed for localization [9, 14], object detection [6, 14, 18, 5] and human pose estimation [19]. Inspired by a neuroscience model of the primate visual cortex, Serre et al. [15] use a series of fixed Gabor filters of different sizes in order to handle multiple scales, similarly to the Inception model.","For immense image collections like Imagenet, the current tendency has been to expand the quantity of layers [12] and layer dimensions [21, 14], utilizing dropout [7] to tackle overfitting. Although there are worries that max-pooling layers lead to lost precise spatial details, the same convolutional network design as [9] has also been effectively used for localization [9, 14], object recognition [6, 14, 18, 5] and human pose analysis [19]. Motivated by a neuroscience model of the primate visual cortex, Serre et al. [15] utilize a sequence of fixed Gabor filters of varying sizes to handle multiple scales, analogous to the Inception model.","For large image datasets such as Imagenet, the latest trend has been to increase the number of layers [12] and layer size [21, 14], applying dropout [7] to address overfitting. Despite concerns that max-pooling layers result in loss of accurate spatial information, the identical convolutional network structure as [9] has also been successfully used for localization [9, 14], object detection [6, 14, 18, 5] and human pose estimation [19]. Inspired by a neuroscience model of the primate visual cortex, Serre et al. [15] use a series of fixed Gabor filters of different sizes to handle multiple scales, similar to the Inception model.","For big image collections like Imagenet, the current tendency has been to expand the amount of layers [12] and layer dimensions [21, 14], while using dropout [7] to tackle overfitting. Although there are concerns that max-pooling layers lead to lost precise spatial information, the same convolutional network design as [9] has also been effectively employed for localization [9, 14], object recognition [6, 14, 18, 5] and human pose analysis [19]. Motivated by a neuroscience model of the primate visual cortex, Serre et al. [15] utilize a sequence of fixed Gabor filters of varying sizes to handle multiple scales, analogous to the Inception model.",A,1
Going deeper with convolutions,"However, contrary to the fixed 2-layer deep model of [15], all filters in the Inception model are learned. Furthermore, Inception layers are repeated many times, leading to a 22-layer deep model in the case of the GoogLeNet model. Network-in-Network is an approach proposed by Lin et al. [12] in order to increase the representational power of neural networks. When applied to convolutional layers, the method could be viewed as additional 1×1 convolutional layers followed typically by the rectified linear activation [9]. This enables it to be easily integrated in the current CNN pipelines. We use this approach heavily in our architecture.","In contrast to the rigid 2-layer deep architecture of [15], all filters in the Inception model are adapted. Moreover, Inception layers are iterated numerous times, resulting in a 22-layer deep model for the GoogLeNet architecture. Network-in-Network is a technique proposed by Lin et al. [12] to boost the representative capacity of neural networks. When used on convolutional layers, the approach can be seen as extra 1×1 convolutional layers commonly followed by the rectified linear activation [9]. This allows it to be easily combined into current CNN pipelines. We utilize this approach extensively in our design.","Unlike the fixed 2-layer deep structure of [15], all filters in the Inception architecture are learned. Furthermore, Inception layers are repeated many times, leading to a 22-layer deep architecture in the case of the GoogLeNet design. Network-in-Network is a strategy proposed by Lin et al. [12] to increase the ability of neural networks to represent features. When applied to convolutional layers, the technique could be viewed as additional 1×1 convolutional layers typically followed by the rectified linear activation function [9]. This enables it to be easily integrated into existing CNN pipelines. We make heavy use of this technique in our model.","In contrast with the inflexible 2-layer deep framework of [15], all filters in the Inception model are adapted through training. Additionally, Inception layers are used repeatedly, resulting in a 22-layer deep architecture for the GoogLeNet model. Network-in-Network is an approach proposed by Lin et al. [12] to improve the representational capacity of neural networks. When used with convolutional layers, the method can be seen as extra 1x1 convolutional layers commonly followed by the rectified linear activation function [9]. This allows it to be easily incorporated into current CNN pipelines. We employ this approach extensively in our design.",A,1
Going deeper with convolutions,"However, in our setting, 1 × 1 convolutions have dual purpose: most critically, they are used mainly as dimension reduction modules to remove computational bottlenecks, that would otherwise limit the size of our networks. This allows for not just increasing the depth, but also the width of our networks without significant performance penalty. The current leading approach for object detection is the Regions with Convolutional Neural Networks (R-CNN) proposed by Girshick et al. [6]. R-CNN decomposes the overall detection problem into two subproblems: to first utilize low-level cues such as color and superpixel consistency for potential object proposals in a category-agnostic fashion, and to then use CNN classifiers to identify object categories at those locations.","Nevertheless, in our framework, 1 × 1 convolutions serve two key purposes: most importantly, they mainly act as dimension reduction modules to eliminate computational limitations that would otherwise constrain the size of our networks. This enables expanding not just the depth, but also the width of our networks without significant performance costs. The prevailing approach for object detection currently is Regions with Convolutional Neural Networks (R-CNN) proposed by Girshick et al. [6]. R-CNN breaks down the full detection task into two subtasks: first using low-level features like color and superpixel consistency to generate potential object proposals in a category-agnostic way, and then utilizing CNN classifiers to identify object categories at those spots.","However, in our configuration, 1 × 1 convolutions have a dual role: most critically, they are utilized primarily as dimension reduction components to remove computational bottlenecks, which would otherwise restrict the scale of our networks. This provides for not just growing the depth, but also the width of our networks without major performance penalties. The current best approach for object detection is Regions with Convolutional Neural Networks (R-CNN) proposed by Girshick et al. [6]. R-CNN separates the complete detection problem into two subproblems: first to use low-level hints like color and superpixel consistency for potential object proposals in a category-agnostic manner, and then to utilize CNN classifiers to identify object categories at those locations.","Nonetheless, in our arrangement, 1 × 1 convolutions serve two purposes: most importantly, they are employed mainly as dimension reduction modules to eliminate computational constraints, which would otherwise limit the magnitude of our networks. This enables increasing not just the depth, but also the width of our networks without significant performance costs. The current premier approach for object detection is Regions with Convolutional Neural Networks (R-CNN) proposed by Girshick et al. [6]. R-CNN decomposes the full detection task into two subtasks: first to leverage low-level cues such as color and superpixel consistency for potential object proposals in a category-agnostic fashion, and then to utilize CNN classifiers to identify object categories at those positions.",A,1
Going deeper with convolutions,"Such a two stage approach leverages the accuracy of bounding box segmentation with low-level cues, as well as the highly powerful classification power of state-of-the-art CNNs. We adopted a similar pipeline in our detection submissions, but have explored enhancements in both stages, such as multi-box [5] prediction for higher object bounding box recall, and ensemble approaches for better categorization of bounding box proposals.","This two part method takes advantage of the precision of bounding box division using low-level hints, and also the extremely effective categorization capability of cutting-edge CNNs. We utilized a comparable workflow in our detection submissions, but have investigated enhancements in both phases, like multi-box [5] forecasting for superior object bounding box recall, and ensemble tactics for improved classification of bounding box proposals.","This approach in two steps leverages the accuracy of bounding box segmentation using basic visual cues, as well as the highly powerful ability of modern CNNs to categorize objects. We used a similar pipeline in our detection submissions, but we explored improvements in both steps, including multi-box [5] prediction to get higher object bounding box recall, and ensemble methods to better classify the proposed bounding boxes. ","This two stage method takes advantage of precise bounding box delineation using low-level visual features, and the extremely powerful classification abilities of state-of-the-art CNNs. We used a similar workflow for our detection submissions, but investigated enhancements at both stages, such as multi-box [5] forecasting to improve object bounding box recall, and ensemble techniques to better categorize the proposed bounding boxes.",A,1
Going deeper with convolutions,"The most straightforward way of improving the performance of deep neural networks is by increasing their size. This includes both increasing the depth – the number of levels – of the network and its width: the number of units at each level. This is as an easy and safe way of training higher quality models, especially given the availability of a large amount of labeled training data. However this simple solution comes with two major drawbacks. Bigger size typically means a larger number of parameters, which makes the enlarged network more prone to overfitting, especially if the number of labeled examples in the training set is limited.","The simplest method to enhance the capabilities of deep neural networks is to make them larger. This means adding more layers to the network as well as more nodes in each layer. This is a straightforward and reliable approach to develop more powerful models, particularly if you have access to a substantial amount of annotated training data. However, this basic tactic has two main weaknesses. Larger size usually increases the quantity of parameters, which amplifies the risk of overfitting, primarily if there is a restricted amount of labeled data available for training.","The most basic technique to improve deep neural networks is expanding their scale. That entails increasing the number of tiers in the network and the count of units at each tier. This is a straightforward and safe means to train more capable models, given the availability of ample labeled training information. However this simple fix has two key problems. Greater size often increases the number of parameters, which makes the enlarged network more susceptible to overfitting, especially if the labeled examples in the training set are scarce.  ","The most elementary way to boost deep neural networks' performance is to make them bigger. That means adding more layers to the network's structure and more nodes to each layer. With plenty of annotated training data, this is an easy and reliable path to developing more powerful models. However, this simple growth approach has two major weaknesses. The larger size typically increases the parameter count, which can lead to overfitting, particularly if labeled training data is limited.",A,1
Going deeper with convolutions,"This can become a major bottleneck, since the creation of high quality training sets can be tricky and expensive, especially if expert human raters are necessary to distinguish between fine-grained visual categories like those in ImageNet (even in the 1000-class ILSVRC subset) as demonstrated by Figure 1. Another drawback of uniformly increased network size is the dramatically increased use of computational resources. For example, in a deep vision network, if two convolutional layers are chained, any uniform increase in the number of their filters results in a quadratic increase of computation. If the added capacity is used inefficiently (for example, if most weights end up to be close to zero), then a lot of computation is wasted.","This process can turn into a huge impediment, since making high-quality training sets is often tricky and costly, particularly if human experts are required to differentiate between precise visual types as shown in ImageNet (even in the 1000-class ILSVRC subset) as depicted in Figure 1. Another disadvantage of uniformly expanding the network's scale is the dramatically amplified use of computing assets. For instance, in a deep vision network, if two convolutional layers are connected, any uniform swell in their filters' number causes a quadratic surge of computation. If the added potential is utilized inefficiently (for example, if most weights end up being near zero), then much calculation is wasted.","This could become a major roadblock, since generating top-notch training data can be challenging and expensive, especially if human specialists must make fine distinctions between intricate visual categories as in ImageNet (even just the 1000-class ILSVRC portion) as shown in Figure 1. Another issue with flatly enlarging network size is the dramatically escalated use of computing power. For example, in a deep vision network, if two convolutional layers are sequenced, any even increase in their filters leads to a quadratic spike in computation. If the extra capacity is used poorly (for instance, if most weights are close to zero), then lots of computation is squandered.  ","This has the potential to be a huge bottleneck, since creating high-quality training sets is often difficult and costly, particularly if human experts need to discriminate between subtle visual types as seen in ImageNet (even just the 1000-class ILSVRC subset) as illustrated in Figure 1. Another downside of uniformly expanding network dimensions is the dramatically intensified use of computing assets. For example, in a deep vision network, if two convolutional layers are connected, any consistent enlargement of their filters causes a quadratic surge in computation. If the added potential is utilized inefficiently (for instance, if most weights end up near zero), then abundant computation is wasted.",A,1
Going deeper with convolutions,"Since in practice the computational budget is always finite, an efficient distribution of computing resources is preferred to an indiscriminate increase of size, even when the main objective is to increase the quality of results. The fundamental way of solving both issues would be by ultimately moving from fully connected to sparsely connected architectures, even inside the convolutions. Besides mimicking biological systems, this would also have the advantage of firmer theoretical underpinnings due to the groundbreaking work of Arora et al. [2]. Their main result states that if the probability distribution of the data-set is representable by a large, very sparse deep neural network, then the optimal network topology can be constructed layer by layer by analyzing the correlation statistics of the activations of the last layer and clustering neurons with highly correlated outputs.","Since in practice there is always a limited computing budget, efficiently distributing computing resources is better than blindly increasing size, even if the main goal is to improve result quality. The key way to solve both issues would be to ultimately switch from fully connected to sparsely connected architectures, even within convolutions. Besides imitating biological systems, this would also have the benefit of stronger theoretical foundations due to the pioneering work of Arora et al. [2]. Their main finding states that if a dataset's probability distribution can be represented by a large, extremely sparse deep neural network, then the best network design can be built layer-by-layer by analyzing the correlation data of the last layer's activations and clustering neurons with highly related outputs.","Because in the real world there are always constraints on computing power, it is better to wisely allocate computing resources rather than haphazardly make things bigger, even if the primary aim is to enhance result quality. The fundamental solution for both problems would be to finally change from fully connected to sparsely connected architectures, even within convolutions. In addition to mimicking biological systems, this would have the advantage of more solid theoretical justification due to the groundbreaking research of Arora et al. [2]. Their main conclusion is that if a dataset's probability distribution is representable by a very large, extremely sparse deep neural network, then the optimal network design can be constructed layer-by-layer by studying the correlation statistics of the last layer's activations and grouping neurons with highly correlated outputs.","Since in actual practice there are always limits on computing capacity, intelligently distributing computing power is superior to blind expansion, even if the main purpose is to improve result quality. The key solution for both issues would be to ultimately switch from fully connected to sparsely connected architectures, even inside convolutions. Apart from imitating biological systems, this would also have the benefit of stronger theoretical support due to the pioneering work of Arora et al. [2]. Their primary finding is that if a dataset's probability distribution is representable by an extremely large, very sparse deep neural network, then the best network architecture can be built layer-by-layer by analyzing the correlation data of the last layer's activations and clustering neurons with highly related outputs.",A,1
Going deeper with convolutions,"Although the strict mathematical proof requires very strong conditions, the fact that this statement resonates with the well known Hebbian principle – neurons that fire together, wire together – suggests that the underlying idea is applicable even under less strict conditions, in practice. On the downside, todays computing infrastructures are very inefficient when it comes to numerical calculation on non-uniform sparse data structures. Even if the number of arithmetic operations is reduced by 100×, the overhead of lookups and cache misses is so dominant that switching to sparse matrices would not pay off. The gap is widened even further by the use of steadily improving, highly tuned, numerical libraries that allow for extremely fast dense matrix multiplication, exploiting the minute details of the underlying CPU or GPU hardware [16, 9].","While a rigorous mathematical proof demands very stringent conditions, the fact that this statement aligns with the well-known Hebbian principle - neurons that fire together, wire together - implies the underlying concept could apply even with less strict conditions, in practice. However, current computing infrastructure is very inefficient for numerical calculation on irregular sparse data structures. Even if the number of arithmetic operations is reduced by 100 times, the overhead of lookups and cache misses is so predominant that switching to sparse matrices would not be advantageous. This gap is further widened by the use of constantly improving, highly optimized numerical libraries that enable extremely fast dense matrix multiplication, leveraging the minute specifics of the underlying CPU or GPU hardware [16, 9].","Although a strict mathematical proof necessitates very strong prerequisites, the resonance of this statement with the familiar Hebbian principle - neurons that fire together, connect together - hints that the core idea could hold true even with less stringent prerequisites, in reality. Nevertheless, present-day computing frameworks are very ineffective for numerical computation on uneven sparse data organizations. Even if the quantity of arithmetic operations is decreased by 100 times, the overhead of lookups and cache misses is so primary that transitioning to sparse matrices would not be beneficial. This gap is broadened even more by the use of steadily enhancing, highly tuned, numerical libraries that facilitate extremely rapid dense matrix multiplication, exploiting the minute particulars of the underlying CPU or GPU hardware [16, 9].","While a rigid mathematical proof calls for very forceful stipulations, the fact that this statement chimes with the recognized Hebbian tenet - neurons that fire together, intertwine together - proposes that the underlying notion could pertain even under less forceful stipulations, in practice. However, current computing infrastructures are very deficient when it comes to numerical calculation on irregular sparse data configurations. Even if the number of arithmetic operations is condensed by 100 times, the overhead of lookups and cache misses is so foremost that shifting to sparse matrices would not be advantageous. This divide is stretched even further by the use of steadily refining, highly tuned, numerical libraries that enable extremely swift dense matrix multiplication, harnessing the minute specifics of the underlying CPU or GPU hardware [16, 9].",A,1
Going deeper with convolutions,"Also, non-uniform sparse models require more sophisticated engineering and computing infrastructure. Most current vision oriented machine learning systems utilize sparsity in the spatial domain just by the virtue of employing convolutions. However, convolutions are implemented as collections of dense connections to the patches in the earlier layer. ConvNets have traditionally used random and sparse connection tables in the feature dimensions since [11] in order to break the symmetry and improve learning, the trend changed back to full connections with [9] in order to better optimize parallel computing. The uniformity of the structure and a large number of filters and greater batch size allow for utilizing efficient dense computation.","Furthermore, irregular sparse models need more complex engineering and computing infrastructure. Most current vision focused machine learning systems use sparsity in the spatial domain simply by using convolutions. However, convolutions are actualized as groups of dense connections to the patches in the prior layer. ConvNets have historically utilized arbitrary and sparse connection tables in the feature dimensions since [11] to break the symmetry and get better learning, but the trend reverted back to full connections with [9] to better enhance parallel computing. The consistency of the structure and a large number of filters and greater batch size enable utilizing efficient dense computation.","In addition, non-consistent sparse models call for more sophisticated engineering and computing facilities. The majority of current vision oriented machine learning frameworks employ sparseness in the spatial domain just by employing convolutions. Though, convolutions are implemented as sets of dense links to the patches in the earlier layer. ConvNets have customarily utilized random and sparse connection tables in the feature dimensions since [11] in order to break the symmetry and enhance learning, however the trend changed back to full connections with [9] in order to better optimize parallel computing. The uniformity of the structure and a substantial number of filters and greater batch size permit utilizing efficient dense computation.","Moreover, irregular sparse models require more complex engineering and computing infrastructure. Most current vision focused machine learning systems use sparseness in the spatial domain simply by utilizing convolutions. However, convolutions are materialized as collections of dense connections to the patches in the prior layer. ConvNets have historically employed arbitrary and sparse connection tables in the feature dimensions since [11] in order to break the symmetry and ameliorate learning, nevertheless the trend reverted back to full connections with [9] in order to better enhance parallel computing. The consistency of the structure and a significant number of filters and greater batch size enable leveraging efficient dense computation.",A,1
Going deeper with convolutions,"The Inception architecture started out as a case study of the first author for assessing the hypothetical output of a sophisticated network topology construction algorithm that tries to approximate a sparse structure implied by [2] for vision networks and covering the hypothesized outcome by dense, readily available components. Despite being a highly speculative undertaking, only after two iterations on the exact choice of topology, we could already see modest gains against the reference architecture based on [12].","The Inception model was originally a thought experiment by the first writer to evaluate what the theoretical results might be of using a complex neural network design algorithm that tries to mimic the sparse structure suggested by [2] for computer vision networks, but uses more common dense building blocks. Even though it was very speculative, after just two attempts to pick the right structure, we saw small improvements compared to the baseline model from [12].","The Inception network started as a case study by the first author to predict the possible outputs of a sophisticated neural network topology design algorithm that approximates the sparse architecture implied by [2] for computer vision, but uses more available dense components instead. Despite the highly speculative nature, after only two iterations on the exact topology, modest gains were already observed compared to the reference model from [12]. ","The Inception architecture originated from a thought experiment by the first author to assess the potential outputs of a complex neural network topology construction algorithm that aims to emulate the sparse structure proposed by [2] for vision networks while using more accessible dense modules. Although it was a very speculative endeavor, after just two attempts at selecting the precise topology, modest improvements were seen over the baseline model from [12].",A,1
Going deeper with convolutions,"After further tuning of learning rate, hyperparameters and improved training methodology, we established that the resulting Inception architecture was especially useful in the context of localization and object detection as the base network for [6] and [5]. Interestingly, while most of the original architectural choices have been questioned and tested thoroughly, they turned out to be at least locally optimal. One must be cautious though: although the proposed architecture has become a success for computer vision, it is still questionable whether its quality can be attributed to the guiding principles that have lead to its construction.","Following additional adjustments of the learning rate, hyperparameters, and enhanced training approaches, we determined that the resulting Inception model was particularly beneficial for localization and object detection as the foundation for [6] and [5]. Remarkably, even though most of the original architectural selections were extensively questioned and tested, they proved to be at least locally ideal. However, one should be careful: while the suggested architecture has become a triumph for computer vision, it is still uncertain whether its quality can be credited to the guiding principles that led to its development.","After more refinement of the learning rate, hyperparameters, and improved training techniques, we found that the resulting Inception structure was especially useful for localization and object detection as the base network for [6] and [5]. Interestingly, even though the majority of the original architectural choices were thoroughly challenged and evaluated, they turned out to be at least locally optimal. However, one must be cautious: despite the fact that the proposed architecture has become a success for computer vision, it is still debatable whether its quality can be attributed to the guiding tenets that led to its design.  ","Following further adjustment of the learning rate, hyperparameters, and enhanced training methodologies, we established that the resulting Inception model was particularly beneficial in the context of localization and object detection as the foundation for [6] and [5]. Remarkably, even though most of the original architectural selections were extensively questioned and tested, they proved to be at least locally ideal. However, one should be wary: while the suggested architecture has become a triumph for computer vision, it is still uncertain whether its quality can be credited to the guiding principles that resulted in its construction.",A,1
Going deeper with convolutions,"Making sure would require much more thorough analysis and verification: for example, if automated tools based on the principles described below would find similar, but better topology for the vision networks. The most convincing proof would be if an automated system would create network topologies resulting in similar gains in other domains using the same algorithm but with very differently looking global architecture. At very least, the initial success of the Inception architecture yields firm motivation for exciting future work in this direction.","Further investigation and confirmation would be necessary to be certain: for instance, if computerized tools utilizing the principles outlined here could identify comparable, yet superior arrangements for the vision systems. The most persuasive evidence would be if an automated framework generated network designs that achieved similar improvements in other areas, using the same algorithm but with a very different overall structure. At the very minimum, the initial triumph of the Inception design provides solid incentive for promising future work along these lines.","Making absolutely sure would need a lot more thorough checking and validation: for example, automated tools working based on the principles described here finding better but similar structure for the vision networks. The most convincing proof would be an automated system making network designs that give similar gains in other fields using the same algorithm but looking totally different overall. As a bare minimum, the early success of the Inception architecture gives strong motivation for exciting future work in this direction.","Complete certainty would require far more exhaustive review and confirmation: such as, if computerized instruments harnessing the guidelines outlined here identified comparable, superior configurations for the vision systems. The most persuasive validation would be an automated platform generating network architectures that achieved similar enhancements across other domains, utilizing the same algorithm but with a completely distinct holistic form. At the absolute least, the initial triumph of the Inception design furnishes sturdy impetus for promising future work along these lines.",A,1
Going deeper with convolutions,The main idea of the Inception architecture is based on finding out how an optimal local sparse structure in a convolutional vision network can be approximated and covered by readily available dense components. Note that assuming translation invariance means that our network will be built from convolutional building blocks. All we need is to find the optimal local construction and to repeat it spatially. Arora et al. [2] suggests a layer-by layer construction in which one should analyze the correlation statistics of the last layer and cluster them into groups of units with high correlation. These clusters form the units of the next layer and are connected to the units in the previous layer.,The primary concept behind the Inception model depends on determining how to approximate and implement an optimal local sparse structure in a convolutional visual network using existing dense elements. This assumes that translation invariance enables constructing our network from convolutional components. We just need to identify the ideal local design and duplicate it spatially. Arora et al. [2] recommends a layer-wise development in which we examine the correlation data of the last layer and group highly correlated units. These collections constitute the units of the subsequent layer and connect to the previous layer's units.,The essential thought in the Inception architecture revolves around ascertaining how to estimate and reproduce an ideal localized sparse configuration in a convolutional visual system using available dense constituents. Presuming translation invariance means our network originates from convolutional building blocks. We only need to pinpoint the best local pattern and replicate it everywhere. Arora et al. [2] puts forth a layer-by-layer approach that analyzes the correlation statistics of the final layer and clusters units with high correlation. These clusters become the units of the next layer and connect to the prior layer's units.,The fundamental notion of the Inception model involves determining how to approximate and implement an optimal sparse local structure in a convolutional visual network using existing dense components. Assuming translation invariance means constructing our network from convolutional blocks. We just need to identify the optimal local design and duplicate it everywhere. Arora et al. [2] proposes a layer-by-layer technique that examines the correlation data of the final layer and groups highly correlated units. These groups become the units of the next layer and connect to the previous layer's units.,A,1
Going deeper with convolutions,"We assume that each unit from the earlier layer corresponds to some region of the input image and these units are grouped into filter banks. In the lower layers (the ones close to the input) correlated units would concentrate in local regions. This means, we would end up with a lot of clusters concentrated in a single region and they can be covered by a layer of 1×1 convolutions in the next layer, as suggested in [12]. It also means that the suggested architecture is a combination of all those layers with their output filter banks concatenated into a single output vector forming the input of the next stage.","We suppose that each component from the previous layer matches to a particular area of the input image, and these components are assembled into filter banks. In the lower layers (the ones near the input), correlated components would focus in local zones. This signifies we would finish with numerous clusters focused in a single area that can be enclosed by a layer of 1×1 convolutions in the next layer, as proposed in [12]. It also denotes that the recommended architecture is a fusion of all those layers with their output filter banks joined into a single output vector constituting the input of the next phase.","We think that every unit from the prior stratum relates to a region of the input image, and these units are categorized into filter banks. In the lower strata (the ones adjacent to the input), correlated units would concentrate in local areas. This entails we would end up with many clusters focused in a single region that can be covered by a layer of 1×1 convolutions in the next stratum, as indicated in [12]. It also means the suggested architecture is a combination of all those strata with their output filter banks merged into a single output vector forming the input of the next stage.  ","We posit that each constituent from the earlier echelon corresponds to some precinct of the input image, and these constituents are assembled into filter banks. In the lower echelons (the ones proximate to the input), correlated constituents would amass in local precincts. This signifies we would conclude with copious clusters concentrated in a single precinct that can be encompassed by a layer of 1×1 convolutions in the next echelon, as proposed in [12]. It also denotes that the suggested architecture is an amalgamation of all those echelons with their output filter banks coalesced into a single output vector constituting the input of the next phase.",A,1
Going deeper with convolutions,"Additionally, since pooling operations have been essential for the success in current state of the art convolutional networks, it suggests that adding an alternative parallel pooling path in each such stage should have additional beneficial effect, too (see Figure 2(a)). As these “Inception modules” are stacked on top of each other, their output correlation statistics are bound to vary: as features of higher abstraction are captured by higher layers, their spatial concentration is expected to decrease suggesting that the ratio of 3×3 and 5×5 convolutions should increase as we move to higher layers.","Moreover, because pooling operations have been critical for the success of current state-of-the-art convolutional networks, it implies that incorporating an extra parallel pooling pathway in each of these stages should also have added positive impact (refer to Figure 2(a)). As these ""Inception modules"" are piled on top of one another, their output correlation data is likely to change: as higher layers capture more abstract features, their spatial density is predicted to decrease, suggesting that the proportion of 3×3 and 5×5 convolutions should grow as we progress to higher layers.","Furthermore, since pooling operations have been vital for the achievements in present best convolutional networks, it hints that adding a supplementary parallel pooling route in all such phases should have extra helpful effect too (see Figure 2(a)). As these ""Inception modules"" are stacked above one another, their output correlation numbers are bound to vary: as more abstract features are captured by higher layers, their spatial concentration is expected to decrease implying that the ratio of 3×3 and 5×5 convolutions should increase as we go to higher layers.  ","In addition, because pooling operations have been essential for the success in current best convolutional networks, it proposes that incorporating an alternative parallel pooling pathway in each of these stages should also have supplementary positive impact (refer to Figure 2(a)). As these ""Inception modules"" are piled on top of each other, their output correlation statistics are likely to change: as features of higher abstraction are captured by higher layers, their spatial concentration is predicted to decrease suggesting that the proportion of 3×3 and 5×5 convolutions should increase as we progress to higher layers.",A,1
Going deeper with convolutions,"This leads to the second idea of the proposed architecture: judiciously applying dimension reductions and projections wherever the computational requirements would increase too much otherwise. This is based on the success of embeddings: even low dimensional embeddings might contain a lot of information about a relatively large image patch. However, embeddings represent information in a dense, compressed form and compressed information is harder to model.","This brings us to the second concept of the suggested design: carefully using dimensionality decreases and projections wherever the computing needs would escalate excessively otherwise. This relies on the success of embeddings: even low dimensional embeddings may encompass ample details about a relatively large image area. However, embeddings denote facts in a packed, condensed form and condensed knowledge is tougher to exemplify.","This guides us to the second principle of the advised framework: prudently employing dimension cutbacks and projections where the calculation demands would swell immoderately otherwise. This depends on the prosperity of embeddings: even low dimensional embeddings could comprise abundant particulars regarding a fairly big image patch. Though, embeddings characterize intelligence in a thick, compressed way and compressed data is more difficult to model. ","This leads into the second tenet of the recommended plan: sensibly applying dimension shrinkages and projections where the computing necessities would rise overly otherwise. This is founded on the triumph of embeddings: even low dimensional embeddings can hold copious information regarding a relatively immense image patch. However, embeddings denote facts in a dense, compacted form and compacted info is harder to demonstrate.",A,1
Going deeper with convolutions,"We would like to keep our representation sparse at most places (as required by the conditions of [2]) and compress the signals only whenever they have to be aggregated en masse. That is, 1×1 convolutions are used to compute reductions before the expensive 3×3 and 5×5 convolutions. Besides being used as reductions, they also include the use of rectified linear activation which makes them dual-purpose. The final result is depicted in Figure 2(b). In general, an Inception network is a network consisting of modules of the above type stacked upon each other, with occasional max-pooling layers with stride 2 to halve the resolution of the grid.","We want our representation to be sparse in most places (as needed by the requirements of [2]) and compress the signals only when they must be combined together. That means 1×1 convolutions are utilized to calculate reductions before the costly 3×3 and 5×5 convolutions. In addition to being used for reductions, they also employ rectified linear activation which makes them serve two purposes. The end result is shown in Figure 2(b). Broadly, an Inception network consists of modules of the above type stacked on top of each other, with occasional max-pooling layers with stride 2 to reduce the grid resolution by half.","Our goal is to have a sparse representation in most areas (per the constraints of [2]) and consolidate the signals exclusively when they need to be aggregated in large numbers. Specifically, 1×1 convolutions are leveraged to compute reductions prior to the expensive 3×3 and 5×5 convolutions. On top of being used for reductions, they also incorporate rectified linear activation, making them dual-use. The final output is illustrated in Figure 2(b). At a high level, an Inception network comprises modules of the aforementioned type piled on each other, with sporadic max-pooling layers with stride 2 to halve the grid resolution.  ","We want our representation to be sparse for the most part (as stipulated by [2]) and compress the signals only when they must be combined in bulk. That is, 1×1 convolutions are utilized to calculate reductions before the costly 3×3 and 5×5 convolutions. Apart from being used for reductions, they also employ rectified linear activation, serving two purposes. The end result is depicted in Figure 2(b). In a nutshell, an Inception network consists of modules of the above kind stacked on top of one another, with occasional max-pooling layers with stride 2 to reduce the grid resolution by half.",A,1
Going deeper with convolutions,"For technical reasons (memory efficiency during training), it seemed beneficial to start using Inception modules only at higher layers while keeping the lower layers in traditional convolutional fashion. This is not strictly necessary, simply reflecting some infrastructural inefficiencies in our current implementation. One of the main beneficial aspects of this architecture is that it allows for increasing the number of units at each stage significantly without an uncontrolled blow-up in computational complexity. The ubiquitous use of dimension reduction allows for shielding the large number of input filters of the last stage to the next layer, first reducing their dimension before convolving over them with a large patch size.","Due to technical constraints with memory during training, it was helpful to only use Inception modules in the higher layers while keeping the early layers as standard convolutional layers. This was not essential, just showing some inadequacies in our current system. A key benefit of this design is it lets us substantially increase the number of units at each stage without computational complexity spiraling out of control. Using dimension reduction everywhere protects later layers from the large number of input filters of the last stage, first shrinking them before convolving over them with a large patch size.","For efficiency reasons during learning, we found it useful to have Inception modules only in higher tiers with regular convolutional tiers below. This wasn't mandatory, just oversights in our present tools. A major plus of this layout is it enables largely expanding the count of nodes at each tier without computational intricacy rising rapidly. Employing downscaling everywhere shields subsequent tiers from the numerous input filters of the final tier, first compressing them before crossing over them with a big patch extent.","Due to limitations in memory during training, it was advantageous to utilize Inception modules solely in higher levels while retaining standard convolutional layers in lower levels. This was not imperative, simply highlighting some inefficiencies in our current implementation. A primary benefit of this design is it permits substantially increasing the quantity of units at each stage without an uncontrolled surge in computational complexity. The widespread application of dimension reduction shields later layers from the numerous input filters of the final stage, first condensing them before convolving over them with a large patch size.",A,1
Going deeper with convolutions,"Another practically useful aspect of this design is that it aligns with the intuition that visual information should be processed at various scales and then aggregated so that the next stage can abstract features from different scales simultaneously. The improved use of computational resources allows for increasing both the width of each stage as well as the number of stages without getting into computational difficulties.  We have found that all the included the knobs and levers allow for a controlled balancing of computational resources that can result in networks that are 2 − 3× faster than similarly performing networks with non-Inception architecture, however this requires careful manual design at this point.","A further practically helpful part of this plan is that it matches the idea that visual data should be handled at different scales and then combined so the next phase can extract features from multiple scales at the same time. The enhanced use of computing resources enables expanding both the width of each step and the quantity of steps without running into computing problems. We've found that all the included controls and adjustments enable a modulated balancing of computing power that can produce networks 2-3 times quicker than equally performing networks without the Inception design, however this necessitates careful manual configuration currently.","An additional practically useful facet of this blueprint is that it aligns with the notion that visual information ought to be processed at various levels and then brought together so the subsequent stage can abstract traits from different levels simultaneously. The improved harnessing of computing assets enables increasing both the breadth of each phase and the amount of phases without encountering computing difficulties. We've discovered that all the included dials and levers allow for a regulated balancing of computing power that can generate networks that are 2-3 times faster than comparably performing networks without the Inception architecture, however this necessitates meticulous manual design for now.","A further practically helpful element of this model is that it conforms with the concept that visual data should be handled at different scales and then consolidated so the next step can extract attributes from multiple scales concurrently. The enhanced utilization of computing resources permits expanding both the width of each step and the number of steps without encountering computing problems. We've found that all the included controls and adjustments enable a controlled balancing of computing capacity that can yield networks that are 2-3 times quicker than equally performing networks without the Inception design, however this necessitates careful manual configuration at present.",A,1
Going deeper with convolutions,"We chose GoogLeNet as our team-name in the ILSVRC14 competition. This name is an homage to Yann LeCuns pioneering LeNet 5 network [10]. We also use GoogLeNet to refer to the particular incarnation of the Inception architecture used in our submission for the competition. We have also used a deeper and wider Inception network, the quality of which was slightly inferior, but adding it to the ensemble seemed to improve the results marginally. We omit the details of that network, since our experiments have shown that the influence of the exact architectural parameters is relatively minor.","For the ILSVRC14 contest, we selected GoogLeNet to be our team's name. This was a tribute to Yann LeCun's groundbreaking LeNet 5 network [10]. We also use GoogLeNet when referring to the specific version of the Inception architecture we submitted for the competition. We tested a larger and more complex Inception network too, which was slightly worse on its own but helped the ensemble a little when added. We won't go into the details of that network, since our tests showed the exact architecture parameters aren't too important.","When entering the ILSVRC14 challenge, our team chose the name GoogLeNet. This was meant as an homage to Yann LeCun's pioneering LeNet 5 network [10]. GoogLeNet also refers to the particular Inception architecture variation we used for our competition submission. We tried out a bigger and deeper Inception network as well, which was marginally inferior by itself but improved the ensemble results slightly when incorporated. We won't discuss the specifics of that network, since experiments showed the precise architectural details aren't very significant.  ","For the ILSVRC14 competition, our team selected the name GoogLeNet. This was intended as a tribute to Yann LeCun's groundbreaking LeNet 5 network [10]. We also use GoogLeNet to refer to the particular version of the Inception architecture that was part of our submission. Additionally, we tested an even larger and deeper Inception network, which was slightly worse on its own but provided a small boost to the ensemble when added. We won't provide the specifics of that network, since experiments demonstrated the exact architectural parameters are not very important.",A,1
Going deeper with convolutions,"Here, the most successful particular instance (named GoogLeNet) is described in Table 1 for demonstrational purposes. The exact same topology (trained with different sampling methods) was used for 6 out of the 7 models in our ensemble. All the convolutions, including those inside the Inception modules, use rectified linear activation. The size of the receptive field in our network is 224×224 taking RGB color channels with mean subtraction. “#3×3 reduce” and “#5×5 reduce” stands for the number of 1×1 filters in the reduction layer used before the 3×3 and 5×5 convolutions. One can see the number of 1×1 filters in the projection layer after the built-in max-pooling in the pool proj column. All these reduction/projection layers use rectified linear activation as well.","The most successful specific case (called GoogLeNet) is illustrated in Table 1 for example purposes. The very same design (trained with various sampling techniques) was utilized for 6 out of the 7 models in our collection. All the convolutions, including those inside the Inception modules, employ rectified linear activation. The size of the receptive field in our network is 224×224 taking RGB color channels with mean subtraction. “#3×3 reduce” and “#5×5 reduce” represents the number of 1×1 filters in the reduction layer used before the 3×3 and 5×5 convolutions. One can observe the number of 1×1 filters in the projection layer after the built-in max-pooling in the pool proj column. All these reduction/projection layers utilize rectified linear activation too.","The most effective individual instance (dubbed GoogLeNet) is shown in Table 1 as an example. The precise same architecture (trained with different sampling procedures) was employed for 6 out of the 7 models in our group. All the convolutions, including those inside the Inception modules, use rectified linear activation. The size of the receptive field in our network is 224×224 taking RGB color channels with mean subtraction. “#3×3 reduce” and “#5×5 reduce” denotes the number of 1×1 filters in the reduction layer used before the 3×3 and 5×5 convolutions. One can discern the number of 1×1 filters in the projection layer after the built-in max-pooling in the pool proj column. All these reduction/projection layers utilize rectified linear activation too.","The most triumphant specific case (termed GoogLeNet) is delineated in Table 1 for illustrative purposes. The very same design (trained with diverse sampling techniques) was employed for 6 out of the 7 models in our collection. All the convolutions, including those inside the Inception modules, utilize rectified linear activation. The size of the receptive field in our network is 224×224 taking RGB color channels with mean subtraction. “#3×3 reduce” and “#5×5 reduce” signifies the number of 1×1 filters in the reduction layer used before the 3×3 and 5×5 convolutions. One can discern the number of 1×1 filters in the projection layer after the built-in max-pooling in the pool proj column. All these reduction/projection layers use rectified linear activation too.",A,1
Going deeper with convolutions,"The network was designed with computational efficiency and practicality in mind, so that inference can be run on individual devices including even those with limited computational resources, especially with low-memory footprint. The network is 22 layers deep when counting only layers with parameters (or 27 layers if we also count pooling). The overall number of layers (independent building blocks) used for the construction of the network is about 100. However this number depends on the machine learning infrastructure system used. The use of average pooling before the classifier is based on [12], although our implementation differs in that we use an extra linear layer.","The network was built focusing on computational efficiency and practicality, so that inference could be executed on various devices including those with constrained computational capabilities, particularly with a small memory footprint. The network has 22 layers if only accounting for layers containing parameters (or 27 layers including pooling layers). Approximately 100 separate building blocks were utilized to construct the network. However this quantity is contingent on the machine learning framework leveraged. Using average pooling before the classifier is inspired by [12], however our implementation has an additional linear layer.","The network was engineered with computational performance and real-world applicability in mind, enabling deployment on individual gadgets even those with limited computing power, especially regarding memory usage. The network contains 22 parameter-holding layers (or 27 total layers counting pooling). The overall number of discrete components used to build the network is around 100, but depends on the machine learning platform used. Applying average pooling before the classifier comes from [12], but our version uses an extra linear layer.  ","The network was designed for computational efficiency and practical use, allowing it to run on various devices including those with constrained computing abilities, particularly a small memory footprint. The network has 22 parameter-containing layers (or 27 with pooling layers). Approximately 100 modular building blocks were used to construct the network, but this number varies based on the machine learning system. Using average pooling before the classifier is based on [12], but our implementation has an additional linear layer.",A,1
Going deeper with convolutions,"This enables adapting and fine-tuning our networks for other label sets easily, but it is mostly convenience and we do not expect it to have a major effect. It was found that a move from fully connected layers to average pooling improved the top-1 accuracy by about 0.6%, however the use of dropout remained essential even after removing the fully connected layers. Given the relatively large depth of the network, the ability to propagate gradients back through all the layers in an effective manner was a concern.","This makes it easy to adjust and refine our networks for other label groups without much effort, but we don't think it will have a big impact. We found that switching from fully connected layers to average pooling increased top-1 accuracy by around 0.6%. However, dropout was still crucial even after getting rid of the fully connected layers. With the network being quite deep, being able to pass gradients backward through all layers efficiently was a worry.","This allows us to simply tune and optimize our networks for other label sets, though it likely won't make a major difference. Replacing fully connected layers with average pooling was found to boost top-1 accuracy by 0.6% or so, but dropout remained key even minus the fully connected layers. Given how deep the network is, being able to propagate gradients back through all layers effectively was a concern. ","This lets us easily adapt and fine-tune our networks for other label groups, but we don't expect a big effect. We found top-1 accuracy rose about 0.6% from switching fully connected layers to average pooling, yet dropout was still vital without the fully connected layers. With the network's large depth, propagating gradients back through all layers efficiently was worrisome.",A,1
Going deeper with convolutions,"One interesting insight is that the strong performance of relatively shallower networks on this task suggests that the features produced by the layers in the middle of the network should be very discriminative. By adding auxiliary classifiers connected to these intermediate layers, we would expect to encourage discrimination in the lower stages in the classifier, increase the gradient signal that gets propagated back, and provide additional regularization. These classifiers take the form of smaller convolutional networks put on top of the output of the Inception (4a) and (4d) modules. During training, their loss gets added to the total loss of the network with a discount weight (the losses of the auxiliary classifiers were weighted by 0.3). At inference time, these auxiliary networks are discarded.","One fascinating observation is that the good performance of somewhat shallower networks on this task hints that the representations generated by the layers in the center of the network should be very informative for discrimination. By appending auxiliary classifiers linked to these intermediate layers, we would expect to promote discrimination in the earlier phases in the classifier, amplify the gradient signal that is propagated backward, and furnish extra regularization. These classifiers are smaller convolutional networks placed on top of the output of the Inception (4a) and (4d) modules. During training, their loss is added to the total loss of the network with a discount weight (the losses of the auxiliary classifiers were weighted by 0.3). At inference time, these auxiliary networks are discarded.","One intriguing insight is that the strong capabilities of relatively less deep networks on this task implies that the features created by the layers in the middle of the network should be very useful for distinguishing between classes. By introducing auxiliary classifiers connected to these intermediate layers, we would anticipate encouraging discrimination in the earlier stages of the classifier, enlarging the gradient signal that gets backpropagated, and providing supplementary regularization. These classifiers are smaller convolutional networks attached to the output of the Inception (4a) and (4d) modules. During training, their loss gets incorporated into the total loss of the network with a discount weight (the losses of the auxiliary classifiers were weighted by 0.3). At inference time, these auxiliary networks are removed.","One fascinating realization is that the robust performance of relatively shallower networks on this task hints that the representations formed by the layers in the center of the network should be very informative for discrimination. By integrating auxiliary classifiers linked to these intermediate layers, we would expect to promote discrimination in the earlier phases of the classifier, boost the gradient signal that propagates backward, and introduce extra regularization. These classifiers are smaller convolutional networks connected to the output of the Inception (4a) and (4d) modules. During training, their loss is combined with the total loss of the network with a discount weight (the losses of the auxiliary classifiers were weighted by 0.3). At inference time, these auxiliary networks are taken away.",A,1
Going deeper with convolutions,"Our networks were trained using the DistBelief [4] distributed machine learning system using modest amount of model and data-parallelism. Although we used CPU based implementation only, a rough estimate suggests that the GoogLeNet network could be trained to convergence using few high-end GPUs within a week, the main limitation being the memory usage. Our training used asynchronous stochastic gradient descent with 0.9 momentum [17], fixed learning rate schedule (decreasing the learning rate by 4% every 8 epochs).","Our neural networks were developed using the DistBelief distributed machine learning framework with a small degree of model and data parallelism. We utilized only CPU implementations, but projections indicate the GoogLeNet architecture could reach convergence in around 7 days using several high-performance GPUs, with memory consumption being the primary constraint. Training leveraged asynchronous stochastic gradient descent with momentum of 0.9, and a fixed learning rate schedule (reducing the rate by 4% every 8 epochs).","Our models were built leveraging the DistBelief distributed learning system applying moderate model and data parallelism. We used CPU-only implementations, however estimates show the GoogLeNet model could fully train in about a week using a few top-tier GPUs, with memory usage being the main limitation. Training employed asynchronous stochastic gradient descent with 0.9 momentum, and a static learning rate plan (lowering the rate by 4% every 8 epochs).  ","Our neural networks were constructed using the DistBelief distributed machine learning platform applying limited model and data parallelism. Although we utilized CPU-based implementations exclusively, projections indicate the GoogLeNet architecture could converge within a week using several high-end GPUs, with memory demands being the primary constraint. Training made use of asynchronous stochastic gradient descent with momentum of 0.9, and a fixed learning rate agenda (reducing the rate by 4% every 8 epochs).",A,1
Going deeper with convolutions,"Polyak averaging [13] was used to create the final model used at inference time. Our image sampling methods have changed substantially over the months leading to the competition, and already converged models were trained on with other options, sometimes in conjunction with changed hyperparameters, like dropout and learning rate, so it is hard to give a definitive guidance to the most effective single way to train these networks. To complicate matters further, some of the models were mainly trained on smaller relative crops, others on larger ones, inspired by [8].","We utilized Polyak averaging [13] for the final model we used during inference. The image sampling techniques we employed changed considerably over the months prior to the competition. Models that had already converged were further trained using different options, at times together with modified hyperparameters like dropout rate and learning rate. Therefore, it is challenging to provide definitive guidance on the single most successful approach for training these neural networks. Additionally, some of the models were primarily trained on smaller relative crops while others used larger ones, following [8].","We made use of Polyak averaging [13] for creating the final model utilized during inference. Our approaches to image sampling went through substantial changes in the months leading up to the competition. Models that had already converged were further trained using other techniques, sometimes paired with altered hyperparameters such as dropout and learning rate. As a result, it is difficult to definitively recommend the single most effective way to train these networks. To make things more complicated, some of the models were mostly trained on smaller relative crops while others used larger ones, inspired by [8].  ","Polyak averaging [13] was leveraged to produce the final model used at inference time. The image sampling methods we used changed significantly over the months prior to the competition. Models that had already converged were further trained using other options, at times in combination with modified hyperparameters like dropout and learning rate. Therefore, it is challenging to definitively point to the single most successful approach for training these networks. Additionally, some of the models were primarily trained on smaller relative crops while others utilized larger ones, based on [8].",A,1
ImageNet A Large_Scale Hierarchical Image Database,"The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called “ImageNet”, a largescale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500- 1000 clean and full resolution images.","The rapid growth of pictorial information online could help develop more advanced systems for cataloging, finding, arranging and using visual data. However, the best ways to control and structure this information are still unclear. We present a new database called ""ImageNet"" here, a large taxonomy of photos constructed using WordNet as a framework. ImageNet seeks to provide most of the 80,000 WordNet categories with 500 to 1000 high quality, full size photos on average.","The explosion of images on the web enables more refined tools and algorithms for indexing, searching, organizing and interacting with visual content and multimedia. But how to actually leverage and systematize this data remains an open question. We introduce a novel database named ""ImageNet"" which categorizes images on a large scale using the WordNet hierarchy. ImageNet aims to attach 500 to 1000 pristine, high resolution images to a majority of the 80,000 WordNet synsets.","The rapid expansion of pictorial data online can facilitate more advanced models and techniques for cataloging, finding, arranging and using visual and multimedia content. However, the best methods for harnessing and structuring this data are still unclear. We present here a new database called ""ImageNet"" which taxonomizes images extensively using the WordNet categories as a framework. ImageNet seeks to link most of WordNet's 80,000 synsets with an average of 500 to 1000 high quality, full resolution photographs.",A,1
ImageNet A Large_Scale Hierarchical Image Database,"This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.","This document provides an in-depth examination of the present condition of ImageNet: 12 subgroups containing 5247 concepts and a total of 3.2 million photos. We demonstrate that ImageNet is considerably larger in size and variety and much more precise than other existing image collections. Assembling such an enormous database is difficult. We explain the data gathering process using Amazon Mechanical Turk. Finally, we highlight the usefulness of ImageNet through three basic applications in object identification, image categorization, and automatic object clustering. We believe the scale, precision, diversity, and hierarchical organization of ImageNet can provide unmatched prospects for experts in computer vision and other fields.","This paper gives a thorough analysis of the current state of ImageNet: 12 subcategories with 5247 definitions and 3.2 million total images. We establish that ImageNet has a much greater scale and diversity and is far more accurate than other present image datasets. Building such a massive database is challenging. We delineate the data accumulation method utilizing Amazon Mechanical Turk. In closing, we demonstrate the value of ImageNet through three straightforward use cases in object recognition, image classification, and automatic object clustering. We anticipate that the size, accuracy, variety, and hierarchical structure of ImageNet can furnish unrivaled opportunities for researchers in computer vision and other areas.  ","This article provides an in-depth review of ImageNet in its present form: 12 subgroups containing 5247 concepts and a total of 3.2 million photographs. We prove that ImageNet is significantly larger in magnitude and variety and considerably more precise than existing image collections. Assembling such a huge database is difficult. We characterize the data gathering technique employing Amazon Mechanical Turk. In conclusion, we exhibit the usefulness of ImageNet through three simple applications in object identification, image categorization, and automatic object clustering. We expect the scale, precision, diversity, and hierarchical organization of ImageNet to offer unmatched possibilities for experts in computer vision and beyond.",A,1
ImageNet A Large_Scale Hierarchical Image Database,"The digital era has brought with it an enormous explosion of data. The latest estimations put a number of more than 3 billion photos on Flickr, a similar number of video clips on YouTube and an even larger number for images in the Google Image Search database. More sophisticated and robust models and algorithms can be proposed by exploiting these images, resulting in better applications for users to index, retrieve, organize and interact with these data. But exactly how such data can be utilized and organized is a problem yet to be solved. In this paper, we introduce a new image database called “ImageNet”, a large-scale ontology of images.","The digital age has led to a massive increase in data. Current estimates show over 3 billion photos on Flickr, a comparable amount of videos on YouTube, and even more images in Google Image Search. By using these images in new and better models and algorithms, improved applications could be created to categorize, find, arrange, and interact with this data. However, how to actually use and structure this data remains an unsolved issue. This paper presents a new image database called ""ImageNet"", which is a large-scale organization of images.","The digital era has resulted in a huge explosion of information. Current approximations indicate over 3 billion photographs on Flickr, a similar quantity of videos on YouTube, and an even greater number for pictures in the Google Image Search system. More refined and robust frameworks and formulas can be suggested by leveraging these visuals, resulting in superior applications for users to catalog, access, coordinate and connect with this content. However, precisely how such information can be used and structured is an issue that has yet to be addressed. In this paper, we present a new image repository called ""ImageNet"", a large-scale categorization of images.","The digital age has brought an enormous increase in information. Current estimates show more than 3 billion photos on Flickr, a comparable number of videos on YouTube, and even more images in Google Image Search. By utilizing these images in more advanced and robust models and algorithms, better applications could be created for users to index, find, organize and interact with this information. However, exactly how to use and structure this data remains an unresolved problem. This paper introduces a new image database called ""ImageNet"", which is a large-scale classification of images.",A,1
ImageNet A Large_Scale Hierarchical Image Database,"We believe that a large-scale ontology of images is a critical resource for developing advanced, large-scale content-based image search and image understanding algorithms, as well as for providing critical training and benchmarking data for such algorithms. ImageNet uses the hierarchical structure of WordNet [9]. Each meaningful concept in WordNet, possibly described by multiple words or word phrases, is called a “synonym set” or “synset”. There are around 80, 000 noun synsets in WordNet. In ImageNet, we aim to provide on average 500-1000 images to illustrate each synset. Images of each concept are quality-controlled and human-annotated as described in Sec. 3.2.","Our view is that a big ontology of images is an essential tool for making advanced, large-scale content-based image search and understanding systems, and for giving key training and benchmark data for these systems. ImageNet takes advantage of the hierarchical structure of WordNet [9]. Every meaningful idea in WordNet, possibly described by multiple words or phrases, is called a ""synonym set"" or ""synset"". There are around 80,000 noun synsets in WordNet. In ImageNet, our goal is to give an average of 500-1000 images to demonstrate each synset. Images of each concept are quality-checked and human-labeled as described in Sec. 3.2.","We think a large-scale organization of images is a vital resource for developing sophisticated, large-scale image search and comprehension algorithms based on content, and for supplying important training and benchmarking information for such algorithms. ImageNet utilizes the hierarchical arrangement of WordNet [9]. Each meaningful notion in WordNet, possibly expressed by multiple words or phrases, is termed a ""synonym set"" or ""synset"". There are about 80,000 noun synsets in WordNet. In ImageNet, we strive to provide typically 500-1000 images to exemplify each synset. Images of each concept are quality-controlled and human-annotated as delineated in Sec. 3.2.","Our belief is that a big taxonomy of images is a key asset for creating advanced, large-scale image search and understanding systems based on content, as well as for furnishing crucial training and benchmarking data for such systems. ImageNet leverages the hierarchical structure of WordNet [9]. Each meaningful idea in WordNet, possibly conveyed by multiple words or expressions, is called a ""synonym set"" or ""synset"". There are around 80,000 noun synsets in WordNet. In ImageNet, our objective is to supply on average 500-1000 images to illustrate each synset. Images of each concept are quality-checked and human-labeled as described in Sec. 3.2.",A,1
ImageNet A Large_Scale Hierarchical Image Database,"ImageNet, therefore, will offer tens of millions of cleanly sorted images. In this paper, we report the current version of ImageNet, consisting of 12 “subtrees”: mammal, bird, fish, reptile, amphibian, vehicle, furniture, musical instrument, geological formation, tool, flower, fruit. These subtrees contain 5247 synsets and 3.2 million images. Fig. 1 shows a snapshot of two branches of the mammal and vehicle subtrees The rest of the paper is organized as follows: We first show that ImageNet is a large-scale, accurate and diverse image database (Section 2).","ImageNet provides many millions of well-organized images. This report describes the current form of ImageNet, with 12 categories: mammals, birds, fish, reptiles, amphibians, vehicles, furniture, musical instruments, geological formations, tools, flowers, and fruits. These categories have 5247 concepts and 3.2 million images. Figure 1 displays parts of the mammal and vehicle categories. The rest of the report covers how ImageNet is a big, precise, and varied image database (Section 2).","ImageNet offers a great number of neatly classified images. This article presents the existing version of ImageNet, made up of 12 ""subcategories"": mammals, birds, fish, reptiles, amphibians, vehicles, furniture, musical instruments, geological formations, tools, flowers, fruits. These subcategories contain 5247 concepts and 3.2 million images. Figure 1 shows two branches of the mammal and vehicle subcategories. The rest of the article is structured as follows: We first demonstrate that ImageNet is a large-scale, accurate and diverse image database (Section 2).","ImageNet provides many millions of well-organized images. This paper describes the current iteration of ImageNet, comprising 12 ""subtrees"": mammals, birds, fish, reptiles, amphibians, vehicles, furniture, musical instruments, geological formations, tools, flowers, fruits. These subtrees contain 5247 concepts and 3.2 million images. Figure 1 displays a sample of two branches of the mammal and vehicle subtrees. The remainder of the paper is structured thus: We first illustrate that ImageNet is a large-scale, precise, and varied image database (Section 2).",A,1
ImageNet A Large_Scale Hierarchical Image Database,"In Section 4, we present a few simple application examples by exploiting the current ImageNet, mostly the mammal and vehicle subtrees. Our goal is to show that ImageNet can serve as a useful resource for visual recognition applications such as object recognition, image classification and object localization. In addition, the construction of such a large-scale and high-quality database can no longer rely on traditional data collection methods. Sec. 3 describes how ImageNet is constructed by leveraging Amazon Mechanical Turk.","In Part 4, we give some basic use cases by making use of the present ImageNet, primarily the mammal and vehicle subcategories. Our aim is to demonstrate that ImageNet can be a helpful resource for visual identification applications like object recognition, image grouping and object positioning. Furthermore, building such a large-scale and high-quality database can't depend on conventional data gathering methods anymore. Section 3 illustrates how ImageNet is built by harnessing Amazon Mechanical Turk.","In Section 4, we provide a few straightforward application examples by utilizing the current ImageNet, chiefly the mammal and vehicle subgroups. Our objective is to indicate that ImageNet can function as a beneficial asset for visual recognition applications such as object identification, image classification and object localization. Additionally, the development of such a large-scale and superior quality database cannot continue to rely on traditional data collection techniques. Section 3 delineates how ImageNet is constructed by leveraging Amazon Mechanical Turk.","In Part 4, we present some simple use cases by making use of the existing ImageNet, primarily the mammal and vehicle subcategories. Our goal is to demonstrate that ImageNet can serve as a useful tool for visual recognition applications like object detection, image categorization and object pinpointing. Furthermore, constructing such a large-scale and high-quality dataset can no longer depend on conventional data gathering approaches. Section 3 illustrates how ImageNet is built by harnessing Amazon Mechanical Turk.",A,1
ImageNet A Large_Scale Hierarchical Image Database,"ImageNet is built upon the hierarchical structure provided by WordNet. In its completion, ImageNet aims to contain in the order of 50 million cleanly labeled full resolution images (500-1000 per synset). At the time this paper is written, ImageNet consists of 12 subtrees. Most analysis will be based on the mammal and vehicle subtrees. ImageNet aims to provide the most comprehensive and diverse coverage of the image world.","ImageNet makes use of the hierarchical structure from WordNet. The goal of ImageNet is to have around 50 million high quality, fully labeled images (500-1000 for each concept). Currently, ImageNet is made up of 12 subsections. A lot of the analysis will focus on the mammal and vehicle subsections. ImageNet strives to give the most thorough and varied representation of images.","ImageNet utilizes the hierarchical framework from WordNet. When finished, ImageNet hopes to have about 50 million cleanly categorized full resolution pictures (500-1000 for each idea). At this time, ImageNet contains 12 subcategories. Much of the analysis will examine the mammal and vehicle subcategories. ImageNet aims to provide the most extensive and diverse coverage of images.","ImageNet takes advantage of the hierarchical organization from WordNet. ImageNet aims to eventually have around 50 million high-res, cleanly labeled images (500-1000 per concept). Right now, ImageNet is composed of 12 subgroups. A lot of the analysis will look at the mammal and vehicle subgroups. ImageNet wants to give the most comprehensive and varied representation of the image world.",A,1
ImageNet A Large_Scale Hierarchical Image Database,"The current 12 subtrees consist of a total of 3.2 million cleanly annotated images spread over 5247 categories (Fig. 2). On average over 600 images are collected for each synset. Fig. 2 shows the distributions of the number of images per synset for the current ImageNet . To our knowledge this is already the largest clean image dataset available to the vision research community, in terms of the total number of images, number of images per category as well as the number of categories.","The present 12 subtrees have a sum of 3.2 million images that are cleanly labeled and divided into 5247 types (Fig. 2). On normal over 600 photos exist for each synset. Fig. 2 exhibits the distributions of image quantity per synset for the current ImageNet. As far as we know this is presently the most substantial clean image dataset accessible to the vision science community, regarding total image count, images per type and number of categories.","The existing 12 subtrees hold a total of 3.2 million images that have been properly annotated and separated into 5247 groups (Fig. 2). Typically more than 600 images are accumulated for each synset. Fig. 2 demonstrates the distributions of the amount of images per synset for the current ImageNet. To our understanding, this is now the most extensive clean image collection available to the vision research society, considering the total quantity of images, quantity of images per group, and number of groups.  ","The now existing 12 subtrees have an aggregate of 3.2 million images that have been accurately labeled and categorized into 5247 sets (Fig. 2). On median over 600 images have been gathered for each synset. Fig. 2 exhibits the distributions of image count per synset for the present ImageNet. As far as we are aware, this is presently the most substantial clean image database accessible to the vision science community, with regards to total image count, images per category, and category count.",A,1
ImageNet A Large_Scale Hierarchical Image Database,"ImageNet organizes the different classes of images in a densely populated semantic hierarchy. The main asset of WordNet [9] lies in its semantic structure, i.e. its ontology of concepts. Similarly to WordNet, synsets of images in ImageNet are interlinked by several types of relations, the “IS-A” relation being the most comprehensive and useful. Although one can map any dataset with category labels into a semantic hierarchy by using WordNet, the density of ImageNet is unmatched by others. For example, to our knowledge no existing vision dataset offers images of 147 dog categories. Fig. 3 compares the “cat” and “cattle” subtrees of ImageNet and the ESP dataset [25]. We observe that ImageNet offers much denser and larger trees.","ImageNet sorts the various types of pictures into a packed semantic structure. The primary value of WordNet [9] is in its semantic organization, meaning its system of concepts. Like WordNet, groups of related images in ImageNet are connected through several kinds of relationships, with ""IS-A"" being the most extensive and beneficial. While any labeled dataset can be mapped to a semantic hierarchy using WordNet, ImageNet's density is unparalleled. For instance, no current visual dataset provides photos of 147 dog types. Fig. 3 contrasts the ""cat"" and ""cattle"" subdivisions of ImageNet and the ESP dataset [25]. We see that ImageNet has much more dense and larger structures.","ImageNet arranges the different image categories into a tightly packed semantic network. The main strength of WordNet [9] is its semantic structure, or ontology of concepts. Similar to WordNet, clusters of related images in ImageNet are linked through various relation types, with ""IS-A"" being the most comprehensive and useful. Although any dataset with labels can be mapped to a semantic hierarchy through WordNet, ImageNet's density is unmatched. For example, no existing visual dataset has images of 147 dog breeds. Fig. 3 compares the ""cat"" and ""cattle"" subgroups of ImageNet and the ESP dataset [25]. We observe that ImageNet has much more dense and extensive subgroups.  ","ImageNet organizes the various image classes into a packed semantic network. The primary value of WordNet [9] is its semantic design, meaning its system of concepts. As with WordNet, groups of related images in ImageNet are interconnected through several relation types, with ""IS-A"" being the broadest and most useful. While any labeled dataset could be mapped to a semantic hierarchy via WordNet, ImageNet's density is unparalleled. For instance, no current visual dataset provides images of 147 dog varieties. Fig. 3 contrasts the ""cat"" and ""cattle"" subcategories of ImageNet and the ESP dataset [25]. We see that ImageNet has much more dense and expansive subcategories.",A,1
ImageNet A Large_Scale Hierarchical Image Database,"We would like to offer a clean dataset at all levels of the WordNet hierarchy. Fig. 4 demonstrates the labeling precision on a total of 80 synsets randomly sampled at different tree depths. An average of 99.7% precision is achieved on average. Achieving a high precision for all depths of the ImageNet tree is challenging because the lower in the hierarchy a synset is, the harder it is to classify, e.g. Siamese cat versus Burmese cat.","We want to provide a pristine dataset at all tiers of the WordNet structure. Fig. 4 shows the labeling accuracy on a total of 80 synsets arbitrarily chosen at various tree profundities. An normal of 99.7% accuracy is accomplished on average. Attaining a high accuracy for all depths of the ImageNet tree is tough because the lower in the hierarchy a synset is, the more problematic it is to categorize, e.g. Siamese cat versus Burmese cat.","Our goal is to make available an uncontaminated dataset across all levels of the WordNet taxonomy. Fig. 4 displays the labeling precision on 80 synsets randomly selected from different tree heights. On average, 99.7% precision is reached on average. Getting high precision across all layers of the ImageNet tree is challenging because the further down the hierarchy a synset is, the harder it is to classify it correctly, for example Siamese cat vs Burmese cat.","We aspire to furnish an unpolluted dataset at every grade of the WordNet categorization. Fig. 4 exhibits the labeling accuracy on a sum of 80 synsets arbitrarily picked from various tree depths. An standard of 99.7% accuracy is realized on average. Attaining high accuracy across all tiers of the ImageNet tree is difficult since the lower in the categorization a synset is, the more complicated it is to identify correctly, e.g. Siamese cat versus Burmese cat.",A,1
ImageNet A Large_Scale Hierarchical Image Database,"In an attempt to tackle the difficult problem of quantifying image diversity, we compute the average image of each synset and measure lossless JPG file size which reflects the amount of information in an image. Our idea is that a synset containing diverse images will result in a blurrier average image, the extreme being a gray image, whereas a synset with little diversity will result in a more structured, sharper average image. We therefore expect to see a smaller JPG file size of the average image of a more diverse synset. Fig. 5 compares the image diversity in four randomly sampled synsets in Caltech101 [8] 3 and the mammal subtree of ImageNet.","To address the challenging issue of measuring image variety, we find the mean image for each category and evaluate the size of lossless JPG files, which shows the quantity of data in an image. Our concept is that a category with diverse images will produce a more blurred average image, the most extreme being a gray image, while a category with little variety will result in a more structured, sharper average image. Thus, we anticipate observing a smaller JPG file size for the average image of a more diverse category. Fig. 5 compares the image diversity in four randomly chosen categories in Caltech101 [8] and the mammal sub-tree of ImageNet.","In an attempt to quantify the complexity of image diversity, we determine the typical image for each grouping and assess the lossless JPG file size, which represents the information content in an image. Our thinking is that a grouping containing varied images will yield a more fuzzy average image, the most extreme being a gray image, while a grouping with little variety will produce a more structured, sharper average image. Therefore, we expect to see a smaller JPG file size for the average image of a more diverse grouping. Fig. 5 contrasts the image diversity in four randomly selected groupings in Caltech101 [8] and the mammal sub-tree of ImageNet.  ","To tackle the tricky issue of measuring image heterogeneity, we calculate the representative image for each category and evaluate the lossless JPG file dimension, which indicates the data amount in an image. Our rationale is that a category with heterogeneous images will generate a more blurred average image, the most extreme being a gray image, whereas a category with little heterogeneity will produce a more structured, sharper average image. Hence, we predict observing a smaller JPG file dimension for the average image of a more heterogeneous category. Fig. 5 compares the image heterogeneity in four randomly chosen categories in Caltech101 [8] and the mammal sub-branch of ImageNet.",A,1
ImageNet A Large_Scale Hierarchical Image Database,"A number of well labeled small datasets (Caltech101/256 [8, 12], MSRC [22], PASCAL [7] etc.) have served as training and evaluation benchmarks for most of today’s computer vision algorithms. As computer vision research advances, larger and more challenging datasets are needed for the next generation of algorithms. The current ImageNet offers 20× the number of categories, and 100× the number of total images than these datasets. TinyImage [24] is a dataset of 80 million 32 × 32 low resolution images, collected from the Internet by sending all words in WordNet as queries to image search engines.","Several small image datasets with good labels (Caltech101/256 [8, 12], MSRC [22], PASCAL [7] etc.) have been used to train and test most modern computer vision systems. But as computer vision research moves forward, larger and tougher datasets are required for the next wave of algorithms. ImageNet has 20 times more categories and 100 times more total images than those small datasets. TinyImage [24] contains 80 million 32x32 low resolution pictures gathered from the web by using all the words in WordNet as search terms on image search engines.","A few small, well-annotated image datasets (Caltech101/256 [8, 12], MSRC [22], PASCAL [7], etc.) have served as benchmarks for training and evaluating most contemporary computer vision models. However, as computer vision research progresses, bigger and more difficult datasets are necessary for developing cutting-edge algorithms. ImageNet has 20 times the number of classes and 100 times the total images compared to those small datasets. TinyImage [24] is a collection of 80 million 32x32 low-res images from the internet, obtained by querying image search engines with every word in WordNet.","Several small image datasets with thorough labeling (including Caltech101/256 [8, 12], MSRC [22], PASCAL [7], etc.) have been utilized for training and assessing the majority of modern computer vision systems. But larger, more challenging datasets are required as computer vision research continues to advance, in order to develop the next generation of algorithms. ImageNet provides 20 times the number of categories and 100 times the total number of images compared to those small datasets. TinyImage [24] contains 80 million 32x32 low resolution images gathered from the web by using all the words in WordNet to search image search engines.",A,1
ImageNet A Large_Scale Hierarchical Image Database,"The ESP dataset is acquired through an online game [25]. Two players independently propose labels to one image with the goal of matching as many words as possible in a certain time limit. Millions of images are labeled through this game, but its speeded nature also poses a major drawback. Rosch and Lloyd [20] have demonstrated that humans tend to label visual objects at an easily accessible semantic level termed as “basic level” (e.g. bird), as opposed to more specific level (“sub-ordinate level”, e.g. sparrow), or more general level (“super-ordinate level”, e.g. vertebrate). Labels collected from the ESP game largely concentrate at the “basic level” of the semantic hierarchy as illustrated by the color bars in Fig. 6.","The ESP information was obtained through an online activity [25]. Two individuals separately put forward tags for one picture with the aim of coordinating as many terms as feasible within a certain timeframe. Many images were classified through this game, but its rushed essence also presents a significant disadvantage. Rosch and Lloyd [20] have shown that people have a tendency to identify visual items at an easily reachable semantic level called “basic level” (e.g. bird), rather than more explicit level (“subordinate level”, e.g. sparrow), or more general level (“superordinate level”, e.g. vertebrate). Labels gathered from the ESP game largely focus at the “basic level” of the semantic hierarchy as illustrated by the color bars in Fig. 6.","The ESP data was collected via an online pastime [25]. Two participants independently come up with labels for one photo with the goal of matching as many words as possible within a set time limit. Many pictures were tagged through this game, but its hurried nature also introduces a major drawback. Rosch and Lloyd [20] have exhibited that humans tend to describe visual objects at an easily accessible semantic level called “basic level” (e.g. bird), rather than more precise level (“subordinate level”, e.g. sparrow), or more broad level (“superordinate level”, e.g. vertebrate). Labels obtained from the ESP game largely concentrate at the “basic level” of the semantic hierarchy as shown by the color bars in Fig. 6.  ","The ESP information was gathered through an online activity [25]. Two players separately propose tags for one image with the objective of coordinating as many terms as feasible within a certain time constraint. Many pictures were classified through this game, but its rushed aspect also presents a significant shortcoming. Rosch and Lloyd [20] have demonstrated that people tend to characterize visual items at an easily accessible semantic level termed “basic level” (e.g. bird), rather than more explicit level (“subordinate level”, e.g. sparrow), or more general level (“superordinate level”, e.g. vertebrate). Labels collected from the ESP game largely focus at the “basic level” of the semantic hierarchy as depicted by the color bars in Fig. 6.",A,1
ImageNet A Large_Scale Hierarchical Image Database,"ImageNet, however, demonstrates a much more balanced distribution of images across the semantic hierarchy. Another critical difference between ESP and ImageNet is sense disambiguation. When human players input the word “bank”, it is unclear whether it means “a river bank” or a “financial institution”. At this large scale, disambiguation becomes a nontrivial task. Without it, the accuracy and usefulness of the ESP data could be affected. ImageNet, on the other hand, does not have this problem by construction. See section 3.2 for more details. Lastly, most of the ESP dataset is not publicly available. Only 60K images and their labels can be accessed [1].","In contrast, ImageNet has a more even distribution of pictures across semantic categories. Another major difference between ESP and ImageNet is clarifying the meaning of words. When human participants enter ""bank"", it's ambiguous if it refers to ""a river bank"" or ""a financial institution"". At this large size, making this distinction becomes a challenging job. Without it, the precision and usefulness of the ESP information could suffer. ImageNet, however, does not have this issue since each image depicts a specific meaning. See section 3.2 for more details. Additionally, most of the ESP data set cannot be publicly accessed. Only 60K photos and their tags are available [1].","On the other hand, ImageNet has a much more balanced spread of images across the semantic taxonomy. A further critical distinction between ESP and ImageNet is word sense disambiguation. When human users input the word ""bank"", it's unclear if it means ""a river bank"" or ""a financial institution"". At this large scale, making this distinction becomes a difficult task. Without it, the accuracy and utility of the ESP information could be negatively impacted. ImageNet, however, does not have this issue since each image illustrates a specific meaning. Refer to section 3.2 for more information. Furthermore, most of the ESP dataset cannot be publicly accessed. Only 60K images and their labels are available [1].  ","In contrast, ImageNet exhibits a far more balanced distribution of images across semantic categories. Another key difference between ESP and ImageNet is resolving the meaning of ambiguous words. When human users enter ""bank"", it's unclear whether it refers to ""a river bank"" or ""a financial institution"". At this large scale, resolving this ambiguity becomes a challenging task. Without doing so, the precision and usefulness of the ESP data could suffer. ImageNet, however, does not have this issue since each image depicts a specific meaning. See section 3.2 for further details. Additionally, most of the ESP dataset is not publicly accessible. Only 60K images and their labels can be accessed [1].",A,1
ImageNet A Large_Scale Hierarchical Image Database,"LabelMe [21] and the Lotus Hill dataset [27] provide 30k and 50k labeled and segmented images, respectively. These two datasets provide complementary resources for the vision community compared to ImageNet. Both only have around 200 categories, but the outlines and locations of objects are provided. ImageNet in its current form does not provide detailed object outlines (see potential extensions in Sec. 5.1), but the number of categories and the number of images per category already far exceeds these two datasets. In addition, images in these two datasets are largely uploaded or provided by users or researchers of the dataset, whereas ImageNet contains images crawled from the entire Internet. The Lotus Hill dataset is only available through purchase.","The LabelMe dataset [21] and the Lotus Hill collection [27] offer 30,000 and 50,000 images respectively that are labeled and segmented. These two resources are complementary for computer vision experts compared to ImageNet. Both only contain about 200 categories, but they include outlines and locations of objects. ImageNet currently does not have detailed object outlines (refer to possible extensions in Section 5.1), however the quantity of categories and images per category already greatly exceeds these two datasets. Also, ImageNet has images crawled from across the internet, while the images in the other two datasets are largely provided by users or researchers involved with those datasets. The Lotus Hill dataset is only accessible through purchase.","The LabelMe [21] and Lotus Hill [27] datasets provide 30k and 50k images with labels and segmentation. These are complementary for computer vision versus ImageNet. They have ~200 categories each but include object outlines and locations, which ImageNet lacks (see Section 5.1). However, ImageNet has far more categories and images per category, with images crawled from the full internet rather than provided by dataset users/researchers. Lotus Hill requires purchase for access.  ","The LabelMe [21] and Lotus Hill [27] datasets have 30,000 and 50,000 labeled and segmented images. These are complementary for computer vision compared to ImageNet. They only have about 200 categories each, but they provide object outlines and locations, unlike ImageNet (see potential additions in Section 5.1). However, ImageNet already has far more categories and images per category, with images crawled from across the internet rather than provided by dataset users or researchers. Lotus Hill is only available through purchase.",A,1
ImageNet A Large_Scale Hierarchical Image Database,"ImageNet is an ambitious project. Thus far, we have constructed 12 subtrees containing 3.2 million images. Our goal is to complete the construction of around 50 million images in the next two years. We describe here the method we use to construct ImageNet, shedding light on how properties of Sec. 2 can be ensured in this process","ImageNet is an ambitious undertaking. So far, we have built 12 subgroups containing 3.2 million pictures. Our aim is to finish building around 50 million images in the next 24 months. We explain here the technique we utilize to build ImageNet, illuminating how attributes of Sec. 2 can be guaranteed in this procedure.","ImageNet is a bold endeavor. Up to this point, we have assembled 12 subcategories containing 3.2 million photos. Our objective is to complete the assembly of around 50 million pictures in the following two years. We portray here the strategy we use to assemble ImageNet, clarifying how qualities of Sec. 2 can be ensured in this interaction. ","ImageNet is an ambitious project. Until now, we have constructed 12 subsets containing 3.2 million visuals. Our goal is to finish constructing around 50 million visuals in the next 24 months. We elucidate here the approach we employ to construct ImageNet, illuminating how properties of Sec. 2 can be ensured in this process.",A,1
ImageNet A Large_Scale Hierarchical Image Database,"The first stage of the construction of ImageNet involves collecting candidate images for each synset. The average accuracy of image search results from the Internet is around 10% [24]. ImageNet aims to eventually offer 500-1000 clean images per synset. We therefore collect a large set of candidate images. After intra-synset duplicate removal, each synset has over 10K images on average. We collect candidate images from the Internet by querying several image search engines.","The initial phase in building ImageNet requires gathering possible pictures for each concept. Search engine image results on the internet tend to be about 10% accurate [24]. ImageNet seeks to one day provide 500-1000 pristine photos per concept. So we assemble a large collection of possible photos. After removing identical photos within a concept, each concept has over 10,000 images on average. We find possible photos on the internet by searching several image search tools.","Constructing ImageNet starts with accumulating potential images for every idea. Web image search typically gives around 10% correct results [24]. ImageNet aims to eventually have 500-1000 flawless images for each idea. We thus compile a large group of candidate images. With duplicate removal inside an idea, each idea has above 10,000 images typically. We obtain candidate images by querying multiple image search services on the web.  ","The first part of making ImageNet is collecting possible images for each meaning. Images from internet searches are about 10% right on average [24]. ImageNet wants to end up with 500-1000 perfect images for each meaning. So we gather a big collection of possible images. After taking out duplicate images within a meaning, each meaning has over 10,000 images on average. We find possible images by searching several image search engines on the internet.",A,1
ImageNet A Large_Scale Hierarchical Image Database,"For each synset, the queries are the set of WordNet synonyms. Search engines typically limit the number of images retrievable (in the order of a few hundred to a thousand). To obtain as many images as possible, we expand the query set by appending the queries with the word from parent synsets, if the same word appears in the gloss of the target synset. For example, when querying “whippet”, according to WordNet’s gloss a “small slender dog of greyhound type developed in England”, we also use “whippet dog” and “whippet greyhound”. To further enlarge and diversify the candidate pool, we translate the queries into other languages [10], including Chinese, Spanish, Dutch and Italian.","For every group of synonymous words, the search terms are the collection of WordNet synonyms. Web search tools often restrict the quantity of pictures that can be found (typically a few hundred to a thousand). To get as many images as feasible, we augment the search terms by adding the words from higher-level synsets, if that word shows up in the definition of the target synset. For instance, when looking for ""whippet"", according to WordNet's explanation a ""diminutive agile dog of greyhound type cultivated in England"", we also utilize ""whippet dog"" and ""whippet greyhound"". To further expand and diversify the pool of candidates, we translate the search terms into other languages [10], like Chinese, Spanish, Dutch and Italian.","For each set of words with the same meaning, the queries are the assortment of WordNet synonyms. Image search engines often limit how many pictures can be retrieved (usually a couple hundred to a thousand). To obtain the maximum number of images possible, we augment the query collection by attaching the words from parent synsets, if that word is present in the definition of the target synset. For example, when looking for ""whippet"", according to WordNet's description a ""small nimble dog of greyhound type developed in England"", we also use ""whippet dog"" and ""whippet greyhound"". To additionally increase and diversify the pool of candidates, we translate the queries into other languages [10], such as Chinese, Spanish, Dutch and Italian.  ","For every synset, the search terms are the array of WordNet synonyms. Photo search tools commonly restrict the amount of images findable (typically a few hundred to a thousand). To collect as many pictures as feasible, we expand the search term set by adding the words from higher level synsets, if that word is in the explanation of the target synset. For instance, when searching for ""whippet"", according to WordNet's definition a ""slight swift dog of greyhound type originated in England"", we also use ""whippet dog"" and ""whippet greyhound"". To further enlarge and diversify the group of candidates, we translate the search terms into other languages [10], including Chinese, Spanish, Dutch and Italian.",A,1
ImageNet A Large_Scale Hierarchical Image Database,"To collect a highly accurate dataset, we rely on humans to verify each candidate image collected in the previous step for a given synset. This is achieved by using the service of Amazon Mechanical Turk (AMT), an online platform on which one can put up tasks for users to complete and to get paid. AMT has been used for labeling vision data [23]. With a global user base, AMT is particularly suitable for large scale labeling. In each of our labeling tasks, we present the users with a set of candidate images and the definition of the target synset (including a link to Wikipedia). We then ask the users to verify whether each image contains objects of the synset.","In order to gather a very precise set of data, we depend on people to check each potential image gathered in the prior step for a particular concept. This is done by utilizing the Amazon Mechanical Turk (AMT) service, an online platform where one can post tasks for users to finish and receive payment. AMT has been used to tag visual information [23]. With a worldwide user base, AMT is especially appropriate for large scale tagging. In each of our labeling activities, we show the users a collection of candidate images and the description of the target concept (including a link to Wikipedia). We then request the users to confirm whether each image has objects of the concept.","To assemble a highly correct dataset, we rely on human verification of every possible image collected in the preceding step for a given idea. We accomplish this by employing Amazon Mechanical Turk (AMT), an online marketplace where tasks can be posted for users to complete in exchange for payment. AMT has been utilized for visual data annotation [23]. With a global user pool, AMT is well-suited for large-scale annotation. In each of our annotation tasks, we provide users with a set of candidate images and the definition of the target idea (with a Wikipedia link). We then ask users to confirm whether each image contains objects representing the idea.  ","In order to build an extremely accurate dataset, we depend on human checkers to validate every prospective image gathered in the prior step for a particular notion. We do this using Amazon Mechanical Turk (AMT), an online platform where people can post jobs for users to finish for compensation. AMT has been used to label visual information [23]. With a worldwide user base, AMT is especially appropriate for large-scale labeling. In each of our labeling jobs, we show checkers candidate images and the description of the target notion (including a Wikipedia link). We then request checkers to verify whether each image portrays objects representing the notion.",A,1
ImageNet A Large_Scale Hierarchical Image Database,"We encourage users to select images regardless of occlusions, number of objects and clutter in the scene to ensure diversity. While users are instructed to make accurate judgment, we need to set up a quality control system to ensure this accuracy. There are two issues to consider. First, human users make mistakes and not all users follow the instructions. Second, users do not always agree with each other, especially for more subtle or confusing synsets, typically at the deeper levels of the tree. Fig. 7(left) shows an example of how users’ judgments differ for “Burmese cat”.","We prompt users to pick images no matter if there are obstructions, quantity of items or disorder in the scene to guarantee variety. Although users are told to make precise choices, we must create a quality assurance system to ensure correctness. There are two concerns to think about. Initially, people make errors and not every user follows the guidelines. Also, users do not constantly concur with one another, particularly for more subtle or perplexing concepts, usually at the further levels of the hierarchy. Fig. 7(left) demonstrates how users' assessments differ for ""Burmese cat"".","We encourage users to choose images independent of impediments, number of articles and disarray in the shot to ensure diversity. While we advise users to make accurate evaluations, we must implement a quality control framework to guarantee precision. There are two considerations. Firstly, humans commit mistakes and not every person abides by the instructions. Secondly, individuals do not always agree with each other, especially for more subtle or confusing ideas, typically deeper in the structure. Fig. 7(left) shows how users' judgments vary for ""Burmese cat"".  ","We prompt users to select photos irrespective of barriers, quantity of objects and disorder in the scene to ensure variety. Although we direct users to make precise appraisals, we must establish a quality assurance process to guarantee accuracy. There are two factors. Initially, people err and not every person follows the guidelines. Additionally, individuals do not always concur with each other, particularly for more subtle or perplexing concepts, generally further down the hierarchy. Fig. 7(left) illustrates how users' evaluations differ for ""Burmese cat"".",A,1
ImageNet A Large_Scale Hierarchical Image Database,"The solution to these issues is to have multiple users independently label the same image. An image is considered positive only if it gets a convincing majority of the votes. We observe, however, that different categories require different levels of consensus among users. For example, while five users might be necessary for obtaining a good consensus on “Burmese cat” images, a much smaller number is needed for “cat” images. We develop a simple algorithm to dynamically determine the number of agreements needed for different categories of images. For each synset, we first randomly sample an initial subset of images.","The way to solve these problems is to get a number of different people to categorize the same photo. A photo is only seen as positive if most of the people agree that it fits in that category. However, we see that the level of agreement needed among the people depends on the specific category. For instance, while 5 people may be required to get good agreement on ""Burmese cat"" photos, many fewer are required for general ""cat"" photos. We created a simple algorithm to automatically decide the number of agreements required for each image category. For every synset, we first randomly select a sample of photos.","The solution for these issues is having multiple users independently classify the same picture. A picture is only considered positive if a convincing majority of users say it is. But we find different categories need different levels of consensus from users. For example, while five users may be needed to get good consensus on ""Burmese cat"" pictures, far fewer are required for ""cat"" pictures. We made a simple algorithm to dynamically choose the number of agreements required for each category of pictures. For every synset, we first randomly choose an initial subset of pictures.","The fix for these problems is getting multiple people to label the identical image separately. An image is seen as positive only if a persuasive majority of the people label it that way. However, we see that different categories need different amounts of agreement from the people. For instance, while five people might be necessary to get good agreement on ""Burmese cat"" images, many less are needed for ""cat"" images. We made a simple algorithm to automatically determine the number of agreements required for each image category. For every synset, we first randomly select a sample of images.",A,1
ImageNet A Large_Scale Hierarchical Image Database,"At least 10 users are asked to vote on each of these images. We then obtain a confidence score table, indicating the probability of an image being a good image given the user votes (Fig. 7(right) shows examples for “Burmese cat” and “cat”). For each of remaining candidate images in this synset, we proceed with the AMT user labeling until a pre-determined confidence score threshold is reached. It is worth noting that the confidence table gives a natural measure of the “semantic difficulty” of the synset. For some synsets, users fail to reach a majority vote for any image, indicating that the synset cannot be easily illustrated by images . Fig. 4 shows that our algorithm successfully filters the candidate images, resulting in a high percentage of clean images per synset.","At minimum 10 people are requested to cast a vote on all of these photos. We then get a confidence score table, showing the likelihood of a photo being a good photo given the user votes (Fig. 7(right) displays examples for ""Burmese cat"" and ""cat""). For every other nominee photo in this synset, we continue with the AMT user tagging until a predetermined confidence score limit is met. Notably, the confidence table provides a natural gauge of the ""semantic difficulty"" of the synset. For some synsets, users fail to achieve a majority vote for any photo, signifying that the synset can't be easily illustrated by photos. Fig. 4 demonstrates that our algorithm successfully filters the candidate photos, resulting in a high percentage of clean photos per synset.","We ask at least 10 users to provide a vote for each of these images. This gives us a confidence score table, which shows the probability that an image is good based on the user votes (Fig. 7(right) has examples for ""Burmese cat"" and ""cat""). For any other candidate images in this synset, we keep having AMT users label them until we reach a predetermined confidence score threshold. Importantly, the confidence table naturally measures the ""semantic difficulty"" of the synset. For some synsets, users can't agree on any image, meaning the synset probably can't be easily illustrated with images. Fig. 4 shows our algorithm successfully filters the candidate images, giving a high percentage of clean images per synset.","We get 10 or more people to vote on all of these pictures. This produces a confidence score table, displaying the chance a picture is good according to the user votes (Fig. 7(right) provides examples for ""Burmese cat"" and ""cat""). For any other possible pictures in this synset, we continue having AMT users label them until we meet a set confidence score limit. Notably, the confidence table naturally assesses the ""semantic difficulty"" of the synset. For some synsets, users can't agree on any picture, suggesting the synset likely can't be easily illustrated with pictures. Fig. 4 shows our algorithm successfully filters the candidate pictures, resulting in a high percentage of clean pictures per synset.",A,1
ImageNet A Large_Scale Hierarchical Image Database,"In this section, we show three applications of ImageNet. The first set of experiments underline the advantages of having clean, full resolution images. The second experiment exploits the tree structure of ImageNet, whereas the last experiment outlines a possible extension and gives more insights into the data. Given an image containing an unknown object, we would like to recognize its object class by querying similar images in ImageNet.","In this part, we demonstrate three uses of ImageNet. The first group of tests emphasize the benefits of having pristine, full resolution pictures. The second trial leverages the tree arrangement of ImageNet, while the final trial describes a potential add-on and provides more understanding into the information. If given a photo with an unfamiliar object, we would want to identify its object type by searching for comparable images in ImageNet.","Here, we exhibit three implementations of ImageNet. The initial set of analyses spotlight the advantages of possessing uncorrupted, maximum resolution graphics. The next analysis capitalizes on the hierarchical structure of ImageNet, though the concluding analysis outlines a feasible extension and imparts further discernment into the evidence. If presented with a photo possessing an unidentified article, we would aspire to recognize its object variety by interrogating analogous depictions in ImageNet. ","In this portion, we present three applications of ImageNet. The first batch of evaluations accentuate the perks of having immaculate, full detail images. The second appraisal exploits the tree layout of ImageNet, while the final appraisal summarizes a prospective add-on and provides further comprehension of the data. If given an image with an unrecognized object, we would want to identify its object type by querying comparable images in ImageNet.",A,1
ImageNet A Large_Scale Hierarchical Image Database,"Torralba et al. [24] has demonstrated that, given a large number of images, simple nearest neighbor methods can achieve reasonable performances despite a high level of noise. We show that with a clean set of full resolution images, object recognition can be more accurate, especially by exploiting more feature level information. We run four different object recognition experiments. In all experiments, we test on images from the 16 common categories 7 between Caltech256 and the mammal subtree. We measure classification performance on each category in the form of an ROC curve. For each category, the negative set consists of all images from the other 15 categories. We now describe in detail our experiments and results (Fig. 8).","Torralba and colleagues [24] showed that basic nearest neighbor techniques can perform decently on recognizing objects in images, even with lots of noise present. We demonstrate that using high quality, full resolution images allows for even better object recognition, particularly by utilizing more features from the images. We conducted four experiments testing object recognition. In all experiments, we evaluated images from 16 shared categories between Caltech256 and mammals. We assessed classification accuracy for each category using ROC curves. For every category, the negative examples were images from the other 15 categories. We now explain the experiments and results in depth (Fig. 8).","The research team of Torralba [24] proved that straightforward nearest neighbor algorithms can achieve moderate performance for object recognition in images, despite high noise levels. Our work indicates that cleaner, full resolution images enables more precise object recognition, especially through leveraging more feature-level data. We performed four object recognition experiments. In all experiments, we evaluated on images from 16 mutual categories between Caltech256 and mammals. We measured classification performance per category using ROC curves. For each category, the negative images were from the other 15 categories. We will now discuss the experiments and results thoroughly (Fig. 8).  ","Torralba and co-authors [24] demonstrated that even with substantial noise, basic nearest neighbor techniques can perform reasonably well at recognizing objects in images. Our research shows that using pristine, full resolution images allows even better object recognition performance, particularly by making use of more feature information. We conducted four object recognition experiments. In all experiments, we tested on images from 16 overlapping categories between Caltech256 and mammals. We assessed classification accuracy for each category via ROC curves. For every category, the negative set contained images from the other 15 categories. We will now explain the experiments and results in detail (Fig. 8).",A,1
ImageNet A Large_Scale Hierarchical Image Database,"First we replicate one of the experiments described in [24], which we refer to as “NN-voting” hereafter. To imitate the TinyImage dataset (i.e. images collected from search engines without human cleaning), we use the original candidate images for each synset (Section 3.1) and downsample them to 32 × 32. Given a query image, we retrieve 100 of the nearest neighbor images by SSD pixel distance from the mammal subtree. Then we perform classification by aggregating votes (number of nearest neighbors) inside the tree of the target category.","Initially, we reproduce one of the tests outlined in [24], which we will call ""NN-voting"" moving forward. To mimic the TinyImage dataset (meaning images gathered from search engines without human curation), we utilize the primary candidate photos for each synset (Section 3.1) and scale them down to 32 × 32. When given a query image, we obtain 100 of the most similar neighbor images by SSD pixel distance from the mammal subtree. We then conduct classification by totaling up votes (quantity of nearest neighbors) inside the tree of the intended category.","To start, we duplicate one of the experiments presented in [24], which we refer to as ""NN-voting"" from here on out. To simulate the TinyImage dataset (i.e., images found on search engines without human editing), we employ the original candidate images for each synset (Section 3.1) and reduce them to 32 × 32. Provided a query image, we recover 100 of the closest matching neighbor images by SSD pixel distance from the mammal subtree. We then execute classification by accumulating votes (number of closest neighbors) within the tree of the target category. ","As a beginning step, we reproduce one of the trials described in [24], which we will call ""NN-voting"" moving forward. To imitate the TinyImage dataset (meaning images sourced from search engines without human refinement), we use the primary candidate photos for each synset (Section 3.1) and shrink them to 32 × 32. When fed a query image, we obtain 100 of the most similar neighbor images by SSD pixel distance from the mammal subtree. We then conduct classification by tallying up votes (count of nearest neighbors) inside the tree of the intended category.",A,1
ImageNet A Large_Scale Hierarchical Image Database,"Compared to other available datasets, ImageNet provides image data in a densely populated hierarchical structure. Many possible algorithms could be applied to exploit a hierarchical data structure (e.g. [16, 17, 28, 18]). In this experiment, we choose to illustrate the usefulness of the ImageNet hierarchy by a simple object classification method which we call the “tree-max classifier”. Imagine you have a classifier at each synset node of the tree and you want to decide whether an image contains an object of that synset or not.","In contrast to other existing image datasets, ImageNet gives image information organized in a very detailed hierarchical way. There are many potential algorithms that could make use of this hierarchical organization of data (for instance [16, 17, 28, 18]). For this test, we decided to show the value of the ImageNet hierarchy using a straightforward object classification approach we refer to as the ""tree-max classifier"". Picture having a classifier at every node in the tree to determine if an image has an object of that node or not.","Compared with other available image datasets, ImageNet provides image data structured in a very dense hierarchical way. Numerous possible algorithms could leverage this hierarchical data structure (such as [16, 17, 28, 18]). For this experiment, we opted to demonstrate the usefulness of the ImageNet hierarchy through a simple object classification technique we call the ""tree-max classifier"". Envision having a classifier at each node in the hierarchy to classify if an image contains an object represented by that node. ","In comparison to other existing image datasets, ImageNet gives image data organized in a very populated hierarchical structure. There are many conceivable algorithms that could utilize this hierarchical data organization (for example [16, 17, 28, 18]). For this experiment, we chose to showcase the value of the ImageNet hierarchy by using a straightforward object classification approach we refer to as the ""tree-max classifier"". Picture having a classifier at every node in the hierarchy to determine if an image has an object represented by that node.",A,1
ImageNet A Large_Scale Hierarchical Image Database,"The idea is to not only consider the classification score at a node such as “dog”, but also of its child synsets, such as “German shepherd”, “English terrier”, etc. The maximum of all the classifier responses in this subtree becomes the classification score of the query image. Fig. 9 illustrates the result of our experiment on the mammal subtree. Note that our algorithm is agnostic to any method used to learn image classifiers for each synset. In this case, we use an AdaBoost-based classifier proposed by [6].","The concept is to take into account not just the categorization result at a node like ""dog"", but also of its more specific descendant concepts, such as ""German shepherd"", ""English terrier"", and so on. The highest of all the classifier outputs in this sub-hierarchy becomes the categorization result of the query image. Fig. 9 shows the consequence of our test on the mammal sub-hierarchy. Note that our algorithm does not depend on any specific technique used to train image classifiers for each concept. In this situation, we utilize an AdaBoost-based classifier proposed by [6].","The plan is to examine not solely the classification mark at a node such as ""dog"", but also of its more detailed child ideas, like ""German shepherd"", ""English terrier"", and so forth. The maximum of all the classifier reactions in this sub-tree becomes the classification mark of the query image. Fig. 9 illustrates the outcome of our experiment on the mammal sub-tree. Note that our algorithm does not rely on any particular method used to learn image classifiers for each idea. In this case, we use an AdaBoost-based classifier proposed by [6].  ","The intention is to evaluate not just the categorization score at a node such as ""dog"", but also of its more granular descendant notions, like ""German shepherd"", ""English terrier"", and so on. The highest of all the classifier outputs in this sub-tree becomes the categorization score of the query image. Fig. 9 shows the result of our trial on the mammal sub-tree. Note that our algorithm does not depend on any specific technique used to develop image classifiers for each notion. In this instance, we utilize an AdaBoost-based classifier proposed by [6].",A,1
ImageNet A Large_Scale Hierarchical Image Database,"For each synset, we randomly sample 90% of the images to form the positive training image set, leaving the rest of the 10% as testing images. We form a common negative image set by aggregating 10 images randomly sampled from each synset. When training an image classifier for a particular synset, we use the positive set from this synset as well as the common negative image set excluding the images drawn from this synset, and its child and parent synsets. We evaluate the classification results by AUC (the area under ROC curve). Fig. 9 shows the results of AUC for synsets at different levels of the hierarchy, compared with an independent classifier that does not exploit the tree structure of ImageNet.","For every group of synonyms, we arbitrarily choose 90% of the photos to create the positive training image collection, leaving the other 10% as testing photos. We form a shared negative image collection by bringing together 10 randomly selected photos from each group of synonyms. When developing an image classifier for a specific group of synonyms, we utilize the positive collection from this group as well as the shared negative image collection excluding the photos drawn from this group, and its subordinate and parent groups. We assess the classification outcomes by AUC (the area under ROC curve). Fig. 9 displays the outcomes of AUC for groups of synonyms at various levels of the hierarchy, compared with an independent classifier that does not take advantage of the tree design of ImageNet.","For all synonym sets, we randomly pick 90% of the images to make up the affirmative training image set, keeping the remaining 10% as testing images. We assemble a common negative image set by aggregating 10 arbitrarily chosen images from each synonym set. When constructing an image classifier for a particular synonym set, we employ the affirmative set from this synonym set and the common negative image set leaving out the images taken from this synonym set, and its child and parent synonym sets. We evaluate the classification results by AUC (the area under ROC curve). Fig. 9 exhibits the outcomes of AUC for synonym sets at different tiers of the hierarchy, compared with an independent classifier that does not utilize the tree architecture of ImageNet.  ","For every collection of synonymous words, we haphazardly take 90% of the pictures to form the positive training image collection, retaining the other 10% as testing pictures. We assemble a shared negative image collection by bringing together 10 randomly picked pictures from each collection of synonymous words. When developing an image classifier for a specific collection of synonymous words, we use the positive collection from this collection of synonymous words and the shared negative image collection excluding the pictures taken from this collection of synonymous words, and its subordinate and parent collections of synonymous words. We appraise the classification results by AUC (the area under ROC curve). Fig. 9 displays the outcomes of AUC for collections of synonymous words at various levels of the hierarchy, compared with an independent classifier that does not exploit the tree design of ImageNet.",A,1
ImageNet A Large_Scale Hierarchical Image Database,"The plot indicates that images are easier to classify at the bottom of the tree (e.g. star-nosed mole, minivan, polar bear) as opposed to the top of the tree (e.g. vehicles, mammal, artifact, etc.). This is most likely due to stronger visual coherence near the leaf nodes of the tree. At nearly all levels, the performance of the tree-max classifier is consistently higher than the independent classifier. This result shows that a simple way of exploiting the ImageNet hierarchy can already provide substantial improvement for the image classification task without additional training or model learning.","The graph shows that pictures are more simply categorized towards the base of the tree structure (for instance, star-nosed mole, minivan, polar bear) rather than the top of the tree structure (for example, vehicles, mammal, artifact, and so on). This is probably because of stronger visual unity near the leaf nodes of the tree. At nearly all levels, the effectiveness of the tree-max classifier is steadily higher than the independent classifier. This outcome displays that a straightforward way of leveraging the ImageNet hierarchy can already give considerable enhancement for the image classification task without extra training or model learning.","The data indicates that images can be more easily labeled at the bottom of the hierarchy (like star-nosed mole, minivan, polar bear) compared to the top of the hierarchy (such as vehicles, mammal, artifact, and so on). This is most likely attributable to stronger visual cohesion near the terminal nodes of the tree. At nearly all tiers, the performance of the tree-max classifier is consistently superior to the independent classifier. This finding shows that a simple method of exploiting the ImageNet hierarchy can already provide substantial improvement for the image classification task without additional training or model development.","The plot shows that images can be more simply identified at the base of the structure (for example, star-nosed mole, minivan, polar bear) versus the top of the structure (like vehicles, mammal, artifact, and so on). This is probably owing to stronger visual unity near the end nodes of the tree. At nearly all stages, the effectiveness of the tree-max classifier is steadily better than the independent classifier. This result indicates that a straightforward way of leveraging the ImageNet hierarchy can already furnish considerable enhancement for the image classification task without supplementary training or model learning.",A,1
ImageNet A Large_Scale Hierarchical Image Database,"ImageNet can be extended to provide additional information about each image. One such information is the spatial extent of the objects in each image. Two application areas come to mind. First, for training a robust object detection algorithm one often needs localized objects in different poses and under different viewpoints. Second, having localized objects in cluttered scenes enables users to use ImageNet as a benchmark dataset for object localization algorithms.","The ImageNet dataset could be expanded to include more details about each of the images. For instance, the locations of objects within each image could be added. This extra information would be useful for two main purposes. First, it would help in developing better object detection algorithms by providing examples of objects shown in different positions and angles. Second, it would allow ImageNet to be used as a benchmark dataset for evaluating object localization techniques in cluttered images.",The ImageNet image database has the potential to be enhanced with additional annotations for each picture. One useful annotation would be marking the spatial boundaries around objects in the images. Providing this object localization data would have two valuable applications. One is that it would assist in training more robust object detection systems that can identify objects posed at varying orientations and viewpoints. Another is that it would enable benchmarking object localization algorithms on ImageNet's images containing many objects.,"ImageNet could be expanded by adding more metadata for each image in the dataset. An example is delineating the location and extent of objects within each image. This extra information would have two main uses. First, it would facilitate developing object detection models that are more resilient to different object poses and camera angles, since examples would be provided. Second, it would allow ImageNet to benchmark performance of object localization techniques on cluttered images with many objects.",A,1
ImageNet A Large_Scale Hierarchical Image Database,"Finally, one bounding box is put around the region which accumulates the highest likelihood. We annotated 100 images in 22 different categories of the mammal and vehicle subtrees with bounding boxes around the objects of that category. Fig. 10 shows precision and recall values. Note that precision is low due to extreme variability of the objects and because of small objects which have hardly any salient regions. Fig. 11 shows sampled bounding boxes on different classes.","In conclusion, a single bounding box is placed surrounding the area with the highest probability. We labeled 100 images in 22 different groups of the mammal and vehicle subcategories with bounding boxes around the objects of that group. Fig. 10 displays the precision and recall values. Note that precision is low because of the extreme variability of the objects and because of small objects which have barely any salient areas. Fig. 11 displays example bounding boxes on different classes.","To summarize, one bounding region is positioned around the zone with the maximum likelihood. We marked 100 photos in 22 distinct subsets of the mammal and vehicle subclasses with bounding regions around the objects of that subset. Fig. 10 exhibits the precision and recall figures. Observe that precision is low owing to extreme changeability of the objects and because of tiny objects which have hardly any prominent zones. Fig. 11 exhibits sampled bounding regions on various classes.  ","In closing, a single bounding area is situated around the space accumulating the top probability. We annotated 100 pictures in 22 varying categories within the mammal and vehicle subcategories with bounding areas surrounding the objects of that category. Fig. 10 shows the precision and recall values. Take note that precision is low because of the extreme variability of the objects and the tiny objects with barely any significant regions. Fig. 11 displays example bounding areas on different classes.",A,1
ImageNet Classification with Deep Convolutional Neural Networks,"We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax.","We used a large, deep neural network with many layers to categorize the 1.2 million high-resolution photos in the ImageNet LSVRC-2010 competition into 1000 different groups. On the test information, we got top-1 and top-5 mistake percentages of 37.5% and 17.0% which is much better than the previous best. The neural network, which has 60 million settings and 650,000 nerve cells, is made up of five layers that apply convolutions, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax.","We educated a big, deep convolutional neural network with 60 million parameters and 650,000 neurons to sort the 1.2 million high-def images in the ImageNet LSVRC-2010 challenge into 1000 distinct types. On the evaluation data, we achieved first and fifth error rates of 37.5% and 17.0% which substantially surpasses the previous best. The network consists of five convolutional layers, some followed by max-pooling layers, and three fully-connected layers ending in a 1000-way softmax.  ","We built a large, deep convolutional neural network with 60 million adjustable parameters and 650,000 nodes to categorize the 1.2 million high-resolution pictures in the ImageNet LSVRC-2010 competition into 1000 classes. On the test set, we obtained top-1 and top-5 error percentages of 37.5% and 17.0%, considerably exceeding prior results. The network has five convolutional layers, some followed by max-pooling layers, three fully-connected layers, and a final 1000-way softmax classifier.",A,1
ImageNet Classification with Deep Convolutional Neural Networks,"To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called “dropout” that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.","We accelerated training by using neurons that don't saturate and a GPU that implements convolution very efficiently. To decrease overfitting in the fully-connected layers, we used a new regularization technique called ""dropout"" that was highly effective. We also entered a modified version of this model in the ILSVRC-2012 contest and obtained a top-5 test error rate of 15.3%, compared to 26.2% for the second place entry.","For faster training, non-saturating neurons and a GPU with a very efficient convolution operation implementation were utilized. To reduce overfitting in the fully-connected layers, we applied a recently invented regularization approach named ""dropout"" which proved very useful. We also submitted a variant of this model to the ILSVRC-2012 competition, achieving a top-5 test error rate of 15.3%, versus 26.2% for the next best submission.  ","To accelerate training, we employed non-saturating neurons along with a GPU with a highly optimized convolution operation. To decrease overfitting in the fully-connected layers, we used a new regularization method called ""dropout"" which was highly beneficial. We also entered a modified form of this model in the ILSVRC-2012 contest, attaining a top-5 test error rate of 15.3%, compared to 26.2% for the second best performer.",A,1
ImageNet Classification with Deep Convolutional Neural Networks,"Current approaches to object recognition make essential use of machine learning methods. To improve their performance, we can collect larger datasets, learn more powerful models, and use better techniques for preventing overfitting. Until recently, datasets of labeled images were relatively small — on the order of tens of thousands of images (e.g., NORB [16], Caltech-101/256 [8, 9], and CIFAR-10/100 [12]). Simple recognition tasks can be solved quite well with datasets of this size, especially if they are augmented with label-preserving transformations. For example, the current best error rate on the MNIST digit-recognition task (<0.3%) approaches human performance [4].","The way objects are currently recognized relies heavily on machine learning techniques. To make these techniques better, larger datasets can be collected, more powerful models can be learned, and better ways to prevent overfitting can be used. Until recently, datasets containing labeled images were fairly small - tens of thousands of images (like NORB, Caltech-101/256, and CIFAR-10/100). Basic recognition tasks can be handled well with datasets this size, particularly if label-preserving transformations are added. For instance, the lowest error rate on the MNIST digit recognition task (<0.3%) is close to human performance.","Modern object recognition utilizes machine learning extensively. Performance can be improved by gathering more data, developing stronger models, and using techniques to reduce overfitting. In the past, labeled image datasets were limited - only tens of thousands of images (NORB, Caltech-101/256, CIFAR-10/100). Even with small datasets, simple recognition tasks can be accomplished well, especially when label-preserving changes are incorporated. The current best error rate for MNIST digit recognition (<0.3%) is nearing human-level performance.  ","Current object recognition relies heavily on machine learning. Larger datasets, more powerful models, and better overfitting prevention can enhance performance. Until recently, labeled image datasets were small - tens of thousands of images (like NORB, Caltech-101/256, CIFAR-10/100). Even so, basic recognition tasks can be handled well, particularly when label-preserving transformations are used. For example, the lowest error rate for MNIST digit recognition (<0.3%) is approaching human performance.",A,1
ImageNet Classification with Deep Convolutional Neural Networks,"But objects in realistic settings exhibit considerable variability, so to learn to recognize them it is necessary to use much larger training sets. And indeed, the shortcomings of small image datasets have been widely recognized (e.g., Pinto et al. [21]), but it has only recently become possible to collect labeled datasets with millions of images. The new larger datasets include LabelMe [23], which consists of hundreds of thousands of fully-segmented images, and ImageNet [6], which consists of over 15 million labeled high-resolution images in over 22,000 categories.","However, objects in real-world environments display a lot of differences, so you need to utilize much larger sets of training images to learn to identify them correctly. Furthermore, the problems with small collections of images have been commonly acknowledged (for example by Pinto et al. [21]), but we have only recently been capable of gathering labeled datasets with millions of photos. These new, larger datasets consist of LabelMe [23], containing hundreds of thousands of completely segmented pictures, and ImageNet [6], containing over 15 million high-resolution labeled images in over 22,000 types.","Nevertheless, objects in authentic settings exhibit substantial variability, so significantly bigger training image sets are essential to learn to recognize them accurately. Additionally, the deficiencies of small image collections have been extensively recognized (see Pinto et al. [21]), but only lately has it become feasible to assemble labeled datasets with millions of photos. These new, larger datasets encompass LabelMe [23], with hundreds of thousands of fully delineated pictures, and ImageNet [6], with over 15 million high-resolution categorized images in more than 22,000 categories.  ","However, objects in life-like environments display considerable differences, so much larger training image sets are needed to learn to identify them properly. Furthermore, the shortfalls of small image collections have been widely acknowledged (for instance by Pinto et al. [21]), but only recently has it become possible to compile labeled datasets with millions of photographs. These new, bigger datasets include LabelMe [23], consisting of hundreds of thousands of completely outlined images, and ImageNet [6], consisting of over 15 million high-resolution classified photographs in over 22,000 types.",A,1
ImageNet Classification with Deep Convolutional Neural Networks,"To learn about thousands of objects from millions of images, we need a model with a large learning capacity. However, the immense complexity of the object recognition task means that this problem cannot be specified even by a dataset as large as ImageNet, so our model should also have lots of prior knowledge to compensate for all the data we don’t have. Convolutional neural networks (CNNs) constitute one such class of models [16, 11, 13, 18, 15, 22, 26]. Their capacity can be controlled by varying their depth and breadth, and they also make strong and mostly correct assumptions about the nature of images (namely, stationarity of statistics and locality of pixel dependencies).","To become knowledgeable about countless objects from an enormous number of images, our model needs to have a substantial ability to learn. However, the extremely intricate nature of recognizing objects means that even a massive dataset like ImageNet cannot fully define this task. Thus, our model should also contain extensive prior understanding to make up for all the data we lack. Convolutional neural networks (CNNs) represent one such type of model [16, 11, 13, 18, 15, 22, 26]. Their learning ability can be adjusted by changing their depth and width, and they also make robust and largely accurate assumptions about the essence of images (namely, consistency of statistics and locality of pixel relationships).","To gain familiarity with a great many objects from millions upon millions of images, our system requires expansive learning potential. However, the tremendously complex essence of object identification means that not even a gargantuan dataset such as ImageNet can fully delineate this challenge. Therefore, our system should also hold abundant previous knowledge to compensate for all the data we do not possess. Convolutional neural networks (CNNs) constitute one such class of systems [16, 11, 13, 18, 15, 22, 26]. Their learning potential can be controlled by modifying their depth and breadth, and they also make sturdy, generally correct assumptions regarding the nature of images (namely, uniformity of statistics and locality of pixel connections).  ","To become well-versed about uncountable objects from countless images, our model necessitates far-reaching learning capacity. However, the exceptionally intricate character of object recognition signifies that even a massive dataset such as ImageNet cannot completely characterize this task. Accordingly, our model should also contain extensive prior comprehension to offset all the data we lack. Convolutional neural networks (CNNs) represent one such model type [16, 11, 13, 18, 15, 22, 26]. Their learning capacity is adjustable by varying their depth and width, and they also make robust, largely precise assumptions regarding the essence of images (namely, consistency of statistics and locality of pixel linkages).",A,1
ImageNet Classification with Deep Convolutional Neural Networks,"We wrote a highly-optimized GPU implementation of 2D convolution and all the other operations inherent in training convolutional neural networks, which we make available publicly . Our network contains a number of new and unusual features which improve its performance and reduce its training time, which are detailed in Section 3. The size of our network made overfitting a significant problem, even with 1.2 million labeled training examples, so we used several effective techniques for preventing overfitting, which are described in Section 4.","We created and openly shared an extremely efficient graphics processing unit code for two-dimensional convolution and other procedures used when teaching convolutional neural networks. Our network has several new and special aspects that enhance its capabilities and minimize the time needed for training, which we explain in Section 3. The large scale of our network made overfitting a major issue, even with 1.2 million labeled examples for training, so we utilized multiple effective techniques to prevent overfitting, detailed in Section 4.","We developed and publicly released a highly-optimized implementation on graphics cards of two-dimensional convolution and other key operations used in convolutional neural network training. Our network incorporates various novel and atypical features that boost its performance and accelerate its training, described further in Section 3. Due to the size of our network, overfitting was a substantial challenge, despite having 1.2 million annotated training samples, thus we employed multiple successful methods to avoid overfitting, outlined in Section 4.  ","We engineered and openly provided a very efficient graphics processing unit version of two-dimensional convolution and other important procedures involved in training convolutional neural networks. Our network has a number of new and unusual attributes that enhance its capabilities and reduce the time required for training, explained in more detail in Section 3. Because of the large scale of our network, overfitting was a major difficulty, even with access to 1.2 million labeled training examples, so we made use of several effective techniques to prevent overfitting, summarized in Section 4.",A,1
ImageNet Classification with Deep Convolutional Neural Networks,"Despite the attractive qualities of CNNs, and despite the relative efficiency of their local architecture, they have still been prohibitively expensive to apply in large scale to high-resolution images. Luckily, current GPUs, paired with a highly-optimized implementation of 2D convolution, are powerful enough to facilitate the training of interestingly-large CNNs, and recent datasets such as ImageNet contain enough labeled examples to train such models without severe overfitting. The specific contributions of this paper are as follows: we trained one of the largest convolutional neural networks to date on the subsets of ImageNet used in the ILSVRC-2010 and ILSVRC-2012 competitions [2] and achieved by far the best results ever reported on these datasets.","Even with the appealing features of CNNs and the comparative efficiency of their local design, they have still been too costly to use extensively on high-resolution images. Thankfully, modern GPUs, together with a highly-optimized implementation of 2D convolution, are strong enough to enable the training of impressively-large CNNs, and recent datasets like ImageNet have sufficient labeled examples to train such models without extreme overfitting. The particular contributions of this paper are: we trained one of the biggest convolutional neural networks so far on the subsets of ImageNet utilized in the ILSVRC-2010 and ILSVRC-2012 competitions [2] and accomplished by far the best results ever documented on these datasets.","Despite the attractive attributes of CNNs, and notwithstanding the relative efficiency of their local architecture, they have still been prohibitively expensive to apply broadly to high-resolution images. Fortunately, current GPUs, paired with a highly-optimized implementation of 2D convolution, are powerful enough to facilitate the training of interestingly-large CNNs, and recent datasets such as ImageNet contain sufficient labeled examples to train such models without severe overfitting. The specific contributions of this paper are the following: we trained one of the largest convolutional neural networks up to now on the subsets of ImageNet used in the ILSVRC-2010 and ILSVRC-2012 competitions [2] and achieved by far the best results ever reported on these datasets.","Even considering the appealing qualities of CNNs, and considering the relative efficiency of their local architecture, they have still been too costly to apply extensively to high-resolution images. Thankfully, modern GPUs, together with a highly-optimized implementation of 2D convolution, are strong enough to make possible the training of impressively-large CNNs, and recent datasets like ImageNet have enough labeled examples to train such models without extreme overfitting. The particular contributions of this paper are: we trained one of the biggest convolutional neural networks so far on the subsets of ImageNet used in the ILSVRC-2010 and ILSVRC-2012 competitions [2] and accomplished the best results ever documented on these datasets.",A,1
ImageNet Classification with Deep Convolutional Neural Networks,"Our final network contains five convolutional and three fully-connected layers, and this depth seems to be important: we found that removing any convolutional layer (each of which contains no more than 1% of the model’s parameters) resulted in inferior performance. In the end, the network’s size is limited mainly by the amount of memory available on current GPUs and by the amount of training time that we are willing to tolerate. Our network takes between five and six days to train on two GTX 580 3GB GPUs. All of our experiments suggest that our results can be improved simply by waiting for faster GPUs and bigger datasets to become available.",The neural network architecture we settled on has 5 convolutional layers and 3 fully connected layers. This depth appears crucial - taking out any of the convolutional layers (which each have only 1% or less of the total parameters) hurt performance. The network's scale is constrained by the memory on modern GPUs and how long we're willing to train for. It takes 5-6 days to train our network on a pair of GTX 580 3GB GPUs. Our experiments imply we could further improve results by using more powerful GPUs and larger datasets when they become accessible.,Our final neural network contains 5 convolutional layers and 3 fully connected layers. This depth is important - removing any convolutional layer (which have only 1% or less of the total parameters) resulted in worse performance. The network's size is limited by the memory available on current GPUs and the amount of training time we find acceptable. It takes our network 5-6 days to train using 2 GTX 580 3GB GPUs. All our experiments show we could get better results by using faster GPUs and larger datasets when they become available.  ,The neural network we ended up with has 5 convolutional layers and 3 fully connected layers. This depth seems crucial - taking out any of the convolutional layers (each of which has no more than 1% of the total parameters) led to worse performance. The network's size is constrained by the memory on today's GPUs and the training time we find tolerable. Our network takes 5-6 days to train using 2 GTX 580 3GB GPUs. All our experiments indicate we could improve the results by using more powerful GPUs and bigger datasets when they become available.,A,1
ImageNet Classification with Deep Convolutional Neural Networks,"ImageNet is a dataset of over 15 million labeled high-resolution images belonging to roughly 22,000 categories. The images were collected from the web and labeled by human labelers using Amazon’s Mechanical Turk crowd-sourcing tool. Starting in 2010, as part of the Pascal Visual Object Challenge, an annual competition called the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC) has been held. ILSVRC uses a subset of ImageNet with roughly 1000 images in each of 1000 categories. In all, there are roughly 1.2 million training images, 50,000 validation images, and 150,000 testing images. ILSVRC-2010 is the only version of ILSVRC for which the test set labels are available, so this is the version on which we performed most of our experiments.","ImageNet is a large dataset containing over 15 million high-resolution images that are categorized into around 22,000 different classes. The images were found on the internet and labeled by human workers using Amazon's Mechanical Turk online platform. Beginning in 2010, an annual competition called the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC) has been held as part of the Pascal Visual Object Challenge. ILSVRC uses a subset of ImageNet with about 1000 images in each of 1000 categories. Overall, there are about 1.2 million training images, 50,000 validation images, and 150,000 test images. ILSVRC-2010 is the only version of ILSVRC where the test set labels are available, so this is the version we used for most of our experiments.","ImageNet is a dataset with over 15 million high-quality images split into roughly 22,000 different categories. The images were sourced from the internet and classified by human labelers utilizing Amazon's Mechanical Turk crowdsourcing service. Starting in 2010, a yearly competition called the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC) has been held as part of the Pascal Visual Object Challenge. ILSVRC uses a subset of ImageNet with around 1000 images in each of 1000 categories. There are about 1.2 million training images, 50,000 validation images, and 150,000 test images total. ILSVRC-2010 is the only ILSVRC version where the test set labels are accessible, so we performed most experiments on this version.  ","ImageNet is a dataset containing more than 15 million high-resolution images organized into approximately 22,000 classes. The images were found online and labeled by human workers using Amazon's Mechanical Turk crowdworking platform. Since 2010, an annual competition called the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC) has been held as part of the Pascal Visual Object Challenge. ILSVRC uses a subset of ImageNet with around 1000 images in each of 1000 categories. There are roughly 1.2 million training images, 50,000 validation images, and 150,000 test images overall. ILSVRC-2010 is the sole ILSVRC version with available test set labels, hence we conducted most experiments on this version.",A,1
ImageNet Classification with Deep Convolutional Neural Networks,"Since we also entered our model in the ILSVRC-2012 competition, in Section 6 we report our results on this version of the dataset as well, for which test set labels are unavailable. On ImageNet, it is customary to report two error rates: top-1 and top-5, where the top-5 error rate is the fraction of test images for which the correct label is not among the five labels considered most probable by the model. ImageNet consists of variable-resolution images, while our system requires a constant input dimensionality.","Given that our model was also used in the ILSVRC-2012 contest, we present our findings on this variant of the data in Section 6 too, even though the test set labels are not public. For ImageNet, two mistake percentages are usually given: top-1 and top-5, where top-5 is the amount of test photos for which the right tag is not one of the five labels the model thinks are most probable. ImageNet has images of differing resolutions, whereas our system needs a steady input size.","Since our model participated in the ILSVRC-2012 competition as well, we also share our results on this form of the data in Section 6, despite test set labels being private. For ImageNet, people tend to provide two error percentages: top-1 and top-5, with top-5 being the proportion of test images where the accurate label is not in the top five labels the model deems most likely. ImageNet has images of varying sizes, but our system requires a fixed input dimension.","Considering our model was entered in the ILSVRC-2012 contest too, we present our findings on this variant of the information in Section 6 also, even with test set tags being confidential. For ImageNet, two mistake rates are commonly given: top-1 and top-5, where top-5 is the percentage of test pictures where the correct tag is not amongst the top five labels the model thinks are most probable. ImageNet contains images of different resolutions, whereas our system needs a consistent input size.",A,1
ImageNet Classification with Deep Convolutional Neural Networks,"Therefore, we down-sampled the images to a fixed resolution of 256 × 256. Given a rectangular image, we first rescaled the image such that the shorter side was of length 256, and then cropped out the central 256×256 patch from the resulting image. We did not pre-process the images in any other way, except for subtracting the mean activity over the training set from each pixel. So we trained our network on the (centered) raw RGB values of the pixels.","As a result, we decreased the resolution of the images to a consistent size of 256 × 256 pixels. With a rectangular image, we first resized the image so that the shorter side was 256 pixels long, and then extracted the central 256×256 pixel area from the resized image. We did not preprocess the images at all, other than subtracting the average pixel value across the training set from each pixel. Therefore, we trained our neural network on the (centered) raw RGB pixel values.","Consequently, we reduced the resolution of the images down to 256 × 256 pixels. For a rectangular image, we first adjusted the size so the shorter edge was 256 pixels, then cropped out the middle 256×256 patch from the adjusted image. We did not alter the images in any way, besides removing the mean pixel value over the training images from each pixel. Thus, we trained our network using the (centered) original RGB pixel values.  ","As a result, we lowered the resolution of the images to 256 × 256 pixels. Given a rectangular image, we first resized it so the shorter side was 256 pixels long, then cut out the 256×256 pixel section from the center of the resized image. We did not change the images at all, other than subtracting the average pixel value from the training set from each pixel. Therefore, we trained our network on the (centered) raw RGB pixel values.",A,1
ImageNet Classification with Deep Convolutional Neural Networks,"A single GTX 580 GPU has only 3GB of memory, which limits the maximum size of the networks that can be trained on it. It turns out that 1.2 million training examples are enough to train networks which are too big to fit on one GPU. Therefore we spread the net across two GPUs. Current GPUs are particularly well-suited to cross-GPU parallelization, as they are able to read from and write to one another’s memory directly, without going through host machine memory.","A solitary GTX 580 graphics processing unit contains just 3 gigabytes of storage, restricting the greatest dimensions of neural networks trainable on it. As it happens, 1.2 million training instances suffice for educating networks that are too large to reside in a single GPU. Hence we distribute the network over two GPUs. Modern GPUs are especially appropriate for parallelization across GPUs, since they can directly access each other's memory without traversing the host computer's memory.","A single GTX 580 GPU has only 3 GB of remembrance, bounding the maximal enormity of the neural nets that are able to be learned on it. It emerges that 1.2 million exemplars for practice are adequate for developing networks that are too capacious to be contained in one GPU. Consequently we disperse the net over two GPUs. Current GPUs are peculiarly well-suited to cross-GPU parallelization, as they can read and inscribe each other's storage directly, sans going through host computer memory.  ","A lone GTX 580 graphics card has just 3 gigabytes of storage space, which constrains the maximum magnitude of neural networks trainible on it. It turns out 1.2 million training samples are sufficient to educate networks that are too huge to fit within one graphics card. Therefore we spread the network across two cards. Modern graphics cards are particularly appropriate for parallelization across cards, since they can directly access each other's memory without traversing the host computer's memory.",A,1
ImageNet Classification with Deep Convolutional Neural Networks,"The parallelization scheme that we employ essentially puts half of the kernels (or neurons) on each GPU, with one additional trick: the GPUs communicate only in certain layers. This means that, for example, the kernels of layer 3 take input from all kernel maps in layer 2. However, kernels in layer 4 take input only from those kernel maps in layer 3 which reside on the same GPU. Choosing the pattern of connectivity is a problem for cross-validation, but this allows us to precisely tune the amount of communication until it is an acceptable fraction of the amount of computation.","The method of parallelization we use splits the kernels (or neurons) evenly between the two GPUs. We also utilize one additional technique: the GPUs only exchange information during certain layers. This means the kernels in layer 3 get inputs from all kernel maps in layer 2, while kernels in layer 4 only get inputs from kernel maps in layer 3 on the same GPU. Picking which layers connect is an issue for cross-validation, but it lets us finely adjust the communication to computation ratio.","Our parallelization approach divides the kernels (or neurons) in half, assigning each half to one of the two GPUs. We also employ one extra trick: the GPUs only communicate with each other during specific layers. So layer 3 kernels take inputs from all layer 2 kernel maps, but layer 4 kernels only take inputs from layer 3 maps on their GPU. Determining the connectivity pattern requires cross-validation, however this enables precise tuning of the communication to computation percentage.  ","The parallelization system we use allocates half the kernels (or neurons) to each of the two GPUs. Additionally, we utilize one other technique: the GPUs only exchange data during particular layers. For instance, layer 3 kernels receive inputs from all layer 2 kernel maps, while layer 4 kernels only get inputs from layer 3 maps on their same GPU. Selecting which layers connect necessitates cross-validation, but it allows precise adjustment of the communication to computation ratio.",A,1
ImageNet Classification with Deep Convolutional Neural Networks,"The resultant architecture is somewhat similar to that of the “columnar” CNN employed by Ciresan et al. [5], except that our columns are not independent (see Figure 2). This scheme reduces our top-1 and top-5 error rates by 1.7% and 1.2%, respectively, as compared with a net with half as many kernels in each convolutional layer trained on one GPU. The two-GPU net takes slightly less time to train than the one-GPU net .","The final design of the architecture is fairly comparable to the ""column-structured"" CNN used by Ciresan and colleagues [5], however our columns are interdependent (refer to Figure 2). This structure decreases our top-1 and top-5 error percentages by 1.7% and 1.2%, respectively, contrasted with a network with half as many kernels in each convolutional layer trained on a single GPU. The two-GPU network takes a bit less time to train versus the one-GPU network.","The resulting architecture bears some semblance to the ""columnar"" CNN employed by Ciresan's group [5], except our columns are not separate (see Figure 2). This configuration lowers our top-1 and top-5 error rates by 1.7% and 1.2%, in comparison to a net with half as many kernels in all convolutional tiers trained on one GPU. The two-GPU net requires slightly less training time than the one-GPU net.  ","The final architecture is quite similar to the ""column-based"" CNN used by Ciresan and team [5], however our columns are interconnected (refer to Figure 2). This setup reduces our top-1 and top-5 error percentages by 1.7% and 1.2%, respectively, compared to a network with half as many kernels in every convolutional layer trained on a single GPU. The two-GPU network takes a bit less training time versus the one-GPU network.",A,1
ImageNet Classification with Deep Convolutional Neural Networks,"This sort of response normalization implements a form of lateral inhibition inspired by the type found in real neurons, creating competition for big activities amongst neuron outputs computed using different kernels. The constants k, n, α, and β are hyper-parameters whose values are determined using a validation set; we used k = 2, n = 5, α = 10−4 , and β = 0.75. We applied this normalization after applying the ReLU nonlinearity in certain layers (see Section 3.5).","This kind of standardization of the response creates a type of lateral suppression similar to what is seen in real neurons. It causes competition for large activities between neuron outputs that are calculated using different kernels. The constants k, n, α, and β are hyperparameters whose values are set using a validation set. We used k = 2, n = 5, α = 10−4, and β = 0.75. We applied this normalization after putting the ReLU nonlinearity in certain layers (see Section 3.5).","This response normalization implements a form of lateral inhibition, like what is found in actual neurons, that creates rivalry for big activities among neuron outputs computed with different kernels. The hyperparameters k, n, α, and β have values fixed using a validation set. We used k = 2, n = 5, α = 10−4, and β = 0.75. We put this normalization after applying the ReLU nonlinearity in some layers (refer to Section 3.5). ","This kind of flattening of the response implements a type of side suppression similar to that seen in real neurons, generating competition for large activities between neuron outputs calculated using various kernels. The constants k, n, α, and β are tuning parameters whose values are identified using a validation set. We used k = 2, n = 5, α = 10−4, and β = 0.75. We applied this normalization after putting the ReLU nonlinearity in certain layers (see Section 3.5).",A,1
ImageNet Classification with Deep Convolutional Neural Networks,"This scheme bears some resemblance to the local contrast normalization scheme of Jarrett et al. [11], but ours would be more correctly termed “brightness normalization”, since we do not subtract the mean activity. Response normalization reduces our top-1 and top-5 error rates by 1.4% and 1.2%, respectively. We also verified the effectiveness of this scheme on the CIFAR-10 dataset: a four-layer CNN achieved a 13% test error rate without normalization and 11% with normalization.","This plan has some similarities to the local contrast standardization approach of Jarrett and colleagues [11], however ours would be more accurately called ""brightness standardization"", since we do not subtract the average activity. Response standardization decreases our top-1 and top-5 error percentages by 1.4% and 1.2%, respectively. We also confirmed the effectiveness of this approach on the CIFAR-10 data set: a four-layer CNN attained a 13% test error rate without standardization and 11% with standardization.","This system has some parallels to the local contrast normalization method of Jarrett's group [11], but ours would be more precisely termed ""brightness normalization"", as we do not subtract the mean activity. Normalizing the response reduces our top-1 and top-5 error rates by 1.4% and 1.2%, in that order. We also verified the usefulness of this system on the CIFAR-10 data set: a four-layer CNN achieved a 13% test error rate without normalization and 11% with normalization. ","This approach has some similarities to the local contrast normalization technique of Jarrett and co-authors [11], however ours would be more accurately called ""brightness normalization"", since we do not subtract the average activity. Normalizing the response decreases our top-1 and top-5 error percentages by 1.4% and 1.2%, respectively. We also confirmed the effectiveness of this technique on the CIFAR-10 dataset: a four-layer CNN obtained a 13% test error rate without normalization and 11% with normalization.",A,1
ImageNet Classification with Deep Convolutional Neural Networks,"Now we are ready to describe the overall architecture of our CNN. As depicted in Figure 2, the net contains eight layers with weights; the first five are convolutional and the remaining three are fully connected. The output of the last fully-connected layer is fed to a 1000-way softmax which produces a distribution over the 1000 class labels. Our network maximizes the multinomial logistic regression objective, which is equivalent to maximizing the average across training cases of the log-probability of the correct label under the prediction distribution.","We have now laid the groundwork to explain the general structure of our convolutional neural network (CNN). As shown in Figure 2, the network has eight layers with weights - the first five are convolutional layers and the last three are fully connected layers. The output from the final fully connected layer is input to a 1000-way softmax which generates a probability distribution over the 1000 class labels. Our network optimizes the multinomial logistic regression loss, which equals maximizing the average across all training examples of the log-probability of the correct label under the predicted distribution.","We can now describe the full architecture of our convolutional neural network (CNN). As illustrated in Figure 2, there are eight weighted layers in the network - the first five are convolutional layers and the final three are fully-connected layers. The output of the last fully-connected layer feeds into a 1000-way softmax that produces a distribution over the 1000 class labels. Our network maximizes the multinomial logistic regression loss function, which is the same as maximizing the average over all training cases of the log-probability of the true label under the predicted distribution.","We are now prepared to explain the complete structure of our convolutional neural network (CNN). As shown in Figure 2, the network has eight weighted layers - the first five are convolutional layers and the last three are fully-connected layers. The output of the final fully-connected layer is input to a 1000-way softmax which generates a distribution over the 1000 class labels. Our network optimizes the multinomial logistic regression loss function, which equals maximizing the average over all training examples of the log-probability of the correct label under the predicted distribution.",A,1
ImageNet Classification with Deep Convolutional Neural Networks,"The kernels of the second, fourth, and fifth convolutional layers are connected only to those kernel maps in the previous layer which reside on the same GPU (see Figure 2). The kernels of the third convolutional layer are connected to all kernel maps in the second layer. The neurons in the fully-connected layers are connected to all neurons in the previous layer. Response-normalization layers follow the first and second convolutional layers. Max-pooling layers, of the kind described in Section 3.4, follow both response-normalization layers as well as the fifth convolutional layer. The ReLU non-linearity is applied to the output of every convolutional and fully-connected layer.","The cores of the second, fourth, and fifth layers of convolution are linked exclusively to those kernel diagrams in the prior stratum which are positioned on the identical graphics processing unit (refer to Figure 2). The cores of the third convolution layer are linked to all kernel diagrams in the second layer. The nerve cells in the fully-connected tiers are linked to all nerve cells in the preceding tier. Response-normalization layers succeed the first and second convolution layers. Max-pooling layers, of the kind elucidated in Section 3.4, follow both response-normalization layers and the fifth convolution layer. The ReLU non-linearity is employed on the yield of every convolution and fully-connected layer.","The centers of the second, fourth, and fifth convolutional tiers are connected solely to those kernel maps in the earlier tier that are situated on the same GPU (see Figure 2). The centers of the third convolutional tier link to all kernel maps in the second tier. The neurons in the fully-connected levels connect to all neurons in the prior level. Response-normalization layers come after the first and second convolutional tiers. Max-pooling layers, of the type explained in Section 3.4, follow both response-normalization layers and the fifth convolutional tier. The ReLU non-linearity is used on the output of each convolutional and fully-connected layer.  ","The nuclei of the second, fourth, and fifth convolution layers are tied only to those kernel diagrams in the preceding layer that dwell on the identical graphics card (refer to Figure 2). The nuclei of the third convolution layer are tied to all kernel diagrams in the second layer. The nerve cells in the fully-connected ranks are tied to all nerve cells in the previous rank. Response-normalization layers come after the first and second convolution layers. Max-pooling layers, of the variety clarified in Section 3.4, follow both response-normalization layers and the fifth convolution layer. The ReLU non-linearity is employed on the yield of each convolution and fully-connected layer.",A,1
ImageNet Classification with Deep Convolutional Neural Networks,"The second convolutional layer takes as input the (response-normalized and pooled) output of the first convolutional layer and filters it with 256 kernels of size 5 × 5 × 48. The third, fourth, and fifth convolutional layers are connected to one another without any intervening pooling or normalization layers. The third convolutional layer has 384 kernels of size 3 × 3 × 256 connected to the (normalized, pooled) outputs of the second convolutional layer. The fourth convolutional layer has 384 kernels of size 3 × 3 × 192 , and the fifth convolutional layer has 256 kernels of size 3 × 3 × 192. The fully-connected layers have 4096 neurons each.","The next convolutional layer gets the (normalized and pooled) result from the first convolutional layer as input, filtering it using 256 kernels that are 5 × 5 × 48 in size. The 3rd, 4th, and 5th convolutional layers connect to one another without any pooling or normalization layers between them. The 3rd convolutional layer contains 384 kernels measuring 3 × 3 × 256 attached to the (normalized, pooled) outputs from the 2nd convolutional layer. The 4th convolutional layer has 384 kernels of 3 × 3 × 192 in size, and the 5th convolutional layer contains 256 kernels that are 3 × 3 × 192 in size. Both fully-connected layers have 4096 neurons each.","The following convolutional layer accepts the (response-normalized and pooled) yield of the initial convolutional layer as input, sifting through it utilizing 256 kernels measuring 5 × 5 × 48. The 3rd, 4th, and 5th convolutional layers link to one another lacking any intervening pooling or normalization layers. The 3rd convolutional layer possesses 384 kernels of dimensions 3 × 3 × 256 connected to the (normalized, pooled) outputs of the 2nd convolutional layer. The 4th convolutional layer owns 384 kernels of size 3 × 3 × 192, and the 5th convolutional layer has 256 kernels of dimensions 3 × 3 × 192. The fully-connected layers each contain 4096 neurons.  ","The next convolutional layer takes the (response-normalized and pooled) product of the first convolutional layer as input, filtering it through 256 kernels of dimensions 5 × 5 × 48. The third, fourth, and fifth convolutional layers connect to one another with no pooling or normalization layers between them. The third convolutional layer holds 384 kernels measuring 3 × 3 × 256 attached to the (normalized, pooled) yields of the second convolutional layer. The fourth convolutional layer contains 384 kernels sized 3 × 3 × 192, and the fifth convolutional layer possesses 256 kernels of size 3 × 3 × 192. Both fully-connected layers have 4096 neurons.",A,1
ImageNet Classification with Deep Convolutional Neural Networks,"Our neural network architecture has 60 million parameters. Although the 1000 classes of ILSVRC make each training example impose 10 bits of constraint on the mapping from image to label, this turns out to be insufficient to learn so many parameters without considerable overfitting. Below, we describe the two primary ways in which we combat overfitting. The easiest and most common method to reduce overfitting on image data is to artificially enlarge the dataset using label-preserving transformations (e.g., [25, 4, 5]).","Our design for the neural network has 60 million adjustable settings. Even though the 1000 types in ILSVRC make each training sample add 10 bits of limit on the mapping from picture to name, this ends up being not enough to learn so many settings without major overfitting. Next, we talk about the two main ways we fight overfitting. The simplest and most popular way to decrease overfitting on image information is to falsely expand the dataset using transformations that keep the labels the same (e.g., [25, 4, 5]).","Our neural network model contains 60 million modifiable parameters. Despite the fact that the 1000 classes of ILSVRC constrain each training case to impose 10 bits of restriction on the association from image to tag, this turns out to be insufficient to learn so many parameters without significant overfitting. Below, we describe the two primary methods we use to combat overfitting. The easiest and most common technique to reduce overfitting on image data is to artificially increase the dataset using transformations that preserve the labels (e.g., [25, 4, 5]).","Our neural network design has 60 million tunable weights. Even though the 1000 categories of ILSVRC constrain each training example to only 10 bits of limitation on the function from image to label, this is not enough to learn so many weights without major overfitting. Next, we explain the two main ways we mitigate overfitting. The simplest and most prevalent approach to decrease overfitting on image data is to artificially expand the dataset using label-preserving transforms (e.g., [25, 4, 5]).",A,1
ImageNet Classification with Deep Convolutional Neural Networks,"We employ two distinct forms of data augmentation, both of which allow transformed images to be produced from the original images with very little computation, so the transformed images do not need to be stored on disk. In our implementation, the transformed images are generated in Python code on the CPU while the GPU is training on the previous batch of images. So these data augmentation schemes are, in effect, computationally free. The first form of data augmentation consists of generating image translations and horizontal reflections.","We make use of two different types of data augmentation. Both allow transformed images to be made from the original images with very little computing, so the transformed images don't need to be kept on disk. In our system, the transformed images are created in Python code on the CPU while the GPU is learning on the previous batch of images. So these data augmentation plans are, essentially, computationally free. The first type of data augmentation includes generating image shifts and horizontal flips.","We utilize two unique forms of data augmentation. Both permit altered images to be produced from the original images with very minimal processing, so the altered images don't require storage on disk. In our setup, the altered images are generated in Python code on the CPU while the GPU is practicing on the prior set of images. Thus these data augmentation techniques are, in effect, computationally gratis. The first form of data augmentation consists of creating image translations and horizontal reversals.","We make use of two distinct modes of data augmentation. Both enable transformed images to be formed from the original images with very small computing, so the transformed images don't require saving on disk. In our implementation, the transformed images are created in Python code on the CPU while the GPU is learning on the previous group of images. So these data augmentation plans are, essentially, computationally free of charge. The first mode of data augmentation includes generating image shifts and horizontal flips.",A,1
ImageNet Classification with Deep Convolutional Neural Networks,"We do this by extracting random 224 × 224 patches (and their horizontal reflections) from the 256×256 images and training our network on these extracted patche . This increases the size of our training set by a factor of 2048, though the resulting training examples are, of course, highly interdependent. Without this scheme, our network suffers from substantial overfitting, which would have forced us to use much smaller networks. At test time, the network makes a prediction by extracting five 224 × 224 patches (the four corner patches and the center patch) as well as their horizontal reflections (hence ten patches in all), and averaging the predictions made by the network’s softmax layer on the ten patches.","We expand the training set data by taking out random 224 x 224 image segments (plus their horizontal flips) from the 256x256 photos and using those segments to train the neural network. This grows the training set by 2048 times, although the resulting examples are quite interrelated. Without this method, our network has considerable overfitting, which would have necessitated much smaller networks. During testing, the network makes a forecast by taking five 224 x 224 segments (the four corner segments and the center segment) and their horizontal flips (so ten segments total), and averaging the predictions from the softmax layer across the ten segments.","We augment the training information by extracting arbitrary 224 x 224 patches (and their left-right reflections) from the 256x256 images and utilizing those patches to optimize our model. This amplifies the size of the training data by 2048 times, despite the resulting training samples being highly dependent. Without this approach, our model suffers from major overfitting, which would have compelled us to employ much smaller models. At prediction time, the model makes a forecast by cropping five 224 x 224 patches (the four corner patches and the center patch) as well as their horizontal reflections (thus ten patches total), and taking the average of the predictions made by the model's softmax layer on the ten patches.  ","We expand the training data by taking random 224 x 224 sections (plus mirrored versions) from the 256x256 images and feeding those sections into the network during training. This grows the training data by 2048 times, although the resulting samples are highly related. Without this technique, our network has substantial overfitting, necessitating much smaller networks. During inference, the network generates a prediction by extracting five 224 x 224 crops (the four corner crops and the center crop) and their horizontal flips (10 crops total), then averaging the predictions from the softmax layer over the 10 crops.",A,1
ImageNet Classification with Deep Convolutional Neural Networks,"Combining the predictions of many different models is a very successful way to reduce test errors [1, 3], but it appears to be too expensive for big neural networks that already take several days to train. There is, however, a very efficient version of model combination that only costs about a factor of two during training. The recently-introduced technique, called “dropout” [10], consists of setting to zero the output of each hidden neuron with probability 0.5. The neurons which are “dropped out” in this way do not contribute to the forward pass and do not participate in backpropagation.","Putting together the forecasts from numerous separate models has proven very effective for decreasing errors on exams [1, 3], but this seems prohibitively costly for large neural networks that already require multiple days of training. Fortunately, there is a very efficient form of model combination that only increases training time by around a factor of two. This recently presented approach, termed ""dropout"" [10], works by randomly setting the output of each hidden neuron to zero with 0.5 probability. The neurons that are ""dropped out"" in this manner do not add to the forward pass and are not involved in backpropagation.","Integrating the projections from many distinct models has been shown to be a very useful way to lower mistakes on tests [1, 3], however this appears too expensive for big neural networks which already need several days to train. Luckily, there is a very cost-effective version of model integration that only raises training time by about twice as much. This newly introduced method, called ""dropout"" [10], functions by randomly setting the output of every hidden neuron to zero with a 0.5 probability. The neurons that are ""dropped out"" in this way do not contribute to the forward pass and are excluded from backpropagation.","Combining the anticipations from numerous different models has proven very effectual for reducing inaccuracies on evaluations [1, 3], nevertheless this seems too costly for large neural networks that already necessitate multiple days of training. Fortunately, there is a very efficient form of model amalgamation that only enlarges training time by around twice as much. This recently presented technique, termed ""dropout"" [10], operates by arbitrarily setting the output of each hidden neuron to zero with a 0.5 probability. The neurons that are ""dropped out"" in this fashion do not add to the forward pass and are precluded from backpropagation.",A,1
ImageNet Classification with Deep Convolutional Neural Networks,"So every time an input is presented, the neural network samples a different architecture, but all these architectures share weights. This technique reduces complex co-adaptations of neurons, since a neuron cannot rely on the presence of particular other neurons. It is, therefore, forced to learn more robust features that are useful in conjunction with many different random subsets of the other neurons. At test time, we use all the neurons but multiply their outputs by 0.5, which is a reasonable approximation to taking the geometric mean of the predictive distributions produced by the exponentially-many dropout networks.","Thus, whenever new information is given, the neural network generates a distinct design, though all these plans utilize the same coefficients. This approach decreases intricate co-dependencies of nerve cells, since a nerve cell is unable to depend on the existence of specific other nerve cells. Consequently, it is compelled to learn more durable characteristics that work together with numerous arbitrary subgroups of the other nerve cells. During testing, we employ all the nerve cells but increase their outputs by 0.5, which reasonably estimates taking the geometric average of the predictive distributions formed by the exponentially-numerous dropout networks.","So, every time there is new data, the neural network comes up with a different structure, but all these structures use the same weights. This method lowers complex co-adaptations of neurons, because a neuron can't rely on particular other neurons being present. It is thus forced to learn more robust features that are useful with many different random subsets of the other neurons. When testing, we use all the neurons but multiply their outputs by 0.5, which approximates taking the geometric mean of the predictive distributions made by the exponentially many dropout networks. ","Therefore, whenever there is new input, the neural network generates a new architecture, though all these designs share the same weights. This technique decreases complicated co-dependencies of neurons, since a neuron is unable to depend on specific other neurons existing. Consequently, it must learn more durable features that work with many random subgroups of the other neurons. During testing, we utilize all the neurons but increase their outputs by 0.5, which reasonably estimates taking the geometric average of the predictive distributions formed by the exponentially numerous dropout networks.",A,1
ImageNet Classification with Deep Convolutional Neural Networks,"We use dropout in the first two fully-connected layers of Figure 2. Without dropout, our network exhibits substantial overfitting. Dropout roughly doubles the number of iterations required to converge. We trained our models using stochastic gradient descent with a batch size of 128 examples, momentum of 0.9, and weight decay of 0.0005. We found that this small amount of weight decay was important for the model to learn. In other words, weight decay here is not merely a regularizer: it reduces the model’s training error.","We utilize dropout in the initial two fully-connected layers of Figure 2. Without utilizing dropout, our neural network displays considerable overfitting. Dropout approximately doubles the quantity of iterations essential to converge. We educated our models employing stochastic gradient descent with a batch dimension of 128 samples, momentum of 0.9, and weight decay of 0.0005. We discovered that this small quantity of weight decay was vital for the model to learn. In other terms, weight decay here is not just a regularizer: it lowers the model's training error.","We make use of dropout in the first pair of fully-connected tiers of Figure 2. Omitting dropout, our network shows substantial overfitting. Dropout roughly increases twofold the number of iterations needed to converge. We trained our models applying stochastic gradient descent with a batch amount of 128 instances, momentum of 0.9, and weight deterioration of 0.0005. We found that this little amount of weight deterioration was important for the model to learn. Put differently, weight deterioration here is not only a regularizer: it reduces the model's training error.  ","We employ dropout in the initial two fully-connected layers of Figure 2. Without utilizing dropout, our neural network displays significant overfitting. Dropout approximately doubles the number of iterations necessary to converge. We trained our models using stochastic gradient descent with a batch size of 128 samples, momentum of 0.9, and weight decay of 0.0005. We discovered that this small amount of weight decay was critical for the model to learn. In other words, weight decay here is not just a regularizer: it decreases the model's training error.",A,1
ImageNet Classification with Deep Convolutional Neural Networks,"We initialized the weights in each layer from a zero-mean Gaussian distribution with standard deviation 0.01. We initialized the neuron biases in the second, fourth, and fifth convolutional layers, as well as in the fully-connected hidden layers, with the constant 1. This initialization accelerates the early stages of learning by providing the ReLUs with positive inputs. We initialized the neuron biases in the remaining layers with the constant 0. We used an equal learning rate for all layers, which we adjusted manually throughout training.","We set the weights in every layer randomly from a normal distribution with a mean of zero and standard deviation of 0.01. We set the neuron biases in the second, fourth, fifth convolutional layers, and in the fully-connected hidden layers, to 1. This speeds up the initial phase of learning by giving the ReLUs positive inputs. We set the neuron biases in the other layers to 0. We utilized the same learning rate for all layers, which we tuned by hand during training.","We initialized the parameters in each layer by sampling from a Gaussian with zero mean and 0.01 standard deviation. We set the biases in the second, fourth, fifth conv layers and fully connected hidden layers to 1. This boosts early learning by providing ReLUs with positive inputs. We set the biases in the other layers to 0. We used a uniform learning rate for all layers, manually adjusting it over the course of training.  ","We randomly initialized the weights in every layer based on a normal distribution with zero mean and standard deviation of 0.01. We set the biases in the second, fourth, fifth conv layers and fully connected hidden layers to 1. This accelerates the initial learning phase by giving ReLUs positive values. We initialized the biases in the remaining layers to 0. We employed the same learning rate for all layers, tuning it by hand during training.",A,1
ImageNet Classification with Deep Convolutional Neural Networks," The heuristic which we followed was to divide the learning rate by 10 when the validation error rate stopped improving with the current learning rate. The learning rate was initialized at 0.01 and reduced three times prior to termination. We trained the network for roughly 90 cycles through the training set of 1.2 million images, which took five to six days on two NVIDIA GTX 580 3GB GPUs.","The rule of thumb we used was to decrease the learning rate by a factor of 10 when the validation error percentage stopped getting better with the current learning rate. We set the learning rate at 0.01 initially and lowered it 3 times before stopping. We trained the neural network for about 90 passes through the training dataset of 1.2 million images, which took 5 to 6 days using two NVIDIA GTX 580 3GB GPUs.","The heuristic we employed was to reduce the learning rate by dividing it by 10 when the validation error rate ceased to improve at the present learning rate. The learning rate began at 0.01 and was diminished 3 times before completion. We trained the neural network for roughly 90 iterations through the 1.2 million image training set, which required 5 to 6 days utilizing two NVIDIA GTX 580 3GB GPUs.  ","The rule of thumb we utilized was to decrease the learning rate by 10 times when the validation error stopped declining with the current learning rate. We initialized the learning rate at 0.01 and lowered it 3 times before ending. We trained the neural net for about 90 epochs through the training dataset of 1.2 million images, taking 5 to 6 days on two NVIDIA GTX 580 3GB GPUs.",A,1
ImageNet Classification with Deep Convolutional Neural Networks,"We also entered our model in the ILSVRC-2012 competition and report our results in Table 2. Since the ILSVRC-2012 test set labels are not publicly available, we cannot report test error rates for all the models that we tried. In the remainder of this paragraph, we use validation and test error rates interchangeably because in our experience they do not differ by more than 0.1% (see Table 2). The CNN described in this paper achieves a top-5 error rate of 18.2%. Averaging the predictions of five similar CNNs gives an error rate of 16.4%.","Furthermore, we submitted our system to the ILSVRC-2012 contest and present our outcomes in Table 2. Given that the ILSVRC-2012 test set labels are confidential, we are unable to disclose test error percentages for all the models we evaluated. For the rest of this section, we treat validation and test error rates as interchangeable since in our experience they do not diverge by over 0.1% (refer to Table 2). The CNN presented in this report attains a top-5 error percentage of 18.2%. Taking the average of the predictions from five analogous CNNs produces an error percentage of 16.4%.","In addition, we entered our algorithm in the ILSVRC-2012 competition and document our scores in Table 2. Because the ILSVRC-2012 test set classifications are not public, we can't disclose test error measurements for all the systems we tried out. For the remainder of this excerpt, we regard validation and test error measurements as the same since in our trials they do not differ by more than 0.1% (see Table 2). The CNN illustrated in this article accomplishes a top-5 error measurement of 18.2%. Calculating the mean of the forecasts from five similar CNNs yields an error measurement of 16.4%.  ","Moreover, we submitted our model to the ILSVRC-2012 contest and chronicle our outcomes in Table 2. Since the ILSVRC-2012 test set labels are private, we are unable to publish test error ratios for all the models we evaluated. For the rest of this excerpt, we treat validation and test error ratios as interchangeable because in our trials they do not diverge by over 0.1% (refer to Table 2). The CNN delineated in this paper achieves a top-5 error ratio of 18.2%. Computing the average of the predictions from five comparable CNNs produces an error ratio of 16.4%.",A,1
ImageNet Classification with Deep Convolutional Neural Networks,"Training one CNN, with an extra sixth convolutional layer over the last pooling layer, to classify the entire ImageNet Fall 2011 release (15M images, 22K categories), and then “fine-tuning” it on ILSVRC-2012 gives an error rate of 16.6%. Averaging the predictions of two CNNs that were pre-trained on the entire Fall 2011 release with the aforementioned five CNNs gives an error rate of 15.3%. The second-best contest entry achieved an error rate of 26.2% with an approach that averages the predictions of several classifiers trained on FVs computed from different types of densely-sampled features [7].","Educating a single CNN, by appending an additional sixth convolutional layer after the final pooling layer, to categorize the complete ImageNet Autumn 2011 edition (15 million images, 22 thousand categories), followed by calibrating it on ILSVRC-2012, yields an error percentage of 16.6%. Taking the mean of the forecasts of two CNNs that were pre-trained on the full Autumn 2011 edition along with the aforementioned five CNNs provides an error percentage of 15.3%. The runner-up contest entry attained an error percentage of 26.2% using a tactic that computes the average of numerous classifiers educated on FVs produced from varied densely-sampled characteristics [7].","Developing a solitary CNN, through attaching one more 6th convolutional stratum succeeding the closing pooling stratum, to organize the entire ImageNet Fall 2011 print (15 million depictions, 22 thousand divisions), accompanied by tuning it on ILSVRC-2012, begets an err percentage of 16.6%. Acquiring the midpoint of the projections of two CNNs that were pre-developed on the complete Fall 2011 print along with the aforesaid five CNNs yields an err percentage of 15.3%. The second-place contest entrance achieved an err percentage of 26.2% employing a policy that computes the mean of abundant classifiers cultivated on FVs spawned from divers densely-sampled attributes [7].  ","Cultivating a single CNN, by fastening an extra 6th convolutional sheet after the final pooling sheet, to categorize the whole ImageNet Autumn 2011 issue (15 million portrayals, 22 thousand categories), followed by calibrating it on ILSVRC-2012, produces an error rate of 16.6%. Obtaining the average of the predictions of two CNNs that were pre-developed on the entire Autumn 2011 issue along with the aforementioned five CNNs gives an error rate of 15.3%. The runner-up contest submission attained an error rate of 26.2% utilizing a strategy that calculates the average of multiple classifiers trained on FVs generated from varied densely-sampled features [7].",A,1
ImageNet Classification with Deep Convolutional Neural Networks,"Finally, we also report our error rates on the Fall 2009 version of ImageNet with 10,184 categories and 8.9 million images. On this dataset we follow the convention in the literature of using half of the images for training and half for testing. Since there is no established test set, our split necessarily differs from the splits used by previous authors, but this does not affect the results appreciably. Our top-1 and top-5 error rates on this dataset are 67.4% and 40.9%, attained by the net described above but with an additional, sixth convolutional layer over the last pooling layer. The best published results on this dataset are 78.1% and 60.9% [19].","In closing, we also share the rates of mistakes from our model on the Autumn 2009 form of ImageNet which had 10,184 types and 8.9 million photos. For this data, we keep to the standard of employing half the images for training and half for testing. Since there is no fixed test set, our division is different from previous researchers, but this does not really change the outcomes. Our top-1 and top-5 error percentages on this data are 67.4% and 40.9%, reached by the network described above but with one extra convolutional layer over the final pooling layer. The top published numbers on this dataset are 78.1% and 60.9% [19].","Lastly, we provide the error percentages generated by our model on the Fall 2009 edition of ImageNet containing 10,184 categories and 8.9 million images. For this dataset, we follow the convention of utilizing half the images for training and the other half for testing. Our split is different from previous work since there is no established test set, but this does not significantly impact the results. Our top-1 and top-5 error rates on this data are 67.4% and 40.9%, achieved by the network mentioned above plus an additional sixth convolutional layer over the final pooling layer. The best published error rates on this dataset are 78.1% and 60.9% [19].  ","In conclusion, we report the mistake rates from our model on the Fall 2009 ImageNet dataset with 10,184 classes and 8.9 million photos. For this data, we adhere to the standard practice of employing half the images for training and the other half for evaluation. Our division differs from prior work as there is no fixed test set, but this does not materially affect the outcomes. Our top-1 and top-5 error percentages on this dataset are 67.4% and 40.9%, obtained by the described network with an extra sixth convolutional layer over the final pooling layer. The best documented error rates on this data are 78.1% and 60.9% [19].",A,1
ImageNet Classification with Deep Convolutional Neural Networks,"Figure 3 shows the convolutional kernels learned by the network’s two data-connected layers. The network has learned a variety of frequency- and orientation-selective kernels, as well as various colored blobs. Notice the specialization exhibited by the two GPUs, a result of the restricted connectivity described in Section 3.5. The kernels on GPU 1 are largely color-agnostic, while the kernels on GPU 2 are largely color-specific. This kind of specialization occurs during every run and is independent of any particular random weight initialization (modulo a renumbering of the GPUs).","The visual representation in Figure 3 displays the convolutional kernels that were learned by the two layers in the network that share data connections. The network learned kernels that are selective for different frequencies, orientations, and colored shapes. Observe the specialized roles of the two GPUs, which is due to the limited connectivity explained in Section 3.5. The kernels on GPU 1 are mostly indifferent to color, while the kernels on GPU 2 are mostly sensitive to specific colors. This sort of division of labor occurs in every run and does not depend on any specific random weight initialization (except for which GPU gets which role).","The diagram in Figure 3 exhibits the convolutional kernels that were acquired by the pair of data-linked layers of the network. The network gained kernels that prefer certain frequencies, orientations, and colored blobs. Note the expertise shown by the two GPUs, a consequence of the constrained connections discussed in Section 3.5. The kernels on GPU 1 are generally oblivious to color, whereas the kernels on GPU 2 are generally cognizant of specific colors. This kind of expertise emerges during every run and does not rely on any particular random weight initialization (other than which GPU is assigned which role).  ","The visual in Figure 3 portrays the convolutional kernels that were learned by the two data-connected layers of the network. The network obtained kernels that favor certain frequencies, orientations, and colored shapes. Perceive the specialization displayed by the two GPUs, a result of the limited connections described in Section 3.5. The kernels on GPU 1 are mostly naive to color, while the kernels on GPU 2 are mostly discerning of precise colors. This sort of expertise materializes during every run and is not contingent on any exact random weight initialization (except for which GPU receives which function).",A,1
Language Models are Few-Shot Learners,"Recent years have featured a trend towards pre-trained language representations in NLP systems, applied in increasingly flexible and task-agnostic ways for downstream transfer. First, single-layer representations were learned using word vectors [MCCD13, PSM14] and fed to task-specific architectures, then RNNs with multiple layers of representations and contextual state were used to form stronger representations [DL15, MBXS17, PNZtY18] (though still applied to task-specific architectures), and more recently pre-trained recurrent or transformer language models [VSP+17] have been directly fine-tuned, entirely removing the need for task-specific architectures [RNSS18, DCLT18, HR18].","In the field of natural language processing, there has been a recent trend of using pre-trained language models that can be applied flexibly to various downstream tasks. Initially, single-layer word embeddings were learned and input to specialized models. Then, multilayer RNNs were used to build more powerful contextual representations, though still fed into specialized models. Most recently, large pretrained recurrent and transformer language models have been directly fine-tuned for tasks, removing the need for specialized architectures.","Over the past few years, NLP systems have increasingly made use of pre-trained language representations in flexible, task-agnostic ways for transfer learning. First, single-layer word vectors were learned and given to task-specific models. Next, multilayer RNNs formed stronger contextual representations, still applied to specialized models. Now, large pretrained recurrent or transformer LMs are directly fine-tuned, eliminating specialized architectures.","In recent times, NLP has seen a trend of using pre-trained language models in adaptable, task-general ways for transfer. Initially, single-layer word embeddings were learned and used in task-focused models. After that, multilayer RNNs created more powerful contextual representations, although still fed to specialized models. Most recently, large pretrained recurrent or transformer LMs have been directly fine-tuned, removing the need for specialized architectures.",A,1
Language Models are Few-Shot Learners,"This last paradigm has led to substantial progress on many challenging NLP tasks such as reading comprehension, question answering, textual entailment, and many others, and has continued to advance based on new architectures and algorithms [RSR+19, LOG+19, YDY+19, LCG+19]. However, a major limitation to this approach is that while the architecture is task-agnostic, there is still a need for task-specific datasets and task-specific fine-tuning: to achieve strong performance on a desired task typically requires fine-tuning on a dataset of thousands to hundreds of thousands of examples specific to that task.","This most recent framework has resulted in considerable improvements on many difficult NLP tasks including reading comprehension, question answering, textual entailment, and more. It has continued to progress thanks to new architectures and algorithms [RSR+19, LOG+19, YDY+19, LCG+19]. However, a significant restriction of this method is that even though the architecture is task-general, task-particular datasets and task-particular fine-tuning are still required: to attain robust performance on a wanted task usually necessitates fine-tuning on thousands to hundreds of thousands of examples exclusive to that task.","This latest paradigm has led to major advancements on numerous challenging natural language processing tasks such as reading comprehension, question answering, textual entailment, and so on. It has kept improving due to novel architectures and algorithms [RSR+19, LOG+19, YDY+19, LCG+19]. However, a considerable limitation of this approach is that although the architecture is not specific to any one task, task-focused datasets and tuning are still needed: achieving strong results on a desired task typically requires tuning on thousands to hundreds of thousands of examples particular to that task.  ","This most recent framework has produced substantial improvements on many tough NLP tasks including reading comprehension, question answering, textual entailment, and more. It has continued advancing thanks to new architectures and algorithms [RSR+19, LOG+19, YDY+19, LCG+19]. However, a big constraint of this method is that while the architecture is general and not tailored to any specific task, task-centered datasets and tuning are still necessary: attaining strong performance on a target task usually requires tuning on thousands to hundreds of thousands of examples specific to that task.",A,1
Language Models are Few-Shot Learners,"Removing this limitation would be desirable, for several reasons. First, from a practical perspective, the need for a large dataset of labeled examples for every new task limits the applicability of language models. There exists a very wide range of possible useful language tasks, encompassing anything from correcting grammar, to generating examples of an abstract concept, to critiquing a short story. For many of these tasks it is difficult to collect a large supervised training dataset, especially when the process must be repeated for every new task. Second, the potential to exploit spurious correlations in training data fundamentally grows with the expressiveness of the model and the narrowness of the training distribution.","It would be beneficial to eliminate this constraint, for multiple motivations. To start with, looking at it practically, requiring a substantial collection of annotated instances for each new job restricts the usefulness of language models. There is a very broad scope of potential useful language tasks, covering everything from fixing grammar, to producing examples of an abstract idea, to reviewing a short story. For many of these tasks it is tough to gather a large supervised training set, especially when the process needs to be repeated for every new task. Furthermore, the capacity to leverage coincidental correlations in training information fundamentally expands with the expressiveness of the model and the limitedness of the training distribution.","Abolishing this limitation would be advantageous, for several justifications. Initially, from a pragmatic angle, needing a voluminous dataset of exemplars with labels for each novel objective curtails the applicability of language models. There subsists a very extensive gamut of conceivable beneficial language tasks, encompassing anything from rectifying grammar, to bringing forth examples of an abstract concept, to evaluating a short story. For many of these tasks it is arduous to assemble a large supervised training dataset, especially when the process must be reiterated for every new task. Moreover, the potential to take advantage of specious correlations in training evidence fundamentally surges with the expressiveness of the model and the narrowness of the training distribution.  ","Removing this constraint would be favorable, for multiple reasons. Firstly, from a practical view, the prerequisite for an expansive dataset of annotated instances for every new objective limits the usefulness of language models. There exists a very wide array of potential useful language tasks, covering anything from amending grammar, to creating examples of an abstract concept, to analyzing a short story. For many of these tasks it is difficult to gather a large supervised training set, especially when the process needs to be repeated for every new task. Furthermore, the ability to leverage illusory correlations in training data fundamentally escalates with the expressiveness of the model and the limitedness of the training distribution.",A,1
Language Models are Few-Shot Learners,"This can create problems for the pre-training plus fine-tuning paradigm, where models are designed to be large to absorb information during pre-training, but are then fine-tuned on very narrow task distributions. There is evidence that suggests that the generalization achieved under this paradigm can be poor because the model is overly specific to the training distribution and does not generalize well outside it [YdC+19, MPL19]. Thus, the performance of fine-tuned models on specific benchmarks, even when it is nominally at human-level, may exaggerate actual performance on the underlying task [GSL+18, NK19].","This approach can cause issues for the pre-train and fine-tune model, where models are made large to learn during pre-training, but then adjusted on very limited task data. There is proof showing that the generalization gained this way can be inadequate because the model is too tailored to the training data and does not generalize well beyond it [YdC+19, MPL19]. Therefore, the performance of fine-tuned models on particular benchmarks, even when nominally human-level, may overstate real performance on the task itself [GSL+18, NK19].","This procedure can create complications for the pre-train then fine-tune paradigm, where models are designed to be sizable to ingest knowledge during pre-training, but are then calibrated on very narrow task examples. There is evidence implying that the generalization attained under this paradigm can be poor because the model is excessively focused on the training examples and does not generalize well exterior to it [YdC+19, MPL19]. Consequently, the performance of fine-tuned models on explicit benchmarks, even when ostensibly at human-level, may exaggerate factual performance on the fundamental task [GSL+18, NK19].  ","This approach can generate problems for the pre-train and then fine-tune model, where models are constructed to be large to internalize information during pre-training, but are then adjusted on very limited task instances. There is data signifying that the generalization gained this way can be inadequate because the model is overly tailored to the training instances and does not generalize well peripheral to it [YdC+19, MPL19]. As a result, the performance of fine-tuned models on particular benchmarks, even when supposedly at human-level, may inflate actual performance on the underlying task [GSL+18, NK19].",A,1
Language Models are Few-Shot Learners,"Third, humans do not require large supervised datasets to learn most language tasks – a brief directive in natural language (e.g. “please tell me if this sentence describes something happy or something sad”) or at most a tiny number of demonstrations (e.g. “here are two examples of people acting brave; please give a third example of bravery”) is often sufficient to enable a human to perform a new task to at least a reasonable degree of competence. Aside from pointing to a conceptual limitation in our current NLP techniques, this adaptability has practical advantages – it allows humans to seamlessly mix together or switch between many tasks and skills, for example performing addition during a lengthy dialogue.","Additionally, people don't need huge datasets with supervision to learn most language activities - a short instruction in normal language (for instance, ""please inform me if this sentence portrays something joyful or sad"") or at most a couple of examples (for example, ""here are two instances of people being courageous; please provide a third case of bravery"") is frequently enough to enable a person to execute a new task decently well. Apart from indicating a conceptual constraint in our present NLP methods, this flexibility has practical benefits - it permits humans to effortlessly combine or alternate between many tasks and abilities, like doing math during a long discussion.","Moreover, humans do not require massive labeled datasets to acquire most linguistic tasks - a brief guide in plain language (e.g. ""tell me if this sentence describes something happy or sad"") or at best a tiny number of demonstrations (e.g. ""here are two examples of brave behavior; now show a third case of bravery"") is often sufficient to allow a person to perform a novel task with at least reasonable competence. Aside from highlighting a conceptual limitation in our current natural language processing techniques, this adaptability has practical advantages - it enables humans to seamlessly integrate or switch between many skills and tasks, such as performing calculations during an extended conversation.","In addition, people don't need huge supervised data to learn most language jobs - a short description in normal words (for example, ""say if this sentence shows something joyful or unhappy"") or at most a couple instances (for instance, ""here are two cases of courage; now provide a third example of bravery"") is often enough to let someone do a new job decently. Apart from indicating a conceptual issue in our current NLP, this flexibility has practical benefits - it allows humans to smoothly mix or alternate between many abilities and tasks, like adding numbers during a long chat.",A,1
Language Models are Few-Shot Learners,"One potential route towards addressing these issues is meta-learning1 – which in the context of language models means the model develops a broad set of skills and pattern recognition abilities at training time, and then uses those abilities at inference time to rapidly adapt to or recognize the desired task (illustrated in Figure 1.1). Recent work [RWC+19] attempts to do this via what we call “in-context learning”, using the text input of a pretrained language model as a form of task specification: the model is conditioned on a natural language instruction and/or a few demonstrations of the task and is then expected to complete further instances of the task simply by predicting what comes next.","A possible way to tackle these problems is meta-learning - which for language models means the model learns a wide range of skills and pattern recognition capabilities during training, and then utilizes those capabilities during inference to quickly adapt to or identify the desired task (shown in Figure 1.1). Recent work [RWC+19] tries to do this through what we term ""in-context learning"", using the text input of a pre-trained language model as a form of task description: the model is conditioned on a natural language instruction and/or a few examples of the task and is then expected to complete more instances of the task just by predicting what comes next.","One avenue to address these challenges is meta-learning - where for language models this means the model develops a diverse set of abilities and pattern recognition skills during training, and subsequently leverages those skills during inference to swiftly tailor to or discern the target task (depicted in Figure 1.1). Recent efforts [RWC+19] attempt this via what we dub ""in-context learning"", utilizing the text input of a pre-trained language model as a type of task specification: the model is primed on a natural language directive and/or a few demonstrations of the task and is then anticipated to complete more cases of the task simply by forecasting what follows. ","A promising approach to tackling these difficulties is meta-learning – which for language models signifies the model acquires a wide repertoire of capabilities and pattern recognition aptitudes during training, then capitalizes on those aptitudes during inference to rapidly accommodate or identify the intended task (portrayed in Figure 1.1). Recent work [RWC+19] seeks to accomplish this through what we call “in-context learning”, leveraging the text input of a pre-trained language model as a form of task delineation: the model is conditioned on a natural language instruction and/or a few instances of the task and is then expected to complete further examples of the task simply by predicting the next steps.",A,1
Language Models are Few-Shot Learners," While it has shown some initial promise, this approach still achieves results far inferior to fine-tuning – for example [RWC+19] achieves only 4% on Natural Questions, and even its 55 F1 CoQa result is now more than 35 points behind the state of the art. Meta-learning clearly requires substantial improvement in order to be viable as a practical method of solving language tasks. Another recent trend in language modeling may offer a way forward. In recent years the capacity of transformer language models has increased substantially, from 100 million parameters [RNSS18], to 300 million parameters [DCLT18], to 1.5 billion parameters [RWC+19], to 8 billion parameters [SPP+19], 11 billion parameters [RSR+19], and finally 17 billion parameters [Tur20].","While displaying some initial promise, this method still produces outcomes far worse than fine-tuning - for instance [RWC+19] only achieves 4% on Natural Questions, and even its 55 F1 CoQa result trails the state-of-the-art by over 35 points now. Meta-learning clearly needs major enhancements before it can be practical as a way to solve language tasks. Another recent fad in language modeling might provide a path forward. In recent times the size of transformer language models has grown substantially, from 100 million parameters [RNSS18], to 300 million [DCLT18], to 1.5 billion [RWC+19], to 8 billion [SPP+19], 11 billion [RSR+19], and finally 17 billion [Tur20].","Although showing some initial potential, this technique still generates results much inferior to fine-tuning - for example [RWC+19] only manages 4% on Natural Questions, and even its 55 F1 CoQa outcome is now more than 35 points behind the best available. Meta-learning obviously requires big improvements to be viable as a practical means of solving language tasks. Another recent trend in language modeling could offer a way ahead. Over recent years the scale of transformer language models has increased substantially, from 100 million parameters [RNSS18], to 300 million [DCLT18], to 1.5 billion [RWC+19], to 8 billion [SPP+19], 11 billion [RSR+19], and finally 17 billion [Tur20].  ","While exhibiting some initial promise, this method still produces outcomes far below fine-tuning – for instance [RWC+19] only accomplishes 4% on Natural Questions, and even its 55 F1 CoQa result now trails the state-of-the-art by over 35 points. Meta-learning clearly necessitates major enhancement before it can be practical as a way of solving language tasks. Another recent movement in language modeling could provide a path forward. In recent times the size of transformer language models has expanded substantially, from 100 million parameters [RNSS18], to 300 million [DCLT18], to 1.5 billion [RWC+19], to 8 billion [SPP+19], 11 billion [RSR+19], and finally 17 billion [Tur20].",A,1
Language Models are Few-Shot Learners,"Each increase has brought improvements in text synthesis and/or downstream NLP tasks, and there is evidence suggesting that log loss, which correlates well with many downstream tasks, follows a smooth trend of improvement with scale [KMH+20]. Since in-context learning involves absorbing many skills and tasks within the parameters of the model, it is plausible that in-context learning abilities might show similarly strong gains with scale.","Every boost has resulted in enhancements in text creation and/or subsequent natural language processing assignments, and there are signs hinting that log loss, which aligns well with many subsequent tasks, pursues a smooth trajectory of enhancement with scale [KMH+20]. Because in-context learning consists of taking in many abilities and assignments within the boundaries of the model, it is believable that in-context learning capabilities might exhibit similarly robust increases with scale.","Each expansion has produced refinements in text synthesis and/or following natural language processing jobs, and there are clues indicating that log loss, which correlates appropriately with many following tasks, follows a smooth course of refinement with scale [KMH+20]. Since in-context learning entails assimilating numerous skills and jobs within the parameters of the model, it is plausible that in-context learning aptitudes might demonstrate similarly formidable gains with scale. ","Every addition has yielded advancements in text generation and/or ensuing natural language processing undertakings, and there are signs denoting that log loss, which aligns suitably with many ensuing undertakings, pursues a smooth trajectory of advancement with scale [KMH+20]. Because in-context learning consists of absorbing many capabilities and undertakings within the confines of the model, it is credible that in-context learning capacities might exhibit similarly sturdy increases with scale.",A,1
Language Models are Few-Shot Learners,"In this paper, we test this hypothesis by training a 175 billion parameter autoregressive language model, which we call GPT-3, and measuring its in-context learning abilities. Specifically, we evaluate GPT-3 on over two dozen NLP datasets, as well as several novel tasks designed to test rapid adaptation to tasks unlikely to be directly contained in the training set. For each task, we evaluate GPT-3 under 3 conditions: (a) “few-shot learning”, or in-context learning where we allow as many demonstrations as will fit into the model’s context window (typically 10 to 100), (b) “one-shot learning”, where we allow only one demonstration, and (c) “zero-shot” learning, where no demonstrations are allowed and only an instruction in natural language is given to the model.","In this report, we examine this theory by teaching a 175 billion parameter self-regressing language program, which we name GPT-3, and calculating its in-context learning skills. Specifically, we review GPT-3 on over two dozen NLP data sets, as well as several new tasks intended to evaluate fast adaptation to tasks unlikely to be straight in the training set. For each task, we assess GPT-3 under 3 circumstances: (a) ""few-shot learning"", or in-context learning where we permit as many examples as will fit into the model's context window (typically 10 to 100), (b) ""one-shot learning"", where we only allow one example, and (c) ""zero-shot"" learning, where no examples are permitted and only an instruction in natural language is provided to the model.","This paper tests the theory by developing a 175 billion parameter self-learning language system called GPT-3, and measuring its ability to learn in context. We specifically judge GPT-3 on over two dozen NLP data sets, plus several new tasks to test quick tuning to tasks probably not in the training set directly. For each task, we rate GPT-3 in 3 ways: (a) ""few-shot learning"", or in-context learning allowing as many examples as fit the context window (usually 10 to 100), (b) ""one-shot learning"" with only one example, and (c) ""zero-shot"" learning without examples, just a natural language instruction.","Here we examine the hypothesis by making a 175 billion parameter autoregressive language program named GPT-3, and testing its in-context learning capacity. We specifically evaluate GPT-3 on over two dozen NLP datasets, and new tasks to evaluate fast tuning to unfamiliar tasks. For each task, we test GPT-3 3 ways: (a) ""few-shot learning"", allowing many context examples (typically 10-100), (b) ""one-shot learning"" with one example, and (c) ""zero-shot learning"" with no examples, only natural language instructions.",A,1
Language Models are Few-Shot Learners,"GPT-3 could also in principle be evaluated in the traditional fine-tuning setting, but we leave this to future work. Figure 1.2 illustrates the conditions we study, and shows few-shot learning of a simple task requiring the model to remove extraneous symbols from a word. Model performance improves with the addition of a natural language task description, and with the number of examples in the model’s context, K. Few-shot learning also improves dramatically with model size. Though the results in this case are particularly striking, the general trends with both model size and number of examples in-context hold for most tasks we study.","GPT-3 has the capability to be tested in the standard fine-tuning configuration, but we are postponing that for the future. Figure 1.2 shows the settings we are investigating, and displays quick learning of a basic task that requires the model to take out unnecessary symbols from a word. The model's performance gets better by adding a natural language description of the task, and with the quantity of examples in the model's context, K. Learning with only a few examples also dramatically improves with the size of the model. Although the results in this situation are especially remarkable, the general tendencies with both the model's size and number of examples in context apply for most of the tasks we investigate.","GPT-3 could theoretically also be evaluated using the conventional approach of fine-tuning, however we are leaving that for future work. Figure 1.2 illustrates the scenarios we are studying, and demonstrates rapid acquisition of a simple task where the model must remove redundant symbols from a word. The model's capabilities improve by providing a natural language description of the task, and by increasing the number of examples in the model's context, K. Learning from only a few examples also greatly improves as the model size increases. While the results in this particular case are especially striking, the general patterns regarding both model size and number of in-context examples hold true for most of the tasks we examine.  ","GPT-3 has the potential to be assessed using the standard fine-tuning methodology as well, however we are postponing that to future work. Figure 1.2 shows the settings we are analyzing, and exhibits fast learning of a straightforward task requiring the model to remove unnecessary symbols from a word. The model's performance enhances by supplying a natural language elucidation of the task, and by raising the quantity of examples in the model's context, K. Learning from just a few examples also dramatically improves as the model size gets bigger. Although the results in this specific case are particularly remarkable, the general trends concerning both model size and number of examples in context apply to most of the tasks we study.",A,1
Language Models are Few-Shot Learners,"We emphasize that these “learning” curves involve no gradient updates or fine-tuning, just increasing numbers of demonstrations given as conditioning. Broadly, on NLP tasks GPT-3 achieves promising results in the zero-shot and one-shot settings, and in the the few-shot setting is sometimes competitive with or even occasionally surpasses state-of-the-art (despite state-of-the-art being held by fine-tuned models). For example, GPT-3 achieves 81.5 F1 on CoQA in the zero-shot setting, 84.0 F1 on CoQA in the one-shot setting, 85.0 F1 in the few-shot setting. Similarly, GPT-3 achieves 64.3% accuracy on TriviaQA in the zero-shot setting, 68.0% in the one-shot setting, and 71.2% in the few-shot setting, the last of which is state-of-the-art relative to fine-tuned models operating in the same closed-book setting.","We want to highlight that these performance graphs do not entail any parameter tuning or model adaptation, just feeding the model more examples as prompts. In a nutshell, on natural language tasks GPT-3 obtains encouraging zero-shot and one-shot results, and in the few-shot regime can sometimes match or even slightly beat state-of-the-art (even though fine-tuned models hold the current state-of-the-art). For instance, GPT-3 reaches 81.5 F1 on CoQA with no shot, 84.0 F1 with one shot, and 85.0 F1 with a few shots. Similarly, GPT-3 gets 64.3% accuracy on TriviaQA with no shot, 68.0% with one shot, and 71.2% with a few shots, the last of which surpasses fine-tuned models working in the same closed-book environment.","We want to stress that these performance curves do not include any parameter updates or model tuning, just providing more examples as prompts to the model. Broadly speaking, on natural language tasks GPT-3 achieves promising zero-shot and one-shot results, and in the few-shot setting can sometimes match or even edge past state-of-the-art (even though fine-tuned models currently hold the state-of-the-art). For example, GPT-3 attains 81.5 F1 score on CoQA with zero shots, 84.0 F1 with one shot, and 85.0 F1 with a few shots. Similarly, GPT-3 reaches 64.3% accuracy on TriviaQA with zero shots, 68.0% with one shot, and 71.2% with a few shots, the last of which beats fine-tuned models operating in the same closed-book setting.","We want to emphasize that these performance graphs do not include any parameter tuning or model fine-tuning, just providing more examples as prompts. In summary, on natural language tasks GPT-3 achieves promising zero-shot and one-shot results, and in the few-shot regime can sometimes equal or even slightly exceed state-of-the-art (despite fine-tuned models currently holding the state-of-the-art). For instance, GPT-3 attains 81.5 F1 on CoQA with zero shots, 84.0 F1 with one shot, and 85.0 F1 with a few shots. Similarly, GPT-3 reaches 64.3% accuracy on TriviaQA with zero shots, 68.0% with one shot, and 71.2% with a few shots, the last of which surpasses fine-tuned models operating in the same closed-book setting.",A,1
Language Models are Few-Shot Learners,"GPT-3 also displays one-shot and few-shot proficiency at tasks designed to test rapid adaption or on-the-fly reasoning, which include unscrambling words, performing arithmetic, and using novel words in a sentence after seeing them defined only once. We also show that in the few-shot setting, GPT-3 can generate synthetic news articles which human evaluators have difficulty distinguishing from human-generated articles. At the same time, we also find some tasks on which few-shot performance struggles, even at the scale of GPT-3. This includes natural language inference tasks like the ANLI dataset, and some reading comprehension datasets like RACE or QuAC.","GPT-3 is adept at rapidly adapting to and reasoning through tasks like unscrambling words, doing math, and utilizing recently defined words in a sentence after only seeing the definition once. With minimal exposure, GPT-3 can also generate fake news articles that humans find very realistic and authentic. However, there are some tasks like making inferences, reading comprehension, and answering questions that GPT-3 struggles with, even when given a few examples. Datasets like ANLI, RACE, and QuAC pose challenges for GPT-3's few-shot learning abilities.","GPT-3 displays proficiency at quickly adapting to and deducing solutions for tasks like unjumbling words, arithmetic calculations, and applying newly explained words in a sentence after a single demonstration. With limited data, GPT-3 can also fabricate news stories that people have trouble discerning from real news written by humans. But some tasks like making logical inferences, reading passages and answering questions still prove difficult for GPT-3, even when given a handful of examples. Datasets such as ANLI, RACE and QuAC are problematic for GPT-3's few-shot learning skills.  ","GPT-3 exhibits adeptness at rapidly acclimating to and reasoning through exercises like descrambling words, executing math operations, and utilizing novel words in a sentence after only seeing the definition once. With minimal data, GPT-3 can also generate sham news reports that humans struggle to differentiate from authentic news authored by people. However, some tasks like making deductions, comprehending texts and responding to questions continue to pose challenges for GPT-3, despite being provided with a few examples. Datasets such as ANLI, RACE and QuAC are problematic for GPT-3's ability to learn from a small number of examples.",A,1
Language Models are Few-Shot Learners,"By presenting a broad characterization of GPT-3’s strengths and weaknesses, including these limitations, we hope to stimulate study of few-shot learning in language models and draw attention to where progress is most needed. A heuristic sense of the overall results can be seen in Figure 1.3, which aggregates the various tasks (though it should not be seen as a rigorous or meaningful benchmark in itself).","We aim to promote research into few-shot learning with language models and highlight areas needing improvement by giving a general depiction of GPT-3's capabilities and flaws, including these restrictions. A rough impression of the full findings can be grasped from Figure 1.3, which brings together the various tasks (but should not be viewed as a strict or meaningful benchmark on its own).","By presenting a wide-ranging portrayal of GPT-3's strengths and limitations, including these constraints, we hope to encourage study of few-shot learning in language models and bring attention to where progress is most required. Figure 1.3, which combines the different tasks (though it should not be considered a rigorous or significant benchmark itself), gives a heuristic sense of the overall results. ","Through providing a broad depiction of GPT-3's abilities and shortcomings, including these caveats, we aim to promote research into few-shot learning with language models and spotlight areas needing enhancement. Figure 1.3, which consolidates the various tasks (albeit it should not be regarded as a strict or meaningful benchmark on its own), gives an approximate feel for the full findings.",A,1
Language Models are Few-Shot Learners,"We also undertake a systematic study of “data contamination” – a growing problem when training high capacity models on datasets such as Common Crawl, which can potentially include content from test datasets simply because such content often exists on the web. In this paper we develop systematic tools to measure data contamination and quantify its distorting effects. Although we find that data contamination has a minimal effect on GPT-3’s performance on most datasets, we do identify a few datasets where it could be inflating results, and we either do not report results on these datasets or we note them with an asterisk, depending on the severity.","Furthermore, we carry out a methodical examination of ""data pollution"" - an increasing issue when teaching high performance models on datasets like Common Crawl, which may inadvertently include content from test datasets just because such material is often found online. In this paper we build systematic tools to quantify data pollution and measure its distorting impacts. Although we determine data pollution has a minimal effect on GPT-3's performance on most datasets, we do pinpoint a few datasets where it could be artificially inflating results, and we either do not document results on these datasets or we denote them with an asterisk, depending on the severity.","In addition, we conduct a meticulous study of ""data contamination"" - a growing dilemma when educating sophisticated models using datasets such as Common Crawl, which could potentially comprise content from test datasets simply because that content is frequently available on the web. Here we develop systematic techniques to evaluate data contamination and quantify its distorting consequences. While we find data contamination has a negligible impact on GPT-3's performance for most datasets, we do identify some datasets where it may be artificially boosting results, so we either omit results for those datasets or flag them with an asterisk, based on the extent.  ","Moreover, we undertake a rigorous analysis of ""data corruption"" - an increasing problem when training advanced models on datasets like Common Crawl, which may include content from test datasets just because such material is commonly found online. In this paper we construct systematic tools to measure data corruption and determine its distorting effects. Although we conclude data corruption has a minimal influence on GPT-3's performance across most datasets, we do recognize a few datasets where it could be artificially inflating results, so we either exclude results for those datasets or denote them with an asterisk, depending on the level of distortion.",A,1
Language Models are Few-Shot Learners,"In addition to all the above, we also train a series of smaller models (ranging from 125 million parameters to 13 billion parameters) in order to compare their performance to GPT-3 in the zero, one and few-shot settings. Broadly, for most tasks we find relatively smooth scaling with model capacity in all three settings; one notable pattern is that the gap between zero-, one-, and few-shot performance often grows with model capacity, perhaps suggesting that larger models are more proficient meta-learners. Finally, given the broad spectrum of capabilities displayed by GPT-3, we discuss concerns about bias, fairness, and broader societal impacts, and attempt a preliminary analysis of GPT-3’s characteristics in this regard.","Furthermore, we educate a range of smaller models (spanning from 125 million factors to 13 billion factors) to contrast their capabilities with GPT-3 in contexts lacking data, with one data point, and with limited data. Overall, for most tasks we see relatively steady improvements with model size in all three contexts; one noticeable pattern is that the difference between performance with no data, one data point, and some data frequently increases with model size, possibly implying that larger models are more skilled at meta-learning. Moreover, given the extensive range of abilities exhibited by GPT-3, we consider issues regarding bias, fairness, and wider societal impacts, and try a preliminary analysis of GPT-3's attributes in this area.","In supplement to the preceding, we also develop a series of smaller systems (ranging from 125 million elements to 13 billion elements) to compare their effectiveness to GPT-3 when no data is available, only one data point is available, and a small amount of data is available. Broadly speaking, for most tasks we observe relatively steady improvements with model scale in all three situations; one noticeable pattern is that the gap between effectiveness with zero data, one data point, and limited data often grows with model size, potentially indicating that larger models are more adept at meta-learning. Furthermore, given the wide array of capabilities shown by GPT-3, we discuss concerns regarding bias, fairness, and broader societal impacts, and attempt a preliminary analysis of GPT-3's properties in this domain.","Additionally, we train a series of smaller models (spanning from 125 million variables to 13 billion variables) to contrast their performance against GPT-3 when no data is present, only one data point is present, and a small amount of data is present. Overall, for most tasks we find relatively consistent improvements in performance as model size increases in all three contexts; one clear pattern is that the difference in performance between having no data, one data point, and a limited amount of data often grows as model size increases, potentially signaling that larger models are more skilled at meta-learning. Moreover, given the extensive capabilities demonstrated by GPT-3, we discuss concerns around bias, fairness, and wider impacts on society, and make a preliminary analysis of GPT-3's attributes in this area.",A,1
Language Models are Few-Shot Learners,"Our basic pre-training approach, including model, data, and training, is similar to the process described in [RWC+19], with relatively straightforward scaling up of the model size, dataset size and diversity, and length of training. Our use of in-context learning is also similar to [RWC+19], but in this work we systematically explore different settings for learning within the context. Therefore, we start this section by explicitly defining and contrasting the different settings that we will be evaluating GPT-3 on or could in principle evaluate GPT-3 on. These settings can be seen as lying on a spectrum of how much task-specific data they tend to rely on.","The fundamental method we used to pre-train our model, including the model architecture, datasets, and training process, closely resembles what was described in [RWC+19]. We scaled up the model size, dataset size and variety, and training duration in a fairly straightforward way. Our technique for in-context learning is also similar to [RWC+19], but in this work we methodically explore different configurations for learning within the context. Therefore, we will start this section by clearly defining and contrasting the different settings that we will be assessing GPT-3 on or that could in principle be used to assess GPT-3. These settings can be viewed as existing on a spectrum of how much task-specific data they tend to utilize.","Our underlying pre-training approach, encompassing the model design, data sources, and training procedures, largely follows the process outlined in [RWC+19]. We increased the model capacity, dataset volume and diversity, and training duration in a relatively simple manner. Our utilization of in-context learning also resembles [RWC+19], but here we systematically investigate various arrangements for learning within the context. As such, we will begin this section by explicitly delineating and differentiating the various configurations that we will evaluate GPT-3 on or that could hypothetically be used to evaluate GPT-3. These configurations can be considered as occupying a range regarding their typical dependence on task-specific data.","The basic pre-training method we employed, including the model architecture, training data, and training procedures, mostly matches what was documented in [RWC+19]. We scaled up the model size, training data volume and variety, and length of training in a fairly straightforward way. Our application of in-context learning is also similar to [RWC+19], but in this work we methodically test different settings for learning within the context. Therefore, we will start this section by clearly defining and contrasting the different arrangements that we will assess GPT-3 on or that could potentially be used to assess GPT-3. These arrangements can be viewed as spanning a spectrum regarding their common reliance on task-specific data.",A,1
Language Models are Few-Shot Learners,"We use the same model and architecture as GPT-2 [RWC+19], including the modified initialization, pre-normalization, and reversible tokenization described therein, with the exception that we use alternating dense and locally banded sparse attention patterns in the layers of the transformer, similar to the Sparse Transformer [CGRS19]. To study the dependence of ML performance on model size, we train 8 different sizes of model, ranging over three orders of magnitude from 125 million parameters to 175 billion parameters, with the last being the model we call GPT-3. Previous work [KMH+20] suggests that with enough training data, scaling of validation loss should be approximately a smooth power law as a function of size; training models of many different sizes allows us to test this hypothesis both for validation loss and for downstream language tasks.","Our model design and structure follows that of GPT-2, including the tweaked initialization, pre-normalization, and reversible tokenization, except that we utilize alternating dense and locally sparse attention patterns in the transformer layers akin to the Sparse Transformer. To analyze how machine learning performance changes with model size, we train 8 models ranging over 3 orders of magnitude from 125 million to 175 billion parameters, the largest being GPT-3. Prior work indicates that with sufficient training data, validation loss scaling should follow a smooth power law with size; training many differently sized models lets us validate this for both validation loss and language tasks.","We employ the same architecture and approach as GPT-2, replicating its adapted initialization, pre-normalization, and reversible tokenization, but use a mix of dense and locally sparse attention patterns in the transformer layers similar to the Sparse Transformer. To study how model performance varies by size, we train 8 models spanning 125 million to 175 billion parameters, the largest called GPT-3. Earlier work proposes that given enough training data, validation loss scaling should follow a smooth power law with size; by training many differently sized models we can test this hypothesis for both validation loss and language tasks.  ","Our model uses the GPT-2 architecture and methodology, including its modified initialization, pre-normalization, and reversible tokenization, except we utilize a combination of dense and locally sparse attention patterns in the transformer layers as in the Sparse Transformer. To examine the relationship between model size and performance, we train 8 models ranging from 125 million to 175 billion parameters, the largest being GPT-3. Previous research suggests that with sufficient training data, validation loss scaling should follow a smooth power law as model size increases; by training models across a wide range of sizes we can evaluate this for validation loss and language tasks.",A,1
Language Models are Few-Shot Learners,"However, we have found that unfiltered or lightly filtered versions of Common Crawl tend to have lower quality than more curated datasets. Therefore, we took 3 steps to improve the average quality of our datasets: (1) we downloaded and filtered a version of CommonCrawl based on similarity to a range of high-quality reference corpora, (2) we performed fuzzy deduplication at the document level, within and across datasets, to prevent redundancy and preserve the integrity of our held-out validation set as an accurate measure of overfitting, and (3) we also added known high-quality reference corpora to the training mix to augment CommonCrawl and increase its diversity.","But we discovered that the unprocessed or lightly processed versions of Common Crawl are often lower in quality than more carefully assembled datasets. So we took 3 actions to enhance the typical quality of our datasets: (1) we obtained and filtered a version of CommonCrawl based on its similarity to various high-quality benchmark corpora, (2) we carried out approximate deduplication at the document level, within and between datasets, to avoid repetition and keep our held-out validation set as a precise gauge of overfitting, and (3) we also incorporated known high-quality benchmark corpora into the training mix to supplement CommonCrawl and boost its diversity.","However, we found that the raw or lightly edited versions of Common Crawl tend to be inferior in quality compared to more selective datasets. Therefore, we implemented 3 procedures to improve the median quality of our datasets: (1) we accessed and filtered a variant of CommonCrawl based on closeness to several premium reference collections, (2) we executed fuzzy duplicate removal at the document level, internally and across datasets, to prevent redundancy and maintain our held-out validation set as an accurate evaluator of overfitting, and (3) we also added established high-quality reference collections to the training combination to enhance CommonCrawl and expand its diversity.","But we determined that the unprocessed or lightly handled versions of Common Crawl are frequently lower in quality versus more selectively compiled datasets. So we undertook 3 steps to enhance the typical quality of our datasets: (1) we sourced and filtered a variant of CommonCrawl based on its resemblance to various premier reference collections, (2) we performed approximate duplicate elimination at the document level, within and between datasets, to avoid repetition and retain our held-out validation set as a precise assessor of overfitting, and (3) we also incorporated established high-quality reference collections into the training amalgamation to augment CommonCrawl and broaden its diversity.",A,1
Language Models are Few-Shot Learners,"Details of the first two points (processing of Common Crawl) are described in Appendix A. For the third, we added several curated high-quality datasets, including an expanded version of the WebText dataset [RWC+19], collected by scraping links over a longer period of time, and first described in [KMH+20], two internet-based books corpora (Books1 and Books2) and English-language Wikipedia. Table 2.2 shows the final mixture of datasets that we used in training. The CommonCrawl data was downloaded from 41 shards of monthly CommonCrawl covering 2016 to 2019, constituting 45TB of compressed plaintext before filtering and 570GB after filtering, roughly equivalent to 400 billion byte-pair-encoded tokens.","The specifics of the initial two elements (handling of Common Crawl) are delineated in Appendix A. Regarding the third, we incorporated multiple hand-picked high-caliber data sets, encompassing an enlarged adaptation of the WebText collection [RWC+19], assembled by web scraping over a more extended timeframe, and initially portrayed in [KMH+20], two internet-sourced books groups (Books1 and Books2) and the English rendition of Wikipedia. Table 2.2 exhibits the definitive blend of data sets utilized for training. The CommonCrawl information was downloaded from 41 fractions of month to month CommonCrawl traversing 2016 to 2019, comprising 45TB of compacted plain content before sifting and 570GB after sifting, approximately comparable to 400 billion byte-pair-encoded tokens.","The particulars of the first pair of points (processing of Common Crawl) are laid out in Appendix A. Regarding the third, we added several carefully chosen top-notch datasets, including an expanded version of the WebText collection [RWC+19], assembled by extracting links over a longer time period, and first illustrated in [KMH+20], two internet-sourced books collections (Books1 and Books2) and the English language Wikipedia. Table 2.2 displays the final mixture of datasets used for training. The CommonCrawl data was obtained from 41 segments of monthly CommonCrawl covering 2016 to 2019, constituting 45TB of compressed plain text before filtering and 570GB after filtering, roughly equivalent to 400 billion byte-pair-encoded tokens.","The specifics of the initial two items (handling of Common Crawl) are described in Appendix A. As for the third, we incorporated multiple hand-selected high-quality datasets, including an extended version of the WebText set [RWC+19], gathered by web scraping over a longer timeframe, and first shown in [KMH+20], two internet-based books collections (Books1 and Books2) and the English Wikipedia. Table 2.2 exhibits the final blend of datasets used in training. The CommonCrawl data was downloaded from 41 portions of monthly CommonCrawl spanning 2016 to 2019, comprising 45TB of compressed plain text before filtering and 570GB after filtering, approximately equal to 400 billion byte-pair-encoded tokens.",A,1
Language Models are Few-Shot Learners,"Note that during training, datasets are not sampled in proportion to their size, but rather datasets we view as higher-quality are sampled more frequently, such that CommonCrawl and Books2 datasets are sampled less than once during training, but the other datasets are sampled 2-3 times. This essentially accepts a small amount of overfitting in exchange for higher quality training data.","Keep in mind that while training the model, we do not sample the datasets according to their size. Instead, we sample the datasets we deem higher quality more often. For example, we sample CommonCrawl and Books2 less than once during training, but sample the other datasets 2-3 times. This allows some overfitting in return for utilizing higher quality training data.","It's important to understand that the datasets are not sampled proportionally to their size during training. Rather, we sample datasets considered higher quality more frequently. For instance, CommonCrawl and Books2 are sampled less than once in training, while the other datasets are sampled 2-3 times. This effectively trades off a small amount of overfitting for access to superior training data. ","During model training, we do not sample the datasets evenly relative to their size. Instead, we sample datasets perceived as higher quality more often. Specifically, CommonCrawl and Books2 are sampled less than once in training, but the other datasets are sampled 2-3 times. This accepts some overfitting in order to leverage higher quality training information.",A,1
Language Models are Few-Shot Learners,"As found in [KMH+20, MKAT18], larger models can typically use a larger batch size, but require a smaller learning rate. We measure the gradient noise scale during training and use it to guide our choice of batch size [MKAT18]. Table 2.1 shows the parameter settings we used. To train the larger models without running out of memory, we use a mixture of model parallelism within each matrix multiply and model parallelism across the layers of the network. All models were trained on V100 GPU’s on part of a high-bandwidth cluster provided by Microsoft. Details of the training process and hyperparameter settings are described in Appendix B.","Research has shown that bigger models are often able to handle larger batch sizes, however they need smaller learning rates [KMH+20, MKAT18]. We evaluate the gradient noise during training which guides our selection of batch size [MKAT18]. Table 2.1 displays the parameters we utilized. To train the larger models without exhausting memory, we implement a combination of model parallelism within each matrix multiplication and across the neural network layers. All models were trained using V100 GPUs on part of a high-bandwidth cluster given by Microsoft. Information about the training procedure and hyperparameter configurations are available in Appendix B.","As demonstrated in prior work [KMH+20, MKAT18], models with more parameters can typically use larger batch sizes, but need smaller learning rates. We measure gradient noise during training to inform our batch size selection [MKAT18]. The settings we used are shown in Table 2.1. To prevent running out of memory when training the bigger models, we use both model parallelism within each matrix multiply and across the layers. The models were all trained on V100 GPUs provided by Microsoft as part of a high-bandwidth cluster. The training process and hyperparameter choices are detailed in Appendix B.  ","Existing research [KMH+20, MKAT18] has found that larger models can handle bigger batch sizes, but need smaller learning rates. We track gradient noise during training to guide batch size [MKAT18]. Our parameter settings are in Table 2.1. To train the larger models without memory issues, we use model parallelism in each matrix multiply and across layers. The models were trained on Microsoft's high-bandwidth cluster using V100 GPUs. Appendix B describes the training and hyperparameters.",A,1
Language Models are Few-Shot Learners,"For few-shot learning, we evaluate each example in the evaluation set by randomly drawing K examples from that task’s training set as conditioning, delimited by 1 or 2 newlines depending on the task. For LAMBADA and Storycloze there is no supervised training set available so we draw conditioning examples from the development set and evaluate on the test set. For Winograd (the original, not SuperGLUE version) there is only one dataset, so we draw conditioning examples directly from it.","To assess few-shot learning, we judge each case in the assessment set by arbitrarily selecting K instances from that task's preparation set as requirements, separated by 1 or 2 line breaks contingent upon the task. For LAMBADA and Storycloze there is no supervised preparation set accessible so we draw requiring examples from the improvement set and assess on the test set. For Winograd (the first, not SuperGLUE form) there is just a single dataset, so we directly draw requiring examples from it.","To evaluate few-shot learning, we appraise each sample in the evaluation collection by randomly choosing K samples from that task's training collection as conditions, demarcated by 1 or 2 line breaks based on the task. For LAMBADA and Storycloze there is no supervised training collection available so we extract conditioning samples from the development collection and evaluate on the test collection. For Winograd (the original, not SuperGLUE version) there is only one dataset, so we directly draw conditioning samples from it.","To test few-shot learning, we judge every case in the assessment set by randomly selecting K cases from that task's training set as prerequisites, separated by 1 or 2 line breaks depending on the task. For LAMBADA and Storycloze there is no supervised training set present so we take conditioning cases from the development set and assess on the test set. For Winograd (the initial, not SuperGLUE form) there is just one dataset, so we directly take conditioning cases from it.",A,1
Language Models are Few-Shot Learners,"K can be any value from 0 to the maximum amount allowed by the model’s context window, which is nctx = 2048 for all models and typically fits 10 to 100 examples. Larger values of K are usually but not always better, so when a separate development and test set are available, we experiment with a few values of K on the development set and then run the best value on the test set. For some tasks (see Appendix G) we also use a natural language prompt in addition to (or for K = 0, instead of) demonstrations.","The variable K is able to take on any whole number from 0 up to the model's context window maximum, which equals 2048 for all models and generally contains somewhere between 10 and 100 instances. More often than not, larger K values prove superior, so when distinct development and test sets exist, we try out a few K values on the development set and subsequently utilize the best value on the test set. For certain tasks (refer to Appendix G), we also employ a natural language prompt along with (or for K = 0, in place of) examples.","K can be any integer between 0 and the limit set by the model's context window, set at 2048 for all models, which tends to be enough for 10 to 100 samples. In most cases, bigger K is better, so with separate dev and test sets, we experiment with some K values on dev then use the best on test. For some tasks (see Appendix G), we also use a natural language prompt with (or instead of, if K=0) the examples.  ","The variable K is able to take on any integer value from 0 up to the maximum permitted by the model's context window, fixed at 2048 for all models, which is typically sufficient for 10 to 100 instances. Larger values of K are generally superior, so given distinct development and test sets, we try out several K values on development then utilize the optimal one for test. For certain tasks (refer Appendix G), we also utilize a natural language prompt along with (or instead of, if K = 0) demonstrations.",A,1
Language Models are Few-Shot Learners,"On tasks that involve binary classification, we give the options more semantically meaningful names (e.g. “True” or “False” rather than 0 or 1) and then treat the task like multiple choice; we also sometimes frame the task similar to what is done by [RSR+19] (see Appendix G) for details. On tasks with free-form completion, we use beam search with the same parameters as [RSR+19]: a beam width of 4 and a length penalty of α = 0.6. We score the model using F1 similarity score, BLEU, or exact match, depending on what is standard for the dataset at hand. Final results are reported on the test set when publicly available, for each model size and learning setting (zero-, one-, and few-shot).","For tasks involving binary classification, we utilize more meaningful names for the options (e.g. ""True"" or ""False"" instead of 0 or 1) and approach it like multiple choice. We also sometimes structure the task similar to the method used in [RSR+19] (see Appendix G). For open-ended completion tasks, we implement beam search with the same settings as [RSR+19]: a beam width of 4 and length penalty of α = 0.6. We evaluate the model using F1 score, BLEU, or exact match, depending on the standard for the given dataset. We report final results on the test set when available, for each model size and learning configuration (zero-, one-, and few-shot).","On binary classification tasks, we use more semantically meaningful option names (for example ""True"" or ""False"" instead of 0 or 1) and frame it as multiple choice. We also sometimes structure the task akin to the approach described in [RSR+19] (see Appendix G). For free-form completion tasks, we utilize beam search with the same hyperparameters as [RSR+19]: beam width of 4 and length penalty α = 0.6. We assess the model using F1, BLEU, or exact match, based on the standard for the dataset. We present final results on the test set when public, for each model size and learning setting (zero-, one-, and few-shot).","For binary classification tasks, we utilize more meaningful option names (such as ""True"" or ""False"" instead of 0 or 1) and frame it as multiple choice. We also occasionally structure the task similar to the method in [RSR+19] (refer to Appendix G). On free-form completion tasks, we employ beam search with the same settings as [RSR+19]: beam width of 4 and length penalty α = 0.6. We evaluate the model using F1, BLEU, or exact match score based on the standard for the given dataset. We report final results on the test set when available for each model size and learning configuration (zero-, one-, and few-shot).",A,1
Language Models are Few-Shot Learners,"In Figure 3.1 we display training curves for the 8 models described in Section 2. For this graph we also include 6 additional extra-small models with as few as 100,000 parameters. As observed in [KMH+20], language modeling performance follows a power-law when making efficient use of training compute. After extending this trend by two more orders of magnitude, we observe only a slight (if any) departure from the power-law. One might worry that these improvements in cross-entropy loss come only from modeling spurious details of our training corpus.","The graph in Figure 3.1 shows the training progress for the 8 models talked about in Section 2, as well as 6 smaller models with only 100,000 parameters. As noted in [KMH+20], language model performance increases rapidly as more computing power is used efficiently. After expanding this trend by 100 times, we see little or no deviation from the rapid increase. One concern is that these gains in reducing cross-entropy loss are just from modeling unimportant details of the training data.","In the graph of Figure 3.1, we have plotted the training improvements over time for the 8 models discussed in Section 2, plus 6 additional very small models with only 100,000 parameters. As shown in [KMH+20], language modeling ability grows extremely quickly if training computation is utilized efficiently. After prolonging this rapid growth trend by two more orders of magnitude, we observe minimal or no slowing of the rapid improvement. There could be worries that these better cross-entropy losses merely come from modeling irrelevant quirks of our training set.","The chart in Figure 3.1 displays the training progress curves for the 8 models covered in Section 2, as well as 6 extra tiny models with only 100,000 parameters. As demonstrated in [KMH+20], language model performance increases rapidly according to a power-law when training computation is used efficiently. After expanding this rapid growth trend by 100 times, we see little or no decrease in the speed of improvement. One possible concern is that these cross-entropy loss gains are simply from modeling unimportant peculiarities of our training corpus.",A,1
Language Models are Few-Shot Learners,"When the test set is private, our model is often too large to fit on the test server, so we report results on the development set. We do submit to the test server on a small number of datasets (SuperGLUE, TriviaQA, PiQa) where we were able to make submission work, and we submit only the 200B few-shot results, and report development set results for everything else.","Since the test data is not public, our system is frequently too large to run on the test platform. Thus, we present findings on the dev set. We did upload to the test platform on a few datasets (SuperGLUE, TriviaQA, PiQa) where we managed to get submission working, and we only submit the 200B few-shot outputs, and show dev set outputs for all other cases.","When the evaluation data is private, our algorithm is often too big to execute on the evaluation server, so we document performance on the development set. We did manage to submit to the evaluation server on a small number of datasets (SuperGLUE, TriviaQA, PiQa) where we got submission functional, and we only submit the 200B few-shot results, and document development set results for everything else.","Since the test data remains confidential, our system is regularly too large to fit on the test computer, therefore we report metrics on the dev set. We did upload to the test computer for a few datasets (SuperGLUE, TriviaQA, PiQa) where we got submission working, and we only submit the 200B few-shot outputs, and show dev set outputs for all other experiments.",A,1
Language Models are Few-Shot Learners,"However, we will see in the following sections that improvements in cross-entropy loss lead to consistent performance gains across a broad spectrum of natural language tasks. Below, we evaluate the 8 models described in Section 2 (the 175 billion parameter parameter GPT-3 and 7 smaller models) on a wide range of datasets. We group the datasets into 9 categories representing roughly similar tasks. In Section 3.1 we evaluate on traditional language modeling tasks and tasks that are similar to language modeling, such as Cloze tasks and sentence/paragraph completion tasks.","Nevertheless, the next parts will demonstrate that enhancements in cross-entropy loss result in steady performance increases across a wide range of natural language assignments. Underneath, we assess the 8 models illustrated in Section 2 (the 175 billion parameter GPT-3 and 7 smaller models) on a broad collection of data sets. We categorize the data sets into 9 groups symbolizing approximately comparable assignments. In Section 3.1 we evaluate on conventional language modeling tasks and assignments that are akin to language modeling, like Cloze tasks and sentence/paragraph completion tasks.","However, as will be shown in the upcoming segments, refinements to cross-entropy loss lead to reliable gains in performance over a diverse array of natural language jobs. In the following, we review the 8 systems outlined in Section 2 (the 175 billion parameter GPT-3 and 7 smaller systems) across a wide selection of data sets. We separate the data sets into 9 categories embodying roughly comparable jobs. In Section 3.1 we measure on standard language modeling jobs and jobs that resemble language modeling, such as Cloze jobs and sentence/paragraph completion jobs.  ","Nonetheless, the forthcoming portions will demonstrate that improvements to cross-entropy loss result in steady gains in performance over a wide spectrum of natural language work. Below, we assess the 8 models described in Section 2 (the 175 billion parameter GPT-3 and 7 smaller models) across a broad array of data sets. We categorize the data sets into 9 groups representing approximately similar work. In Section 3.1 we evaluate on conventional language modeling work and work that is comparable to language modeling, such as Cloze work and sentence/paragraph completion work.",A,1
Language Models are Few-Shot Learners,In Section 3.2 we evaluate on “closed book” question answering tasks: tasks which require using the information stored in the model’s parameters to answer general knowledge questions. In Section 3.3 we evaluate the model’s ability to translate between languages (especially one-shot and few-shot). In Section 3.4 we evaluate the model’s performance on Winograd Schema-like tasks. In Section 3.5 we evaluate on datasets that involve commonsense reasoning or question answering.,"In Part 3.2 we assess performance on question answering tasks where external information cannot be utilized and answers must be deduced from what the model has learned. In Part 3.3 we measure how well the model can translate text between languages, especially with limited examples. In Part 3.4 we test the model on tasks requiring resolution of ambiguities like the Winograd Schema. In Part 3.5 we evaluate the model's skill at common sense reasoning and answering questions.","Section 3.2 looks at the model's ability to answer general knowledge questions using only its stored knowledge, without external information. Section 3.3 examines one-shot and few-shot translation between languages. Section 3.4 focuses on performance on Winograd Schema-style problems that involve resolving ambiguities. Section 3.5 evaluates commonsense reasoning and question answering. ","In portion 3.2 we review performance on question answering where no outside info can be used, only what the model has learned. In portion 3.3 we assess translation, especially with minimal samples. In portion 3.4 we check abilities on ambiguity resolution tasks like Winograd Schema. In portion 3.5 we test commonsense reasoning and answering questions.",A,1
Language Models are Few-Shot Learners,"In Section 3.6 we evaluate on reading comprehension tasks, in Section 3.7 we evaluate on the SuperGLUE benchmark suite, and in 3.8 we briefly explore NLI. Finally, in Section 3.9, we invent some additional tasks designed especially to probe in-context learning abilities – these tasks focus on on-the-fly reasoning, adaptation skills, or open-ended text synthesis. We evaluate all tasks in the few-shot, one-shot, and zero-shot settings.","In part 3.6 we assess performance on reading comprehension activities, in part 3.7 we assess performance on the SuperGLUE benchmark collection, and in 3.8 we briefly investigate NLI. Lastly, in part 3.9, we create some extra activities particularly intended to analyze in-context learning capabilities – these activities concentrate on spontaneous reasoning, adaptation abilities, or open-ended text creation. We assess all activities in the few-shot, one-shot, and zero-shot configurations.","In section 3.6 we appraise on reading understanding assignments, in section 3.7 we appraise on the SuperGLUE benchmark suite, and in 3.8 we briefly explore NLI. Finally, in section 3.9, we invent some additional assignments designed specifically to probe in-context learning skills – these assignments focus on on-the-fly reasoning, adaptation abilities, or open-ended text synthesis. We evaluate all assignments in the few-shot, one-shot, and zero-shot settings.","In portion 3.6 we judge performance on reading comprehension tests, in portion 3.7 we judge performance on the SuperGLUE benchmark collection, and in 3.8 we briefly investigate NLI. At last, in portion 3.9, we create some extra tests particularly meant to analyze in-context learning capabilities – these tests concentrate on spontaneous reasoning, adaptation skills, or open-ended text creation. We judge all tests in the few-shot, one-shot, and zero-shot arrangements.",A,1
Language Models are Few-Shot Learners,"LAMBADA is also a demonstration of the flexibility of few-shot learning as it provides a way to address a problem that classically occurs with this dataset. Although the completion in LAMBADA is always the last word in a sentence, a standard language model has no way of knowing this detail. It thus assigns probability not only to the correct ending but also to other valid continuations of the paragraph. This problem has been partially addressed in the past with stop-word filters [RWC+19] (which ban “continuation” words). The few-shot setting instead allows us to “frame” the task as a cloze-test and allows the language model to infer from examples that a completion of exactly one word is desired.","LAMBADA also shows the adaptability of quick learning as it gives a way to tackle an issue that typically happens with this data. Even though the fill-in-the-blank in LAMBADA is constantly the final word in a sentence, a normal language model has no means of knowing this fact. So it assigns likelihood not just to the right ending but also to other valid ways to continue the section. This problem has been somewhat solved before with stop-word filters [RWC+19] (which prohibit ""continuation"" words). The few-shot framework instead enables us to ""present"" the task as a cloze test and allows the language model to deduce from instances that a completion of precisely one word is wanted.","LAMBADA also demonstrates the flexibility of rapid learning as it provides a method to address a dilemma that traditionally occurs with this dataset. Despite the fact that the completion in LAMBADA is always the last word in a sentence, a standard language model has no way of being aware of this detail. It thus assigns probability not only to the correct ending but also to other valid progressions of the paragraph. This issue has been partially tackled in the past with stop-word filters [RWC+19] (which ban ""continuation"" words). The few-shot setting instead allows us to ""frame"" the task as a fill-in-the-blank test and allows the language model to infer from examples that a completion of exactly one word is desired.  ","LAMBADA also shows the adaptability of quick learning as it gives a technique to handle a problem that typically happens with this dataset. Even with the completion in LAMBADA always being the final word in a sentence, a normal language model has no means of knowing this fact. So it assigns likelihood not just to the accurate ending but also to other valid continuations of the section. This issue has been partially addressed before with stop-word filters [RWC+19] (which prohibit ""continuation"" words). The few-shot framework instead enables us to ""present"" the task as a fill-in-the-blank test and allows the language model to deduce from examples that a completion of precisely one word is wanted.",A,1
Language Models are Few-Shot Learners,"When presented with examples formatted this way, GPT-3 achieves 86.4% accuracy in the few-shot setting, an increase of over 18% from the previous state-of-the-art. We observe that few-shot performance improves strongly with model size. While this setting decreases the performance of the smallest model by almost 20%, for GPT-3 it improves accuracy by 10%. Finally, the fill-in-blank method is not effective one-shot, where it always performs worse than the zero-shot setting. Perhaps this is because all models still require several examples to recognize the pattern.","After being shown instances arranged in this style, GPT-3 has 86.4% precision with minimal preparation, an enhancement of over 18% compared to the preceding best result. We see that capability with little groundwork rises markedly with model extent. Although this configuration drops the smallest model's performance by practically 20%, for GPT-3 it lifts accuracy by 10%. Lastly, the fill-in-the-blank approach is ineffective with one instance, where it always does worse than with no examples. This could be because all models still need multiple illustrations to identify the pattern.","When given examples in this format, GPT-3 reaches 86.4% accuracy with a small number of examples, an improvement of over 18% over the previous highest result. We find that performance with few examples increases substantially as model size grows. While this setting lowers the smallest model's performance by nearly 20%, for GPT-3 it boosts accuracy by 10%. Finally, the fill-in-the-blank method does not work well with just one example, where it always underperforms compared to having zero examples. This is likely because all models still need several examples to learn the pattern.","Presented with instances structured in this way, GPT-3 attains 86.4% precision with minimal preparation, a rise of over 18% from the earlier best. We discern that capability with scarce groundwork ascends markedly with model extent. Although this configuration diminishes the smallest model's performance by almost 20%, for GPT-3 it elevates accuracy by 10%. Lastly, the fill-in-the-blank technique is ineffective with a single case, where it always fares worse than with no precedents. This could be owing to all models still necessitating numerous precedents to ascertain the pattern.",A,1
Language Models are Unsupervised Multitask Learners,"Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset - matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples.","Language processing jobs like responding to questions, converting one language to another, understanding text that is read, and summarizing are usually done by supervised learning on specific datasets for each task. We show that models for language start to learn these jobs without any direct oversight when they are trained on a new dataset of millions of webpages called WebText. When given a document and questions, the answers created by the language model get 55 F1 on the CoQA dataset - equaling or surpassing the performance of 3 out of 4 baseline systems without using the 127,000+ training samples.","Tasks in natural language processing such as providing answers to questions, translating between languages, comprehending text that has been read, and summarizing text are commonly handled using supervised machine learning techniques on datasets tailored to each specific task. Our research demonstrates that language models are able to begin learning how to perform these tasks without any explicit supervision when trained on a novel dataset containing millions of webpages known as WebText. When given a document and questions as input, the language model achieves 55 F1 score on the CoQA benchmark dataset - matching or outperforming 3 out of the 4 baseline systems without requiring the 127,000+ training examples.  ","Jobs in processing natural language like answering questions, translating between languages, understanding text that was read, and summarizing text are usually tackled using supervised learning on datasets particular to each job. Our work shows that models for language can start learning these jobs without any direct supervision when trained on a new dataset with millions of webpages called WebText. When provided a document and questions, the answers produced by the language model achieve 55 F1 score on the CoQA dataset - equaling or exceeding the performance of 3 of the 4 baseline systems without needing the 127,000+ training instances.",A,1
Language Models are Unsupervised Multitask Learners,"The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.","The ability of the language model to understand and generate natural language is critical for accomplishing new tasks without additional training data, and expanding its capacity leads to better performance on tasks in a logarithmic way. Our most advanced model, GPT-2, uses 1.5 billion parameters in a Transformer architecture to set new benchmarks on 7 out of 8 language modeling datasets we tested without any task-specific fine-tuning. But there is still room for improvement on modeling a large web text corpus. Text samples from GPT-2 showcase these advancements and include coherent multi-sentence passages. These results indicate a promising approach to creating language systems that can learn to carry out tasks just from seeing examples in naturally occurring text.","The language comprehension and generation strengths of the model are indispensable for successfully transferring to new tasks without extra training, and increasing these strengths boosts task performance exponentially. GPT-2, our biggest model with 1.5 billion Transformer parameters, establishes state-of-the-art results on 7 of 8 language modeling datasets we evaluated in a zero-shot context, but still has difficulty fully modeling WebText. Text excerpts from GPT-2 reflect these enhancements and have coherent paragraph structure. These findings point to a promising method for developing language processing systems capable of learning tasks from demonstrations found in natural text.","The language model's capacity to understand and generate text is vital to accomplishing zero-shot task transfer, and expanding it improves performance across tasks in a logarithmic fashion. GPT-2, our largest 1.5 billion parameter Transformer model, achieves best-in-class results on 7 of 8 language modeling datasets we tested in a zero-shot environment, but still struggles to fully capture WebText. Text samples from GPT-2 exhibit these improvements and contain coherent multi-sentence passages. These results suggest a promising approach to building language systems that learn to perform tasks solely from naturally occurring examples in text.",A,1
Language Models are Unsupervised Multitask Learners,"Machine learning systems now excel (in expectation) at tasks they are trained for by using a combination of large datasets, high-capacity models, and supervised learning (Krizhevsky et al., 2012) (Sutskever et al., 2014) (Amodei et al., 2016). Yet these systems are brittle and sensitive to slight changes in the data distribution (Recht et al., 2018) and task specification (Kirkpatrick et al., 2017). Current systems are better characterized as narrow experts rather than competent generalists. We would like to move towards more general systems which can perform many tasks – eventually without the need to manually create and label a training dataset for each one.","Machine learning models have gotten very good at carrying out the tasks they are trained to do using big data sets, complex models, and supervised learning (Krizhevsky et al., 2012) (Sutskever et al., 2014) (Amodei et al., 2016). However, these models are fragile and can break down with even small changes to the data or task (Recht et al., 2018) (Kirkpatrick et al., 2017). The current systems are more like specialists focused on a narrow area rather than flexible generalists. Our goal should be to develop more versatile systems that can perform many tasks without needing large labeled training sets for each one.","Machine learning algorithms today excel at the specific jobs they are trained for, utilizing large training sets, high-capacity architectures, and supervised learning techniques (Krizhevsky et al., 2012) (Sutskever et al., 2014) (Amodei et al., 2016). But they are not robust, and can fail with small deviations in data or task (Recht et al., 2018) (Kirkpatrick et al., 2017). Existing systems are narrow experts, not competent generalists. Our aim should be more flexible systems that can do many tasks, eventually without manually labeling training data for each task. ","Modern machine learning models have become very capable at specialized tasks they are trained on, by leveraging big training datasets, complex models, and supervised learning (Krizhevsky et al., 2012) (Sutskever et al., 2014) (Amodei et al., 2016). However, they are fragile, and break with even minor changes to data or task (Recht et al., 2018) (Kirkpatrick et al., 2017). Today's systems are narrow specialists, not versatile generalists. We want to enable more flexible systems that can perform many tasks, eventually without needing labeled training data for each one.",A,1
Language Models are Unsupervised Multitask Learners,"The dominant approach to creating ML systems is to collect a dataset of training examples demonstrating correct behavior for a desired task, train a system to imitate these behaviors, and then test its performance on independent and identically distributed (IID) held-out examples. This has served well to make progress on narrow experts. But the often erratic behavior of captioning models (Lake et al., 2017), reading comprehension systems (Jia & Liang, 2017), and image classifiers (Alcorn et al., 2018) on the diversity and variety of possible inputs highlights some of the shortcomings of this approach.","The most common way to build ML systems is to gather many labeled examples that show the right actions for a task, use those to train a system to copy that behavior, and then evaluate how well it can generalize to new unseen data from the same distribution. This works okay for specialized systems. But the unpredictable responses of image captioners, reading comprehension AI, and object classifiers on the wide range of inputs shows some weaknesses of only using this method.","The predominant method for developing ML models is collecting a set of training samples that demonstrate the desired conduct for a task, using those to teach a system to imitate that conduct, then assessing its skill on new identical data. This has been effective for narrow systems. However, the frequently bizarre actions of systems for image captioning, reading comprehension, and classification on the variety of possible inputs highlights some constraints of this tactic.","The most widespread technique for building ML models is accumulating a dataset of labeled instances that exhibit appropriate behavior for a task, utilizing those to get a system to replicate that behavior, then evaluating its ability on fresh identical data. This has worked for specialized systems. But the often peculiar responses of systems for image description, reading understanding, and categorization on the diversity of potential inputs demonstrates some limitations of this approach.",A,1
Language Models are Unsupervised Multitask Learners,"Our suspicion is that the prevalence of single task training on single domain datasets is a major contributor to the lack of generalization observed in current systems. Progress towards robust systems with current architectures is likely to require training and measuring performance on a wide range of domains and tasks. Recently, several benchmarks have been proposed such as GLUE (Wang et al., 2018) and decaNLP (McCann et al., 2018) to begin studying this. Multitask learning (Caruana, 1997) is a promising framework for improving general performance. However, multitask training in NLP is still nascent.","We believe that the common practice of training only on single tasks with data from one domain is a big reason why current systems fail to generalize well. For current architectures to make progress towards robust systems, training and testing across diverse tasks and domains will likely be necessary. Some benchmarks like GLUE (Wang et al., 2018) and decaNLP (McCann et al., 2018) have recently been introduced to start examining this. Multitask learning (Caruana, 1997) seems like a good framework to improve general performance, but multitask training in NLP is still in early stages.","Our view is that focusing training on individual tasks using data from solitary domains greatly contributes to the poor generalization exhibited by present systems. With existing architectures, advancing towards robust systems will probably need training and assessment over many domains and tasks. A few benchmarks such as GLUE (Wang et al., 2018) and decaNLP (McCann et al., 2018) have recently been put forward to begin analyzing this. Multitask learning (Caruana, 1997) is a promising approach to enhance overall performance, however multitask training in NLP remains in early phases.  ","We think that the habit of training only on single tasks using data from one domain plays a big role in the lack of generalization seen in today's systems. For current architectures to make progress towards robust systems, training and evaluating across diverse tasks and domains is likely necessary. Some benchmarks such as GLUE (Wang et al., 2018) and decaNLP (McCann et al., 2018) have been proposed recently to start investigating this. Multitask learning (Caruana, 1997) seems a good way to improve general performance, but multitask training in NLP is still new.",A,1
Language Models are Unsupervised Multitask Learners,"Recent work reports modest performance improvements (Yogatama et al., 2019) and the two most ambitious efforts to date have trained on a total of 10 and 17 (dataset, objective) pairs respectively (McCann et al., 2018) (Bowman et al., 2018). From a meta-learning perspective, each (dataset, objective) pair is a single training example sampled from the distribution of datasets and objectives. Current ML systems need hundreds to thousands of examples to induce functions which generalize well. This suggests that multitask training many need just as many effective training pairs to realize its promise with current approaches. It will be very difficult to continue to scale the creation of datasets and the design of objectives to the degree that may be required to brute force our way there with current techniques.","The latest research shows small gains in performance (Yogatama et al., 2019) and the two most ambitious attempts so far have practiced on a sum of 10 and 17 (dataset, goal) pairs respectively (McCann et al., 2018) (Bowman et al., 2018). From a meta-learning view, each (dataset, goal) pair is a single practice case taken from the distribution of data and aims. Current ML systems need hundreds to thousands of examples to learn functions that generalize well. This implies that multitask training may need just as many effective training pairs to achieve its potential with current methods. It will be very tough to keep increasing the creation of data and the design of goals to the degree that may be required to get there by brute force with current techniques.","Recent studies demonstrate modest enhancements in results (Yogatama et al., 2019) and the two most ambitious tries thus far have practiced on a total of 10 and 17 (dataset, purpose) pairs respectively (McCann et al., 2018) (Bowman et al., 2018). From a meta-learning angle, each (dataset, purpose) pair is a single practice example sampled from the distribution of data and purposes. Current ML systems require hundreds to thousands of examples to induce functions that generalize well. This hints that multitask training may need just as many effective training pairs to realize its promise with current methods. It will be very difficult to continue scaling the creation of data and the design of purposes to the degree that may be required to get there through brute force with current techniques.","The latest work shows small gains in performance (Yogatama et al., 2019) and the two most ambitious attempts so far have trained on a total of 10 and 17 (dataset, goal) pairs respectively (McCann et al., 2018) (Bowman et al., 2018). From a meta-learning view, each (dataset, goal) pair is a single training case taken from the distribution of data and goals. Current ML systems need hundreds to thousands of examples to learn functions that generalize well. This implies that multitask training may need just as many effective training pairs to realize its potential with current approaches. It will be very tough to keep increasing the creation of data and the design of goals to the degree that may be required to get there through brute force with current techniques.",A,1
Language Models are Unsupervised Multitask Learners,"The current best performing systems on language tasks utilize a combination of pre-training and supervised finetuning. This approach has a long history with a trend towards more flexible forms of transfer. First, word vectors were learned and used as inputs to task-specific architectures (Mikolov et al., 2013) (Collobert et al., 2011), then the contextual representations of recurrent networks were transferred (Dai & Le, 2015) (Peters et al., 2018), and recent work suggests that task-specific architectures are no longer necessary and transferring many self-attention blocks is sufficient (Radford et al., 2018) (Devlin et al., 2018).","The systems that currently achieve the best results on language-related tasks make use of both pre-training and fine-tuning with supervision. This method has a long history, with a tendency toward more flexible forms of transfer learning. Initially, word vectors were learned and fed as inputs into architectures designed for specific tasks (Mikolov et al., 2013) (Collobert et al., 2011). Then, the contextual representations of recurrent networks were transferred (Dai & Le, 2015) (Peters et al., 2018). Recent work indicates task-specific architectures are no longer needed, and transferring numerous self-attention blocks is sufficient (Radford et al., 2018) (Devlin et al., 2018).","The top-performing systems today for language tasks use both pre-training and supervised fine-tuning. This approach has a long tradition, trending toward more adaptable types of transfer learning. First, word embeddings were created and used as inputs to architectures built for particular tasks (Mikolov et al., 2013) (Collobert et al., 2011). Next, the contextual representations of recurrent networks were transferred (Dai & Le, 2015) (Peters et al., 2018). Recent research shows task-specific architectures are unnecessary, and transferring many self-attention blocks is enough (Radford et al., 2018) (Devlin et al., 2018).  ","The currently best systems for language tasks employ both pre-training and supervised fine-tuning. This method has a long history, with a tendency toward more flexible forms of transfer learning. Initially, word vectors were learned and provided as inputs to task-specific models (Mikolov et al., 2013) (Collobert et al., 2011). After that, the contextual representations of recurrent networks were transferred (Dai & Le, 2015) (Peters et al., 2018). Recent studies indicate task-specific models are no longer needed, and transferring numerous self-attention blocks suffices (Radford et al., 2018) (Devlin et al., 2018).",A,1
Language Models are Unsupervised Multitask Learners,"These methods still require supervised training in order to perform a task. When only minimal or no supervised data is available, another line of work has demonstrated the promise of language models to perform specific tasks, such as commonsense reasoning (Schwartz et al., 2017) and sentiment analysis (Radford et al., 2017). In this paper, we connect these two lines of work and continue the trend of more general methods of transfer. We demonstrate language models can perform down-stream tasks in a zero-shot setting – without any parameter or architecture modification. We demonstrate this approach shows potential by highlighting the ability of language models to perform a wide range of tasks in a zero-shot setting.","These techniques still need supervised learning to carry out a job. When there is little or no supervised information available, other research has shown the potential of language models to do certain jobs, like commonsense reasoning (Schwartz et al., 2017) and sentiment analysis (Radford et al., 2017). In this paper, we connect these two areas of research and continue the trend of more general transfer methods. We show language models can do downstream tasks with zero-shot learning - with no parameter or architecture changes. We highlight the potential of this approach by showing language models can do a wide range of tasks with zero-shot learning.","These approaches still require labeled data for training to execute a task. With minimal or no labeled data, other work has exhibited the promise of language models for specific tasks, such as commonsense reasoning (Schwartz et al., 2017) and sentiment analysis (Radford et al., 2017). Here, we link these two threads of work and extend the trend towards more general transfer methods. We demonstrate language models are capable of downstream tasks in a zero-shot setting - with no parameter or architecture adjustments. We exhibit the potential of this approach by underscoring language models' ability to perform diverse tasks in a zero-shot fashion.  ","These techniques still need supervised examples to learn how to complete a task. When there are few or no supervised cases available, other studies have shown the potential of language models for certain tasks, like commonsense reasoning (Schwartz et al., 2017) and sentiment analysis (Radford et al., 2017). In this paper, we connect these two areas of work and push forward the trend of more general transfer methods. We show language models can accomplish downstream tasks with zero-shot learning - with no changes to parameters or architecture. We highlight the promise of this approach by emphasizing language models' capacity to handle a wide variety of tasks in a zero-shot manner.",A,1
Language Models are Unsupervised Multitask Learners,"Learning to perform a single task can be expressed in a probabilistic framework as estimating a conditional distribution p(output|input). Since a general system should be able to perform many different tasks, even for the same input, it should condition not only on the input but also on the task to be performed. That is, it should model p(output|input, task). This has been variously formalized in multitask and meta-learning settings. Task conditioning is often implemented at an architectural level, such as the task specific encoders and decoders in (Kaiser et al., 2017) or at an algorithmic level such as the inner and outer loop optimization framework of MAML (Finn et al., 2017).","Acquiring the skills to carry out a solitary chore can be depicted using probability theory as approximating a conditional distribution p(result|data). Since a universal structure ought to be capable of executing numerous distinct chores, even for the same input, it should be conditional not solely on the input but also on the chore to be executed. That is, it should exemplify p(result|data, chore). This has been articulated in various ways in multitask and meta-learning contexts. Chore conditioning is frequently actualized at an architectural level, like the chore particular encoders and decoders in (Kaiser et al., 2017) or at an algorithmic level like the inner and outer loop enhancement structure of MAML (Finn et al., 2017).","Learning how to complete a single job can be modeled mathematically as estimating a conditional probability p(output|input). Because a general-purpose system should be able to perform many different jobs, even for the same input, it should take into account not just the input but also the specific job to be done. In other words, it should model p(output|input, job). This idea has been formalized in various ways in multi-task and meta-learning settings. Conditioning on the job is often implemented architecturally, as with the job-specific encoders and decoders in (Kaiser et al., 2017), or algorithmically, as with the inner and outer loop optimization approach of MAML (Finn et al., 2017).","Grasping how to execute a solitary assignment can be depicted probabilistically as approximating a conditional distribution p(consequence|information). Since an all-purpose structure ought to have the capacity to play out various distinctive assignments, even for a similar information, it ought to condition not just on the information yet in addition on the assignment to be performed. That is, it ought to demonstrate p(consequence|information, assignment). This has been formalized in various ways in multitask and meta-learning situations. Task conditioning is frequently executed designally, for example, the assignment particular encoders and decoders in (Kaiser et al., 2017) or algorithmically, for example, the internal and outer circle enhancement structure of MAML (Finn et al., 2017).",A,1
Language Models are Unsupervised Multitask Learners,"But as exemplified in McCann et al. (2018), language provides a flexible way to specify tasks, inputs, and outputs all as a sequence of symbols. For example, a translation training example can be written as the sequence (translate to french, english text, french text). Likewise, a reading comprehension training example can be written as (answer the question, document, question, answer). McCann et al. (2018) demonstrated it was possible to train a single model, the MQAN, to infer and perform many different tasks on examples with this type of format.","However, as shown in McCann et al. (2018), language gives a versatile approach to characterize tasks, inputs, and outputs all as a progression of symbols. For instance, a translation preparing model can be composed as the arrangement (decipher into french, english content, french content). Also, a perusing comprehension preparing model can be composed as (reply the inquiry, report, question, answer). McCann et al. (2018) showed it was conceivable to prepare a solitary model, the MQAN, to induce and play out numerous unique errands on models with this organization.","But as exhibited in McCann et al. (2018), language gives a flexible method to determine tasks, information sources, and yields all as a grouping of images. As a model, a translation preparing model can be communicated as the course of action (interpret into french, english substance, french substance). Also, a comprehension preparing model can be communicated as (answer the request, report, question, answer). McCann et al. (2018) showed it was conceivable to prepare a single model, the MQAN, to derive and perform various unique errands on models with this arrangement. ","In any case, as shown in McCann et al. (2018), language gives an adaptable method to characterize undertakings, information sources, and yields all as an arrangement of images. For instance, a translation preparing model can be composed as the grouping (interpret into french, english content, french content). Also, a comprehension preparing model can be composed as (answer the inquiry, archive, question, answer). McCann et al. (2018) showed it was conceivable to prepare a solitary model, the MQAN, to induce and play out various novel errands on models with this organization.",A,1
Language Models are Unsupervised Multitask Learners,"Language modeling is also able to, in principle, learn the tasks of McCann et al. (2018) without the need for explicit supervision of which symbols are the outputs to be predicted. Since the supervised objective is the same as the unsupervised objective but only evaluated on a subset of the sequence, the global minimum of the unsupervised objective is also the global minimum of the supervised objective. In this slightly toy setting, the concerns with density estimation as a principled training objective discussed in (Sutskever et al., 2015) are side stepped. The problem instead becomes whether we are able to, in practice, optimize the unsupervised objective to convergence.","Language models can theoretically learn the tasks of McCann et al. (2018) without needing explicit guidance on which symbols to predict as outputs. Since the supervised and unsupervised goals are identical except the supervised evaluates a sequence subset, the unsupervised minimum is the supervised minimum too. In this somewhat simplistic setting, issues with density estimation as a principled training aim (Sutskever et al., 2015) are avoided. The issue becomes whether we can optimize the unsupervised aim fully in practice.","Language models are capable, in theory, of acquiring the tasks of McCann et al. (2018) with no explicit teaching of which symbols are the outputs to be foreseen. Because the supervised purpose equals the unsupervised purpose but is only appraised on a sequence portion, the global bottom of the unsupervised purpose is also the global bottom of the supervised purpose. In this somewhat basic setting, the concerns with density approximation as a principled training intention discussed in (Sutskever et al., 2015) are bypassed. The problem instead becomes whether we can, in practice, enhance the unsupervised intention to completion. ","Language models can learn the tasks of McCann et al. (2018) in principle without needing clear instruction on which symbols are the outputs to be predicted. Since the supervised goal matches the unsupervised goal but is only measured on part of the sequence, the overall minimum of the unsupervised goal also minimizes the supervised goal. In this somewhat simplified case, issues with density estimation as a sound training aim (Sutskever et al., 2015) are avoided. The question becomes whether we can fully optimize the unsupervised aim in practice.",A,1
Language Models are Unsupervised Multitask Learners,"Preliminary experiments confirmed that sufficiently large language models are able to perform multitask learning in this toy-ish setup but learning is much slower than in explicitly supervised approaches. While it is a large step from the well-posed setup described above to the messiness of “language in the wild”, Weston (2016) argues, in the context of dialog, for the need to develop systems capable of learning from natural language directly and demonstrated a proof of concept – learning a QA task without a reward signal by using forward prediction of a teacher’s outputs. While dialog is an attractive approach, we worry it is overly restrictive.","Initial tests showed that large enough language models can carry out multitask learning in this simplistic configuration, but acquiring knowledge occurs much more slowly than with explicit supervision. Although there is a considerable gap between the well-defined scenario stated above and the complexity of natural language, Weston (2016) contends, regarding dialog, that systems able to learn directly from natural language without rewards are necessary, and provided a demonstration - acquiring a QA task without a reward signal by predicting a teacher's outputs. However, while dialog is appealing, we are concerned it is excessively limiting.","Early experiments proved sufficiently large language models can do multi-task learning in this basic setup, however learning is far slower versus explicitly supervised techniques. While there is a huge difference between the clear-cut case described previously and the chaos of ""real world language"", Weston (2016) argues, in dialog's context, systems that can learn straight from natural language sans rewards are required, and showed a proof of concept - learning a QA task sans a reward signal by forecasting a teacher's outputs. However, even though dialog is attractive, we worry it's overly constraining.  ","Initial trials showed adequately large language models are capable of multi-task learning in this simplified arrangement, but acquiring knowledge is much slower compared to explicit supervision. Although there is a massive gap between the well-defined scenario stated before and the turmoil of ""language in the wild"", Weston (2016) contends, regarding dialog, systems able to learn directly from natural language without rewards are needed, and exhibited a proof of concept - learning a QA task without a reward signal by predicting a teacher's outputs. However, even though dialog is appealing, we are concerned it is excessively restrictive.",A,1
Language Models are Unsupervised Multitask Learners,"The internet contains a vast amount of information that is passively available without the need for interactive communication. Our speculation is that a language model with sufficient capacity will begin to learn to infer and perform the tasks demonstrated in natural language sequences in order to better predict them, regardless of their method of procurement. If a language model is able to do this it will be, in effect, performing unsupervised multitask learning. We test whether this is the case by analyzing the performance of language models in a zero-shot setting on a wide variety of tasks.","The internet has a huge quantity of information that can be accessed without active communication. We think that a language model with enough capability may start to learn to deduce and carry out the activities shown in natural language sequences so it can better foresee them, no matter how they were obtained. If a language model can do this it will be, essentially, doing unsupervised multitask learning. We examine whether this holds true by analyzing the performance of language models in a zero-shot environment across a wide variety of tasks.","The internet harbors a massive volume of data that is available passively without interactive contact. Our hypothesis is that a language model with ample capacity could begin to infer and execute the jobs illustrated in natural language chains so as to more accurately predict them, irrespective of their source. If a language model can accomplish this it would be, in effect, conducting unsupervised multi-task learning. We test whether this is true by assessing the performance of language models in a zero-shot setting on a diverse array of tasks.  ","The internet houses a huge amount of content that can be reached without active communication. Our thinking is that a language model with sufficient ability may start to deduce and undertake the activities shown in natural language sequences so it can better anticipate them, no matter how they were obtained. If a language model can do this it will be, in essence, conducting unsupervised multi-task learning. We examine whether this is the case by evaluating the performance of language models in a zero-shot environment across a wide variety of tasks.",A,1
Language Models are Unsupervised Multitask Learners,"Most prior work trained language models on a single domain of text, such as news articles (Jozefowicz et al., 2016), Wikipedia (Merity et al., 2016), or fiction books (Kiros et al., 2015). Our approach motivates building as large and diverse a dataset as possible in order to collect natural language demonstrations of tasks in as varied of domains and contexts as possible. A promising source of diverse and nearly unlimited text is web scrapes such as Common Crawl. While these archives are many orders of magnitude larger than current language modeling datasets, they have significant data quality issues.","The majority of previous research focused on teaching language models using text from a single area, like news stories (Jozefowicz et al., 2016), Wikipedia (Merity et al., 2016), or fiction novels (Kiros et al., 2015). Our method suggests constructing as large and varied a dataset as feasible to gather natural language examples of tasks across many domains and settings. A promising source of diverse and nearly limitless text is web scrapes like Common Crawl. Although these archives are much bigger than current language modeling datasets, they have considerable data quality problems.","Most earlier work trained language models using text from one field, for instance news reports (Jozefowicz et al., 2016), Wikipedia (Merity et al., 2016), or fiction books (Kiros et al., 2015). Our approach advocates building as extensive and diverse a dataset as possible to collect natural language demonstrations of tasks across the widest variety of domains and contexts. A promising source of varied and almost unlimited text is web scrapes such as Common Crawl. While these archives are orders of magnitude larger than current language modeling datasets, they have significant data quality challenges.  ","The majority of past research trained language models using text from a single area, such as news articles (Jozefowicz et al., 2016), Wikipedia (Merity et al., 2016), or fiction books (Kiros et al., 2015). Our approach promotes constructing as large and varied a dataset as feasible to gather natural language examples of tasks in the broadest range of domains and settings possible. A promising source of diverse and nearly boundless text is web scrapes like Common Crawl. Although these archives are many times bigger than current language modeling datasets, they have considerable data quality issues.",A,1
Language Models are Unsupervised Multitask Learners,"Trinh & Le (2018) used Common Crawl in their work on commonsense reasoning but noted a large amount of documents “whose content are mostly unintelligible”. We observed similar data issues in our initial experiments with Common Crawl. Trinh & Le (2018)’s best results were achieved using a small subsample of Common Crawl which included only documents most similar to their target dataset, the Winograd Schema Challenge. While this is a pragmatic approach to improve performance on a specific task, we want to avoid making assumptions about the tasks to be performed ahead of time. Instead, we created a new web scrape which emphasizes document quality.","Trinh and Le utilized Common Crawl for their research on reasoning and logic but pointed out that many of the documents were mostly incomprehensible. We saw the same problems with meaningless data when we first tried using Common Crawl. Trinh and Le got their best results by only using a small part of Common Crawl that was most similar to the Winograd Schema Challenge dataset they were working with. While that is a practical way to do better on one particular task, we want to avoid assuming what the tasks will be beforehand. So instead, we made a new web scrape that focuses on document quality.","Trinh and Le made use of Common Crawl in their work on common sense reasoning however they highlighted that there was a lot of content that was largely unintelligible. We encountered comparable data quality problems when we initially tested Common Crawl. Trinh and Le achieved their top performance by only using a subsample of Common Crawl containing documents very similar to their target dataset, the Winograd Schema Challenge. Even though that is a sensible tactic for improving results on one specific task, we want to avoid presuming what the tasks will be in advance. Rather, we created a new web scrape prioritizing document quality.  ","Trinh and Le utilized Common Crawl for their research into commonsense reasoning but pointed out many documents had content that was mostly incomprehensible. We saw similar data quality issues when we first experimented with Common Crawl. Trinh and Le obtained their best results by using just a small part of Common Crawl containing documents highly similar to their target dataset, the Winograd Schema Challenge. While that pragmatic approach improves performance on one particular task, we want to avoid predetermining what the tasks will be. Instead, we made a new web scrape emphasizing document quality.",A,1
Language Models are Unsupervised Multitask Learners,"To do this we only scraped web pages which have been curated/filtered by humans. Manually filtering a full web scrape would be exceptionally expensive so as a starting point, we scraped all outbound links from Reddit, a social media platform, which received at least 3 karma. This can be thought of as a heuristic indicator for whether other users found the link interesting, educational, or just funny. The resulting dataset, WebText, contains the text subset of these 45 million links. To extract the text from HTML responses we use a combination of the Dragnet (Peters & Lecocq, 2013) and Newspaper1 content extractors.","For this task, we exclusively gathered data from webpages that were selected/filtered by people. Manually sorting through all the data from a full web scrape would be incredibly costly, so we began by compiling all the outbound links posted on Reddit, a social networking site, that had a karma score of at least 3. This can be viewed as a heuristic sign that other users found the link fascinating, informative, or humorous. The resulting dataset, WebText, comprises the text portions of these 45 million links. To extract the text from the HTML responses we utilize a combination of the Dragnet (Peters & Lecocq, 2013) and Newspaper1 content extraction tools.","To accomplish this we only accumulated content from web pages that were chosen/refined by humans. Sorting through everything from a comprehensive web scrape by hand would be prohibitively expensive, so our starting point was scraping all external links posted on Reddit, a social media platform, that had earned at least 3 karma points. This can be considered a heuristic indicator that other users found the link compelling, educational, or amusing. The resulting data set, WebText, includes the text portions of those 45 million links. To extract the text from the HTML responses we used a combination of the Dragnet (Peters & Lecocq, 2013) and Newspaper1 content extraction utilities.  ","For this endeavor we exclusively gathered data from webpages that were selected/filtered by people. Manually sifting through everything from a full web scrape would be extremely costly, so our initial step was aggregating all outbound links published on Reddit, a social media website, that had accrued at least 3 karma points. This can be viewed as a heuristic sign that other users found the link interesting, informative, or funny. The resulting dataset, WebText, consists of the text segments of those 45 million links. To extract the text from the HTML responses we utilized a combination of the Dragnet (Peters & Lecocq, 2013) and Newspaper1 content extraction tools.",A,1
Language Models are Unsupervised Multitask Learners,All results presented in this paper use a preliminary version of WebText which does not include links created after Dec 2017 and which after de-duplication and some heuristic based cleaning contains slightly over 8 million documents for a total of 40 GB of text. We removed all Wikipedia documents from WebText since it is a common data source for other datasets and could complicate analysis due to overlapping training data with test evaluation tasks.,"The findings detailed in this report utilize an early form of WebText that does not have links formed after December 2017. After eliminating duplicate and unsuitable documents using automated methods, it retains a bit over 8 million texts totaling 40 GB. We excluded all Wikipedia pages from this WebText version since Wikipedia is frequently used to create other datasets, which could muddle analysis because of overlapping training and test data.","All discoveries presented in this publication depend on a preliminary variant of WebText lacking links made after December of 2017. Following duplication removal and some heuristic-powered cleansing, it incorporates somewhat over 8 million documents totaling 40 GB of content. We took out all Wikipedia pages from this WebText edition given Wikipedia's prevalent role in other datasets, which could obscure investigation due to overlapping preparation and assessment information. ","The results outlined in this paper employ an early iteration of WebText without links added after December 2017. Once duplicate and unsuitable content is eliminated using automated techniques, it retains slightly above 8 million documents amounting to 40 GB of text. We omitted all Wikipedia articles from this WebText version since Wikipedia is often utilized in other datasets, which could complicate analysis because of overlapping training and testing data.",A,1
Language Models are Unsupervised Multitask Learners,"A general language model (LM) should be able to compute the probability of (and also generate) any string. Current large scale LMs include pre-processing steps such as lowercasing, tokenization, and out-of-vocabulary tokens which restrict the space of model-able strings. While processing Unicode strings as a sequence of UTF-8 bytes elegantly fulfills this requirement as exemplified in work such as Gillick et al. (2015), current byte-level LMs are not competitive with word-level LMs on large scale datasets such as the One Billion Word Benchmark (Al-Rfou et al., 2018).","A universal language framework should have the capacity to figure the likelihood of (and furthermore produce) any arrangement of characters. Flowing enormous scope LMs join pre-planning steps like changing over to lowercase, separating into tokens, and obscure word tokens which limit the space of strings that can be displayed. While taking care of Unicode strings as an arrangement of UTF-8 bytes flawlessly satisfies this essential as shown in work like Gillick et al. (2015), current byte-level LMs are not serious with word-level LMs on huge scope informational collections like the One Billion Word Benchmark (Al-Rfou et al., 2018).","A broad language system ought to have the option to ascertain the probability of (and additionally create) any progression of characters. Current large-scale LMs join pre-handling steps like changing over to lowercase, partitioning into tokens, and obscure word tokens which limit the space of modelable strings. While taking care of Unicode strings as an arrangement of UTF-8 bytes elegantly satisfies this prerequisite as shown in work like Gillick et al. (2015), flow byte-level LMs are not cutthroat with word-level LMs on enormous scope datasets like the One Billion Word Benchmark (Al-Rfou et al., 2018). ","A wide-ranging language prototype should be capable of computing the likelihood of (and also producing) any string of characters. Prevalent large-scale LMs incorporate pre-processing actions like converting to lowercase, tokenizing, and out-of-vocabulary tokens which constrain the space of representable strings. While processing Unicode strings as a sequence of UTF-8 bytes gracefully accomplishes this requirement as exemplified in research like Gillick et al. (2015), current byte-level LMs are not competitive with word-level LMs on massive scale data sets like the One Billion Word Benchmark (Al-Rfou et al., 2018).",A,1
Language Models are Unsupervised Multitask Learners,"We observed a similar performance gap in our own attempts to train standard byte-level LMs on WebText. Byte Pair Encoding (BPE) (Sennrich et al., 2015) is a practical middle ground between character and word level language modeling which effectively interpolates between word level inputs for frequent symbol sequences and character level inputs for infrequent symbol sequences. Despite its name, reference BPE implementations often operate on Unicode code points and not byte sequences. These implementations would require including the full space of Unicode symbols in order to model all Unicode strings.","In our own tries to teach normal byte-level LMs on WebText, we saw a comparable gap in performance. Byte Pair Encoding (BPE) (Sennrich et al., 2015) is a useful compromise between character and word level language modeling which essentially combines word level inputs for common symbol sequences and character level inputs for rare symbol sequences. Although it's called Byte Pair Encoding, reference implementations of BPE often work on Unicode code points rather than byte sequences. These implementations would need to include the complete set of Unicode symbols to be able to model all Unicode strings.","When we attempted to train standard byte-level language models on WebText, we noticed a similar underperformance. Byte Pair Encoding (BPE) (Sennrich et al., 2015) strikes a balance between character and word level language modeling, effectively fusing word level inputs for frequent symbol sequences with character level inputs for less common symbol sequences. Despite the name Byte Pair Encoding, existing BPE implementations typically operate on Unicode code points rather than bytes. To model all Unicode strings, these implementations would need to incorporate the full range of Unicode symbols.  ","In training conventional byte-level language models on WebText, we saw a comparable deficiency in performance. Byte Pair Encoding (BPE) (Sennrich et al., 2015) combines the best of character and word level language modeling, using word level inputs for common symbol sequences and character level inputs for rare symbol sequences. Though called Byte Pair Encoding, current BPE implementations work with Unicode code points, not bytes. To model all Unicode strings, these implementations would require the complete Unicode symbol set.",A,1
Language Models are Unsupervised Multitask Learners,"This would result in a base vocabulary of over 130,000 before any multi-symbol tokens are added. This is prohibitively large compared to the 32,000 to 64,000 token vocabularies often used with BPE. In contrast, a byte-level version of BPE only requires a base vocabulary of size 256. However, directly applying BPE to the byte sequence results in suboptimal merges due to BPE using a greedy frequency based heuristic for building the token vocabulary. We observed BPE including many versions of common words like dog since they occur in many variations such as dog. dog! dog? . This results in a sub-optimal allocation of limited vocabulary slots and model capacity.","This would lead to a foundational lexicon of over 130,000 before any multi-symbol tokens are incorporated. This is excessively large compared to the 32,000 to 64,000 token vocabularies frequently utilized with BPE. On the other hand, a byte-level variant of BPE only necessitates a base vocabulary of size 256. However, directly implementing BPE on the byte order produces subpar integrations because BPE employs a greedy frequency grounded heuristic for constructing the token vocabulary. We noticed BPE encompassing many editions of common words like dog since they manifest in numerous variations such as dog. dog! dog? . This culminates in a suboptimal allotment of limited vocabulary vacancies and model capacity.","This would produce an elementary word stock of over 130,000 preceding any multi-symbol tokens being added. This is prohibitively enormous juxtaposed with the 32,000 to 64,000 token lexicons frequently engaged with BPE. Conversely, a byte-level form of BPE solely commands a foundational lexicon of enormity 256. Though, straightforwardly administering BPE to the byte arrangement engenders suboptimal coalescences owing to BPE utilizing a rapacious frequency-based heuristic for assembling the token vocabulary. We observed BPE encompassing numerous versions of ubiquitous words like dog since they materialize in copious variations such as dog. dog! dog? . This effectuates a suboptimal apportionment of limited vocabulary positions and model capacity.  ","This would yield a primordial idiom hoard of over 130,000 anterior to any multi-symbol tokens being annexed. This is prohibitively capacious contrasted with the 32,000 to 64,000 token idioms frequently utilized with BPE. In contradistinction, a byte-level variant of BPE solely necessitates a primordial idiom of enormity 256. However, straightforwardly applying BPE to the byte succession engenders suboptimal conflations owed to BPE exercising a rapacious frequency-predicated heuristic for assembling the token idiom. We observed BPE encompassing multitudinous versions of quotidian words like dog since they materialize in multifarious variations such as dog. dog! dog? . This culminates in a suboptimal allotment of limited idiom vacancies and model capacity.",A,1
Language Models are Unsupervised Multitask Learners,"To avoid this, we prevent BPE from merging across character categories for any byte sequence. We add an exception for spaces which significantly improves the compression efficiency while adding only minimal fragmentation of words across multiple vocab tokens. This input representation allows us to combine the empirical benefits of word-level LMs with the generality of byte-level approaches. Since our approach can assign a probability to any Unicode string, this allows us to evaluate our LMs on any dataset regardless of pre-processing, tokenization, or vocab size.","In order to prevent this issue, we stop BPE from combining across character types for all byte sequences. We make an exception for spaces which greatly improves the compression efficiency while only minimally splitting words into multiple vocab tokens. This input form enables us to unite the empirical advantages of word-level LMs with the generality of byte-level methods. Because our approach can assign a probability to any Unicode string, it allows us to assess our LMs on any dataset irrespective of pre-processing, tokenization, or vocabulary size.","To circumvent this problem, we prohibit BPE from merging across character categories for any byte pattern. We create an exception for spaces which notably enhances the compression performance while barely fragmenting words into multiple vocabulary tokens. This input representation gives us the ability to bring together the empirical strengths of word-level LMs with the universality of byte-level techniques. Since our method can assign a likelihood to any Unicode string, it provides us the means to evaluate our LMs on any dataset without regard to pre-processing, tokenization, or vocabulary dimensions.  ","In order to avoid this predicament, we stop BPE from combining across character types for all byte sequences. We make an exclusion for spaces which significantly improves the compression efficiency while minimally dividing words into multiple vocabulary tokens. This input form allows us to unite the empirical strengths of word-level LMs with the generality of byte-level approaches. Because our method can assign a probability to any Unicode string, it enables us to assess our LMs on any dataset independent of pre-processing, tokenization, or vocabulary size.",A,1
Language Models are Unsupervised Multitask Learners,"We trained and benchmarked four LMs with approximately log-uniformly spaced sizes. The architectures are summarized in Table 2. The smallest model is equivalent to the original GPT, and the second smallest equivalent to the largest model from BERT (Devlin et al., 2018). Our largest model, which we call GPT-2, has over an order of magnitude more parameters than GPT. The learning rate of each model was manually tuned for the best perplexity on a 5% held-out sample of WebText. All models still underfit WebText and held-out perplexity has as of yet improved given more training time.","We educated and evaluated four language models with roughly log-uniformly distributed magnitudes. The designs are outlined in Table 2. The smallest model equals the first GPT, and the second smallest equals the biggest model from BERT (Devlin et al., 2018). Our most substantial model, which we term GPT-2, has over ten times more parameters than GPT. The learning pace of each model was manually adapted for the best perplexity on a 5% held-out exemplar of WebText. All models still inadequately fit WebText and held-out perplexity has up to now enhanced given more training time.","We trained and benchmarked four natural language processing models with approximately logarithmically evenly spaced sizes. The architectures are summarized in Table 2. The most diminutive model is the same as the original GPT, and the second smallest the same as the biggest model from BERT (Devlin et al., 2018). Our largest model, which we designate GPT-2, has over ten times more parameters than GPT. The learning velocity of each model was manually calibrated for the optimal perplexity on a 5% retained sample of WebText. All models still insufficiently fit WebText and retained perplexity has so far gotten better given additional training time.  ","We educated and evaluated four natural language models with roughly logarithmically equally allocated magnitudes. The designs are outlined in Table 2. The most minute model equals the inaugural GPT, and the second smallest equals the most substantial model from BERT (Devlin et al., 2018). Our most sizable model, which we entitle GPT-2, has over an order of magnitude more parameters than GPT. The learning pace of each model was manually tuned for the best perplexity on a 5% withheld exemplar of WebText. All models still inadequately accommodate WebText and withheld perplexity has hitherto enhanced given supplementary training time.",A,1
Language Models are Unsupervised Multitask Learners,"As an initial step towards zero-shot task transfer, we are interested in understanding how WebText LM’s perform at zero-shot domain transfer on the primary task they are trained for – language modeling. Since our model operates on a byte level and does not require lossy pre-processing or tokenization, we can evaluate it on any language model benchmark. Results on language modeling datasets are commonly reported in a quantity which is a scaled or exponentiated version of the average negative log probability per canonical prediction unit - usually a character, a byte, or a word.","To start exploring zero-shot task transfer, we want to see how language models trained on WebText perform when transferring to new domains for language modeling, which is their main task, without additional training. Our model works at the byte level so it can be tested on any benchmark without preprocessing or tokenization. Language modeling results are shown in a scaled or exponentiated form of the average negative log probability per standard prediction unit, like a character, byte or word.","As a first step toward zero-shot task transfer, we are curious about how language models trained on WebText do at zero-shot domain transfer for language modeling, which is their primary purpose, without additional training. Because our model operates on raw bytes instead of tokens, we can evaluate it on any language modeling dataset without lossy preprocessing or tokenization. Language modeling results are typically reported using a scaled or exponentiated version of the average negative log probability per common prediction unit, such as a character, byte, or word.  ","To start investigating zero-shot task transfer, we want to understand the performance of WebText language models on zero-shot domain transfer for language modeling, which is their main trained capability, without any additional training. Since our model works directly on byte data without requiring lossy preprocessing or tokenization, we can test it on any language modeling benchmarks. Language modeling results are shown in a scaled or exponentiated form of the average negative log probability per standard prediction unit, which is usually a character, byte, or word.",A,1
Language Models are Unsupervised Multitask Learners,"We evaluate the same quantity by computing the log-probability of a dataset according to a WebText LM and dividing by the number of canonical units. For many of these datasets, WebText LMs would be tested significantly out-of-distribution, having to predict aggressively standardized text, tokenization artifacts such as disconnected punctuation and contractions, shuffled sentences, and even the string <UNK> which is extremely rare in WebText - occurring only 26 times in 40 billion bytes. We report our main results in Table 3 using invertible de-tokenizers which remove as many of these tokenization / pre-processing artifacts as possible.","We assess the same value by calculating the log-probability of a data set based on a WebText language model and dividing by the number of canonical units. For many of these data sets, WebText language models would be tested very out-of-distribution, having to predict highly standardized text, tokenization quirks like disconnected punctuation and contractions, jumbled sentences, and even the string <UNK> which is extremely uncommon in WebText - occurring only 26 times in 40 billion bytes. We present our primary findings in Table 3 utilizing reversible de-tokenizers which eliminate as many of these tokenization / pre-processing oddities as possible.","We appraise the same quantity by working out the log-probability of a collection of data per a WebText linguistic model and splitting by the count of canonical units. For a lot of these collections of data, WebText linguistic models would be evaluated significantly outside-of-distribution, being made to predict strongly standardized text, tokenization peculiarities such as detached punctuation and contractions, shuffled sentences, and even the string <UNK> which is extremely rare in WebText - transpiring only 26 times in 40 billion bytes. We furnish our principal results in Table 3 applying invertible de-tokenizers which remove as many of these tokenization / pre-processing quirks as feasible. ","We measure the same value by computing the log-probability of a dataset per a WebText language model and dividing by the number of canonical units. For many of these datasets, WebText language models would be tested very out-of-distribution, having to predict highly standardized text, tokenization oddities like separated punctuation and contractions, mixed up sentences, and even the string <UNK> which is extremely uncommon in WebText - occurring only 26 times in 40 billion bytes. We provide our main findings in Table 3 using reversible de-tokenizers which take away as many of these tokenization / pre-processing peculiarities as possible.",A,1
Language Models are Unsupervised Multitask Learners,"We observe gains of 2.5 to 5 perplexity for GPT-2 with these de-tokenizers. WebText LMs transfer well across domains and datasets, improving the state of the art on 7 out of the 8 datasets in a zero-shot setting. Large improvements are noticed on small datasets such as Penn Treebank and WikiText-2 which have only 1 to 2 million training tokens. Large improvements are also noticed on datasets created to measure long-term dependencies like LAMBADA (Paperno et al., 2016) and the Children’s Book Test (Hill et al., 2015). Our model is still significantly worse than prior work on the One Billion Word Benchmark (Chelba et al., 2013).","We notice increases of 2.5 to 5 perplexity for GPT-2 when using these de-tokenizers. Language models trained on WebText transfer effectively across areas and data sets, enhancing the current best performance on 7 out of the 8 data sets in a zero-shot environment. Significant improvements are seen on small data sets like Penn Treebank and WikiText-2 which only have 1 to 2 million training tokens. Large improvements are also noticed on data sets created to evaluate long-term dependencies such as LAMBADA (Paperno et al., 2016) and the Children's Book Test (Hill et al., 2015). Our model is still considerably worse than previous work on the One Billion Word Benchmark (Chelba et al., 2013).","We detect gains of 2.5 to 5 perplexity for GPT-2 when utilizing these de-tokenizers. Language models pre-trained on WebText generalize well across domains and collections, surpassing the state-of-the-art on 7 out of the 8 collections in a zero-shot setting. Notable enhancements are observed on small collections like Penn Treebank and WikiText-2 which contain just 1 to 2 million training tokens. Significant improvements are also detected on collections designed to test long-term dependencies such as LAMBADA (Paperno et al., 2016) and the Children's Book Test (Hill et al., 2015). Our model is still markedly inferior to earlier work on the One Billion Word Benchmark (Chelba et al., 2013).","We see increases of 2.5 to 5 perplexity for GPT-2 when applying these de-tokenizers. Language models trained on WebText transfer effectively across subject areas and datasets, improving upon the current state-of-the-art on 7 out of the 8 datasets in a zero-shot environment. Large gains are noticed on small datasets like Penn Treebank and WikiText-2 which have only 1 to 2 million training tokens. Significant improvements are also observed on datasets created to assess long-term dependencies such as LAMBADA (Paperno et al., 2016) and the Children's Book Test (Hill et al., 2015). Our model still performs much worse than previous work on the One Billion Word Benchmark (Chelba et al., 2013).",A,1
Language Models are Unsupervised Multitask Learners,"This is likely due to a combination of it being both the largest dataset and having some of the most destructive pre-processing - 1BW’s sentence level shuffling removes all long-range structure. The Children’s Book Test (CBT) (Hill et al., 2015) was created to examine the performance of LMs on different categories of words: named entities, nouns, verbs, and prepositions. Rather than reporting perplexity as an evaluation metric, CBT reports accuracy on an automatically constructed cloze test where the task is to predict which of 10 possible choices for an omitted word is correct.","This is probably because it is both the biggest dataset and has some of the most damaging pre-processing - 1BW's sentence level shuffling eliminates all long-range structure. The Children's Book Test (CBT) (Hill et al., 2015) was made to analyze the performance of LMs on different word types: named entities, nouns, verbs, and prepositions. Instead of reporting perplexity as an evaluation metric, CBT reports accuracy on a automatically created cloze test where the task is to predict which of 10 possible options for a left out word is correct.","This is likely owing to it being both the most substantial dataset and having some of the most destructive pre-processing - 1BW's sentence order randomization takes away all long-distance structure. The Children's Book Test (CBT) (Hill et al., 2015) was formed to inspect the capabilities of LMs on various word categories: named entities, nouns, verbs, and prepositions. Rather than stating perplexity as an evaluation metric, CBT states accuracy on a automatically made cloze test where the task is to predict which of 10 feasible choices for an omitted word is accurate.","This is probably due to it being both the biggest dataset and having some of the most damaging pre-processing - 1BW's randomizing of sentence order eliminates all long-range structure. The Children's Book Test (CBT) (Hill et al., 2015) was developed to analyze the performance of LMs on different word types: named entities, nouns, verbs, and prepositions. Instead of documenting perplexity as an evaluation metric, CBT documents accuracy on a automatically constructed cloze test where the task is to predict which of 10 possible options for a left out word is correct.",A,1
Language Models are Unsupervised Multitask Learners,"Following the LM approach introduced in the original paper, we compute the probability of each choice and the rest of the sentence conditioned on this choice according to the LM, and predict the one with the highest probability. As seen in Figure 2 performance steadily improves as model size is increased and closes the majority of the gap to human performance on this test. Data overlap analysis showed one of the CBT test set books, The Jungle Book by Rudyard Kipling, is in WebText, so we report results on the validation set which has no significant overlap. GPT-2 achieves new state of the art results of 93.3% on common nouns and 89.1% on named entities.","We used the language model method from the original paper to calculate the probability of each option and the rest of the sentence based on that option using the language model. We predicted the option with the highest probability. As shown in Figure 2, performance steadily got better as we increased the model size. It closed most of the gap between the model and human performance on this test. We analyzed the data overlap between the CBT test set and WebText. We found that one of the CBT test set books, The Jungle Book by Rudyard Kipling, is in WebText. So we report results on the validation set which has no significant overlap. GPT-2 achieved new state of the art results of 93.3% on common nouns and 89.1% on named entities.","Adopting the language modeling technique presented in the first paper, we determined the likelihood of each selection and the remainder of the sentence conditioned on that selection per the language model. We chose the one with the greatest probability. As evident in Figure 2, performance steadily improved as we enlarged the model size and it closed most of the gap with human performance on this evaluation. Examination of data overlap showed one of the books in the CBT test set, The Jungle Book by Rudyard Kipling, is present in WebText, so we present results on the validation set which has no significant overlap. GPT-2 attained new state-of-the-art results of 93.3% on common nouns and 89.1% on named entities.  ","Using the language model approach described in the original publication, we calculated the probability of each alternative and the rest of the sentence based on that alternative according to the language model. We predicted the one with the highest probability. As seen in Figure 2, performance steadily got better as we increased the model size and it closed most of the difference between the model and human performance on this test. Analysis of data overlap revealed one of the books in the CBT test set, The Jungle Book by Rudyard Kipling, is in WebText. Therefore, we report results on the validation set which has no significant overlap. GPT-2 achieved new state-of-the-art results of 93.3% on common nouns and 89.1% on named entities.",A,1
Language Models are Unsupervised Multitask Learners,"The LAMBADA dataset (Paperno et al., 2016) tests the ability of systems to model long-range dependencies in text. The task is to predict the final word of sentences which require at least 50 tokens of context for a human to successfully predict. GPT-2 improves the state of the art from 99.8 (Grave et al., 2016) to 8.6 perplexity and increases the accuracy of LMs on this test from 19% (Dehghani et al., 2018) to 52.66%. Investigating GPT-2’s errors showed most predictions are valid continuations of the sentence, but are not valid final words.","The LAMBADA dataset (Paperno et al., 2016) evaluates the capability of systems to represent long-range reliances in text. The objective is to predict the last word of sentences which need at least 50 tokens of context for a human to successfully predict. GPT-2 enhances the state of the art from 99.8 (Grave et al., 2016) to 8.6 perplexity and expands the precision of LMs on this evaluation from 19% (Dehghani et al., 2018) to 52.66%. Examining GPT-2's mistakes showed most forecasts are legitimate continuations of the sentence, but are not valid final words.","The LAMBADA dataset (Paperno et al., 2016) tests the ability of models to capture long-distance dependencies in language. The goal is to foresee the final term of sentences which require a minimum of 50 tokens of context for a person to accurately predict. GPT-2 improves the previous best from 99.8 (Grave et al., 2016) to 8.6 perplexity and increases the performance of LMs on this benchmark from 19% (Dehghani et al., 2018) to 52.66%. Analyzing GPT-2's errors revealed most predictions are plausible extensions of the sentence, but are not correct final terms.","The LAMBADA dataset (Paperno et al., 2016) evaluates the capacity of systems to represent long-span associations in text. The challenge is to predict the concluding word of sentences which need at least 50 tokens of context for a human to successfully predict. GPT-2 enhances the prior best from 99.8 (Grave et al., 2016) to 8.6 perplexity and boosts the accuracy of LMs on this assessment from 19% (Dehghani et al., 2018) to 52.66%. Reviewing GPT-2's mistakes showed most guesses are valid progressions of the sentence, but are not valid concluding words.",A,1
Language Models are Unsupervised Multitask Learners,"This suggests that the LM is not using the additional useful constraint that the word must be the final of the sentence. Adding a stop-word filter as an approximation to this further increases accuracy to 63.24%, improving the overall state of the art on this task by 4%. The previous state of the art (Hoang et al., 2018) used a different restricted prediction setting where the outputs of the model were constrained to only words that appeared in the context.","This implies that the language model is not leveraging the extra helpful limit that the word needs to be the last one in the sentence. Incorporating a stop-word filter to approximate this additionally boosts precision to 63.24%, enhancing the general state-of-the-art on this task by 4%. The preceding state-of-the-art (Hoang et al., 2018) utilized a different constrained prediction configuration where the outputs of the model were constrained to only words that showed up in the context.","This shows that the language model isn't capitalizing on the additional useful constraint that the word has to be the final one of the sentence. Adding a stop-word filter to act as an approximation of this further increases accuracy to 63.24%, improving the overall best performance on this task by 4%. The previous best performance (Hoang et al., 2018) used a different limited prediction setup where the outputs of the model were limited to only words that were present in the context.  ","This indicates that the language model is not leveraging the extra helpful restriction that the word needs to be the last one of the sentence. Incorporating a stop-word filter as an approximation of this additionally increases accuracy to 63.24%, enhancing the general best achievement on this task by 4%. The previous best achievement (Hoang et al., 2018) utilized a different constrained prediction arrangement where the outputs of the model were constrained to only words that were available in the context.",A,1
Language Models are Unsupervised Multitask Learners,"The Winograd Schema challenge (Levesque et al., 2012) was constructed to measure the capability of a system to perform commonsense reasoning by measuring its ability to resolve ambiguities in text. Recently Trinh & Le (2018) demonstrated significant progress on this challenge using LMs, by predicting the resolution of the ambiguity with higher probability. We follow their problem formulation and visualize the performance of our models with both full and partial scoring techniques in Figure 3. GPT-2 improves state of the art accuracy by 7%, achieving 70.70%. The dataset is quite small with only 273 examples so we recommend reading Trichelair et al. (2018) to help contextualize this result.","The Winograd Schema challenge (Levesque et al., 2012) was made to gauge the ability of a system to use common sense by testing its skill to clear up unclear parts in writing. Not long ago, Trinh & Le (2018) showed major progress on this challenge by using LMs, by guessing the resolution of the ambiguity with greater probability. We follow their formulation of the problem and visualize the performance of our models with both complete and partial scoring techniques in Figure 3. GPT-2 gets better state of the art accuracy by 7%, getting 70.70%. The dataset is quite small with only 273 examples so we suggest reading Trichelair et al. (2018) to help put this result in context.","The Winograd Schema challenge (Levesque et al., 2012) was designed to quantify the skill of a system at using commonsense reasoning by evaluating its competence to resolve ambiguities in text. Recently, Trinh & Le (2018) exhibited significant advancements on this challenge by employing LMs, by predicting the clarification of the ambiguity with higher probability. We adopt their formulation of the problem and depict the performance of our models with both full and partial scoring techniques in Figure 3. GPT-2 improves the state-of-the-art accuracy by 7%, achieving 70.70%. Since the dataset contains only 273 examples, we recommend referring to Trichelair et al. (2018) to assist with contextualizing this result.  ","The Winograd Schema challenge (Levesque et al., 2012) was made to gauge the ability of a system to reason practically by testing its skill to resolve unclear meanings in writing. Not long ago, Trinh & Le (2018) showed major improvements on this challenge by employing LMs, by forecasting the explanation of the ambiguity with greater probability. We take on their formulation of the problem and picture the performance of our models with both complete and incomplete scoring techniques in Figure 3. GPT-2 gets better the best accuracy so far by 7%, getting 70.70%. Because the dataset has only 273 examples, we suggest looking at Trichelair et al. (2018) to help put this result in context.",A,1
Language Models are Unsupervised Multitask Learners,"The Conversation Question Answering dataset (CoQA) Reddy et al. (2018) consists of documents from 7 different domains paired with natural language dialogues between a question asker and a question answerer about the document. CoQA tests reading comprehension capabilities and also the ability of models to answer questions that depend on conversation history (such as “Why?”). Greedy decoding from GPT-2 when conditioned on a document, the history of the associated conversation, and a final token A: achieves 55 F1 on the development set.","The CoQA dataset from Reddy et al. (2018) has documents from 7 areas along with natural language chats between someone asking questions and someone answering questions about the document. CoQA evaluates reading comprehension skills and also the skills of models to respond to questions relying on prior chat history (like ""Why?""). Doing greedy decoding from GPT-2 when provided the document, the history of the related chat, and a final token A: reaches 55 F1 on the dev set.","The Conversation Question Answering collection (CoQA) by Reddy et al. (2018) includes texts from 7 domains paired with natural language dialogs between a questioner and answerer regarding the text. CoQA examines reading understanding abilities and also the capacity of models to respond to questions dependent on conversation background (like ""Why?""). Greedy decoding from GPT-2 when given a text, the history of the related conversation, and a final token A: accomplishes 55 F1 on the development collection.","The CoQA dataset by Reddy et al. (2018) has texts from 7 areas coupled with natural language discussions between someone posing questions and someone replying to questions about the text. CoQA evaluates reading comprehension skills and also the ability of models to answer questions relying on prior discussion history (such as ""Why?""). Doing greedy decoding from GPT-2 when provided the text, the history of the associated discussion, and a final token A: reaches 55 F1 on the dev collection.",A,1
Neural Machine Translation by Jointly Learning To Align and Translate,"Neural machine translation is a newly emerging approach to machine translation, recently proposed by Kalchbrenner and Blunsom (2013), Sutskever et al. (2014) and Cho et al. (2014b). Unlike the traditional phrase-based translation system (see, e.g., Koehn et al., 2003) which consists of many small sub-components that are tuned separately, neural machine translation attempts to build and train a single, large neural network that reads a sentence and outputs a correct translation. Most of the proposed neural machine translation models belong to a family of encoder– decoders (Sutskever et al., 2014; Cho et al., 2014a), with an encoder and a decoder for each language, or involve a language-specific encoder applied to each sentence whose outputs are then compared (Hermann and Blunsom, 2014).","Neural machine translation is a newly emerging method for automated translation between languages that was recently put forward by Kalchbrenner and Blunsom (2013), Sutskever et al. (2014) and Cho et al. (2014b). In contrast to the conventional statistical machine translation system (see, e.g., Koehn et al., 2003) composed of many small individual parts tuned in isolation, neural machine translation tries to construct and train one large neural network that accepts a sentence as input and generates a correct translation as output. Most of the proposed neural machine translation architectures follow an encoder-decoder design (Sutskever et al., 2014; Cho et al., 2014a), with separate encoder and decoder modules for each language, or use language-specific encoders applied to each sentence whose representations are then compared (Hermann and Blunsom, 2014).","Neural machine translation is a novel approach to automated translation that was recently developed by Kalchbrenner and Blunsom (2013), Sutskever et al. (2014) and Cho et al. (2014b). In contrast to traditional statistical machine translation systems (see, e.g., Koehn et al., 2003) made up of many small modular parts tuned in isolation, neural machine translation attempts to construct and train a single large neural network that takes in a sentence and outputs a correct translation. Most proposed neural machine translation architectures follow an encoder-decoder design (Sutskever et al., 2014; Cho et al., 2014a), with distinct encoder and decoder modules for each language, or leverage language-specific encoders applied to each sentence whose encoded representations are then matched (Hermann and Blunsom, 2014). ","Neural machine translation is a novel approach to automated translation recently introduced by Kalchbrenner and Blunsom (2013), Sutskever et al. (2014) and Cho et al. (2014b). Unlike conventional phrase-based translation systems (see, e.g., Koehn et al., 2003) composed of many small components tuned separately, neural machine translation seeks to build and train one large neural network that accepts a sentence as input and generates a correct translation as output. Most neural machine translation models follow an encoder-decoder architecture (Sutskever et al., 2014; Cho et al., 2014a), with distinct encoder and decoder modules for each language, or utilize language-specific encoders applied to each sentence whose encoded representations are then compared (Hermann and Blunsom, 2014).",A,1
Neural Machine Translation by Jointly Learning To Align and Translate,"An encoder neural network reads and encodes a source sentence into a fixed-length vector. A decoder then outputs a translation from the encoded vector. The whole encoder–decoder system, which consists of the encoder and the decoder for a language pair, is jointly trained to maximize the probability of a correct translation given a source sentence. A potential issue with this encoder–decoder approach is that a neural network needs to be able to compress all the necessary information of a source sentence into a fixed-length vector. This may make it difficult for the neural network to cope with long sentences, especially those that are longer than the sentences in the training corpus.","A neural network that encodes text reads and converts a source sentence into a vector of a predefined length. After that, a decoder neural network generates a translation using the encoded vector. The whole encoder-decoder framework, made up of the encoder and decoder for a language pair, is jointly optimized to maximize the chance of producing the right translation from a source sentence. A possible problem with this encoder-decoder method is that a neural network has to be capable of condensing all the required information from a source sentence into a fixed-length vector. This might make it tough for the neural network to handle long sentences, particularly those longer than the sentences in the training data.","An encoder neural net processes and encodes a source sentence into a vector of fixed size. Then, a decoder generates a translation from that encoded vector. The entire encoder-decoder model, composed of the encoder and decoder for a language pair, is trained together to increase the probability of generating a correct translation from a source sentence. One issue with this encoder-decoder technique is that a neural network has to compress all necessary information from a source sentence into a fixed-size vector. This can make it problematic for the neural net to work with long sentences, especially those longer than the sentences in the training set.","An encoding neural network analyzes and converts a source sentence into a vector of predetermined length. Afterward, a decoding neural network produces a translation using that encoded vector. The whole encoder-decoder architecture, consisting of the encoder and decoder for a language pair, is co-trained to maximize the likelihood of correct translation generation given a source sentence. One potential problem with this encoder-decoder method is that a neural net must be capable of condensing all requisite information from a source sentence into a fixed-length vector. This may cause difficulties for the neural net to process long sentences, particularly those exceeding the length of sentences in the training data.",A,1
Neural Machine Translation by Jointly Learning To Align and Translate,"Cho et al. (2014b) showed that indeed the performance of a basic encoder–decoder deteriorates rapidly as the length of an input sentence increases. In order to address this issue, we introduce an extension to the encoder–decoder model which learns to align and translate jointly. Each time the proposed model generates a word in a translation, it (soft-)searches for a set of positions in a source sentence where the most relevant information is concentrated. The model then predicts a target word based on the context vectors associated with these source positions and all the previous generated target words.","Cho and colleagues (2014b) demonstrated that the performance of a simple encoder-decoder model worsens quickly as the length of an input sentence grows. To address this problem, we present a modification to the encoder-decoder model that learns to align and translate together. Each time the proposed model produces a word in a translation, it performs a soft search to find positions in the source sentence where the most useful information is focused. The model then forecasts a target word based on the context vectors linked to those source positions and all previously generated target words.","The research by Cho et al. (2014b) showed that the capabilities of a basic encoder-decoder model deteriorate rapidly when the length of an input sentence is increased. To tackle this issue, we put forward an enhancement to the encoder-decoder model that concurrently learns how to align and translate. Whenever the suggested model outputs a word in a translation, it carries out a soft search to identify locations in a source sentence where the most relevant information is concentrated. The model then predicts a target word relying on the context vectors related to those source locations and all earlier produced target words.  ","The study by Cho and co-authors (2014b) demonstrated that the efficacy of a simple encoder-decoder architecture quickly declines as the length of an input sentence becomes longer. To address this problem, we introduce an augmentation to the encoder-decoder model that learns alignment and translation together. Each instance when the proposed model emits a word in a translation, it executes a soft search to pinpoint positions in a source sentence where the most pertinent information resides. The model then conjectures a target word based on the context vectors linked to those source positions and all previously spawned target words.",A,1
Neural Machine Translation by Jointly Learning To Align and Translate,"The most important distinguishing feature of this approach from the basic encoder–decoder is that it does not attempt to encode a whole input sentence into a single fixed-length vector. Instead, it encodes the input sentence into a sequence of vectors and chooses a subset of these vectors adaptively while decoding the translation. This frees a neural translation model from having to squash all the information of a source sentence, regardless of its length, into a fixed-length vector. We show this allows a model to cope better with long sentences.","The main difference between this method and the standard encoder-decoder is that it does not try to encode an entire input sentence into one vector of fixed length. Rather, it encodes the input sentence into multiple vectors and selectively chooses some of those vectors when generating the translation. This removes the limitation of having to compress all the information from a source sentence, regardless of its length, into a single fixed-size vector. We demonstrate that this enables the model to handle long sentences more effectively.","The most notable feature setting this technique apart from the basic encoder-decoder is that it does not attempt to map a whole input sentence into a solitary static-length vector. Instead, it maps the input sentence into a series of vectors and adaptively picks a subset of these vectors while decoding the translation. This liberates a neural translation model from being forced to cram all the information from a source sentence, irrespective of its length, into one fixed-size vector. We exhibit that this empowers a model to manage long sentences better. ","The most significant distinguishing aspect of this method compared to the standard encoder-decoder is that it does not try to condense an entire input sentence into a single vector of predefined length. Rather, it encodes the input sentence into multiple vectors and selectively utilizes some of those vectors during translation generation. This removes the constraint of having to squeeze all the information in a source sentence, regardless of its length, into one static-size vector. We demonstrate that this allows the model to handle long sentences more effectively.",A,1
Neural Machine Translation by Jointly Learning To Align and Translate,"In this paper, we show that the proposed approach of jointly learning to align and translate achieves significantly improved translation performance over the basic encoder–decoder approach. The improvement is more apparent with longer sentences, but can be observed with sentences of any length. On the task of English-to-French translation, the proposed approach achieves, with a single model, a translation performance comparable, or close, to the conventional phrase-based system. Furthermore, qualitative analysis reveals that the proposed model finds a linguistically plausible (soft) alignment between a source sentence and the corresponding target sentence.","This document demonstrates that the suggested technique of concurrently learning to match and convert text accomplishes notably enhanced translation performance compared to the fundamental encoder-decoder method. The enhancement is more noticeable with lengthier sentences, however can be seen with sentences of any length. For English-to-French translation, the suggested approach accomplishes, with a single model, a translation capability on par with, or close to, the conventional phrase-based framework. Moreover, qualitative examination shows that the proposed model identifies a linguistically probable (soft) alignment between a source sentence and the related target sentence.","In this document, we exhibit that the recommended tactic of simultaneously acquiring to orient and interpret text reaches meaningfully developed translation competence over the elementary encoder-decoder plan. The progress is more evident with wordier sentences, nevertheless can be discerned with sentences of any extent. On the endeavor of English-to-French interpretation, the suggested plan achieves, with a sole prototype, an interpretation capability comparable, or near, to the conventional phrase-founded scheme. Furthermore, qualitative dissection unveils that the offered exemplar spots a linguistically plausible (flexible) alignment between a source sentence and the associated target sentence.  ","This composition demonstrates that the advised system of concurrently educating to coordinate and convert language accomplishes notably enhanced translation ability compared to the basic encoder-decoder method. The improvement is more noticeable with more prolonged sentences, however can be discerned with sentences of any length. On the task of English-to-French translation, the advised approach accomplishes, with a single archetype, a translation capacity on par with, or approximating, the conventional phrase-based model. Moreover, qualitative dissection divulges that the offered exemplar identifies a linguistically probable (pliant) alignment between a source sentence and the associated target sentence.",A,1
Neural Machine Translation by Jointly Learning To Align and Translate,"From a probabilistic perspective, translation is equivalent to finding a target sentence y that maximizes the conditional probability of y given a source sentence x, i.e., arg maxy p(y | x). In neural machine translation, we fit a parameterized model to maximize the conditional probability of sentence pairs using a parallel training corpus. Once the conditional distribution is learned by a translation model, given a source sentence a corresponding translation can be generated by searching for the sentence that maximizes the conditional probability.","Translation can be viewed as finding a target sentence y that has the highest conditional probability given a source sentence x, which is arg maxy p(y | x). Neural machine translation involves fitting a parameterized model to maximize the conditional probabilities of sentence pairs using parallel training data. After the translation model learns the conditional distribution, it can generate a translation for a source sentence by searching for the sentence with the highest conditional probability.","From a probability perspective, translation is finding a target sentence y that gives the maximum conditional probability for y given a source sentence x, or arg maxy p(y | x). Neural machine translation trains a model with parameters to maximize the conditional probabilities of sentence pairs using parallel corpora. Once the model learns the conditional distribution, it can produce a translation for a source sentence by picking the sentence that gives the highest conditional probability.","Translation is like finding a target sentence y that has the best conditional probability p(y | x) given a source sentence x. Neural machine translation fits a model with parameters to maximize the conditional probabilities of sentence pairs using parallel texts for training. After learning the conditional distribution, the translation model can generate a translation for a source sentence by choosing the sentence with the highest conditional probability.",A,1
Neural Machine Translation by Jointly Learning To Align and Translate,"Recently, a number of papers have proposed the use of neural networks to directly learn this conditional distribution (see, e.g., Kalchbrenner and Blunsom, 2013; Cho et al., 2014a; Sutskever et al., 2014; Cho et al., 2014b; Forcada and Neco, 1997). This neural machine translation approach typically consists of two components, the first of which encodes a source sentence x and the second decodes to a target sentence y. For instance, two recurrent neural networks (RNN) were used by (Cho et al., 2014a) and (Sutskever et al., 2014) to encode a variable-length source sentence into a fixed-length vector and to decode the vector into a variable-length target sentence.","In the past few years, several articles have suggested utilizing neural networks to directly learn this conditional probability distribution (refer to, for example, Kalchbrenner and Blunsom, 2013; Cho et al., 2014a; Sutskever et al., 2014; Cho et al., 2014b; Forcada and Neco, 1997). This neural machine translation method usually contains two parts, the first of which encodes a source sentence x and the second decodes to a target sentence y. For instance, two recurrent neural networks (RNNs) were utilized by (Cho et al., 2014a) and (Sutskever et al., 2014) to encode a variable-length source sentence into a fixed-length vector and to decode the vector into a variable-length target sentence.","Recently, multiple papers have put forward the utilization of neural networks to directly acquire this conditional distribution (see, e.g., Kalchbrenner and Blunsom, 2013; Cho et al., 2014a; Sutskever et al., 2014; Cho et al., 2014b; Forcada and Neco, 1997). This neural machine translation methodology typically consists of two elements, the first of which encodes a source sentence x and the second decodes to a target sentence y. For example, two recurrent neural networks (RNNs) were employed by (Cho et al., 2014a) and (Sutskever et al., 2014) to encode a variable-length source sentence into a fixed-length vector and to decode the vector into a variable-length target sentence.  ","In recent years, several studies have advocated the use of neural networks to directly learn this conditional probability distribution (refer to, for instance, Kalchbrenner and Blunsom, 2013; Cho et al., 2014a; Sutskever et al., 2014; Cho et al., 2014b; Forcada and Neco, 1997). This neural machine translation approach usually involves two components, the first of which encodes a source sentence x and the second decodes to a target sentence y. For example, two recurrent neural networks (RNNs) were utilized by (Cho et al., 2014a) and (Sutskever et al., 2014) to encode a variable-length source sentence into a fixed-length vector and to decode the vector into a variable-length target sentence.",A,1
Neural Machine Translation by Jointly Learning To Align and Translate,"Despite being a quite new approach, neural machine translation has already shown promising results. Sutskever et al. (2014) reported that the neural machine translation based on RNNs with long short-term memory (LSTM) units achieves close to the state-of-the-art performance of the conventional phrase-based machine translation system on an English-to-French translation task.1 Adding neural components to existing translation systems, for instance, to score the phrase pairs in the phrase table (Cho et al., 2014a) or to re-rank candidate translations (Sutskever et al., 2014), has allowed to surpass the previous state-of-the-art performance level.","Even though neural machine translation is a relatively recent method, it has already demonstrated encouraging outcomes. Sutskever et al. (2014) showed that neural machine translation using RNNs with LSTM units approaches the best performance of standard phrase-based machine translation systems on English-to-French translation.1 Integrating neural elements into current translation systems, for example to score phrase pairs in the phrase table (Cho et al., 2014a) or rerank translation options (Sutskever et al., 2014), has enabled surpassing previous state-of-the-art levels.","Despite its newness, neural machine translation has quickly produced promising results. Sutskever et al. (2014) found that neural machine translation leveraging RNNs with LSTM units achieves near state-of-the-art performance of conventional phrase-based machine translation on English-to-French translation.1 Incorporating neural components into existing translation systems, like scoring phrase pairs in the phrase table (Cho et al., 2014a) or re-ranking translation candidates (Sutskever et al., 2014), has allowed surpassing previous best performance. ","Though a relatively recent approach, neural machine translation has already generated encouraging outcomes. Sutskever et al. (2014) showed that neural machine translation employing RNNs with LSTM units approaches the leading performance of standard phrase-based machine translation on English-to-French translation.1 Adding neural elements to current translation systems, for example scoring phrase pairs in the phrase table (Cho et al., 2014a) or re-ordering translation options (Sutskever et al., 2014), has enabled exceeding previous state-of-the-art performance.",A,1
Neural Machine Translation by Jointly Learning To Align and Translate,"WMT ’14 contains the following English-French parallel corpora: Europarl (61M words), news commentary (5.5M), UN (421M) and two crawled corpora of 90M and 272.5M words respectively, totaling 850M words. Following the procedure described in Cho et al. (2014a), we reduce the size of the combined corpus to have 348M words using the data selection method by Axelrod et al. (2011). We do not use any monolingual data other than the mentioned parallel corpora, although it may be possible to use a much larger monolingual corpus to pretrain an encoder.","The WMT '14 dataset has these English-French parallel texts: Europarl (61 million words), news commentary (5.5 million), UN (421 million) and two web-crawled sets of 90 million and 272.5 million words, totaling 850 million words altogether. Using the data selection approach described in Axelrod et al. (2011), we reduced the size of the combined corpus down to 348 million words, following the procedure detailed in Cho et al. (2014a). We did not utilize any monolingual data beyond the mentioned parallel corpora, despite the option to pretrain an encoder on a much larger monolingual collection.","The WMT 2014 dataset contains the following English-French parallel corpora: Europarl (61M words), news commentary (5.5M), UN (421M) and two web-crawled sets of 90M and 272.5M words, amounting to 850M words total. Per the method outlined in Cho et al. (2014a), we decreased the size of the combined corpus to 348M words using the data selection technique proposed by Axelrod et al. (2011). We did not employ any monolingual data other than the stated parallel corpora, although pretraining an encoder on a far larger monolingual collection is feasible.","The WMT '14 dataset includes these English-French parallel texts: Europarl (61 million words), news commentary (5.5 million), UN (421 million) and two internet-scraped sets of 90 million and 272.5 million words, totaling 850 million words all together. Following the process explained in Cho et al. (2014a), we reduced the size of the combined corpus to 348 million words using the data selection approach described by Axelrod et al. (2011). We did not make use of any monolingual data beyond the mentioned parallel corpora, despite the possibility to pretrain an encoder on a much bigger monolingual collection.",A,1
Neural Machine Translation by Jointly Learning To Align and Translate,"We train two types of models. The first one is an RNN Encoder–Decoder (RNNencdec, Cho et al., 2014a), and the other is the proposed model, to which we refer as RNNsearch. We train each model twice: first with the sentences of length up to 30 words (RNNencdec-30, RNNsearch-30) and then with the sentences of length up to 50 word (RNNencdec-50, RNNsearch-50). The encoder and decoder of the RNNencdec have 1000 hidden units each. The encoder of the RNNsearch consists of forward and backward recurrent neural networks (RNN) each having 1000 hidden units. Its decoder has 1000 hidden units.","We educate two kinds of prototypes. The first is a RNN Encoder-Decoder (RNNencdec, Cho et al., 2014a), and the other is the suggested model, which we call RNNsearch. We prepare each prototype two times: first with the sentences having at most 30 words (RNNencdec-30, RNNsearch-30) and then with the sentences having at most 50 words (RNNencdec-50, RNNsearch-50). The encoder and decoder of RNNencdec both have 1000 hidden components. The encoder of RNNsearch is made up of forward and backward recurrent neural networks (RNN), each having 1000 hidden components. Its decoder has 1000 hidden components.","We develop two varieties of systems. One is a RNN Encoder-Decoder (RNNencdec, Cho et al., 2014a), the other is the recommended system, referred to as RNNsearch. We construct each system twice: initially for sentences up to 30 words (RNNencdec-30, RNNsearch-30), then for sentences up to 50 words (RNNencdec-50, RNNsearch-50). The RNNencdec encoder and decoder each possess 1000 concealed nodes. The RNNsearch encoder comprises forward and reverse recurrent neural networks (RNN), both containing 1000 concealed nodes. Its decoder has 1000 concealed nodes.  ","We build two kinds of models. One is a RNN Encoder-Decoder (RNNencdec, Cho et al., 2014a), the other is the proposed model, called RNNsearch. We construct each model twice: first for sentences having a maximum of 30 words (RNNencdec-30, RNNsearch-30), then for sentences having a maximum of 50 words (RNNencdec-50, RNNsearch-50). The RNNencdec encoder and decoder have 1000 hidden units each. The RNNsearch encoder is made of forward and backward recurrent neural networks (RNN), each with 1000 hidden units. Its decoder has 1000 hidden units.",A,1
Neural Machine Translation by Jointly Learning To Align and Translate,"In both cases, we use a multilayer network with a single maxout (Goodfellow et al., 2013) hidden layer to compute the conditional probability of each target word (Pascanu et al., 2014). We use a minibatch stochastic gradient descent (SGD) algorithm together with Adadelta (Zeiler, 2012) to train each model. Each SGD update direction is computed using a minibatch of 80 sentences. We trained each model for approximately 5 days. Once a model is trained, we use a beam search to find a translation that approximately maximizes the conditional probability (see, e.g., Graves, 2012; Boulanger-Lewandowski et al., 2013). Sutskever et al. (2014) used this approach to generate translations from their neural machine translation model.","In both situations, we utilize a neural network with a single maxout hidden layer to determine the conditional likelihood of each target word. We employ a mini-batch stochastic gradient descent algorithm with Adadelta to optimize each model's parameters. Each gradient update direction is found using a mini-batch of 80 sentences. We trained each model for about 5 days. After training a model, we use beam search to find a translation that roughly maximizes the conditional probability. Sutskever et al. (2014) used this technique to generate translations from their neural translation model.","For both cases, we make use of a deep neural network containing a single maxout hidden layer for calculating the conditional probability of every target word. We apply minibatch stochastic gradient descent together with Adadelta to learn the parameters of each model. Every SGD update direction is determined using a minibatch of 80 sentences. We trained each model for around 5 days. After a model is trained, we utilize beam search to obtain a translation that approximately maximizes the conditional probability. Sutskever et al. (2014) used this method to generate translations from their neural translation model.  ","In both scenarios, we employ a multi-layer neural network with a single maxout hidden layer to compute the conditional probability of every target word. We use a minibatch stochastic gradient descent algorithm in conjunction with Adadelta to optimize each model's parameters. Each SGD update direction is calculated using a minibatch containing 80 sentences. We trained each model for about 5 days. Once a model is trained, we leverage beam search to identify a translation that roughly maximizes the conditional probability. Sutskever et al. (2014) used this technique to generate translations from their neural translation model.",A,1
Neural Machine Translation by Jointly Learning To Align and Translate,"In Table 1, we list the translation performances measured in BLEU score. It is clear from the table that in all the cases, the proposed RNNsearch outperforms the conventional RNNencdec. More importantly, the performance of the RNNsearch is as high as that of the conventional phrase-based translation system (Moses), when only the sentences consisting of known words are considered. This is a significant achievement, considering that Moses uses a separate monolingual corpus (418M words) in addition to the parallel corpora we used to train the RNNsearch and RNNencdec.","The data in Table 1 demonstrates that our proposed RNNsearch model achieves higher BLEU scores than the standard RNNencdec across all experiments. Notably, the RNNsearch reaches performance on par with the Moses phrase-based system, which utilizes extra monolingual data, when evaluating only on sentences containing known words. This is a substantial accomplishment given that the RNNsearch and RNNencdec were trained on the same parallel corpora without additional monolingual data.","The BLEU scores listed in Table 1 make it evident that the RNNsearch we put forward surpasses the typical RNNencdec in every case. Most significantly, the RNNsearch's performance rivals that of the conventional phrase-based Moses system, when restricting to sentences with only familiar words. This is an important success, since Moses uses an extra standalone monolingual corpus (418M words) on top of the parallel corpora we utilized to train both the RNNsearch and RNNencdec.  ","Analyzing the BLEU scores in Table 1, we can clearly see our proposed RNNsearch model outperforms the standard RNNencdec model in all experiments. Crucially, the RNNsearch achieves comparable performance to the Moses phrase-based system, which uses additional monolingual data, when evaluating only on sentences with known words. This represents a major achievement given the RNNsearch and RNNencdec were trained on the same parallel corpora without extra monolingual data, unlike Moses.",A,1
Neural Machine Translation by Jointly Learning To Align and Translate,"One of the motivations behind the proposed approach was the use of a fixed-length context vector in the basic encoder–decoder approach. We conjectured that this limitation may make the basic encoder–decoder approach to underperform with long sentences. In Fig. 2, we see that the performance of RNNencdec dramatically drops as the length of the sentences increases. On the other hand, both RNNsearch-30 and RNNsearch-50 are more robust to the length of the sentences. RNNsearch50, especially, shows no performance deterioration even with sentences of length 50 or more. This superiority of the proposed model over the basic encoder–decoder is further confirmed by the fact that the RNNsearch-30 even outperforms RNNencdec-50 (see Table 1).","A key inspiration for the suggested method was the use of an unchanging context vector in the standard encoder-decoder model. We hypothesized this constraint might cause the standard encoder-decoder approach to underperform on lengthy sentences. In Fig. 2, we observe RNNencdec's performance sharply declines as sentence length grows. In contrast, both RNNsearch-30 and RNNsearch-50 prove more robust to sentence length. Especially RNNsearch50 displays no performance deterioration even at 50+ words per sentence. This proposed model's superiority over basic encoder-decoder is further validated by RNNsearch-30 outperforming RNNencdec-50 (see Table 1).","One impetus for the proposed technique was the static context vector employed in the basic encoder-decoder framework. We surmised this limitation could hamper the basic encoder-decoder with very long sentences. Fig. 2 shows RNNencdec's performance plummets as sentences get longer. Meanwhile, RNNsearch-30 and RNNsearch-50 remain more resilient to sentence length. RNNsearch50 in particular maintains performance despite 50+ word sentences. This proposed model's edge over basic encoder-decoder is additionally evidenced by RNNsearch-30 besting RNNencdec-50 (see Table 1).  ","A driving force behind the suggested approach was the fixed-size context vector used in the standard encoder-decoder structure. We speculated this constraint might inhibit the standard encoder-decoder on very lengthy sentences. Fig. 2 displays RNNencdec's performance diving as sentence length increases. In contrast, RNNsearch-30 and RNNsearch-50 prove more robust to sentence length. RNNsearch50 especially sustains performance even with 50+ word sentences. This proposed model's advantage over basic encoder-decoder is further supported by RNNsearch-30 exceeding RNNencdec-50 (see Table 1).",A,1
Neural Machine Translation by Jointly Learning To Align and Translate,"The proposed approach provides an intuitive way to inspect the (soft-)alignment between the words in a generated translation and those in a source sentence. This is done by visualizing the annotation weights αij from Eq. (6), as in Fig. 3. Each row of a matrix in each plot indicates the weights associated with the annotations. From this we see which positions in the source sentence were considered more important when generating the target word. We can see from the alignments in Fig. 3 that the alignment of words between English and French is largely monotonic.","The suggested method offers an instinctive manner to examine the (soft-)correlation between the terms in a produced translation and those in an original sentence. This is accomplished by picturing the annotation weights αij from Eq. (6), as in Fig. 3. Each row of a matrix in each plot signifies the weights linked with the annotations. From this we discern which locations in the source sentence were deemed more vital when creating the target word. We can perceive from the alignments in Fig. 3 that the alignment of words between English and French is mostly sequential.","The proposed technique provides an intuitive approach to inspect the (soft-)association between the words in a generated translation and those in a source sentence. This is achieved by visualizing the annotation coefficients αij from Equation (6), as shown in Figure 3. Each row of a matrix in each graph indicates the coefficients related to the annotations. From this we can see which positions in the source sentence were considered more important when producing the target word. We can discern from the alignments in Figure 3 that the alignment of words between English and French is largely monotonic.  ","The suggested approach gives an instinctive way to examine the (soft-)linkage between the terms in a produced translation and those in an original sentence. This is done by picturing the annotation weights αij from Formula (6), as in Figure 3. Each row of a matrix in each plot shows the weights connected with the annotations. From this we can discern which locations in the source sentence were viewed as more critical when generating the target word. We can see from the alignments in Figure 3 that the alignment of words between English and French is mostly sequential.",A,1
Neural Machine Translation by Jointly Learning To Align and Translate,"We see strong weights along the diagonal of each matrix. However, we also observe a number of non-trivial, non-monotonic alignments. Adjectives and nouns are typically ordered differently between French and English, and we see an example in Fig. 3 (a). From this figure, we see that the model correctly translates a phrase [European Economic Area] into [zone economique europ ´ een]. The RNNsearch was able to correctly align [zone] with [Area], jumping ´ over the two words ([European] and [Economic]), and then looked one word back at a time to complete the whole phrase [zone economique europ ´ eenne]. ´","There are robust weights along the main diagonal of each matrix. However, there are also several non-trivial, non-monotonic alignments present. The order of adjectives and nouns is often different between French and English, as shown in Fig. 3 (a). This figure demonstrates that the model accurately translates the phrase [European Economic Area] into [zone economique europ ́een]. The RNNsearch correctly aligned [zone] with [Area], skipping over the two words ([European] and [Economic]), and then looked back word-by-word to complete the whole phrase [zone economique europ ́eenne].","We notice strong values on the diagonal of every matrix. But there are also a number of noteworthy, non-monotonic matches too. Descriptors and nouns are typically sequenced differently in French and English, and we observe an illustration in Fig. 3 (a). From this illustration, we note that the model properly interprets a phrase [European Economic Area] into [zone economique europ ́een]. The RNNsearch managed to correctly match [zone] with [Area], jumping past the two words ([European] and [Economic]), and then examined one word at a time to finish the entire phrase [zone economique europ ́eenne].","There are robust magnitudes along the main diagonal of each matrix. However, there are also several significant, non-monotonic correlations present as well. Adjectives and nouns are often ordered differently between French and English, as evidenced in Fig. 3 (a). This figure shows that the model accurately translates the phrase [European Economic Area] into [zone economique europ ́een]. The RNNsearch correctly matched [zone] with [Area], bypassing the two words ([European] and [Economic]), and then inspected one word back sequentially to complete the whole phrase [zone economique europ ́eenne].",A,1
Neural Machine Translation by Jointly Learning To Align and Translate,"The strength of the soft-alignment, opposed to a hard-alignment, is evident, for instance, from Fig. 3 (d). Consider the source phrase [the man] which was translated into [l’ homme]. Any hard alignment will map [the] to [l’] and [man] to [homme]. This is not helpful for translation, as one must consider the word following [the] to determine whether it should be translated into [le], [la], [les] or [l’]. Our soft-alignment solves this issue naturally by letting the model look at both [the] and [man], and in this example, we see that the model was able to correctly translate [the] into [l’]. We observe similar behaviors in all the presented cases in Fig. 3.","The power of the flexible alignment, as opposed to a rigid alignment, is clear, for example, from Fig. 3 (d). Look at the source phrase [the man] which was translated into [l' homme]. Any rigid alignment will map [the] to [l'] and [man] to [homme]. This is not useful for translation, as one must consider the word after [the] to decide if it should be translated into [le], [la], [les] or [l']. Our flexible alignment solves this issue naturally by letting the model look at both [the] and [man], and in this example, we see that the model was able to correctly translate [the] into [l']. We notice similar behaviors in all the presented cases in Fig. 3.","The advantage of the adaptable matching, in contrast to an inflexible matching, is obvious, as seen in Fig. 3 (d). Examine the source phrase [the man] which was converted into [l' homme]. Any inflexible matching will connect [the] to [l'] and [man] to [homme]. This is not beneficial for translation, as one must look at the word following [the] to decide if it should be translated into [le], [la], [les] or [l']. Our adaptable matching resolves this issue naturally by permitting the model to examine both [the] and [man], and in this example, we observe that the model was able to accurately translate [the] into [l']. We notice similar behaviors in all the shown cases in Fig. 3.  ","The benefit of the adjustable alignment, as opposed to a fixed alignment, is apparent, as shown in Fig. 3 (d). Focus on the source phrase [the man] which was rendered as [l' homme]. Any fixed alignment will link [the] to [l'] and [man] to [homme]. This is not useful for translation, as one must look at the word after [the] to determine if it should be translated as [le], [la], [les] or [l']. Our adjustable alignment resolves this issue organically by allowing the model to analyze both [the] and [man], and in this example, we discern that the model was able to accurately translate [the] into [l']. We detect similar behaviors in all the presented cases in Fig. 3.",A,1
Neural Machine Translation by Jointly Learning To Align and Translate,"A similar approach of aligning an output symbol with an input symbol was proposed recently by Graves (2013) in the context of handwriting synthesis. Handwriting synthesis is a task where the model is asked to generate handwriting of a given sequence of characters. In his work, he used a mixture of Gaussian kernels to compute the weights of the annotations, where the location, width and mixture coefficient of each kernel was predicted from an alignment model.","A related method of matching an output character to an input character was put forward not long ago by Graves (2013) for generating handwritten text. Handwriting generation involves getting the model to synthesize handwriting for a particular sequence of letters. He made use of a combination of Gaussian kernels to determine the weights of the annotations, where the placement, size and blend coefficient of each kernel was inferred from an alignment algorithm.","Recently Graves (2013) proposed an analogous system of associating output symbols with input symbols for handwriting synthesis. In handwriting synthesis the model aims to produce handwritten versions of given strings of letters. His approach utilized a mixture of Gaussian kernels for computing annotation weights, with the site, width and mixture weight of each kernel predicted by an alignment scheme. ","A comparable tactic of pairing output characters with input characters was described in 2013 by Graves for handwriting generation. Handwriting generation requires the model to create handwritten forms of specific letter sequences. Graves' method involved Gaussian kernel mixtures to ascertain annotation weights, where the location, scale and mixture component of each kernel was deduced from an alignment model.",A,1
Neural Machine Translation by Jointly Learning To Align and Translate,"More specifically, his alignment was restricted to predict the location such that the location increases monotonically. The main difference from our approach is that, in (Graves, 2013), the modes of the weights of the annotations only move in one direction. In the context of machine translation, this is a severe limitation, as (long-distance) reordering is often needed to generate a grammatically correct translation (for instance, English-to-German). Our approach, on the other hand, requires computing the annotation weight of every word in the source sentence for each word in the translation. This drawback is not severe with the task of translation in which most of input and output sentences are only 15–40 words.","To be more precise, his method was limited to forecasting the position in a way that increased steadily. The main contrast with our technique is that, in Graves (2013), the peaks of the annotation weights were only permitted to shift in one direction. For machine translation, this severely restricts the approach, since reordering words significantly (over long distances) is frequently necessary to produce a grammatically correct translation (for example, from English to German). Our method, on the other hand, needs to calculate the annotation weight for every word in the source sentence for each word in the translation. This downside is not very problematic for translation tasks where most input and output sentences are only 15-40 words long.","Specifically, his system could only predict the location while enforcing that it increased monotonically. The primary difference from our method is that, in Graves (2013), the modes of the annotation weights were constrained to move in just one direction. In machine translation, this greatly limits the approach, because extensive reordering of words (over long distances) is often essential to generate a grammatically correct translation (for instance, English to German). Our approach, conversely, requires computing the annotation weight for every source sentence word for each translated word. This disadvantage is not too concerning for translation tasks where most input and output sentences are only 15-40 words in length.  ","More exactly, his system was limited to predicting the position in a way that increased monotonically. The main divergence from our technique is that, in Graves (2013), the peaks of the annotation weights were restricted to shift in only one direction. For machine translation, this seriously hampers the method, since substantially reordering words (over long spans) is often crucial for producing a grammatically correct translation (for example, English to German). Our technique, on the other hand, necessitates calculating the annotation weight for every source sentence word for each translated word. This drawback is not very problematic for translation tasks in which most input and output sentences are only 15-40 words long.",A,1
Neural Machine Translation by Jointly Learning To Align and Translate,"Since Bengio et al. (2003) introduced a neural probabilistic language model which uses a neural network to model the conditional probability of a word given a fixed number of the preceding words, neural networks have widely been used in machine translation. However, the role of neural networks has been largely limited to simply providing a single feature to an existing statistical machine translation system or to re-rank a list of candidate translations provided by an existing system.","Ever since Bengio and colleagues published their 2003 paper on utilizing neural networks to estimate the probability of a word based on a fixed quantity of prior words, neural networks have become very popular in machine translation. Still, neural networks have mostly just given one extra feature to current statistical machine translation systems or reranked translation options generated by current systems.","Neural networks have become very common in machine translation after Bengio's group described in 2003 how to use them to predict a word's probability given a certain number of preceding words. But so far, neural networks have not done much more than contribute a single feature to existing statistical MT systems or reorder translation candidates already produced by existing systems.  ","Neural networks gained popularity in machine translation following Bengio et al.'s 2003 paper introducing a neural network model to estimate a word's conditional probability given a fixed number of previous words. However, neural networks have played a limited role, either providing an additional feature to existing statistical MT systems or rescoring translation options already produced by existing systems. Their capabilities have not been fully utilized.",A,1
Neural Machine Translation by Jointly Learning To Align and Translate,"For instance, Schwenk (2012) proposed using a feedforward neural network to compute the score of a pair of source and target phrases and to use the score as an additional feature in the phrase-based statistical machine translation system. More recently, Kalchbrenner and Blunsom (2013) and Devlin et al. (2014) reported the successful use of the neural networks as a sub-component of the existing translation system. Traditionally, a neural network trained as a target-side language model has been used to rescore or rerank a list of candidate translations (see, e.g., Schwenk et al., 2006).","As an illustration, Schwenk (2012) suggested utilizing a feedforward neural network for calculating the score of a pair of source and target phrases. He proposed incorporating the score as a supplementary characteristic in the phrase-based statistical machine translation framework. More recently, Kalchbrenner and Blunsom (2013) as well as Devlin et al. (2014) announced the fruitful application of neural networks as a sub-module of current translation systems. Historically, a neural network educated as a target-side language model has been leveraged for rescoring or reranking a collection of candidate translations (refer to, for example, Schwenk et al., 2006).","To give an example, Schwenk (2012) recommended harnessing a feedforward neural network to determine the rating of a source and target phrase pair. He advised integrating the rating as an extra feature in the phrase-based statistical machine translation structure. More recently, Kalchbrenner and Blunsom (2013) together with Devlin et al. (2014) declared the successful utilization of neural networks as a sub-component of existing translation frameworks. Conventionally, a neural network trained as a target-language model has been exploited to rescore or rerank a set of translation options (see, for instance, Schwenk et al., 2006).  ","As a case in point, Schwenk (2012) put forth employing a feedforward neural network for evaluating the score of a source and target phrase pair. He suggested incorporating the score as an additional characteristic in the phrase-based statistical machine translation model. More recently, Kalchbrenner and Blunsom (2013) along with Devlin et al. (2014) proclaimed the effective application of neural networks as a sub-part of current translation architectures. Traditionally, a neural network conditioned as a target-tongue model has been leveraged to rescore or rerank a collection of translation candidates (refer to, for example, Schwenk et al., 2006).",A,1
Neural Machine Translation by Jointly Learning To Align and Translate,"Although the above approaches were shown to improve the translation performance over the state-of-the-art machine translation systems, we are more interested in a more ambitious objective of designing a completely new translation system based on neural networks. The neural machine translation approach we consider in this paper is therefore a radical departure from these earlier works. Rather than using a neural network as a part of the existing system, our model works on its own and generates a translation from a source sentence directly.","While the methods described earlier were able to enhance the capabilities of current machine translation systems, our aim is to take a more ambitious step of creating an entirely new neural network-based translation system. The neural machine translation model we examine here thus represents a major shift from prior efforts. Instead of incorporating a neural network into a pre-existing framework, our model functions independently to produce translations directly from source sentences.","Although previous techniques improved performance compared to state-of-the-art machine translation systems, we are more interested in the bolder goal of building a brand new translation system using neural networks. The neural machine translation approach we look at in this paper therefore diverges sharply from earlier work. Rather than employing a neural network within an existing framework, our model works on its own to generate translations straight from source sentences.  ","Despite prior methods demonstrating enhanced results over current machine translation systems, our focus is on the more ambitious aim of creating an entirely novel translation system leveraging neural networks. The neural machine translation model considered here is thus a radical change from previous attempts. In contrast to incorporating a neural network into a legacy framework, our model operates autonomously to produce translations directly from source sentences.",A,1
Neural Machine Translation by Jointly Learning To Align and Translate,"The conventional approach to neural machine translation, called an encoder–decoder approach, encodes a whole input sentence into a fixed-length vector from which a translation will be decoded. We conjectured that the use of a fixed-length context vector is problematic for translating long sentences, based on a recent empirical study reported by Cho et al. (2014b) and Pouget-Abadie et al. (2014). In this paper, we proposed a novel architecture that addresses this issue. We extended the basic encoder–decoder by letting a model (soft-)search for a set of input words, or their annotations computed by an encoder, when generating each target word.","The standard way of doing neural machine translation, known as encoder-decoder, turns an entire input sentence into a vector of fixed length that a translation will be created from. We hypothesized that using a fixed-length context vector causes issues for translating long sentences, based on a recent study by Cho et al. (2014b) and Pouget-Abadie et al. (2014). In this paper, we suggested a new architecture that solves this problem. We expanded the basic encoder-decoder by letting a model (softly) look for a set of input words, or their encodings generated by an encoder, when producing each target word.","The typical approach to neural machine translation, called encoder-decoder, encodes an entire input sentence into a static-length vector that a translation will be generated from. We theorized that utilizing a static-length context vector is problematic for translating lengthy sentences, based on a recent empirical study by Cho et al. (2014b) and Pouget-Abadie et al. (2014). In this paper, we proposed an innovative architecture that resolves this issue. We extended the fundamental encoder-decoder by enabling a model to (softly) search for a set of input words, or their representations computed by an encoder, when creating each target word.","The conventional technique for neural machine translation, known as encoder-decoder, transforms an entire input sentence into a fixed-size vector that a translation will be derived from. We conjectured that employing a fixed-size context vector causes difficulties for translating long sentences, based on a recent empirical study by Cho et al. (2014b) and Pouget-Abadie et al. (2014). In this paper, we put forth a novel architecture that addresses this problem. We augmented the basic encoder-decoder by allowing a model to (softly) look for a set of input words, or their encodings generated by an encoder, when producing each target word.",A,1
Neural Machine Translation by Jointly Learning To Align and Translate,"This frees the model from having to encode a whole source sentence into a fixed-length vector, and also lets the model focus only on information relevant to the generation of the next target word. This has a major positive impact on the ability of the neural machine translation system to yield good results on longer sentences. Unlike with the traditional machine translation systems, all of the pieces of the translation system, including the alignment mechanism, are jointly trained towards a better log-probability of producing correct translations. We tested the proposed model, called RNNsearch, on the task of English-to-French translation.","This allows the model to avoid compressing an entire source sentence into a single fixed-length vector. It can instead focus only on the information needed to generate the next word in the target language. This significantly improves the neural machine translation system's ability to handle longer sentences. With this approach, all parts of the translation system, including the alignment component, are trained together to maximize the log probability of generating accurate translations. We evaluated this proposed RNNsearch model on English-to-French translation.","This removes the requirement to encode a full source sentence into a static vector. Rather, it enables concentrating exclusively on the relevant details for producing the next target word. This greatly enhances the neural machine translation system's capacity to process longer sentences. In contrast to traditional machine translation systems, all elements of the translation system, including the alignment mechanism, are trained jointly to optimize the log probability of producing correct translations. We tested the RNNsearch model, which uses this approach, on English-to-French translation.  ","This exempts the model from having to compress an entire source sentence into an inflexible vector. It also allows focusing solely on the information needed to generate the next word in the target language. This significantly boosts the neural machine translation system's ability to handle longer sentences well. Unlike traditional machine translation systems, all components of the translation system, including alignment, are trained together to maximize the log probability of generating accurate translations. We evaluated the proposed RNNsearch model, which utilizes this approach, on English-to-French translation.",A,1
Neural Machine Translation by Jointly Learning To Align and Translate,"The experiment revealed that the proposed RNNsearch outperforms the conventional encoder–decoder model (RNNencdec) significantly, regardless of the sentence length and that it is much more robust to the length of a source sentence. From the qualitative analysis where we investigated the (soft-)alignment generated by the RNNsearch, we were able to conclude that the model can correctly align each target word with the relevant words, or their annotations, in the source sentence as it generated a correct translation. Perhaps more importantly, the proposed approach achieved a translation performance comparable to the existing phrase-based statistical machine translation.","The test showed that the suggested RNNsearch is much better than the standard encoder-decoder RNN model (RNNencdec), no matter the length of the sentence. It is also far more capable of handling source sentences of different lengths. Looking at the soft-alignment produced by RNNsearch, we saw it could properly connect each translated word with the right words or notes in the original sentence to produce an accurate translation. Most importantly, this method reached a translation quality on par with current phrase-based statistical machine translation systems.","The experiment demonstrated the proposed RNNsearch significantly outdoes the typical encoder-decoder RNN (RNNencdec) across all source sentence lengths and handles length variability much better. Analyzing the soft-alignments from RNNsearch revealed it accurately links each target word to the relevant source words or annotations to produce correct translations. Perhaps most significantly, this approach achieved translation accuracy comparable to existing statistical machine translation based on phrases. ","The test showed the suggested RNNsearch far surpasses the normal encoder-decoder RNN (RNNencdec) for all sentence lengths and handles length changes much more robustly. Studying the soft-alignment from RNNsearch, we found it properly associates each translated word with the right source words or notes to generate an accurate translation. Most importantly, this method reached translation quality equal to current statistical machine translation using phrases.",A,1
Neural Machine Translation by Jointly Learning To Align and Translate,"For the activation function f of an RNN, we use the gated hidden unit recently proposed by Cho et al. (2014a). The gated hidden unit is an alternative to the conventional simple units such as an element-wise tanh. This gated unit is similar to a long short-term memory (LSTM) unit proposed earlier by Hochreiter and Schmidhuber (1997), sharing with it the ability to better model and learn long-term dependencies. This is made possible by having computation paths in the unfolded RNN for which the product of derivatives is close to 1. These paths allow gradients to flow backward easily without suffering too much from the vanishing effect (Hochreiter, 1991; Bengio et al., 1994; Pascanu et al., 2013a).","The activation function we use for the RNN is the gated hidden unit recently developed by Cho et al. (2014a). This gated unit is an alternative to more basic units like element-wise tanh. It is similar to the long short-term memory unit proposed earlier by Hochreiter and Schmidhuber (1997), in that it shares the ability to better model and learn long-term dependencies. This is achieved by having computation paths in the unfolded RNN where the product of derivatives is close to 1. These paths let gradients flow backward without too much vanishing (Hochreiter, 1991; Bengio et al., 1994; Pascanu et al., 2013a).","For the RNN activation function, we utilize the gated hidden unit recently created by Cho et al. (2014a). This gated unit is a substitute for conventional simple units like element-wise tanh. It resembles the long short-term memory unit proposed earlier by Hochreiter and Schmidhuber (1997), sharing the capacity to better model and learn long-term dependencies. This is accomplished by having computation paths in the unfolded RNN where the product of derivatives is near 1. These paths enable gradients to flow backward without excessive vanishing (Hochreiter, 1991; Bengio et al., 1994; Pascanu et al., 2013a).  ","The activation function we employ for the RNN is the gated hidden unit recently invented by Cho et al. (2014a). This gated unit is an alternative to traditional basic units such as element-wise tanh. It is similar to the long short-term memory unit proposed earlier by Hochreiter and Schmidhuber (1997), in that it shares the ability to better model and learn long-term dependencies. This is achieved by having computation paths in the unfolded RNN where the product of derivatives is close to 1. These paths allow gradients to flow backward without too much vanishing (Hochreiter, 1991; Bengio et al., 1994; Pascanu et al., 2013a).",A,1
Neural Machine Translation by Jointly Learning To Align and Translate,"It is a striking result, considering that the proposed architecture, or the whole family of neural machine translation, has only been proposed as recently as this year. We believe the architecture proposed here is a promising step toward better machine translation and a better understanding of natural languages in general. One of challenges left for the future is to better handle unknown, or rare words. This will be required for the model to be more widely used and to match the performance of current state-of-the-art machine translation systems in all contexts.","This is a remarkable finding, given that the suggested design, or the entire collection of neural machine translation systems, was only put forward this year. We think the framework presented here is a positive step toward enhanced machine translation and improved comprehension of natural languages overall. One of the difficulties still to be addressed is better management of unfamiliar, or uncommon words. This will be necessary for the model to have broader application and equal the performance of current best-in-class machine translation systems in all contexts.","It is an astonishing outcome, considering that the recommended model, or the whole group of neural machine translation approaches, was just brought up this year. We believe the structure described here is a promising advance toward superior machine translation and better understanding of natural languages in general. One of the challenges still ahead is to better process unknown, or rare words. This will be needed for the system to be more extensively used and to match the capabilities of today's state-of-the-art machine translation systems in all situations.","This is a striking finding, given that the proposed design, or the entire set of neural machine translation techniques, was only introduced this year. We think the framework outlined here is a positive step toward better machine translation and increased comprehension of natural languages overall. One of the issues still to be tackled is improved handling of unfamiliar, or uncommon words. This will be required for the system to have broader usefulness and equal the capabilities of current best-in-class machine translation systems in all contexts.",A,1
Neural Machine Translation by Jointly Learning To Align and Translate,"Each SGD update direction was computed with a minibatch of 80 sentences. At each update our implementation requires time proportional to the length of the longest sentence in a minibatch. Hence, to minimize the waste of computation, before every 20-th update, we retrieved 1600 sentence pairs, sorted them according to the lengths and split them into 20 minibatches. The training data was shuffled once before training and was traversed sequentially in this manner. In Tables 2 we present the statistics related to training all the models used in the experiments.","For each SGD update step, we used a batch of 80 sentences. Our implementation needs time proportional to the length of the longest sentence in a batch. So to reduce wasted computation, every 20 updates we took 1600 pairs, sorted them by length, and split into 20 batches. We shuffled the data once before training and went through it sequentially this way. Table 2 has statistics for training all the models we experimented with.","The direction of each SGD update was found using a mini-batch of 80 sentences. Our code takes time relative to the longest sentence in a mini-batch. Therefore, before every 20th update, we got 1600 pairs, ordered them by length, and divided into 20 mini-batches, to minimize wasted computation. The training data was randomized once before training and was gone through in order for updates. Table 2 shows information about training all the models used in the tests.","Each SGD update vector was determined using a batch of 80 sentences. Our implementation takes time proportional to the length of the longest sentence in a batch. So before every 20th update, we took 1600 pairs, sorted them by length, and split into 20 batches, to reduce wasted computation. The training data was shuffled once before training and traversed sequentially this way for updates. Table 2 presents statistics about training all the models used in the experiments.",A,1
RoBERTa_A Robustly Optimized BERT Pretraining Approach,"Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al. , 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements.","Pretraining language models has resulted in major performance improvements, but carefully comparing different approaches is challenging. Training takes a lot of computation, is often done using private datasets of varying sizes, and, as we show, choices of hyperparameters have a big impact on final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the effect of many key hyperparameters and training data size. We find BERT was significantly undertrained, and can match or surpass the performance of all models published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported gains.","Though pretraining language models leads to big performance gains, carefully comparing different approaches is difficult. Training demands lots of computation, frequently uses private datasets of different sizes, and, as we demonstrate, choices of hyperparameters greatly affect final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that thoroughly measures the impact of many key hyperparameters and amount of training data. We find BERT was substantially undertrained, and can equal or outperform all models published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously disregarded design choices, and raise questions about the source of recently published improvements.  ","While pretraining language models results in major performance improvements, carefully contrasting different approaches is challenging. Training requires extensive computation, regularly utilizes private datasets of varying sizes, and, as we exhibit, selections of hyperparameters significantly influence final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that meticulously gauges the effect of many key hyperparameters and quantity of training data. We find BERT was meaningfully undertrained, and can match or surpass all models published after it. Our best model accomplishes state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously ignored design choices, and raise questions regarding the source of recently documented advancements.",A,1
RoBERTa_A Robustly Optimized BERT Pretraining Approach,"Self-training methods such as ELMo (Peters et al. , 2018), GPT (Radford et al. , 2018), BERT (Devlin et al. , 2019), XLM (Lample and Conneau , 2019), and XLNet (Yang et al. , 2019) have brought significant performance gains, but it can be challenging to determine which aspects of the methods contribute the most. Training is computationally expensive, limiting the amount of tuning that can be done, and is often done with private training data of varying sizes, limiting our ability to measure the effects of the modeling advances.","Recently developed unsupervised learning techniques like ELMo, GPT, BERT, XLM, and XLNet have substantially improved performance. However, pinpointing the most critical components of these methods can be tricky. Training takes a lot of computing power, restricting optimization opportunities. Training data is usually private and varies in size, making it hard to quantify modeling improvements.","New unsupervised learning systems such as ELMo, GPT, BERT, XLM, and XLNet have led to major gains. But determining the most important parts of these systems is difficult. Training demands extensive computing resources, limiting tuning opportunities. Training data tends to be proprietary and inconsistent in size, obstructing measurement of modeling enhancements.  ","Novel self-supervised learning models including ELMo, GPT, BERT, XLM, and XLNet have achieved impressive progress. Though, isolating the most vital aspects of these approaches is challenging. Training is computationally demanding, constraining hyperparameter tuning. Training sets are typically private and inconsistent in amount, hindering quantification of modeling developments.",A,1
RoBERTa_A Robustly Optimized BERT Pretraining Approach,"We present a replication study of BERT pretraining (Devlin et al., 2019), which includes a careful evaluation of the effects of hyperparmeter tuning and training set size. We find that BERT was significantly undertrained and propose an improved recipe for training BERT models, which we call RoBERTa, that can match or exceed the performance of all of the post-BERT methods. Our modifications are simple, they include: (1) training the model longer, with bigger batches, over more data; (2) removing the next sentence prediction objective; (3) training on longer sequences; and (4) dynamically changing the masking pattern applied to the training data.","We conducted a replication analysis of BERT pretraining (Devlin et al., 2019). Our study thoroughly evaluated the impacts of hyperparameter tuning and training dataset size. We determined BERT was considerably undertrained. We suggest an enhanced procedure for training BERT models, which we call RoBERTa, that can equal or surpass the performance of all post-BERT approaches. Our modifications are straightforward. They involve: (1) training the model longer, with larger batches, on more data; (2) eliminating the next sentence prediction goal; (3) training on extended sequences; and (4) dynamically altering the masking pattern used on the training data.","We performed a reproduction analysis of BERT pretraining (Devlin et al., 2019). Our examination carefully assessed the effects of hyperparameter adjustment and size of the training set. We found BERT was significantly undertrained. We propose an improved method for developing BERT models, which we call RoBERTa, that can match or exceed the capabilities of all post-BERT techniques. Our changes are simple. They include: (1) training the model for longer, with larger batches, using more data; (2) removing the next sentence forecasting objective; (3) training on longer sequences; and (4) dynamically modifying the masking pattern applied to the training data.  ","We conducted a replication study of BERT pretraining (Devlin et al., 2019). Our evaluation thoroughly examined the influences of hyperparameter tuning and amount of training data. We determined BERT was substantially undertrained. We present an enhanced procedure for constructing BERT models, called RoBERTa, that can equal or surpass all post-BERT methods. Our modifications are straightforward. They consist of: (1) training the model longer, with bigger batches, on increased data; (2) eliminating the next sentence prediction goal; (3) training on extended sequences; and (4) dynamically changing the masking pattern used on the training data.",A,1
RoBERTa_A Robustly Optimized BERT Pretraining Approach,"We also collect a large new dataset (CC-NEWS) of comparable size to other privately used datasets, to better control for training set size effects. When controlling for training data, our improved training procedure improves upon the published BERT results on both GLUE and SQuAD. When trained for longer over additional data, our model achieves a score of 88.5 on the public GLUE leaderboard, matching the 88.4 reported by Yang et al. (2019). Our model establishes a new state-of-the-art on 4/9 of the GLUE tasks: MNLI, QNLI, RTE and STS-B. We also match state-of-the-art results on SQuAD and RACE.","Furthermore, we assemble a large new dataset (CC-NEWS) comparable in size to other privately used datasets, to better account for effects of training set size. When accounting for training data, our enhanced training procedure surpasses the published BERT results on both GLUE and SQuAD. When trained longer over supplementary data, our model accomplishes a score of 88.5 on the public GLUE leaderboard, equaling the 88.4 reported by Yang et al. (2019). Our model sets a new state-of-the-art on 4/9 of the GLUE tasks: MNLI, QNLI, RTE and STS-B. We also match state-of-the-art results on SQuAD and RACE.","In addition, we compile a large novel dataset (CC-NEWS) of similar scale to other privately utilized datasets, to better regulate for training set size effects. When regulating for training data, our improved training process outperforms the published BERT results on both GLUE and SQuAD. When trained for an extended time over extra data, our model achieves a score of 88.5 on the public GLUE leaderboard, equalling the 88.4 reported by Yang et al. (2019). Our model establishes a new state-of-the-art on 4/9 of the GLUE tasks: MNLI, QNLI, RTE and STS-B. We also match state-of-the-art results on SQuAD and RACE.  ","Moreover, we gather a large new dataset (CC-NEWS) comparable in magnitude to other privately used datasets, to better control for training set size impacts. When controlling for training data, our enhanced training methodology surpasses the published BERT outcomes on both GLUE and SQuAD. When trained longer over supplementary data, our model attains a score of 88.5 on the public GLUE leaderboard, equaling the 88.4 reported by Yang et al. (2019). Our model institutes a new state-of-the-art on 4/9 of the GLUE tasks: MNLI, QNLI, RTE and STS-B. We also match state-of-the-art outcomes on SQuAD and RACE.",A,1
RoBERTa_A Robustly Optimized BERT Pretraining Approach,"In summary, the contributions of this paper are: (1) We present a set of important BERT design choices and training strategies and introduce alternatives that lead to better downstream task performance; (2) We use a novel dataset, CCNEWS, and confirm that using more data for pretraining further improves performance on downstream tasks; (3) Our training improvements show that masked language model pretraining, under the right design choices, is competitive with all other recently published methods. We release our model, pretraining and fine-tuning code implemented in PyTorch (Paszke et al., 2017).","To summarize, this paper makes the following key contributions: (1) We put forward several important architectural and training decisions for BERT, as well as alternative options that result in superior performance on downstream tasks; (2) Using a new dataset called CCNEWS, we validate that pretraining on more data leads to additional gains on downstream tasks; (3) Our training enhancements demonstrate that masked language modeling, with optimal design choices, can be as effective as all other recently published techniques. We are releasing our model code, pretraining and fine-tuning procedures built in PyTorch (Paszke et al., 2017).","In brief, this paper makes the following contributions: (1) We introduce a set of critical design and training strategies for BERT, as well as substitutes that improve performance on downstream tasks; (2) Leveraging a novel CCNEWS dataset, we show that larger pretraining data further boosts downstream task accuracy; (3) Our training advancements exhibit that masked language modeling pretraining, with the right configurations, is on par with all other state-of-the-art methods recently published. We provide our model, pretraining and fine-tuning implementations in PyTorch (Paszke et al., 2017).","To recap, the key contributions of this paper are: (1) We present several important architectural and training choices for BERT and suggest alternatives that lead to superior downstream task accuracy; (2) Using a new CCNEWS dataset, we verify that more pretraining data further enhances downstream performance; (3) Our training enhancements demonstrate that masked language modeling pretraining, with optimal settings, is competitive with all other recently reported techniques. We release our model, pretraining and fine-tuning code built using PyTorch (Paszke et al., 2017).",A,1
RoBERTa_A Robustly Optimized BERT Pretraining Approach,"NSP is a binary classification loss for predicting whether two segments follow each other in the original text. Positive examples are created by taking consecutive sentences from the text corpus. Negative examples are created by pairing segments from different documents. Positive and negative examples are sampled with equal probability. The NSP objective was designed to improve performance on downstream tasks, such as Natural Language Inference (Bowman et al., 2015), which require reasoning about the relationships between pairs of sentences.","NSP is a binary classification error function for guessing if two chunks come one after the other in the first text. Good examples are made by getting back to back sentences from the text collection. Bad examples are made by coupling chunks from varying files. Good and bad examples are taken with equal chance. The NSP goal was intended to get better outcomes on downstream errands, like Natural Language Inference (Bowman et al., 2015), which need thinking regarding the connections between sets of sentences.","NSP is a binary classification cost function for determining if two segments are consecutive in the source text. Positive instances are produced by taking sequential sentences from the text dataset. Negative instances are produced by pairing segments from separate documents. Positive and negative instances are sampled with equal likelihood. The NSP cost function was devised to enhance performance on downstream tasks, such as Natural Language Inference (Bowman et al., 2015), which necessitate reasoning about the relationships between pairs of sentences. ","NSP is a binary classification loss function for predicting whether two excerpts follow each other in the original text. Affirmative examples are generated by taking successive sentences from the text collection. Contrary examples are generated by coupling excerpts from different documents. Affirmative and contrary examples are sampled with equal probability. The NSP loss function was designed to improve performance on downstream tasks, such as Natural Language Inference (Bowman et al., 2015), which require inferencing about the relationships between pairs of sentences.",A,1
RoBERTa_A Robustly Optimized BERT Pretraining Approach,"We reimplement BERT in FAIRSEQ (Ott et al., 2019). We primarily follow the original BERT optimization hyperparameters, given in Section 2, except for the peak learning rate and number of warmup steps, which are tuned separately for each setting. We additionally found training to be very sensitive to the Adam epsilon term, and in some cases we obtained better performance or improved stability after tuning it. Similarly, we found setting β2 = 0.98 to improve stability when training with large batch sizes. We pretrain with sequences of at most T = 512 tokens.","We re-create BERT using the FAIRSEQ framework (Ott et al., 2019). We mostly use the same optimization hyperparameters from the original BERT, listed in Section 2, except we adjust the maximum learning rate and number of warmup steps for each scenario. We also found that tweaking the Adam epsilon parameter improved performance and stability in some cases. Likewise, setting β2 = 0.98 helped stabilize training when using large batch sizes. Our pretraining used sequences with up to T = 512 tokens.","We implement BERT again in the FAIRSEQ library (Ott et al., 2019). We follow most of the original BERT optimization hyperparameters given in Section 2, but tune the peak learning rate and warmup steps separately for each setting. We also discovered that modifying the Adam epsilon term improved performance and stability sometimes. Similarly, setting β2 = 0.98 enhanced stability during training with large batches. Our pretraining used sequences with a maximum of T = 512 tokens.  ","We build BERT again using FAIRSEQ (Ott et al., 2019). We adhere to most of the original BERT optimization hyperparameters from Section 2, except we customize the maximum learning rate and number of warmup steps per setting. We also found that adjusting the Adam epsilon parameter boosted performance and stability in certain cases. Likewise, using β2 = 0.98 helped stabilize training when employing large batch sizes. Our pretraining utilized sequences capped at T = 512 tokens.",A,1
RoBERTa_A Robustly Optimized BERT Pretraining Approach,"BERT-style pretraining crucially relies on large quantities of text. Baevski et al. (2019) demonstrate that increasing data size can result in improved end-task performance. Several efforts have trained on datasets larger and more diverse than the original BERT (Radford et al., 2019; Yang et al., 2019; Zellers et al., 2019). Unfortunately, not all of the additional datasets can be publicly released. For our study, we focus on gathering as much data as possible for experimentation, allowing us to match the overall quality and quantity of data as appropriate for each comparison.","BERT-style pre-training fundamentally depends on huge amounts of text. Baevski et al. (2019) show that expanding data volume can lead to enhanced end-task results. Multiple attempts have trained on datasets bigger and more varied than the original BERT (Radford et al., 2019; Yang et al., 2019; Zellers et al., 2019). Sadly, not all the extra datasets can be made public. For our analysis, we concentrate on gathering as much data as feasible for experimentation, enabling us to match the overall quality and amount of data as suitable for each comparison.","BERT-style pre-training is critically reliant on massive amounts of text. Baevski et al. (2019) demonstrate that increasing the size of the data can produce better end-task performance. Several efforts have trained using datasets larger and more diverse than the original BERT (Radford et al., 2019; Yang et al., 2019; Zellers et al., 2019). Unfortunately, not all the additional datasets can be released publicly. For our study, we focus on collecting as much data as possible for experimentation, allowing us to match the general quality and quantity of data as appropriate for each comparison.","BERT-style pre-training fundamentally hinges on huge quantities of text. Baevski et al. (2019) show that expanding data size can result in improved end-task results. Multiple attempts have trained using datasets bigger and more varied than the original BERT (Radford et al., 2019; Yang et al., 2019; Zellers et al., 2019). Regrettably, not all the extra datasets can be made public. For our analysis, we concentrate on gathering as much data as feasible for experimentation, permitting us to match the general quality and amount of data as suitable for each comparison.",A,1
RoBERTa_A Robustly Optimized BERT Pretraining Approach,"The General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2019b) is a collection of 9 datasets for evaluating natural language understanding systems.6 Tasks are framed as either single-sentence classification or sentence-pair classification tasks. The GLUE organizers provide training and development data splits as well as a submission server and leaderboard that allows participants to evaluate and compare their systems on private held-out test data.","The General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2019b) is a set of 9 data collections used to assess natural language processing systems. The tasks are structured as either classifying a single sentence or classifying a pair of sentences. The GLUE organizers give training and development data splits and also a submission server and leaderboard which lets participants evaluate and contrast their systems using private held-out test data that is not released publicly.","The General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2019b) consists of 9 datasets for testing natural language understanding systems. The tasks are designed as either categorizing one sentence or categorizing two sentences. The GLUE organizers provide splits of training data and development data, as well as a submission server and leaderboard that enables participants to measure and compare their systems using unseen test data that is kept private. ","The General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2019b) contains 9 collections of data for assessing natural language processing systems. The tasks are set up as classifying either a single sentence or a pair of sentences. The GLUE organizers make available splits of training data and development data, and also provide a submission server and leaderboard which allows participants to evaluate and benchmark their systems using held-out test data that is kept confidential.",A,1
RoBERTa_A Robustly Optimized BERT Pretraining Approach,"For the replication study in Section 4, we report results on the development sets after finetuning the pretrained models on the corresponding singletask training data (i.e., without multi-task training or ensembling). Our finetuning procedure follows the original BERT paper (Devlin et al., 2019). In Section 5 we additionally report test set results obtained from the public leaderboard. These results depend on a several task-specific modifications, which we describe in Section 5.1.","Regarding the replication experiment detailed in Section 4, we present findings on the development sets after customizing the pre-trained models on the matching single-task training information (meaning, without multi-task learning or assembling). Our customization process adheres to the original BERT publication (Devlin et al., 2019). In Section 5 we further present test set conclusions attained from the public leaderboard. These conclusions depend on numerous task-specific changes, which we explain in Section 5.1.","For the reproduction analysis discussed in Section 4, we document conclusions on the development sets subsequent to tuning the pre-existing models on the related single-task preparation statistics (that is, excluding multi-task optimization or combining). Our tuning approach mirrors the original BERT manuscript (Devlin et al., 2019). In Section 5 we additionally document test set conclusions derived from the public leaderboard. These conclusions hinge on several task-explicit alterations, which we elucidate in Section 5.1.  ","Regarding the replication review outlined in Section 4, we convey deductions on the development sets succeeding adapting the pre-trained prototypes on the associated single-task instruction evidence (specifically, minus multi-task education or integrating). Our adapting system reflects the original BERT draft (Devlin et al., 2019). In Section 5 we further convey test set inferences obtained from the public leaderboard. These inferences depend on numerous task-defined changes, which we clarify in Section 5.1.",A,1
RoBERTa_A Robustly Optimized BERT Pretraining Approach,"The Stanford Question Answering Dataset (SQuAD) provides a paragraph of context and a question. The task is to answer the question by extracting the relevant span from the context. We evaluate on two versions of SQuAD: V1.1 and V2.0 (Rajpurkar et al., 2016, 2018). In V1.1 the context always contains an answer, whereas in V2.0 some questions are not answered in the provided context, making the task more challenging. For SQuAD V1.1 we adopt the same span prediction method as BERT (Devlin et al., 2019). For SQuAD V2.0, we add an additional binary classifier to predict whether the question is answerable, which we train jointly by summing the classification and span loss terms.","The Stanford Question Answering Dataset (SQuAD) gives a paragraph of background information and a question. The goal is to respond to the question by finding the relevant snippet from the context. We evaluate on two versions of SQuAD: V1.1 and V2.0 (Rajpurkar et al., 2016, 2018). In V1.1 the background always has an answer, while in V2.0 some questions are not answered in the provided context, making the task more tricky. For SQuAD V1.1 we use the same span prediction approach as BERT (Devlin et al., 2019). For SQuAD V2.0, we add an extra binary classifier to predict if the question can be answered, which we train together by adding up the classification and span loss terms.","The Stanford Question Answering Dataset (SQuAD) provides a passage of background information and a question. The objective is to respond to the question by extracting the relevant excerpt from the context. We assess on two versions of SQuAD: V1.1 and V2.0 (Rajpurkar et al., 2016, 2018). In V1.1 the background always includes an answer, while in V2.0 some questions are not answered in the given context, making the task more challenging. For SQuAD V1.1 we use the same span prediction method as BERT (Devlin et al., 2019). For SQuAD V2.0, we append an additional binary classifier to predict if the question can be answered, which we train jointly by combining the classification and span loss terms.  ","The Stanford Question Answering Dataset (SQuAD) provides a passage of context and a question. The aim is to reply to the question by pulling out the relevant section from the context. We evaluate on two versions of SQuAD: V1.1 and V2.0 (Rajpurkar et al., 2016, 2018). In V1.1 the context always has an answer, while in V2.0 some questions are not answered in the given context, making the task more difficult. For SQuAD V1.1 we use the same span prediction approach as BERT (Devlin et al., 2019). For SQuAD V2.0, we append an extra binary classifier to predict if the question is answerable, which we train together by summing the classification and span loss quantities.",A,1
RoBERTa_A Robustly Optimized BERT Pretraining Approach,"The ReAding Comprehension from Examinations (RACE) (Lai et al., 2017) task is a large-scale reading comprehension dataset with more than 28,000 passages and nearly 100,000 questions. The dataset is collected from English examinations in China, which are designed for middle and high school students. In RACE, each passage is associated with multiple questions. For every question, the task is to select one correct answer from four options. RACE has significantly longer context than other popular reading comprehension datasets and the proportion of questions that requires reasoning is very large.","The ReAding Comprehension from Exams (RACE) dataset (Lai et al., 2017) is a large reading comprehension dataset containing over 28,000 passages and close to 100,000 questions. The data was gathered from English tests in China, designed for middle school and high school learners. In RACE, each passage has multiple associated questions. For each question, the task is to choose one right answer from four choices. RACE has much longer contexts than other common reading comprehension datasets, and most questions require reasoning.","The Reading Comprehension from Exams (RACE) dataset (Lai et al., 2017) is a sizable reading comprehension data collection containing more than 28,000 passages and nearly 100,000 questions. The information was accumulated from English examinations in China, intended for middle and high school pupils. In RACE, every passage has various related questions. For each inquiry, the job is to select one accurate response from four options. RACE has significantly more extended contexts than other prevalent reading comprehension data collections, and the majority of questions need logical reasoning.  ","The Reading Comprehension from Exams (RACE) data set (Lai et al., 2017) is a large-scale reading comprehension data collection with over 28,000 passages and close to 100,000 questions. The information was gathered from English tests in China, designed for middle school and high school students. In RACE, every passage has multiple associated questions. For each question, the task is to pick one correct response out of four choices. RACE has much longer contexts than other common reading comprehension data sets, and a high proportion of the questions require reasoning.",A,1
RoBERTa_A Robustly Optimized BERT Pretraining Approach,"As discussed in Section 2, BERT relies on randomly masking and predicting tokens. The original BERT implementation performed masking once during data preprocessing, resulting in a single static mask. To avoid using the same mask for each training instance in every epoch, training data was duplicated 10 times so that each sequence is masked in 10 different ways over the 40 epochs of training. Thus, each training sequence was seen with the same mask four times during training. We compare this strategy with dynamic masking where we generate the masking pattern every time we feed a sequence to the model. This becomes crucial when pretraining for more steps or with larger datasets.","As mentioned in Section 2, BERT uses random masking and guessing of words. The first version of BERT did masking one time while preparing the data, making a single fixed mask. To not use the same mask for every training example in all epochs, the training data was copied 10 times so each sequence was masked 10 different ways over the 40 training epochs. So each training sequence saw the same mask 4 times during training. We compare this approach to dynamic masking where we create the masking pattern whenever we give a sequence to the model. This becomes important when pretraining for more steps or with larger datasets.","As described in Section 2, BERT utilizes random obscuring and predicting of tokens. The original implementation of BERT performed obscuring once while processing the data, resulting in a single static obscuring. To avoid utilizing the same obscuring for every training instance in each epoch, the training data was duplicated 10 times so each sequence is obscured in 10 distinct ways over the 40 epochs of training. Thus, each training sequence was seen with the same obscuring four times during training. We compare this tactic with dynamic obscuring where we generate the obscuring pattern whenever we provide a sequence to the model. This becomes crucial when pretraining for more steps or with larger datasets.  ","As talked about in Section 2, BERT uses random masking and guessing of tokens. The first version of BERT did masking one time while getting the data ready, making a single fixed mask. To avoid using the same mask for every training example in all cycles of training, the training data was copied 10 times so each sequence was masked 10 different ways over the 40 training cycles. So each training sequence saw the same mask 4 times during training. We compare this method to dynamic masking where we make the masking pattern whenever we feed a sequence to the model. This becomes important when pretraining for more steps or with larger datasets.",A,1
RoBERTa_A Robustly Optimized BERT Pretraining Approach,"In the original BERT pretraining procedure, the model observes two concatenated document segments, which are either sampled contiguously from the same document (with p = 0.5) or from distinct documents. In addition to the masked language modeling objective, the model is trained to predict whether the observed document segments come from the same or distinct documents via an auxiliary Next Sentence Prediction (NSP) loss.","The first version of BERT was pretrained by showing it two sections of text joined together. Half the time these were from the same document, and half the time they were from different documents. As well as predicting the masked words, BERT had to predict whether the two sections were from the same document or not. This was an extra task called Next Sentence Prediction that BERT was trained on.","In the original pretraining process for BERT, it would see two parts of text joined together. These parts were either taken from the same document (50% of the time) or from different documents. BERT had two training objectives: masked language modeling to predict masked words, and next sentence prediction where it predicted if the two text parts were from the same document or not. The latter was an auxiliary loss function. ","When BERT was first pretrained, it was shown pairs of text segments that were concatenated together. These segments were sampled either from the same document (50% probability) or from different documents. BERT was trained on two tasks: masked language model prediction to predict masked words, and next sentence prediction where it predicted whether the two segments came from the same document or not. The latter was an extra training loss.",A,1
RoBERTa_A Robustly Optimized BERT Pretraining Approach,"We next compare training without the NSP loss and training with blocks of text from a single document (DOC-SENTENCES). We find that this setting outperforms the originally published BERTBASE results and that removing the NSP loss matches or slightly improves downstream task performance, in contrast to Devlin et al. (2019). It is possible that the original BERT implementation may only have removed the loss term while still retaining the SEGMENT-PAIR input format","In the next experiment, we evaluate training without using the next sentence prediction loss and training using blocks of text from the same document (DOC-SENTENCES). We discover that this configuration surpasses the originally published BERTBASE results and removing the NSP loss equals or slightly enhances performance on downstream tasks, contradicting Devlin et al. (2019). One possibility is that the original BERT implementation may have only eliminated the loss function while still keeping the SEGMENT-PAIR input format.","Subsequently, we do a comparison of training minus the next sentence prediction loss and training utilizing chunks of text from a solitary document (DOC-SENTENCES). We find that this setting is superior to the originally published BERTBASE results and that removing the NSP loss matches or marginally improves downstream task performance, contrasting Devlin et al. (2019). It could be that the original BERT implementation perhaps only removed the loss term while still retaining the SEGMENT-PAIR input format. ","In the following experiment, we make a comparison between training without using the next sentence prediction loss and training employing sections of text from one document (DOC-SENTENCES). We determine that this configuration surpasses the originally published BERTBASE results and eliminating the NSP loss equals or slightly boosts downstream task performance, differing from Devlin et al. (2019). One possibility is that the original BERT implementation may have only taken away the loss function while still keeping the SEGMENT-PAIR input format.",A,1
RoBERTa_A Robustly Optimized BERT Pretraining Approach,"Past work in Neural Machine Translation has shown that training with very large mini-batches can both improve optimization speed and end-task performance when the learning rate is increased appropriately (Ott et al., 2018). Recent work has shown that BERT is also amenable to large batch training (You et al., 2019). Devlin et al. (2019) originally trained BERTBASE for 1M steps with a batch size of 256 sequences. This is equivalent in computational cost, via gradient accumulation, to training for 125K steps with a batch size of 2K sequences, or for 31K steps with a batch size of 8K.","Previous research in Neural Machine Translation demonstrated that utilizing very large mini-batches during training can enhance optimization velocity and final task results when the learning rate is suitably amplified (Ott et al., 2018). Recent studies have illustrated that BERT can also handle large batch training (You et al., 2019). Devlin et al. (2019) initially trained BERTBASE for 1M steps with 256 sequences per batch. This is comparable in computational expense, through gradient buildup, to training for 125K steps with 2K sequences per batch, or 31K steps with 8K sequences per batch.","Earlier work in Neural Machine Translation showed that using extremely large mini-batches while training can improve speed of optimization and performance on the end goal task if the learning rate is increased properly (Ott et al., 2018). More recent work has indicated that BERT is also responsive to large batch training (You et al., 2019). Devlin et al. (2019) first trained BERTBASE for 1M steps using a batch size of 256 sequences. This has the same computational cost, by accumulating gradients, as training for 125K steps with a batch size of 2K sequences, or training for 31K steps with a batch size of 8K sequences.","Past research in Neural Machine Translation demonstrated that utilizing very large mini-batches during training can boost speed of optimization and final performance on the task when the learning rate is increased suitably (Ott et al., 2018). Recent studies have shown that BERT can also handle large batch training effectively (You et al., 2019). Devlin et al. (2019) initially trained BERTBASE for 1M steps using batches of 256 sequences. This has equivalent computational expense, through accumulating gradients, as training for 125K steps with batches of 2K sequences, or 31K steps with batches of 8K sequences.",A,1
RoBERTa_A Robustly Optimized BERT Pretraining Approach,"We observe that training with large batches improves perplexity for the masked language modeling objective, as well as end-task accuracy. Large batches are also easier to parallelize via distributed data parallel training,8 and in later experiments we train with batches of 8K sequences. Notably You et al. (2019) train BERT with even larger batche sizes, up to 32K sequences. We leave further exploration of the limits of large batch training to future work.","Our analysis shows that using large sets of data for training enhances perplexity for the masked language modeling goal, and also improves performance on end applications. Large data sets can also be more easily parallelized using distributed data parallel training methods, and later we train using batches of 8K sequences. Notably, You et al. (2019) trained BERT using even bigger batch sizes, up to 32K sequences. Further inspection of the boundaries of large batch training is left for future research.","We find that utilizing substantial batches boosts perplexity on the masked language modeling task, and further increases accuracy on downstream tasks. Bigger batches can also be more easily parallelized with distributed data parallel training, and we later train using batches of 8K examples. It is notable that You et al. (2019) trained BERT with even larger batches, up to 32K sequences. Further exploring the limits of large batch training is left to future work. ","Our observations show that employing large training batches enhances perplexity on masked language modeling, and also improves performance on end applications. Larger batches can also be more readily parallelized using distributed data parallel training methods, and later we train using batches of 8K sequences. Notably, You et al. (2019) trained BERT utilizing even larger batch sizes, up to 32K sequences. Pushing the boundaries of large batch training is left for future investigation.",A,1
RoBERTa_A Robustly Optimized BERT Pretraining Approach,"Byte-Pair Encoding (BPE) (Sennrich et al., 2016) is a hybrid between character- and word-level representations that allows handling the large vocabularies common in natural language corpora. Instead of full words, BPE relies on subwords units, which are extracted by performing statistical analysis of the training corpus. BPE vocabulary sizes typically range from 10K-100K subword units. However, unicode characters can account for a sizeable portion of this vocabulary when modeling large and diverse corpora, such as the ones considered in this work.","Byte-Pair Encoding (BPE) (Sennrich et al., 2016) combines aspects of character-level and word-level representations, enabling it to handle the extensive vocabularies found in natural language data sets. Rather than using full words, BPE uses subword units, which are identified by performing statistical analysis of the training data. BPE vocabularies are often 10K-100K subword units in size. However, unicode characters can make up a considerable part of this vocabulary when modeling large and varied data sets, like the ones examined here.","Byte-Pair Encoding (BPE) (Sennrich et al., 2016) takes a hybrid approach between character- and word-level representations, allowing it to process the large vocabularies seen in natural language corpora. BPE uses subword units instead of full words, extracting these units by conducting statistical analysis of the training data. Typical BPE vocabulary sizes range from 10K-100K subword units. However, unicode characters can constitute a significant portion of this vocabulary when working with large and diverse corpora, such as the corpora considered here.  ","Byte-Pair Encoding (BPE) (Sennrich et al., 2016) combines character-level and word-level representations, enabling it to handle the extensive vocabularies common in natural language data. Rather than full words, BPE uses subword units, extracted via statistical analysis of the training data. BPE vocabularies are often 10K-100K subword units in size. However, unicode characters can make up a sizable part of this vocabulary when modeling large and varied corpora, like the ones examined here.",A,1
RoBERTa_A Robustly Optimized BERT Pretraining Approach,"Radford et al. (2019) introduce a clever implementation of BPE that uses bytes instead of unicode characters as the base subword units. Using bytes makes it possible to learn a subword vocabulary of a modest size (50K units) that can still encode any input text without introducing any “unknown” tokens. The original BERT implementation (Devlin et al., 2019) uses a character-level BPE vocabulary of size 30K, which is learned after preprocessing the input with heuristic tokenization rules.","Radford and colleagues in 2019 present an ingenious way to implement BPE that utilizes bytes rather than unicode characters as the elementary subword parts. Employing bytes enables learning a subword lexicon of moderate dimensions (50K elements) that can still encode any input text without bringing in any “unfamiliar” tokens. The first BERT model (Devlin and others, 2019) uses a character-level BPE vocabulary of 30K size, which is learned after preprocessing the input with heuristic tokenization guidelines.","In 2019, Radford and coauthors introduce a clever way to implement BPE that leverages bytes instead of unicode characters as the basic subword units. Leveraging bytes makes it feasible to learn a subword dictionary of a reasonable size (50K units) that can still code any input text without adding any “unknown” tokens. The original BERT architecture (Devlin et al., 2019) utilizes a character-level BPE vocabulary of dimension 30K, which is learned after pre-processing the input with heuristic tokenization protocols.","Radford and fellow researchers in 2019 present an ingenious implementation of BPE that harnesses bytes rather than unicode characters as the elementary subword components. Capitalizing on bytes enables learning a subword lexicon of moderate scale (50K elements) that can still encode any input text without introducing any “unfamiliar” tokens. The inaugural BERT model (Devlin and coauthors, 2019) employs a character-level BPE vocabulary of 30K magnitude, which is learned after pre-processing the input with heuristic tokenization guidelines.",A,1
RoBERTa_A Robustly Optimized BERT Pretraining Approach,"Following Radford et al. (2019), we instead consider training BERT with a larger byte-level BPE vocabulary containing 50K subword units, without any additional preprocessing or tokenization of the input. This adds approximately 15M and 20M additional parameters for BERTBASE and BERTLARGE, respectively. Early experiments revealed only slight differences between these encodings, with the Radford et al. (2019) BPE achieving slightly worse end-task performance on some tasks. Nevertheless, we believe the advantages of a universal encoding scheme outweighs the minor degredation in performance and use this encoding in the remainder of our experiments. A more detailed comparison of these encodings is left to future work.","As described by Radford and colleagues in 2019, we explore training BERT using a larger byte-level BPE vocabulary with 50K subword elements, without any other preprocessing or tokenization of the input text. This increases the number of parameters for BERTBASE and BERTLARGE by about 15M and 20M, respectively. Initial experiments showed minor differences between these encodings, with the Radford et al. (2019) BPE performing slightly worse on some downstream tasks. However, we believe the benefits of a universal encoding method are more important than the small performance reduction, so we utilize this encoding in the rest of our experiments. A more in-depth analysis comparing these encodings will be conducted in the future.","In line with the work of Radford et al. (2019), rather than standard preprocessing and tokenization, we consider teaching BERT using a larger byte-level BPE lexicon containing 50K subword parts, without any other preprocessing or tokenization of the input. This results in about 15M and 20M extra parameters for BERTBASE and BERTLARGE, respectively. Early tests showed slight differences between these encodings, with the Radford et al. (2019) BPE doing slightly worse on some end tasks. However, we think the advantages of a universal encoding system outweigh the minor performance degradation, so we use this encoding in the rest of our tests. A more thorough comparison of these encodings is left for future work.","As proposed by Radford and coauthors (2019), we explore training BERT with a larger byte-level BPE vocabulary having 50K subword units, without any other preprocessing or tokenization of the input text. This increases the parameters for BERTBASE and BERTLARGE by approximately 15M and 20M, respectively. Preliminary experiments showed minor differences between these encodings, with the Radford et al. (2019) BPE performing marginally worse on some downstream tasks. However, we believe the benefits of a universal encoding method outweigh the small performance reduction, so we utilize this encoding in the remainder of our experiments. A more comprehensive comparison of these encodings is left as future work.",A,1
RoBERTa_A Robustly Optimized BERT Pretraining Approach,"In the previous section we propose modifications to the BERT pretraining procedure that improve end-task performance. We now aggregate these improvements and evaluate their combined impact. We call this configuration RoBERTa for Robustly optimized BERT approach. Specifically, RoBERTa is trained with dynamic masking (Section 4.1), FULL-SENTENCES without NSP loss (Section 4.2), large mini-batches (Section 4.3) and a larger byte-level BPE (Section 4.4). Additionally, we investigate two other important factors that have been under-emphasized in previous work: (1) the data used for pretraining, and (2) the number of training passes through the data.","In the prior part we suggest changes to the BERT pre-training process that enhance performance on end tasks. We now combine these enhancements and assess their total impact. We name this setup RoBERTa for Robustly enhanced BERT method. Namely, RoBERTa is educated with dynamic masking, FULL-SENTENCES without NSP loss, large mini-batches, and a larger byte-level BPE. Furthermore, we examine two other key factors that were under-stressed before: the data utilized for pre-training, and the quantity of training cycles through the data.","In the earlier section we put forward modifications to the BERT pre-training routine that boost end-task results. We now bring together these advancements and evaluate their collective effect. We dub this configuration RoBERTa for Sturdily enhanced BERT approach. To be specific, RoBERTa is trained with dynamic masking, FULL-SENTENCES without NSP loss, large mini-batches, and a larger byte-level BPE. Additionally, we investigate two other crucial factors that were underemphasized previously: the data used for pre-training, and the number of training passes over the data.","In the prior portion we propose changes to the BERT pre-training process that improve performance on end tasks. We now amalgamate these enhancements and assess their combined impact. We name this setup RoBERTa for Robustly optimized BERT method. Namely, RoBERTa is trained with dynamic masking, FULL-SENTENCES without NSP loss, large mini-batches, and a larger byte-level BPE. Furthermore, we examine two other important factors that were understated before: the data used for pre-training, and the quantity of training cycles through the data.",A,1
RoBERTa_A Robustly Optimized BERT Pretraining Approach,"For example, the recently proposed XLNet architecture (Yang et al., 2019) is pretrained using nearly 10 times more data than the original BERT (Devlin et al., 2019). It is also trained with a batch size eight times larger for half as many optimization steps, thus seeing four times as many sequences in pretraining compared to BERT. To help disentangle the importance of these factors from other modeling choices (e.g., the pretraining objective), we begin by training RoBERTa following the BERTLARGE architecture (L = 24, H = 1024, A = 16, 355M parameters). We pretrain for 100K steps over a comparable BOOKCORPUS plus WIKIPEDIA dataset as was used in Devlin et al. (2019).","As an illustration, the newly suggested XLNet design (Yang and colleagues, 2019) is pre-trained utilizing nearly 10 times additional information compared to the first BERT (Devlin and others, 2019). It is also educated with a batch dimension eight times bigger for half as numerous enhancement steps, consequently seeing four times as numerous sequences in pretraining contrasted with BERT. To assist disentangle the significance of these variables from other modeling decisions (for instance, the pretraining goal), we start by preparing RoBERTa following the BERTLARGE design (L = 24, H = 1024, A = 16, 355M parameters). We pretrain for 100K stages over a comparable BOOKCORPUS in addition to WIKIPEDIA dataset as was utilized in Devlin and others (2019).","For instance, the recently proposed XLNet model (Yang et al., 2019) is pre-trained using almost 10 times more data than the original BERT model (Devlin et al., 2019). It is also trained with a batch size eight times larger for half as many optimization iterations, thus seeing four times as many sequences during pretraining compared to BERT. To help isolate the importance of these factors from other modeling choices (such as the pretraining objective), we start by training RoBERTa using the BERTLARGE architecture (L = 24, H = 1024, A = 16, 355M parameters). We pretrain for 100K iterations over a comparable BOOKCORPUS plus WIKIPEDIA dataset as was used in Devlin et al. (2019).  ","As an example, the newly suggested XLNet neural network (Yang and coauthors, 2019) is pre-trained using close to 10 times more training data than the original BERT neural network (Devlin and colleagues, 2019). It is also trained with a batch size eight times larger for half as many training iterations, thus being exposed to four times as many sequences during pretraining compared to BERT. To help determine the importance of these factors apart from other modeling choices (such as the pretraining objective), we begin by training RoBERTa using the BERTLARGE neural network architecture (L = 24, H = 1024, A = 16, 355M parameters). We pretrain for 100K training iterations over a comparable BOOKCORPUS plus WIKIPEDIA dataset as was used in Devlin et al. (2019).",A,1
RoBERTa_A Robustly Optimized BERT Pretraining Approach,"We pretrain our model using 1024 V100 GPUs for approximately one day. We present our results in Table 4. When controlling for training data, we observe that RoBERTa provides a large improvement over the originally reported BERTLARGE results, reaffirming the importance of the design choices we explored in Section 4. Next, we combine this data with the three additional datasets described in Section 3.2. We train RoBERTa over the combined data with the same number of training steps as before (100K). In total, we pretrain over 160GB of text. We observe further improvements in performance across all downstream tasks, validating the importance of data size and diversity in pretraining.","We pre-train our model by using 1024 V100 GPUs for about one day. We show our findings in Table 4. When accounting for training information, we see that RoBERTa gives a big improvement over the BERTLARGE results that were originally documented, confirming again how important the design choices we looked at in Section 4 are. After that, we combine this information with the other 3 datasets talked about in Section 3.2. We train RoBERTa on the combined information with the same number of training steps as before (100K). In total, we pre-train over 160GB of text. We see additional improvements in performance across all downstream tasks, validating how important data size and diversity in pre-training are.","We prime our model utilizing 1024 V100 GPUs for roughly 24 hours. We display our outputs in Table 4. Controlling for training data, we discern that RoBERTa furnishes a substantial enhancement over the initially documented BERTLARGE outcomes, reaffirming the significance of the design selections we investigated in Section 4. Subsequently, we amalgamate this data with the three supplementary datasets delineated in Section 3.2. We drill RoBERTa over the combined data with the same quantity of training steps as antecedently (100K). In totality, we pre-train over 160GB of text. We discern further improvements in performance across all downstream tasks, validating the importance of data magnitude and diversity in pre-training.  ","We pre-condition our model operating 1024 V100 GPUs for about a day. We exhibit our conclusions in Table 4. When regulating for training evidence, we detect that RoBERTa supplies a large refinement over the originally recorded BERTLARGE consequences, reconfirming the weight of the design picks we surveyed in Section 4. Next, we coalesce this evidence with the three additional datasets portrayed in Section 3.2. We discipline RoBERTa over the combined evidence with the same number of training steps as before (100K). In totality, we pre-condition over 160GB of text. We detect further enhancements in performance across all downstream tasks, validating the importance of data extent and diversity in pre-conditioning.",A,1
RoBERTa_A Robustly Optimized BERT Pretraining Approach,"Finally, we pretrain RoBERTa for significantly longer, increasing the number of pretraining steps from 100K to 300K, and then further to 500K. We again observe significant gains in downstream task performance, and the 300K and 500K step models outperform XLNetLARGE across most tasks. We note that even our longest-trained model does not appear to overfit our data and would likely benefit from additional training.","Ultimately, we pre-train RoBERTa extensively, raising the quantity of pre-training phases from 100K to 300K, then additionally to 500K. We see major improvements in performance on downstream assignments, and the 300K and 500K step models surpass XLNetLARGE on most tasks. We highlight that even our longest-trained model does not seem to overfit our data and would probably benefit from more training.","In conclusion, we pretrain RoBERTa for a much longer time, expanding the number of pretraining iterations from 100K to 300K, and further to 500K. We again see significant enhancements in performance on downstream tasks, and the 300K and 500K step models outdo XLNetLARGE on the majority of tasks. We note that even our model trained for the longest time does not appear to overfit our data and would likely improve with more training.  ","To summarize, we pretrain RoBERTa extensively, increasing the amount of pretraining rounds from 100K to 300K, and additionally to 500K. We observe major gains in performance on downstream jobs, and the 300K and 500K step models are superior to XLNetLARGE on most tasks. We point out that even our model trained the longest does not seem to overfit our data and would probably benefit from extra training.",A,1
RoBERTa_A Robustly Optimized BERT Pretraining Approach,"We finetune for 10 epochs and perform early stopping based on each task’s evaluation metric on the dev set. The rest of the hyperparameters remain the same as during pretraining. In this setting, we report the median development set results for each task over five random initializations, without model ensembling. In the second setting (ensembles, test), we compare RoBERTa to other approaches on the test set via the GLUE leaderboard. While many submissions to the GLUE leaderboard depend on multitask finetuning, our submission depends only on single-task finetuning.","We refine the model for 10 cycles and apply early halting predicated on each task's assessment metric on the development set. The remainder of the hyperparameters persist unchanged from pretraining. In this configuration, we document the median development set conclusions for each task over five arbitrary initializations, without model combining. In the second arrangement (ensembles, test), we analyze RoBERTa to other methodologies on the test set via the GLUE leaderboard. Although many entries to the GLUE leaderboard depend on multitask fine-tuning, our submission relies solely on single-task fine-tuning.","We adjust the model for 10 epochs and implement premature stopping based on each task's evaluation measure on the dev collection. The rest of the hyperparameters stay the same as during pre-education. In this case, we present the middle development set results for each task over five random starts, without model blending. In the second case (ensembles, test), we compare RoBERTa to other approaches on the test collection via the GLUE leaderboard. While many submissions to the GLUE leaderboard depend on multitask fine-tuning, our submission depends only on single-task fine-tuning.  ","We refine the model for 10 cycles and use early ending based on each task's assessment metric on the development set. The other hyperparameters remain unchanged from pretraining. In this configuration, we report the median development set results for each task over five arbitrary initializations, without model ensembling. In the second arrangement (ensembles, test), we contrast RoBERTa to other approaches on the test set via the GLUE leaderboard. Although many entries to the GLUE leaderboard rely on multitask fine-tuning, our entry depends only on single-task fine-tuning.",A,1
RoBERTa_A Robustly Optimized BERT Pretraining Approach,"We explore a slightly wider hyperparameter space, described in the Appendix, and ensemble between 5 and 7 models per task. Recent submissions on the GLUE leaderboard adopt a pairwise ranking formulation for the QNLI task, in which candidate answers are mined from the training set and compared to one another, and a single (question, candidate) pair is classified as positive (Liu et al., 2019b,a; Yang et al., 2019). This formulation significantly simplifies the task, but is not directly comparable to BERT (Devlin et al., 2019). Following recent work, we adopt the ranking approach for our test submission, but for direct comparison with BERT we report development set results based on a pure classification approach.","We examine a somewhat broader hyperparameter space, outlined in the Appendix, and combine between 5 and 7 models per task. Recent entries on the GLUE leaderboard use a pairwise ranking formulation for the QNLI task, where possible answers are extracted from the training set and contrasted with one another, and a single (question, candidate) pair is categorized as positive (Liu et al., 2019b,a; Yang et al., 2019). This formulation greatly simplifies the task, but is not directly comparable to BERT (Devlin et al., 2019). Following recent work, we utilize the ranking approach for our test submission, but for direct comparison with BERT we present development set results based on a pure classification method.","We investigate a slightly larger hyperparameter space, described in the Appendix, and ensemble between 5 and 7 models for each task. Recent submissions on the GLUE leaderboard adopt a pairwise ranking formulation for the QNLI task, in which candidate answers are sourced from the training set and juxtaposed with one another, and a single (question, candidate) pair is labeled as positive (Liu et al., 2019b,a; Yang et al., 2019). This formulation significantly streamlines the task, but is not directly comparable to BERT (Devlin et al., 2019). Following recent work, we use the ranking approach for our test submission, but for direct comparison with BERT we report development set results based on a pure classification methodology.  ","We explore a marginally broader hyperparameter space, outlined in the Appendix, and combine between 5 and 7 models per task. Recent entries on the GLUE leaderboard utilize a pairwise ranking formulation for the QNLI task, where possible answers are extracted from the training set and compared against one another, and a single (question, candidate) pair is identified as positive (Liu et al., 2019b,a; Yang et al., 2019). This formulation greatly simplifies the task, but is not directly comparable to BERT (Devlin et al., 2019). Following recent work, we employ the ranking approach for our test submission, but for direct comparison with BERT we present development set results based on a pure classification technique.",A,1
RoBERTa_A Robustly Optimized BERT Pretraining Approach,"We found the provided NLI-format data to be challenging to work with. Instead we use the reformatted WNLI data from SuperGLUE (Wang et al., 2019a), which indicates the span of the query pronoun and referent. We finetune RoBERTa using the margin ranking loss from Kocijan et al. (2019). For a given input sentence, we use spaCy (Honnibal and Montani, 2017) to extract additional candidate noun phrases from the sentence and finetune our model so that it assigns higher scores to positive referent phrases than for any of the generated negative candidate phrases.","The NLI-format data that was given to us was difficult to utilize. Rather, we make use of the reformatted WNLI information from SuperGLUE (Wang et al., 2019a), which points out the extent of the query pronoun and referent. We fine-tune RoBERTa utilizing the margin ranking loss from Kocijan et al. (2019). For any particular input sentence, we leverage spaCy (Honnibal and Montani, 2017) to extract extra nominee noun phrases from the sentence and fine-tune our model so it assigns elevated scores to affirmative referent phrases over any of the formed negative nominee phrases.","We found the NLI-format data provided to be tricky to work with. As an alternative, we utilize the reformatted WNLI information from SuperGLUE (Wang et al., 2019a), which highlights the range of the query pronoun and referent. We refine RoBERTa employing the margin ranking loss from Kocijan et al. (2019). For any given input sentence, we harness spaCy (Honnibal and Montani, 2017) to extract supplementary candidate noun phrases from the sentence and refine our model so it assigns higher ratings to positive referent phrases than any of the produced negative candidate phrases.  ","The NLI-format data given to us was challenging to utilize. Rather, we employ the reformatted WNLI data from SuperGLUE (Wang et al., 2019a), which indicates the extent of the query pronoun and referent. We tune RoBERTa using the margin ranking loss from Kocijan et al. (2019). For any specific input sentence, we use spaCy (Honnibal and Montani, 2017) to extract additional nominee noun phrases from the sentence and tune our model so that it assigns elevated scores to affirmative referent phrases over any of the generated negative nominee phrases.",A,1
RoBERTa_A Robustly Optimized BERT Pretraining Approach,"We present our results in Table 5. In the first setting (single-task, dev), RoBERTa achieves state-of-the-art results on all 9 of the GLUE task development sets. Crucially, RoBERTa uses the same masked language modeling pretraining objective and architecture as BERTLARGE, yet consistently outperforms both BERTLARGE and XLNetLARGE. This raises questions about the relative importance of model architecture and pretraining objective, compared to more mundane details like dataset size and training time that we explore in this work.","The findings are shown in Table 5. In the first configuration (single-task, dev), RoBERTa obtains the best existing results on all 9 of the GLUE task development sets. Importantly, RoBERTa utilizes the same masked language modeling pretraining goal and architecture as BERTLARGE, yet consistently surpasses both BERTLARGE and XLNetLARGE. This brings up questions regarding the relative significance of model architecture and pretraining objective, compared to more common details like dataset size and training time that we investigate here.","Our conclusions are presented in Table 5. In the initial setup (single-task, dev), RoBERTa attains state-of-the-art outcomes on all 9 of the GLUE task development sets. Notably, RoBERTa employs the same masked language modeling pretraining purpose and design as BERTLARGE, yet consistently exceeds both BERTLARGE and XLNetLARGE. This provokes inquiries regarding the relative value of model architecture and pretraining objective, versus more mundane factors like dataset size and training time that we explore in this work. ","The results are shown in Table 5. In the first arrangement (single-task, dev), RoBERTa accomplishes the best existing results on all 9 of the GLUE task development sets. Importantly, RoBERTa uses the same masked language modeling pretraining goal and structure as BERTLARGE, yet consistently surpasses both BERTLARGE and XLNetLARGE. This prompts questions about the relative significance of model architecture and pretraining objective, compared to more common details like dataset size and training time that we examine here.",A,1
RoBERTa_A Robustly Optimized BERT Pretraining Approach,"In the second setting (ensembles, test), we submit RoBERTa to the GLUE leaderboard and achieve state-of-the-art results on 4 out of 9 tasks and the highest average score to date. This is especially exciting because RoBERTa does not depend on multi-task finetuning, unlike most of the other top submissions. We expect future work may further improve these results by incorporating more sophisticated multi-task finetuning procedures.","In the second experiment (groups, evaluation), we enter RoBERTa into the GLUE benchmark and accomplish the best outcomes so far on 4 out of 9 assignments and the highest average mark up to this point. This is particularly thrilling because RoBERTa does not rely on multi-task tuning, unlike most of the other top entries. We anticipate future work may additionally refine these outcomes by joining more complex multi-task tuning techniques.","In the second analysis (collections, assessment), we submit RoBERTa to the GLUE ranking and attain state-of-the-art performances on 4 out of 9 tasks and the top mean score thus far. This is especially exciting since RoBERTa is not dependent on multi-task fine-tuning, contrary to most of the other highest submissions. We expect future efforts may further enhance these results by integrating more sophisticated multi-task fine-tuning processes.  ","In the second trial (assemblies, appraisal), we enter RoBERTa into the GLUE leaderboard and achieve best-in-class results on 4 out of 9 assignments and the peak average mark to this point. This is particularly thrilling as RoBERTa does not hinge on multi-task tuning, unlike most of the other premier entries. We anticipate future work may additionally improve these outcomes by combining more complex multi-task tuning techniques.",A,1
RoBERTa_A Robustly Optimized BERT Pretraining Approach,"We carefully evaluate a number of design decisions when pretraining BERT models. We find that performance can be substantially improved by training the model longer, with bigger batches over more data; removing the next sentence prediction objective; training on longer sequences; and dynamically changing the masking pattern applied to the training data. Our improved pretraining procedure, which we call RoBERTa, achieves state-of-the-art results on GLUE, RACE and SQuAD, without multi-task finetuning for GLUE or additional data for SQuAD.","When creating BERT models, we thoroughly assess various design choices. We discover that performance can be significantly enhanced by having longer training times, larger batch sizes over more data, eliminating the next sentence prediction goal, training on longer sequences, and dynamically altering the masking pattern used on the training information. Our enhanced pretraining method, which we call RoBERTa, accomplishes best-in-class results on GLUE, RACE and SQuAD, without multi-task fine-tuning for GLUE or extra data for SQuAD.","During the pretraining of BERT models, we carefully evaluate multiple design decisions. We find out that extending training duration, expanding batch sizes across more data, removing the next sentence forecasting objective, exercising on longer sequences, and adaptively modifying the masking pattern applied to the training data can substantially improve performance. Our improved pretraining procedure called RoBERTa attains state-of-the-art performance on GLUE, RACE and SQuAD without multi-task tuning for GLUE or additional data for SQuAD.  ","When pretraining BERT models, we thoroughly appraise various design choices. We determine that lengthening training time, increasing batch sizes over more data, eliminating the next sentence prediction goal, training on longer sequences, and dynamically changing the masking pattern used on the training data can significantly boost performance. Our enhanced pretraining approach called RoBERTa achieves best-in-class results on GLUE, RACE and SQuAD, without multi-task fine-tuning for GLUE or extra data for SQuAD.",A,1
RoBERTa_A Robustly Optimized BERT Pretraining Approach,"We present our results in Table 6. On the SQuAD v1.1 development set, RoBERTa matches the state-of-the-art set by XLNet. On the SQuAD v2.0 development set, RoBERTa sets a new state-of-the-art, improving over XLNet by 0.4 points (EM) and 0.6 points (F1). We also submit RoBERTa to the public SQuAD 2.0 leaderboard and evaluate its performance relative to other systems. Most of the top systems build upon either BERT (Devlin et al., 2019) or XLNet (Yang et al., 2019), both of which rely on additional external training data. In contrast, our submission does not use any additional data.","The findings of our work are displayed in Table 6. On the SQuAD v1.1 development dataset, RoBERTa equals the current best performance achieved by XLNet. On the SQuAD v2.0 development dataset, RoBERTa establishes a new top result, surpassing XLNet by 0.4 points on exact match and 0.6 points on F1 score. We also submit RoBERTa to the public SQuAD 2.0 leaderboard to assess its performance compared to other models. The majority of the highest performing systems are built on either BERT or XLNet, both of which utilize extra external training information. However, our submission does not use any supplementary data.","Our results are presented in Table 6. For the SQuAD v1.1 development set, RoBERTa matches the best existing result obtained by XLNet. On the SQuAD v2.0 development set, RoBERTa achieves a new state-of-the-art, outperforming XLNet by 0.4 points on exact match and 0.6 points on F1. We submit RoBERTa to the public SQuAD 2.0 leaderboard to evaluate it against other systems. Most top systems are based on either BERT or XLNet, which use additional external training data. In contrast, our submission does not utilize any extra data.  ","The results of our work are shown in Table 6. On the SQuAD v1.1 development dataset, RoBERTa equals the current top performance set by XLNet. For the SQuAD v2.0 development dataset, RoBERTa establishes a new state-of-the-art, surpassing XLNet by 0.4 points on exact match and 0.6 points on F1 metric. We also submit RoBERTa to the public SQuAD 2.0 leaderboard to assess its performance relative to other models. The majority of the highest performing systems build on either BERT or XLNet, both of which employ supplementary external training data. However, our submission does not use any additional data.",A,1
Sentence Embeddings using Siamese BERT-Networks,"BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations (~65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering.","BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) have established unprecedented performance on sentence pair regression tasks such as semantic textual similarity (STS). However, both sentences must be input to the network, incurring immense computational cost: Finding the most analogous pair in 10,000 sentences necessitates approximately 50 million inference computations (~65 hours) with BERT. The architecture of BERT renders it inappropriate for semantic similarity search and unsupervised tasks like clustering.","BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) have achieved new best results on sentence pair regression problems such as semantic textual similarity (STS). But this requires feeding both sentences into the network, causing massive computational burden: To find the most related pair among 10,000 sentences takes around 50 million inference operations (~65 hours) with BERT. How BERT is constructed makes it unfit for semantic similarity search and unsupervised activities such as clustering.  ","BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) have set new top performance on sentence pair regression tasks including semantic textual similarity (STS). However, it necessitates that both sentences are input to the network, incurring huge computational cost: Identifying the most similar pair in 10,000 sentences needs approximately 50 million inference computations (~65 hours) with BERT. The way BERT is built makes it unsuitable for semantic similarity search and unsupervised jobs like clustering.",A,1
Sentence Embeddings using Siamese BERT-Networks,"In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.","This paper introduces Sentence-BERT (SBERT), a modified version of the pre-trained BERT model that uses siamese and triplet neural networks to create sentence embeddings that capture semantic meaning and can be compared using cosine similarity. This decreases the time to find the most similar pair from 65 hours with BERT/RoBERTa to around 5 seconds with SBERT, while keeping BERT's accuracy. We assess SBERT and SRoBERTa on standard semantic textual similarity tasks and transfer learning tasks, where they surpass other state-of-the-art sentence embedding techniques.","In this paper, we present Sentence-BERT (SBERT), a modification of the pre-trained BERT network using siamese and triplet network architectures to generate semantically meaningful sentence embeddings that can be compared via cosine similarity. This reduces the time to identify the most similar pair from 65 hours with BERT/RoBERTa to about 5 seconds with SBERT, maintaining BERT's accuracy. We evaluate SBERT and SRoBERTa on common semantic textual similarity tasks and transfer learning tasks, outperforming other cutting-edge sentence embedding methods.","This paper introduces Sentence-BERT (SBERT), a variant of the pretrained BERT model using siamese and triplet networks to create sentence embeddings that capture semantic meaning and can be compared via cosine similarity. This lowers the time to find the most similar pair from 65 hours with BERT/RoBERTa to around 5 seconds with SBERT, retaining BERT's accuracy. We test SBERT and SRoBERTa on standard semantic textual similarity and transfer learning tasks, outperforming other state-of-the-art sentence embedding techniques.",A,1
Sentence Embeddings using Siamese BERT-Networks,"BERT set new state-of-the-art performance on various sentence classification and sentence-pair regression tasks. BERT uses a cross-encoder: Two sentences are passed to the transformer network and the target value is predicted. However, this setup is unsuitable for various pair regression tasks due to too many possible combinations. Finding in a collection of n = 10 000 sentences the pair with the highest similarity requires with BERT n·(n−1)/2 = 49 995 000 inference computations. On a modern V100 GPU, this requires about 65 hours.","BERT established unprecedented high-quality performance on various tasks involving categorizing sentences and predicting relationships between sentence pairs using regression. BERT utilizes a cross-encoder structure: Two sentences are input to the transformer network and it predicts the target value. However, this architecture is not well-suited for several pair regression tasks because there are too many potential combinations. To find the most similar pair among a set of n = 10,000 sentences using BERT requires n·(n−1)/2 = 49,995,000 inference computations. On a current V100 GPU, this would take about 65 hours.","BERT achieved new best results on multiple jobs like sorting sentences into categories and predicting connections between pairs of sentences using regression. BERT works by feeding two sentences into its transformer network and outputting a target score. But this setup struggles with various pair regression tasks since there are very many combinations. If you have n = 10,000 sentences, finding the most related pair with BERT takes n·(n−1)/2 = 49,995,000 inference runs. On a modern V100 GPU, that's around 65 hours.","BERT produced new state-of-the-art scores on various tasks including categorizing individual sentences and predicting relationships between sentence pairs using regression. BERT employs a cross-encoder structure where two sentences are input into its transformer network which then predicts the target value. However, this design struggles with multiple pair regression tasks because of the vast number of combinations. To find the top matching pair among n = 10,000 sentences requires n·(n−1)/2 = 49,995,000 inference computations with BERT. On a current generation V100 GPU, this would need about 65 hours.",A,1
Sentence Embeddings using Siamese BERT-Networks,"Similar, finding which of the over 40 million existent questions of Quora is the most similar for a new question could be modeled as a pair-wise comparison with BERT, however, answering a single query would require over 50 hours. A common method to address clustering and semantic search is to map each sentence to a vector space such that semantically similar sentences are close. Researchers have started to input individual sentences into BERT and to derive fixed size sentence embeddings. The most commonly used approach is to average the BERT output layer (known as BERT embeddings) or by using the output of the first token (the [CLS] token).","Likewise, determining which of the more than 40 million current questions on Quora is the most related for a new question could be modeled as a pair-wise comparison using BERT. However, responding to one query would take over 50 hours. A prevalent approach to handle clustering and semantic search is to map each sentence into a vector space so that semantically similar sentences are close together. Researchers have started to input separate sentences into BERT and obtain fixed size sentence embeddings. The most widely used method is to take the mean of the BERT output layer (known as BERT embeddings) or by utilizing the output of the first token (the [CLS] token).","Similarly, finding which of the over 40 million existing questions on Quora is the most comparable for a new question could be done as a pair-wise comparison with BERT. Though, providing an answer to a single query would need over 50 hours. A common technique to manage clustering and semantic search is to project each sentence into a vector space so that semantically related sentences are proximal. Researchers have begun to input distinct sentences into BERT and derive fixed size sentence embeddings. The most commonly used approach is to average the BERT output layer (referred to as BERT embeddings) or by leveraging the output of the first token (the [CLS] token).  ","Likewise, determining which of the more than 40 million current questions on Quora is the most similar for a new question could be accomplished as a pair-wise comparison utilizing BERT. However, responding to one query would necessitate over 50 hours. A prevalent method to handle clustering and semantic search is to map each sentence into a vector space such that semantically analogous sentences are in close proximity. Researchers have started to input separate sentences into BERT and obtain fixed size sentence embeddings. The most widely used technique is to take the mean of the BERT output layer (known as BERT embeddings) or by harnessing the output of the first token (the [CLS] token).",A,1
Sentence Embeddings using Siamese BERT-Networks,"As we will show, this common practice yields rather bad sentence embeddings, often worse than averaging GloVe embeddings (Pennington et al., 2014). To alleviate this issue, we developed SBERT. The siamese network architecture enables that fixed-sized vectors for input sentences can be derived. Using a similarity measure like cosine similarity or Manhattan / Euclidean distance, semantically similar sentences can be found. These similarity measures can be performed extremely efficient on modern hardware, allowing SBERT to be used for semantic similarity search as well as for clustering.","Our analysis demonstrates that this widespread technique results in quite poor sentence vectors, frequently inferior to averaging GloVe vectors (Pennington et al., 2014). To address this problem, we created SBERT. The siamese architecture means fixed-length embeddings can be produced for input sentences. Using a similarity function such as cosine similarity or Manhattan/Euclidean distance, semantically related sentences can be identified. These similarities can be computed very efficiently on modern hardware, enabling SBERT to be utilized for semantic search and clustering.","As we show, this common practice leads to rather unsuitable sentence representations, often worse than taking the mean of GloVe embeddings (Pennington et al., 2014). To fix this issue, we built SBERT. The siamese network design means fixed-size vectors can be generated for input sentences. Utilizing a similarity metric like cosine similarity or Manhattan/Euclidean distance, semantically equivalent sentences can be detected. These similarities can be calculated extremely fast on current hardware, allowing SBERT to be leveraged for semantic search and clustering.","Our analysis proves that this prevalent approach results in quite poor sentence embeddings, often inferior to averaging GloVe embeddings (Pennington et al., 2014). To resolve this problem, we invented SBERT. The siamese architecture permits fixed-length vectors to be derived for input sentences. Employing a similarity function such as cosine similarity or Manhattan/Euclidean distance, semantically close sentences can be identified. These similarities can be performed very efficiently on modern hardware, enabling SBERT to be used for semantic similarity search and clustering.",A,1
Sentence Embeddings using Siamese BERT-Networks,"By using optimized index structures, finding the most similar Quora question can be reduced from 50 hours to a few milliseconds (Johnson et al., 2017). We fine-tune SBERT on NLI data, which creates sentence embeddings that significantly outperform other state-of-the-art sentence embedding methods like InferSent (Conneau et al., 2017) and Universal Sentence Encoder (Cer et al., 2018). On seven Semantic Textual Similarity (STS) tasks, SBERT achieves an improvement of 11.7 points compared to InferSent and 5.5 points compared to Universal Sentence Encoder.","Through applying enhanced indexing systems, the time to identify the most related Quora inquiry can be lowered from 50 hours to just a couple of milliseconds (Johnson et al., 2017). We adjust SBERT utilizing NLI information, which produces sentence embeddings that substantially surpass other cutting edge sentence embedding techniques like InferSent (Conneau et al., 2017) and Universal Sentence Encoder (Cer et al., 2018). On seven Semantic Textual Similarity (STS) assignments, SBERT accomplishes an improvement of 11.7 points contrasted with InferSent and 5.5 points contrasted with Universal Sentence Encoder.","By utilizing improved file organizing frameworks, the procedure of finding the most comparable Quora question can be brought down dramatically from 50 hours to just a couple of milliseconds (Johnson et al., 2017). We tweak SBERT with NLI information, making sentence embeddings that immensely outflank other best in class sentence embedding procedures like InferSent (Conneau et al., 2017) and Universal Sentence Encoder (Cer et al., 2018). Over seven Semantic Textual Similarity (STS) errands, SBERT achieves a 11.7 point improvement over InferSent and a 5.5 point improvement over Universal Sentence Encoder.  ","Through harnessing enhanced indexing architectures, identifying the most related Quora question can be reduced tremendously from 50 hours to just a few milliseconds (Johnson et al., 2017). We fine-tune SBERT utilizing NLI data, generating sentence embeddings that vastly surpass other cutting edge sentence embedding techniques like InferSent (Conneau et al., 2017) and Universal Sentence Encoder (Cer et al., 2018). Across seven Semantic Textual Similarity (STS) tasks, SBERT accomplishes an 11.7 point enhancement over InferSent and a 5.5 point enhancement over Universal Sentence Encoder.",A,1
Sentence Embeddings using Siamese BERT-Networks,"On SentEval (Conneau and Kiela, 2018), an evaluation toolkit for sentence embeddings, we achieve an improvement of 2.1 and 2.6 points, respectively. SBERT can be adapted to a specific task. It sets new state-of-the-art performance on a challenging argument similarity dataset (Misra et al., 2016) and on a triplet dataset to distinguish sentences from different sections of a Wikipedia article (Dor et al., 2018).","On SentEval (Conneau and Kiela, 2018), a tool for assessing sentence embeddings, we see gains of 2.1 and 2.6 points. SBERT is customizable for a particular task. It establishes top performance on a difficult argument similarity dataset (Misra et al., 2016) and on a triplet dataset for telling apart sentences from various parts of a Wikipedia page (Dor et al., 2018).","With SentEval (Conneau and Kiela, 2018), a toolkit for evaluating sentence vectors, we get improvements of 2.1 and 2.6 points. SBERT is adaptable to a certain objective. It achieves new best results on a tricky argument closeness dataset (Misra et al., 2016) and on a triplet set to differentiate sentences from multiple sections of a Wikipedia entry (Dor et al., 2018).  ","On SentEval (Conneau and Kiela, 2018), a framework for testing sentence representations, we obtain boosts of 2.1 and 2.6 points. SBERT can be tailored to a given task. It establishes unprecedented performance on a challenging argument resemblance dataset (Misra et al., 2016) and on a triplet collection to tell apart sentences from various portions of a Wikipedia page (Dor et al., 2018).",A,1
Sentence Embeddings using Siamese BERT-Networks,"The paper is structured in the following way: Section 3 presents SBERT, section 4 evaluates SBERT on common STS tasks and on the challenging Argument Facet Similarity (AFS) corpus (Misra et al., 2016). Section 5 evaluates SBERT on SentEval. In section 6, we perform an ablation study to test some design aspect of SBERT. In section 7, we compare the computational efficiency of SBERT sentence embeddings in contrast to other state-of-the-art sentence embedding methods.","The composition of the document is as follows: Part 3 introduces SBERT, part 4 assesses SBERT on prevalent STS tasks and on the demanding Argument Facet Similarity (AFS) collection (Misra et al., 2016). Part 5 assesses SBERT on SentEval. In part 6, we execute an ablation analysis to evaluate some architectural aspects of SBERT. In part 7, we contrast the computational capability of SBERT sentence embeddings versus other cutting-edge sentence embedding techniques.","The paper's structure is like this: Segment 3 presents SBERT, segment 4 judges SBERT on common STS assignments and on the tough Argument Facet Similarity (AFS) corpus (Misra et al., 2016). Segment 5 judges SBERT on SentEval. In segment 6, we do an ablation review to test some design parts of SBERT. In segment 7, we equate the computational proficiency of SBERT sentence embeddings contrasted with other state-of-the-art sentence embedding methodologies.","The composition of the paper is: Portion 3 introduces SBERT, portion 4 appraises SBERT on prevalent STS tasks and on the demanding Argument Facet Similarity (AFS) collection (Misra et al., 2016). Portion 5 appraises SBERT on SentEval. In portion 6, we implement an ablation analysis to evaluate some architectural components of SBERT. In portion 7, we contrast the computational capability of SBERT sentence embeddings versus other leading-edge sentence embedding techniques.",A,1
Sentence Embeddings using Siamese BERT-Networks,"We first introduce BERT, then, we discuss state of-the-art sentence embedding methods. BERT (Devlin et al., 2018) is a pre-trained transformer network (Vaswani et al., 2017), which set for various NLP tasks new state-of-the-art results, including question answering, sentence classification, and sentence-pair regression. The input for BERT for sentence-pair regression consists of the two sentences, separated by a special [SEP] token. Multi-head attention over 12 (base-model) or 24 layers (large-model) is applied and the output is passed to a simple regression function to derive the final label. Using this setup, BERT set a new state-of-the-art performance on the Semantic Textual Semilarity (STS) benchmark (Cer et al., 2017).","We begin by presenting BERT, then we examine cutting-edge sentence embedding techniques. BERT (Devlin et al., 2018) is a pre-trained transformer network (Vaswani et al., 2017) that established new state-of-the-art results on various NLP tasks, including question answering, sentence classification, and sentence-pair regression. The input for BERT for sentence-pair regression is made up of the two sentences, separated by a special [SEP] token. Multi-head attention over 12 (base-model) or 24 layers (large-model) is used and the output is given to a simple regression function to get the final label. Using this method, BERT achieved new state-of-the-art performance on the Semantic Textual Similarity (STS) benchmark (Cer et al., 2017).","We first discuss BERT, then we review advanced sentence embedding methods. BERT (Devlin et al., 2018) is a pre-trained transformer network (Vaswani et al., 2017) that set new benchmarks on multiple NLP tasks, like question answering, sentence classification, and sentence-pair regression. The input for BERT for sentence-pair regression is the two sentences, separated by a special [SEP] token. Multi-head attention over 12 (base-model) or 24 layers (large-model) is employed and the output is fed to a simple regression function to get the final label. Using this architecture, BERT established a new state-of-the-art performance on the Semantic Textual Similarity (STS) benchmark (Cer et al., 2017).  ","We start by presenting BERT, then we survey cutting-edge sentence embedding techniques. BERT (Devlin et al., 2018) is a pre-trained transformer network (Vaswani et al., 2017) that achieved new state-of-the-art results across various NLP tasks, including question answering, sentence classification, and sentence-pair regression. The input for BERT for sentence-pair regression is the two sentences, separated by a special [SEP] token. Multi-head attention over 12 (base-model) or 24 layers (large-model) is utilized and the output is given to a simple regression function to derive the final label. Using this setup, BERT obtained a new state-of-the-art performance on the Semantic Textual Similarity (STS) benchmark (Cer et al., 2017).",A,1
Sentence Embeddings using Siamese BERT-Networks,"RoBERTa (Liu et al., 2019) showed, that the performance of BERT can further improved by small adaptations to the pre-training process. We also tested XLNet (Yang et al., 2019), but it led in general to worse results than BERT. A large disadvantage of the BERT network structure is that no independent sentence embeddings are computed, which makes it difficult to derive sentence embeddings from BERT. To bypass this limitations, researchers passed single sentences through BERT and then derive a fixed sized vector by either averaging the outputs (similar to average word embeddings) or by using the output of the special CLS token.","The research by Liu et al. (2019) demonstrated that minor tweaks to the pre-training approach can further enhance BERT's capabilities. We conducted experiments with XLNet (Yang et al., 2019) as well, but its performance was generally inferior to BERT's. A significant drawback of BERT's architecture is the lack of independent sentence embeddings, making it challenging to extract sentence vectors from BERT. To overcome this limitation, researchers have tried techniques like passing individual sentences through BERT and then deriving fixed-sized vectors by averaging the outputs (akin to average word embeddings) or utilizing the output of the special CLS token.","The study by Liu and colleagues in 2019 showed that small changes to how BERT is pre-trained can improve its performance even more. We tried using XLNet by Yang et al. from 2019 too, but overall it did worse than BERT. A big weakness of BERT's structure is that it doesn't calculate separate sentence embeddings, which makes getting sentence vectors from BERT difficult. To get around this problem, researchers have passed single sentences into BERT and then created vectors of a fixed size by either taking the average of the outputs (like with average word embeddings) or using the output of the special CLS token.","The research conducted by Liu and co-authors in 2019 demonstrated that minor modifications to BERT's pre-training approach can further boost its capabilities. Our experiments with XLNet by Yang and others from 2019 were also carried out, however its performance was generally worse compared to BERT. A significant disadvantage of BERT's architecture is the lack of independent embeddings for each sentence, which poses challenges for extracting sentence vectors from BERT. To tackle this limitation, researchers have experimented with techniques such as feeding individual sentences into BERT and then generating fixed-length vectors by taking the average of the outputs (similar to average word embeddings) or leveraging the output of the special CLS token.",A,1
Sentence Embeddings using Siamese BERT-Networks,"These two options are also provided by the popular bert-as-a-service-repository3 . Up to our knowledge, there is so far no evaluation if these methods lead to useful sentence embeddings. Sentence embeddings are a well studied area with dozens of proposed methods. Skip-Thought (Kiros et al., 2015) trains an encoder-decoder architecture to predict the surrounding sentences. InferSent (Conneau et al., 2017) uses labeled data of the Stanford Natural Language Inference dataset (Bowman et al., 2015) and the MultiGenre NLI dataset (Williams et al., 2018) to train a siamese BiLSTM network with max-pooling over the output.","These two choices are also given by the well-known bert-as-a-service-repository as well. As far as we know, there has not been any assessment done yet on whether these approaches generate useful sentence representations. Sentence representations are a thoroughly researched area with many proposed techniques. Skip-Thought (Kiros et al., 2015) teaches an encoder-decoder model to predict the nearby sentences. InferSent (Conneau et al., 2017) utilizes labeled data from the Stanford Natural Language Inference dataset (Bowman et al., 2015) and the MultiGenre NLI dataset (Williams et al., 2018) to educate a siamese BiLSTM network with max-pooling over the output.","These two alternatives are also provided by the popular bert-as-a-service-repository too. To our understanding, there has not been any evaluation yet on if these ways lead to beneficial sentence vectors. Sentence vectors are a well studied field with many suggested approaches. Skip-Thought (Kiros et al., 2015) develops an encoder-decoder structure to foresee the surrounding sentences. InferSent (Conneau et al., 2017) employs labeled information from the Stanford Natural Language Inference dataset (Bowman et al., 2015) and the MultiGenre NLI dataset (Williams et al., 2018) to develop a siamese BiLSTM network with max-pooling over the output.  ","These two options are also given by the well-known bert-as-a-service-repository as well. As far as we are aware, there is still no assessment if these techniques result in useful sentence embeddings. Sentence embeddings are an extensively studied area with many proposed methods. Skip-Thought (Kiros et al., 2015) builds an encoder-decoder model to predict the adjacent sentences. InferSent (Conneau et al., 2017) harnesses labeled data from the Stanford Natural Language Inference dataset (Bowman et al., 2015) and the MultiGenre NLI dataset (Williams et al., 2018) to construct a siamese BiLSTM network with max-pooling over the output.",A,1
Sentence Embeddings using Siamese BERT-Networks,"Conneau et al. showed, that InferSent consistently outperforms unsupervised methods like SkipThought. Universal Sentence Encoder (Cer et al., 2018) trains a transformer network and augments unsupervised learning with training on SNLI. Hill et al. (2016) showed, that the task on which sentence embeddings are trained significantly impacts their quality. Previous work (Conneau et al., 2017; Cer et al., 2018) found that the SNLI datasets are suitable for training sentence embeddings.","Conneau and colleagues demonstrated that InferSent persistently surpasses unsupervised techniques such as SkipThought. The Universal Sentence Encoder (Cer and others, 2018) educates a transformer system and complements unsupervised learning with preparation on SNLI. Hill and colleagues (2016) exhibited that the undertaking on which sentence embeddings are prepared essentially affects their quality. Earlier work (Conneau and others, 2017; Cer and others, 2018) discovered that the SNLI informational collections are appropriate for preparing sentence embeddings.","Conneau and co-authors showed that InferSent consistently exceeds unsupervised approaches such as SkipThought. The Universal Sentence Encoder (Cer et al., 2018) develops a transformer framework and supplements unsupervised learning with preparation on SNLI. Hill et al. (2016) demonstrated that the assignment on which sentence embeddings are prepared essentially impacts their quality. Past work (Conneau et al., 2017; Cer et al., 2018) found that the SNLI datasets are reasonable for preparing sentence embeddings.","Conneau and colleagues exhibited that InferSent persistently beats unsupervised techniques like SkipThought. The Universal Sentence Encoder (Cer et al., 2018) trains a transformer network and augments unsupervised learning with training on SNLI. Hill and co-authors (2016) showed that the task on which sentence embeddings are trained significantly affects their quality. Prior work (Conneau et al., 2017; Cer et al., 2018) found that the SNLI data sets are suitable for training sentence embeddings.",A,1
Sentence Embeddings using Siamese BERT-Networks,"However, polyencoders have the drawback that the score function is not symmetric and the computational overhead is too large for use-cases like clustering, which would require O(n 2 ) score computations. Previous neural sentence embedding methods started the training from a random initialization. In this publication, we use the pre-trained BERT and RoBERTa network and only fine-tune it to yield useful sentence embeddings. This reduces significantly the needed training time: SBERT can be tuned in less than 20 minutes, while yielding better results than comparable sentence embedding methods.","Nevertheless, polyencoders have the disadvantage that the score function is not balanced and the computational burden is excessive for use cases like clustering, which would need O(n^2) score calculations. Earlier neural sentence embedding techniques began training from an arbitrary initialization. In this paper, we utilize the pre-trained BERT and RoBERTa models and only fine-tune them to produce effective sentence embeddings. This greatly decreases the required training duration: SBERT can be adjusted in under 20 minutes, while generating superior results compared to similar sentence embedding approaches.","However, polyencoders have the shortcoming that the score function lacks symmetry and the computational cost is too high for applications like clustering, which would call for O(n^2) score computations. Previous neural sentence embedding algorithms started training from a random initialization. In this publication, we leverage the pre-trained BERT and RoBERTa models and only fine-tune them to yield useful sentence representations. This significantly reduces the needed training time: SBERT can be tuned in less than 20 minutes, while producing better outcomes than comparable sentence embedding techniques.","Nonetheless, polyencoders have the downside that the score function is not symmetric and the computational expense is excessive for use cases like clustering, which would necessitate O(n^2) score calculations. Earlier neural sentence embedding methods began the training from an arbitrary initialization. In this paper, we employ the pre-trained BERT and RoBERTa architectures and only fine-tune them to generate effective sentence embeddings. This greatly lowers the required training time: SBERT can be adjusted in under 20 minutes, while generating superior performance compared to similar sentence embedding methods.",A,1
Sentence Embeddings using Siamese BERT-Networks,"State-of-the-art methods often learn a (complex) regression function that maps sentence embeddings to a similarity score. However, these regression functions work pair-wise and due to the combinatorial explosion those are often not scalable if the collection of sentences reaches a certain size. Instead, we always use cosine-similarity to compare the similarity between two sentence embeddings. We ran our experiments also with negative Manhatten and negative Euclidean distances as similarity measures, but the results for all approaches remained roughly the same.","Current best practices frequently learn a (complicated) regression function that converts sentence representations into a similarity value. However, these regression functions operate on pairs and because of the combinatorial explosion those are regularly not adaptable if the collection of sentences grows beyond a certain amount. Rather, we always utilize cosine-similarity to analyze the similarity between two sentence representations. We also conducted our experiments with negative Manhatten and negative Euclidean distances as similarity metrics, but the outcomes for all methods stayed approximately the same.","State-of-the-art techniques often develop a (complex) regression model that maps sentence vectors to a similarity score. Though, these regression models work two at a time and due to the combinatorial growth those are frequently not scalable when the set of sentences becomes too large. As an alternative, we consistently leverage cosine-similarity to evaluate the similarity between two sentence vectors. We also executed our experiments with negative Manhatten and negative Euclidean distances as similarity measures, however the results for all approaches persisted roughly unchanged.  ","Leading-edge approaches frequently learn a (sophisticated) regression function that relates sentence embeddings to a similarity value. However, these regression functions operate pairwise and because of the exponential increase those are regularly not scalable if the collection of sentences exceeds a certain magnitude. As a substitute, we always employ cosine-similarity to contrast the similarity between two sentence embeddings. We also conducted our experiments with negative Manhatten and negative Euclidean distances as similarity metrics, nevertheless the outcomes for all methods persisted approximately equivalent.",A,1
Sentence Embeddings using Siamese BERT-Networks,"We showed in (Reimers et al., 2016) that Pearson correlation is badly suited for STS. Instead, we compute the Spearman’s rank correlation between the cosine-similarity of the sentence embeddings and the gold labels. The setup for the other sentence embedding methods is equivalent, the similarity is computed by cosine similarity. The results are depicted in Table 1. The results shows that directly using the output of BERT leads to rather poor performances. Averaging the BERT embeddings achieves an average correlation of only 54.81, and using the CLStoken output only achieves an average correlation of 29.19.","Our previous work (Reimers et al., 2016) demonstrated that Pearson correlation is not well-suited for semantic textual similarity (STS). We instead calculate Spearman's rank correlation between the cosine similarity of the sentence embeddings and the gold standard labels. The setup for the other sentence embedding techniques is the same, with similarity computed by cosine similarity. The results are shown in Table 1. The results indicate that directly utilizing the output of BERT leads to quite poor performance. Taking the average of the BERT embeddings achieves a mediocre correlation of just 54.81, and employing the CLS token output only reaches an average correlation of 29.19.","Our prior research (Reimers et al., 2016) showed that Pearson correlation does not work well for semantic textual similarity (STS). We compute Spearman's rank correlation between the cosine similarity of the sentence representations and the gold labels instead. The configuration for the other sentence representation methods is the same, using cosine similarity for the similarity calculation. The outcomes are presented in Table 1. The outcomes demonstrate that directly applying the output of BERT results in rather unsatisfactory performance. Averaging the BERT embeddings produces a moderate correlation of only 54.81, and utilizing the CLS token output achieves an average correlation of just 29.19.  ","Our previous paper (Reimers et al., 2016) established that Pearson correlation is poorly suited for semantic textual similarity (STS). We calculate Spearman's rank correlation between the cosine similarity of the sentence vectors and the gold standard labels instead. The setup for the other sentence vector methods is equivalent, with similarity determined by cosine similarity. The results are shown in Table 1. The results indicate that directly leveraging the output of BERT leads to rather poor performance. Taking the average of the BERT vectors achieves a middling correlation of only 54.81, and using the CLS token output manages an average correlation of just 29.19.",A,1
Sentence Embeddings using Siamese BERT-Networks,"Both are worse than computing average GloVe embeddings. Using the described siamese network structure and fine-tuning mechanism substantially improves the correlation, outperforming both InferSent and Universal Sentence Encoder substantially. The only dataset where SBERT performs worse than Universal Sentence Encoder is SICK-R. Universal Sentence Encoder was trained on various datasets, including news, question-answer pages and discussion forums, which appears to be more suitable to the data of SICK-R. In contrast, SBERT was pre-trained only on Wikipedia (via BERT) and on NLI data.","The siamese network architecture and fine-tuning approach are superior to simply calculating the mean GloVe embeddings. This method leads to much better correlation, substantially outperforming InferSent and Universal Sentence Encoder on nearly all datasets. The one exception is SICK-R, where Universal Sentence Encoder does better. This is likely because Universal Sentence Encoder was trained on diverse data like news, QA pages and forums, making it more suitable for SICK-R. Meanwhile, SBERT relies only on Wikipedia (through BERT) and NLI data for pre-training.","Using the siamese network design and fine-tuning technique is far better than just taking the average GloVe embeddings. It boosts correlation considerably, beating InferSent and Universal Sentence Encoder by a wide margin on most datasets. SICK-R is the only one where Universal Sentence Encoder wins. This may be because it was trained on news, QA sites, forums etc which seem more relevant to SICK-R. In comparison, SBERT's pre-training comes solely from Wikipedia (via BERT) and NLI data.  ","The described siamese network framework and fine-tuning process is superior to simply computing mean GloVe vectors. It enhances correlation tremendously, substantially outperforming InferSent and Universal Sentence Encoder on nearly all datasets. The exception is SICK-R, where Universal Sentence Encoder prevails. This could be attributed to its diverse training data including news, QA pages and discussion forums, making it more suited to SICK-R. Meanwhile, SBERT relies exclusively on Wikipedia (through BERT) and NLI data for pre-training.",A,1
Sentence Embeddings using Siamese BERT-Networks,"While RoBERTa was able to improve the performance for several supervised tasks, we only observe minor difference between SBERT and SRoBERTa for generating sentence embeddings. 4.2 Supervised STS The STS benchmark (STSb) (Cer et al., 2017) provides is a popular dataset to evaluate supervised STS systems. The data includes 8,628 sentence pairs from the three categories captions, news, and forums. It is divided into train (5,749), dev (1,500) and test (1,379).","Although RoBERTa was able to enhance the results for some tasks requiring supervision, we only saw small differences between SBERT and SRoBERTa when generating embeddings for sentences. 4.2 STS with Supervision The STS benchmark dataset (STSb) (Cer et al., 2017) is a widely used collection for assessing STS systems utilizing supervision. The data comprises 8,628 pairs of sentences from captions, news, and forums. It is split into training (5,749), development (1,500) and test (1,379) sets.","While RoBERTa managed to boost performance on certain supervised learning tasks, we observed minimal differences between SBERT and SRoBERTa in producing sentence embeddings. 4.2 Supervised Semantic Textual Similarity The STS benchmark (STSb) (Cer et al., 2017) is a popular dataset for evaluating supervised semantic textual similarity systems. The dataset contains 8,628 sentence pairs from captions, news articles, and forums. It is separated into training (5,749), validation (1,500) and test (1,379) splits. ","Although RoBERTa was able to enhance results on some supervised tasks, we only noticed small discrepancies between SBERT and SRoBERTa when generating sentence embeddings. 4.2 STS with Supervision The STS benchmark (STSb) dataset (Cer et al., 2017) is commonly used to assess supervised semantic textual similarity systems. The data has 8,628 sentence pairs from captions, news, and forums. It is divided into train (5,749), dev (1,500) and test (1,379) sets.",A,1
Sentence Embeddings using Siamese BERT-Networks,"The data was annotated on a scale from 0 (“different topic”) to 5 (“completely equivalent”). The similarity notion in the AFS corpus is fairly different to the similarity notion in the STS datasets from SemEval. STS data is usually descriptive, while AFS data are argumentative excerpts from dialogs. To be considered similar, arguments must not only make similar claims, but also provide a similar reasoning. Further, the lexical gap between the sentences in AFS is much larger. Hence, simple unsupervised methods as well as state-of-the-art STS systems perform badly on this dataset (Reimers et al., 2019).","The information was labeled on a range from 0 (""unrelated subject"") to 5 (""fully identical""). The concept of similarity in the AFS collection is quite distinct from the concept of similarity in the STS datasets from SemEval. STS information is typically descriptive, while AFS information consists of contentious excerpts from discussions. To be seen as alike, arguments need to not just make comparable assertions, but also put forward comparable reasoning. Additionally, the lexical difference between the sentences in AFS is much more significant. Therefore, simple unsupervised techniques and cutting-edge STS systems perform poorly on this data set (Reimers et al., 2019).","The data was marked on a scale going from 0 (""different matter"") to 5 (""totally the same""). The notion of similarity in the AFS corpus differs greatly from the notion of similarity in the STS datasets from SemEval. STS data tends to be descriptive, whereas AFS data are quarrelsome snippets from dialogues. For arguments to be considered similar, they must not only make comparable claims, but also present comparable justification. Furthermore, the lexical gap between the sentences in AFS is much wider. As a result, basic unsupervised approaches and state-of-the-art STS systems do not perform well on this dataset (Reimers et al., 2019).  ","The information was categorized on a range starting at 0 (""unrelated topic"") up to 5 (""fully matching""). The concept of similarity in the AFS collection varies substantially from the idea of similarity in the STS datasets from SemEval. STS information is characteristically descriptive, while AFS information consists of contentious excerpts from conversations. For arguments to be viewed as comparable, they need to not just state similar claims, but also put forth similar reasoning. Also, the lexical difference between the sentences in AFS is much more pronounced. Consequently, straightforward unsupervised methods and leading-edge STS systems have poor performance on this data set (Reimers et al., 2019).",A,1
Sentence Embeddings using Siamese BERT-Networks,"We evaluate SBERT on this dataset in two scenarios: 1) As proposed by Misra et al., we evaluate SBERT using 10-fold cross-validation. A drawback of this evaluation setup is that it is not clear how well approaches generalize to different topics. Hence, 2) we evaluate SBERT in a cross-topic setup. Two topics serve for training and the approach is evaluated on the left-out topic. We repeat this for all three topics and average the results. SBERT is fine-tuned using the Regression Objective Function.","We assess SBERT's performance on this dataset in two situations: 1) As suggested by Misra et al., we appraise SBERT utilizing 10-fold cross-validation. A limitation of this assessment configuration is that it is uncertain how well methods generalize to different subjects. Therefore, 2) we evaluate SBERT in a cross-subject experimental design. Two subjects are utilized for training and the approach is appraised on the omitted subject. We repeat this for all three subjects and take the average of the outcomes. SBERT is tuned using the Regression Objective Function.","We judge SBERT's effectiveness on this data collection in two cases: 1) As proposed by Misra et al., we rate SBERT employing 10-fold cross-checking. A weakness of this judging arrangement is that it is vague how well techniques generalize to distinct topics. As a result, 2) we assess SBERT in a cross-topic test setup. Two topics are used for instructing and the approach is judged on the excluded topic. We reiterate this for all three topics and calculate the mean of the results. SBERT is tailored using the Regression Objective Function. ","We analyze SBERT's performance on this dataset under two circumstances: 1) As suggested by Misra et al., we measure SBERT using 10-fold cross-examination. A shortcoming of this analysis configuration is that it is uncertain how well methods extend to different subjects. Therefore, 2) we test SBERT in a cross-subject evaluation design. Two subjects are utilized for training and the approach is measured on the omitted subject. We repeat this for all three subjects and find the average of the results. SBERT is adjusted using the Regression Objective Function.",A,1
Sentence Embeddings using Siamese BERT-Networks,"The similarity score is computed using cosine-similarity based on the sentence embeddings. We also provide the Pearson correlation r to make the results comparable to Misra et al. However, we showed (Reimers et al., 2016) that Pearson correlation has some serious drawbacks and should be avoided for comparing STS systems. The results are depicted in Table 3. Unsupervised methods like tf-idf, average GloVe embeddings or InferSent perform rather badly on this dataset with low scores.","The likeness rating is determined using cosine-similarity founded on the sentence representations. We also give the Pearson correlation r to make the outcomes comparable to Misra et al. Though, we demonstrated (Reimers et al., 2016) that Pearson correlation has some grave shortcomings and ought to be avoided for contrasting STS frameworks. The consequences are delineated in Table 3. Unsupervised techniques like tf-idf, normal GloVe embeddings or InferSent perform somewhat severely on this informational collection with low scores.","The similarity value is figured utilizing cosine-similarity dependent on the sentence implantings. We likewise give the Pearson relationship r to make the outcomes practically identical to Misra et al. In any case, we showed (Reimers et al., 2016) that Pearson relationship has some genuine disadvantages and ought to be kept away from for looking at STS frameworks. The outcomes are depicted in Table 3. Unsupervised strategies like tf-idf, normal GloVe embeddings or InferSent perform rather ineffectively on this dataset with low scores. ","The resemblance rating is ascertained using cosine-similarity founded on the sentence representations. We also make available the Pearson correlation r to make the results comparable to Misra et al. However, we exhibited (Reimers et al., 2016) that Pearson correlation has some serious shortcomings and should be avoided for comparing STS systems. The consequences are illustrated in Table 3. Unsupervised approaches like tf-idf, mean GloVe embeddings or InferSent perform quite poorly on this data set with low scores.",A,1
Sentence Embeddings using Siamese BERT-Networks,"BERT is able to use attention to compare directly both sentences (e.g. word-by-word comparison), while SBERT must map individual sentences from an unseen topic to a vector space such that arguments with similar claims and reasons are close. This is a much more challenging task, which appears to require more than just two topics for training to work on-par with BERT.","BERT can utilize attention to directly contrast both sentences (for instance, contrasting each word), whereas SBERT has to change individual sentences from an unknown subject into a vector space so that contentions with comparable claims and rationales are near each other. This is a considerably more troublesome errand, which seems to require more than only two subjects for preparing to work similarly too BERT.","BERT can use attention to straightforwardly analyze both sentences word for word, while SBERT needs to change singular sentences from an obscure point into a vector space with the goal that conflicts with comparable cases and legitimizations are close. This is a significantly more troublesome assignment, which seems to require more than two subjects for preparing to perform similarly to BERT. ","BERT can straightforwardly look at both sentences word for word using attention, though SBERT needs to change singular sentences from a dark point into a vector space so debates with comparable cases and legitimizations are near one another. This is a significantly more troublesome task, which appears to require over two subjects for preparing to act comparably to BERT.",A,1
Sentence Embeddings using Siamese BERT-Networks,"The purpose of SBERT sentence embeddings are not to be used for transfer learning for other tasks. Here, we think fine-tuning BERT as described by Devlin et al. (2018) for new tasks is the more suitable method, as it updates all layers of the BERT network. However, SentEval can still give an impression on the quality of our sentence embeddings for various tasks. We compare the SBERT sentence embeddings to other sentence embeddings methods on the following seven SentEval transfer tasks.","The intent of SBERT sentence embeddings is not for transfer learning to other jobs. We believe fine-tuning BERT as outlined by Devlin and colleagues (2018) for new jobs is the more appropriate technique, since it modifies all tiers of the BERT structure. Though, SentEval can still provide an idea of the excellence of our sentence embeddings for various jobs. We analyze the SBERT sentence embeddings to other sentence embedding techniques on the next seven SentEval transfer jobs.","The aim of SBERT sentence embeddings is not to be utilized for transfer learning for other undertakings. Here, we think tuning BERT as portrayed by Devlin et al. (2018) for new undertakings is the more reasonable strategy, as it refreshes all layers of the BERT organization. In any case, SentEval can in any case give a feeling on the nature of our sentence embeddings for different undertakings. We analyze the SBERT sentence embeddings to other sentence embedding techniques on the accompanying seven SentEval transfer undertakings. ","The rationale of SBERT sentence embeddings isn't to be utilized for transfer learning for other assignments. In this unique situation, we accept adjusting BERT as depicted by Devlin et al. (2018) for new assignments is the more appropriate technique, as it refreshes all layers of the BERT design. Notwithstanding, SentEval can in any case give a feeling of the nature of our sentence embeddings for different assignments. We analyze the SBERT sentence embeddings to other sentence embedding procedures on the accompanying seven SentEval transfer assignments.",A,1
Sentence Embeddings using Siamese BERT-Networks,"Scores are based on a 10-fold cross-validation. It appears that the sentence embeddings from SBERT capture well sentiment information: We observe large improvements for all sentiment tasks (MR, CR, and SST) from SentEval in comparison to InferSent and Universal Sentence Encoder. The only dataset where SBERT is significantly worse than Universal Sentence Encoder is the TREC dataset. Universal Sentence Encoder was pre-trained on question-answering data, which appears to be beneficial for the question-type classification task of the TREC dataset.","The ratings are determined using a 10-fold cross-validation. It seems that the sentence embeddings from SBERT effectively capture sentiment data: We see big improvements for all sentiment tasks (MR, CR, and SST) from SentEval compared to InferSent and Universal Sentence Encoder. The only data set where SBERT is considerably worse than Universal Sentence Encoder is the TREC data set. Universal Sentence Encoder was pre-trained on question-answering information, which appears to be helpful for the question-type classification task of the TREC data set.","The scores are calculated using a 10-fold cross-validation. It looks like the sentence representations from SBERT successfully encode sentiment knowledge: We notice large gains for all sentiment assignments (MR, CR, and SST) from SentEval versus InferSent and Universal Sentence Encoder. The sole dataset where SBERT is meaningfully inferior to Universal Sentence Encoder is the TREC dataset. Universal Sentence Encoder was pre-trained on question-answering content, which seems to be beneficial for the question-type categorization job of the TREC dataset.  ","The marks are established using a 10-fold cross-validation. It seems the sentence vectors from SBERT effectively capture sentiment understanding: We see big improvements for all sentiment tasks (MR, CR, and SST) from SentEval compared to InferSent and Universal Sentence Encoder. The only set where SBERT is considerably worse than Universal Sentence Encoder is the TREC set. Universal Sentence Encoder was pre-trained on question-answering data, which appears helpful for the question-type classification task of the TREC set.",A,1
Sentence Embeddings using Siamese BERT-Networks,"In this section, we perform an ablation study of different aspects of SBERT in order to get a better understanding of their relative importance. We evaluated different pooling strategies (MEAN, MAX, and CLS). For the classification objective function, we evaluate different concatenation methods. For each possible configuration, we train SBERT with 10 different random seeds and average the performances. The objective function (classification vs. regression) depends on the annotated dataset.","This part investigates the impact of different components of SBERT to better grasp their comparative significance. We looked at various pooling techniques (MEAN, MAX, CLS). For the classification goal function, we consider alternate joining approaches. For every potential setup, we teach SBERT with 10 distinct arbitrary seeds and take the average performances. The goal function (classification or regression) is contingent on the annotated dataset.","In this portion, we do an exploratory analysis of diverse facets of SBERT to acquire enhanced comprehension of their relative weight. We evaluated multiple pooling methodologies (MEAN, MAX, CLS). For the categorization cost function, we assess alternative concatenation procedures. For every feasible arrangement, we develop SBERT with 10 unique stochastic seeds and calculate the mean enactments. The cost function (grouping or regression) hinges on the annotated information. ","Here, we conduct an examination of various aspects of SBERT to gain better insight into their comparative importance. We assessed different pooling strategies (MEAN, MAX, CLS). For the classification loss function, we review alternative combination techniques. For every possible configuration, we train SBERT with 10 distinct random initializations and take the average performances. The loss function (classification or regression) depends on the labeled dataset.",A,1
Sentence Embeddings using Siamese BERT-Networks,"Note, that the concatenation mode is only relevant for training the softmax classifier. At inference, when predicting similarities for the STS benchmark dataset, only the sentence embeddings u and v are used in combination with cosine-similarity. The element-wise difference measures the distance between the dimensions of the two sentence embeddings, ensuring that similar pairs are closer and dissimilar pairs are further apart. When trained with the regression objective function, we observe that the pooling strategy has a large impact.","Observe that the concatenation method is only applicable when teaching the softmax classifier. During inference, when anticipating similarities for the STS benchmark dataset, only the sentence embeddings u and v are utilized along with cosine-similarity. The element-wise variance calculates the distance between the dimensions of the two sentence embeddings, guaranteeing that comparable pairs are nearer and dissimilar pairs are more separated. When educated with the regression objective function, we notice that the pooling approach has a major effect.","Note that joining sentences is only important when training the softmax classifier. When making predictions, to estimate similarities for the STS benchmark dataset, we just use the sentence embeddings u and v together with cosine-similarity. The element-wise difference measures how far apart the dimensions of the two sentence embeddings are, making sure related pairs are closer and unrelated pairs are farther apart. With regression as the objective function, we see that how sentences are pooled together has a big impact. ","Recognize that concatenating is only useful for teaching the softmax classifier. During inference, when forecasting similarities for the STS benchmark dataset, only the sentence embeddings u and v are employed along with cosine-similarity. The element-wise variance calculates the distance between the dimensions of the two sentence embeddings, ensuring comparable pairs are nearer and different pairs are more separated. When trained with the regression goal function, we discern that the pooling method has a major effect.",A,1
Sentence Embeddings using Siamese BERT-Networks,"Sentence embeddings need potentially be computed for Millions of sentences, hence, a high computation speed is desired. In this section, we compare SBERT to average GloVe embeddings, InferSent (Conneau et al., 2017), and Universal Sentence Encoder (Cer et al., 2018). For our comparison we use the sentences from the STS benchmark (Cer et al., 2017). We compute average GloVe embeddings using a simple for-loop with python dictionary lookups and NumPy. InferSent4 is based on PyTorch.","Sentence representations may need to be generated for a very large number of sentences, so high speed of computation is wanted. Here, we contrast SBERT with standard GloVe embeddings, InferSent (Conneau et al., 2017), and Universal Sentence Encoder (Cer et al., 2018). We utilize sentences from the STS benchmark (Cer et al., 2017) for our comparison. We compute typical GloVe embeddings utilizing a simple for-loop with python dictionary lookups and NumPy. InferSent is built on PyTorch.","Sentence vectors might have to be produced for millions of sentences, thus, fast computation is desirable. In this part, we measure SBERT against mean GloVe vectors, InferSent (Conneau et al., 2017), and Universal Sentence Encoder (Cer et al., 2018). For our evaluation we employ the sentences from the STS benchmark (Cer et al., 2017). We generate average GloVe vectors using a basic for-loop with python dictionary lookups and NumPy. InferSent is constructed on PyTorch.  ","Sentence representations could need to be generated for a massive number of sentences, so high speed of processing is preferred. Here, we analyze SBERT compared to standard GloVe embeddings, InferSent (Conneau et al., 2017), and Universal Sentence Encoder (Cer et al., 2018). For our analysis we utilize the sentences from the STS benchmark (Cer et al., 2017). We produce typical GloVe embeddings using a simple for-loop with python dictionary lookups and NumPy. InferSent is implemented on PyTorch.",A,1
Sentence Embeddings using Siamese BERT-Networks,"We showed that BERT out-of-the-box maps sentences to a vector space that is rather unsuitable to be used with common similarity measures like cosine-similarity. The performance for seven STS tasks was below the performance of average GloVe embeddings. To overcome this shortcoming, we presented Sentence-BERT (SBERT). SBERT fine-tunes BERT in a siamese / triplet network architecture. We evaluated the quality on various common benchmarks, where it could achieve a significant improvement over state-of-the-art sentence embeddings methods.","Our experiments demonstrated that the standard BERT model maps sentences into a vector space not well-suited for typical similarity measures such as cosine similarity. Using seven STS tasks, we found it performed worse than average GloVe embeddings. To address this weakness, we introduced Sentence-BERT (SBERT), which fine-tunes BERT using a siamese/triplet network structure. We tested SBERT on various benchmarks and showed substantial gains over other state-of-the-art sentence embedding techniques.","We established that out-of-the-box BERT transforms sentences into a vector space poorly compatible with common similarity metrics like cosine distance. It was inferior to typical GloVe vectors on seven STS jobs. As a solution, we created Sentence-BERT (SBERT), fine-tuning BERT within a siamese/triplet framework. SBERT was evaluated on diverse test sets, substantially outperforming other leading sentence embedding methods. ","Our studies showed vanilla BERT maps sentences into a space not optimal for standard similarity measures such as cosine closeness. It was worse than regular GloVe on seven STS tasks. To improve on this, we introduced Sentence-BERT (SBERT), fine-tuning BERT in a siamese/triplet structure. We tested SBERT extensively, and it significantly beat other state-of-the-art sentence embedding techniques.",A,1
Sentence Embeddings using Siamese BERT-Networks,"Replacing BERT with RoBERTa did not yield a significant improvement in our experiments. SBERT is computationally efficient. On a GPU, it is about 9% faster than InferSent and about 55% faster than Universal Sentence Encoder. SBERT can be used for tasks which are computationally not feasible to be modeled with BERT. For example, clustering of 10,000 sentences with hierarchical clustering requires with BERT about 65 hours, as around 50 Million sentence combinations must be computed. With SBERT, we were able to reduce the effort to about 5 seconds.","Substituting RoBERTa for BERT did not substantially boost performance in our tests. SBERT has low computational costs. Using a GPU, it's roughly 9% quicker than InferSent and 55% faster than Universal Sentence Encoder. SBERT enables modeling tasks not feasible with BERT due to computational limits. For instance, clustering 10,000 sentences with hierarchical clustering takes around 65 hours with BERT since approximately 50 million sentence pairs must be evaluated. With SBERT, we cut this to about 5 seconds.","Exchanging BERT for RoBERTa failed to meaningfully enhance results in our trials. SBERT is efficient to compute. On a GPU, it's about 9% swifter than InferSent and 55% faster than Universal Sentence Encoder. SBERT makes it possible to tackle tasks too computationally demanding for BERT. As an example, grouping 10,000 sentences with hierarchical clustering needs about 65 hours with BERT, as around 50 million sentence combinations must be assessed. With SBERT, we reduced this to around 5 seconds.  ","Using RoBERTa instead of BERT did not significantly boost performance in our experiments. SBERT has low computational cost. When using a GPU, it is about 9% faster than InferSent and 55% faster than Universal Sentence Encoder. SBERT enables modeling tasks that are computationally infeasible with BERT. For example, clustering 10,000 sentences using hierarchical clustering takes about 65 hours with BERT, because around 50 million sentence pairs need to be computed. With SBERT, we were able to reduce this to about 5 seconds.",A,1
Sequence to Sequence Learning with Neural Networks,"Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector.","Sophisticated neural network models called DNNs have demonstrated remarkable capabilities on challenging learning tasks, provided substantial labeled training information is available. However, DNNs are unable to map input sequences to output sequences. This paper puts forth a general technique for sequence learning that presumes little about sequence structure. Our technique employs a multilayer Long Short-Term Memory neural network to encode the input sequence into a fixed-size vector, and then decodes the target sequence from that vector using another deep Long Short-Term Memory network.","Advanced deep neural networks (DNNs) can achieve superb performance on difficult learning problems when supplied with ample labeled training data. But DNNs cannot map input sequences to output sequences. Here we introduce a generic end-to-end approach to sequence learning that makes minimal assumptions about sequence structure. It uses a multilayer Long Short-Term Memory (LSTM) network to encode the input sequence into a fixed-size vector, then decodes the target sequence from that vector using another deep LSTM.","Sophisticated deep neural network models called DNNs can deliver excellent results on challenging learning tasks, but require substantial labeled training data. However, DNNs cannot map sequences to sequences. This paper presents a general technique for sequence learning that makes minimal assumptions about sequence structure. It employs a multilayer Long Short-Term Memory (LSTM) network to map the input sequence into a fixed-size vector, then decodes the target sequence from that vector using another deep LSTM network.",A,1
Sequence to Sequence Learning with Neural Networks,"Our main result is that on an English to French translation task from the WMT’14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM’s BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task.","The primary finding is that on an English to French translation exercise utilizing the WMT'14 data, the translations created by the LSTM get a BLEU score of 34.8 on the whole test set, even though the LSTM's BLEU score was penalized for out-of-vocabulary words. Furthermore, the LSTM did not struggle with long sentences. In comparison, a phrase-based SMT framework accomplishes a BLEU score of 33.3 on the same data. When we utilized the LSTM to rerank the 1000 hypotheses generated by the aforementioned SMT framework, its BLEU score rises to 36.5, which is near the previous top result on this task.","Our principal conclusion is that on an English to French translation job using the WMT'14 dataset, the translations generated by the LSTM obtain a BLEU score of 34.8 on the full test set, despite the LSTM's BLEU score being penalized for out-of-vocabulary terms. Additionally, the LSTM had no issues with long sentences. For context, a phrase-based SMT system reaches a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses created by the above SMT system, its BLEU score increases to 36.5, which is close to the previous best performance on this task.  ","The key takeaway is that on an English to French translation exercise leveraging the WMT'14 data, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, even with the LSTM's BLEU score being penalized for out-of-vocabulary words. Further, the LSTM had no trouble with long sentences. For comparison, a phrase-based SMT model attains a BLEU score of 33.3 on the identical dataset. When we leveraged the LSTM to rerank the 1000 hypotheses generated by the aforementioned SMT model, its BLEU score rises to 36.5, which is near the prior top result on this task.",A,1
Sequence to Sequence Learning with Neural Networks,"The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM’s performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.","The LSTM also developed reasonable phrase and sentence representations that are responsive to word order and are somewhat unaffected by active and passive voice. Additionally, we discovered that flipping the order of the words in all source sentences (but not target sentences) enhanced the LSTM's performance substantially, because doing so presented many short-term dependencies between the source and target sentence that made the optimization task simpler.","The LSTM also learned sensible phrase and sentence depictions that are sensitive to word sequence and are fairly invariant to active and passive voice. Finally, we established that reversing the sequence of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, since doing so introduced numerous short-range dependencies between the source and the target sentence which simplified the optimization challenge. ","The LSTM also acquired reasonable phrase and sentence characterizations that are attentive to word ordering and are moderately unaffected by active versus passive voice. Furthermore, we ascertained that inverting the word order in all source sentences (but not target sentences) enhanced the LSTM's performance substantially, because implementing this introduced many short-term associations between the source and target sentence that simplified the optimization difficulty.",A,1
Sequence to Sequence Learning with Neural Networks,"Deep Neural Networks (DNNs) are extremely powerful machine learning models that achieve excellent performance on difficult problems such as speech recognition [13, 7] and visual object recognition [19, 6, 21, 20]. DNNs are powerful because they can perform arbitrary parallel computation for a modest number of steps. A surprising example of the power of DNNs is their ability to sort N N-bit numbers using only 2 hidden layers of quadratic size [27]. So, while neural networks are related to conventional statistical models, they learn an intricate computation.","Deep Neural Networks (DNNs) are very capable machine learning models that attain superb performance on challenging tasks like speech recognition [13, 7] and visual object identification [19, 6, 21, 20]. DNNs are potent because they can execute any parallel calculation for a moderate number of steps. A startling illustration of the might of DNNs is their capacity to organize N N-bit numbers utilizing just 2 hidden layers of quadratic size [27]. Therefore, even though neural networks are linked to standard statistical models, they learn a complex computation.","Deep Neural Networks (DNNs) are extremely effective machine learning models that achieve outstanding results on tough problems such as speech recognition [13, 7] and visual object detection [19, 6, 21, 20]. DNNs are strong because they can carry out any simultaneous computation for a modest number of steps. A surprising demonstration of the strength of DNNs is their ability to sort N N-bit numbers with only 2 hidden layers of quadratic dimensions [27]. Hence, although neural networks are associated with conventional statistical models, they learn an intricate calculation.  ","Deep Neural Networks (DNNs) are remarkably capable machine learning models that attain excellent performance on challenging tasks such as speech recognition [13, 7] and visual object identification [19, 6, 21, 20]. DNNs are powerful because they can execute any parallel computation for a moderate number of steps. A startling example of the capability of DNNs is their ability to organize N N-bit numbers using just 2 hidden layers of quadratic size [27]. Therefore, while neural networks are related to standard statistical models, they learn a complex computation.",A,1
Sequence to Sequence Learning with Neural Networks,"Furthermore, large DNNs can be trained with supervised backpropagation whenever the labeled training set has enough information to specify the network’s parameters. Thus, if there exists a parameter setting of a large DNN that achieves good results (for example, because humans can solve the task very rapidly), supervised backpropagation will find these parameters and solve the problem. Despite their flexibility and power, DNNs can only be applied to problems whose inputs and targets can be sensibly encoded with vectors of fixed dimensionality.","Moreover, big deep neural networks are able to be trained through supervised backpropagation if the labeled training data has sufficient information to define the network's parameters. Therefore, if there is a parameter configuration of a large deep neural network that achieves good performance (for instance, because humans can solve the task very quickly), supervised backpropagation will determine these parameters and solve the problem. However, despite their adaptability and capability, deep neural networks can only be utilized for problems where the inputs and outputs can be reasonably encoded as vectors with fixed dimensions.","In addition, large deep neural networks can be educated via supervised backpropagation as long as the tagged training information has enough specifics to establish the network's variables. Hence, if a parameter setup of a big deep neural network exists that accomplishes great outcomes (for example, since humans can accomplish the task very fast), supervised backpropagation will discover these parameters and resolve the issue. Though, even with their flexibility and power, deep neural networks can only be applied to problems where the inputs and outputs can be logically encoded as vectors with permanent dimensions. ","Also, big deep neural networks are able to be trained through supervised backpropagation if the labeled training data contains adequate details to define the network's parameters. Therefore, if a parameter configuration of a large deep neural network prevails that achieves good performance (for example, because humans can solve the task very quickly), supervised backpropagation will identify these parameters and solve the problem. However, despite their adaptability and capability, deep neural networks can only be used for problems where the inputs and outputs can be reasonably encoded as vectors with fixed dimensions.",A,1
Sequence to Sequence Learning with Neural Networks," It is a significant limitation, since many important problems are best expressed with sequences whose lengths are not known a-priori. For example, speech recognition and machine translation are sequential problems. Likewise, question answering can also be seen as mapping a sequence of words representing the question to a sequence of words representing the answer. It is therefore clear that a domain-independent method that learns to map sequences to sequences would be useful. Sequences pose a challenge for DNNs because they require that the dimensionality of the inputs and outputs is known and fixed.","This is a major constraint, since many critical issues are best conveyed with sequences whose lengths are unknown ahead of time. For instance, speech recognition and machine translation are sequential challenges. Similarly, question answering can also be viewed as mapping a sequence of words denoting the question to a sequence of words signifying the answer. Thus, it is evident that a domain-independent technique that learns to map sequences to sequences would be beneficial. Sequences present a difficulty for DNNs because they necessitate that the dimensionality of the inputs and outputs is determined and static.","This is a significant drawback, because many vital problems are best expressed with sequences whose lengths are not predetermined. As an illustration, speech processing and machine translation are sequential difficulties. In the same way, question responding can also be seen as converting a sequence of words embodying the question into a sequence of words typifying the answer. Therefore, it is clear that a domain-neutral approach that learns to transform sequences into sequences would be advantageous. Sequences pose an obstacle for DNNs since they require the dimensionality of the inputs and outputs to be identified and fixed.","This is a major shortcoming, since many crucial issues are best articulated with sequences whose lengths are unknown ahead of schedule. For instance, speech identification and machine translation are sequential challenges. Similarly, question solving can also be viewed as converting a sequence of words symbolizing the question into a sequence of words epitomizing the answer. Thus, it is apparent that a domain-independent technique that learns to translate sequences into sequences would be beneficial. Sequences present an impediment for DNNs because they necessitate that the dimensionality of the inputs and outputs is determined and stationary.",A,1
Sequence to Sequence Learning with Neural Networks,"In this paper, we show that a straightforward application of the Long Short-Term Memory (LSTM) architecture [16] can solve general sequence to sequence problems. The idea is to use one LSTM to read the input sequence, one timestep at a time, to obtain large fixed dimensional vector representation, and then to use another LSTM to extract the output sequence from that vector (fig. 1). The second LSTM is essentially a recurrent neural network language model [28, 23, 30] except that it is conditioned on the input sequence.","This paper demonstrates that a simple implementation of the Long Short-Term Memory (LSTM) model [16] is capable of handling general sequence-to-sequence tasks. The approach utilizes one LSTM to process the input sequence incrementally to generate a large fixed-size vector representation. Then a second LSTM is used to decode the output sequence from that vector (fig. 1). The second LSTM is basically a recurrent neural network language model [28, 23, 30] that is conditioned on the input sequence.","In this work, we establish that directly applying the Long Short-Term Memory (LSTM) framework [16] enables solving generic sequence-to-sequence problems. The methodology employs one LSTM for reading in the input sequence step-by-step to create a high-dimensional fixed-size vector embedding. Afterwards, another LSTM is leveraged for producing the output sequence from said vector (fig. 1). This second LSTM resembles a recurrent neural network language model [28, 23, 30] apart from being dependent on the input sequence. ","This paper exhibits that a simple Long Short-Term Memory (LSTM) architecture [16] suffices to tackle general sequence-to-sequence tasks. The technique feeds an input sequence into one LSTM incrementally to form a large static vector representation. Then a second LSTM takes this vector and decodes an output sequence (fig. 1). This second LSTM resembles a recurrent neural network language model [28, 23, 30] conditioned on the input sequence.",A,1
Sequence to Sequence Learning with Neural Networks,"The LSTM’s ability to successfully learn on data with long range temporal dependencies makes it a natural choice for this application due to the considerable time lag between the inputs and their corresponding outputs (fig. 1). There have been a number of related attempts to address the general sequence to sequence learning problem with neural networks. Our approach is closely related to Kalchbrenner and Blunsom [18] who were the first to map the entire input sentence to vector, and is related to Cho et al. [5] although the latter was used only for rescoring hypotheses produced by a phrase-based system.","The LSTM is well-suited for this task because its architecture allows it to learn associations between inputs and outputs that are separated by long time periods. There have been other attempts to tackle sequence-to-sequence learning using neural networks. Our method is similar to Kalchbrenner and Blunsom's approach of encoding the full input sentence into a vector. It is also related to Cho et al.'s model, although they only used it to rescore outputs from a phrase-based system.","The LSTM can successfully learn from data with long lags between inputs and outputs. This makes it a natural fit for this problem, where there are sizable delays between the inputs and targets (see figure 1). Other researchers have tried using neural networks for general sequence-to-sequence learning. Our approach is comparable to Kalchbrenner and Blunsom's technique of mapping the whole input sentence into a vector. It is also akin to Cho et al., though they only used it to rescore hypotheses from a phrase-based system.  ","The LSTM is adept at learning from data where the inputs and outputs are separated substantially in time. This ability suits it well to this task, given the large gap between the inputs and targets (fig. 1). There have been several attempts to tackle the general sequence-to-sequence problem with neural networks. Our method bears similarity to Kalchbrenner and Blunsom's approach of encoding the full input sentence in a vector. It is also related to Cho et al., although they only used it to rescore outputs from a phrase-based system.",A,1
Sequence to Sequence Learning with Neural Networks,"Graves [10] introduced a novel differentiable attention mechanism that allows neural networks to focus on different parts of their input, and an elegant variant of this idea was successfully applied to machine translation by Bahdanau et al. [2]. The Connectionist Sequence Classification is another popular technique for mapping sequences to sequences with neural networks, but it assumes a monotonic alignment between the inputs and the outputs [11].","Graves [10] presented a new differentiable focus mechanism that enables neural networks to concentrate on various aspects of their inputs. Bahdanau et al. [2] skillfully adapted this concept for machine translation. The Connectionist Sequence Classification is another prevalent approach for mapping sequences to sequences with neural networks, but presumes a monotonic association between inputs and outputs [11].","Graves [10] brought forth an original differentiable attention system permitting neural networks to accentuate diverse constituents of their inputs. Bahdanau et al. [2] fruitfully employed this notion for machine translation. The Connectionist Sequence Classification constitutes another pervasive technique to map sequences onto sequences via neural networks, albeit assuming a monotonic linkage between inputs and outputs [11]. ","Graves [10] pioneered an innovative differentiable attention appliance empowering neural networks to spotlight sundry facets of their inputs. Bahdanau et al. [2] astutely applied this construct to machine translation. The Connectionist Sequence Classification typifies another ubiquitous avenue to channel sequences into sequences within neural networks, notwithstanding presuming a monotonic tethering of inputs to outputs [11].",A,1
Sequence to Sequence Learning with Neural Networks,"The main result of this work is the following. On the WMT’14 English to French translation task, we obtained a BLEU score of 34.81 by directly extracting translations from an ensemble of 5 deep LSTMs (with 384M parameters and 8,000 dimensional state each) using a simple left-to-right beam search decoder. This is by far the best result achieved by direct translation with large neural networks. For comparison, the BLEU score of an SMT baseline on this dataset is 33.30 [29]. The 34.81 BLEU score was achieved by an LSTM with a vocabulary of 80k words, so the score was penalized whenever the reference translation contained a word not covered by these 80k.","The principal finding of this work is as follows. On the WMT'14 English to French translation task, we attained a BLEU score of 34.81 by straightforwardly deriving translations from a group of 5 deep LSTMs (with 384M parameters and 8,000 dimensional state apiece) employing a simple left-to-right beam search decoder. This is by a wide margin the best result accomplished by direct translation with large neural networks. As a comparison, the BLEU score of an SMT baseline on this dataset is 33.30 [29]. The 34.81 BLEU score was reached by an LSTM with a vocabulary of 80k words, so the score was penalized whenever the reference translation contained a word not included in these 80k.","The primary outcome of this research can be summarized as: On the WMT'14 English to French translation challenge, we obtained a BLEU score of 34.81 by directly extracting translations from an ensemble of 5 deep LSTMs (each with 384M parameters and 8,000 dimensional state) using a straightforward left-to-right beam search decoder. This substantially surpasses the best result achieved so far by direct translation with large neural networks. For reference, the BLEU score of an SMT baseline system on this dataset is 33.30 [29]. The 34.81 BLEU score was produced by an LSTM with a vocabulary of 80k words, so the score was lowered whenever the reference translation had a word outside of these 80k.  ","The key finding of this study is: On the WMT'14 English to French translation task, we reached a BLEU score of 34.81 by directly pulling translations from a group of 5 deep LSTMs (each with 384M parameters and 8,000 dimensional state) leveraging a simple left-to-right beam search decoder. This vastly exceeds the top result obtained previously via direct translation with large neural networks. As a point of comparison, the BLEU score of an SMT baseline system on this dataset is 33.30 [29]. The 34.81 BLEU score resulted from an LSTM with a vocabulary of 80k words, so the score was reduced whenever the reference translation had a word outside of these 80k.",A,1
Sequence to Sequence Learning with Neural Networks,"This result shows that a relatively unoptimized small-vocabulary neural network architecture which has much room for improvement outperforms a phrase-based SMT system. Finally, we used the LSTM to rescore the publicly available 1000-best lists of the SMT baseline on the same task [29]. By doing so, we obtained a BLEU score of 36.5, which improves the baseline by 3.2 BLEU points and is close to the previous best published result on this task (which is 37.0 [9]). Surprisingly, the LSTM did not suffer on very long sentences, despite the recent experience of other researchers with related architectures [26].","This finding indicates that a fairly basic small-vocabulary neural network model, which has ample room for enhancement, surpasses a phrase-based statistical machine translation system. Additionally, we leveraged the LSTM to rescore the publicly available 1000-best lists of the baseline SMT system on the same task [29]. This resulted in a BLEU score of 36.5, which boosts the baseline by 3.2 BLEU points and is near the prior best published outcome on this task (which is 37.0 [9]). Interestingly, the LSTM was not negatively impacted by very long sentences, despite the recent experiences of other researchers with similar architectures [26].","These results demonstrate that an unrefined small-vocabulary neural network design, with much potential for further optimization, is superior to a phrase-based statistical MT system. We also utilized the LSTM to rescore the publicly accessible 1000-best hypotheses of the SMT baseline on the identical task [29]. In doing so, we achieved a BLEU score of 36.5, which enhances the baseline by 3.2 BLEU points and is proximate to the preceding top published score on this task (which is 37.0 [9]). Surprisingly, the LSTM was not impaired by very lengthy sentences, contrary to the recent trials of other scholars with analogous architectures [26].  ","This outcome exhibits that a relatively crude small-vocabulary neural network model, with ample room for refinement, surpasses a phrase-based statistical machine translation system. Furthermore, we harnessed the LSTM to rescore the publicly available 1000-best possibilities of the SMT baseline on the same task [29]. By this approach, we obtained a BLEU score of 36.5, which improves the baseline by 3.2 BLEU points and is near to the prior highest published mark on this task (which is 37.0 [9]). Remarkably, the LSTM was not encumbered by very long sentences, despite the recent undertakings of other experts with similar architectures [26].",A,1
Sequence to Sequence Learning with Neural Networks,"We were able to do well on long sentences because we reversed the order of words in the source sentence but not the target sentences in the training and test set. By doing so, we introduced many short term dependencies that made the optimization problem much simpler (see sec. 2 and 3.3). As a result, SGD could learn LSTMs that had no trouble with long sentences. The simple trick of reversing the words in the source sentence is one of the key technical contributions of this work. A useful property of the LSTM is that it learns to map an input sentence of variable length into a fixed-dimensional vector representation.","Our method performed effectively on lengthy sentences because we changed the order of the words in the source sentences but not the target sentences used for training and testing. This introduced numerous brief dependencies that greatly simplified the optimization task (refer to sections 2 and 3.3). Consequently, SGD was able to learn LSTMs that handled long sentences without issue. One of the key technical innovations of this work was the simple tactic of reversing the word order of the source sentences. An advantageous aspect of the LSTM is its ability to convert input sentences of differing lengths into fixed-dimensional vector representations.","We achieved strong results with long sentences through inverting the word sequence in the source sentences while leaving target sentences unchanged for training and evaluation. That brought in many short-term dependencies that dramatically reduced the complexity of the optimization problem (see sections 2 and 3.3). As a result, SGD could successfully train LSTMs that had no difficulties processing extended sentences. A core technical contribution here was the basic approach of reversing the order of words in the source sentences. A useful LSTM feature is mapping input sentences of varying lengths to fixed-dimensional vector embeddings.","Our system performed well on lengthy sentences by reversing the word order in the source sentences while keeping target sentences normal for training and testing. This introduced numerous short dependencies that greatly simplified the optimization challenge (refer sections 2 and 3.3). Consequently, SGD could effectively learn LSTMs that handled long sentences with ease. A key technical innovation was the simple technique of reversing words in the source. A valuable LSTM trait is taking input sentences of different lengths and mapping to fixed-dimensional vector representations.",A,1
Sequence to Sequence Learning with Neural Networks,"Given that translations tend to be paraphrases of the source sentences, the translation objective encourages the LSTM to find sentence representations that capture their meaning, as sentences with similar meanings are close to each other while different sentences meanings will be far. A qualitative evaluation supports this claim, showing that our model is aware of word order and is fairly invariant to the active and passive voice.","Since translations are typically rewordings of the original sentences, the translation goal motivates the LSTM to generate sentence representations that encapsulate their meaning. Sentences with comparable meanings will have similar representations, while different sentence meanings will have distinct representations. A qualitative assessment upholds this idea, demonstrating that our model comprehends word order and is reasonably unaffected by active versus passive voice.","Because translations are usually restatements of the source sentences, the translation objective prompts the LSTM to find sentence representations that convey their meaning. Sentences with related meanings will have representations that are close together, while sentences with different meanings will be farther apart representationally. A qualitative review supports this notion, showing our model grasps word order and is fairly stable regardless of active or passive voice.  ","Given that translations tend to be paraphrases of the original sentences, the translation objective spurs the LSTM to identify sentence representations that capture their meaning. Sentences with similar meanings will have representations that are near each other, while sentences with dissimilar meanings will be representationally distant. A qualitative assessment corroborates this idea, exhibiting that our model understands word order and is reasonably invariant to active vs. passive voice.",A,1
Sequence to Sequence Learning with Neural Networks,"The RNN can easily map sequences to sequences whenever the alignment between the inputs the outputs is known ahead of time. However, it is not clear how to apply an RNN to problems whose input and the output sequences have different lengths with complicated and non-monotonic relationships. The simplest strategy for general sequence learning is to map the input sequence to a fixed-sized vector using one RNN, and then to map the vector to the target sequence with another RNN (this approach has also been taken by Cho et al. [5]).","Recurrent neural networks are adept at mapping input sequences to output sequences when the correlation between the inputs and outputs is predetermined. However, it is unclear how to utilize RNNs for problems where the input and output sequences vary in length and have intricate, nonlinear relationships. A basic tactic for general sequence learning is to encode the input sequence into a fixed-size vector using one RNN, then decode the vector into the target sequence with another RNN (this method was also used by Cho et al. [5]).","RNNs can easily convert sequences to sequences when the mapping between the inputs and outputs is established ahead of time. But it's not obvious how to use RNNs on problems where the input and output sequences are different lengths with complicated, non-monotonic connections. The most basic approach for general sequence learning is to encode the input sequence into a static vector using one RNN, then decode that vector into the target sequence with another RNN (Cho et al. [5] also used this technique).  ","Recurrent neural networks can smoothly translate sequences into sequences when the link between the inputs and the outputs is predetermined. However, it is uncertain how to employ an RNN on problems where the input and output sequences are of differing lengths with intricate, nonlinear relationships. The most straightforward plan for general sequence learning is to convert the input sequence into a fixed-size vector utilizing one RNN, then convert that vector into the target sequence with another RNN (this plan was also utilized by Cho et al. [5]).",A,1
Sequence to Sequence Learning with Neural Networks,"While it could work in principle since the RNN is provided with all the relevant information, it would be difficult to train the RNNs due to the resulting long term dependencies (figure 1) [14, 4, 16, 15]. However, the Long Short-Term Memory (LSTM) [16] is known to learn problems with long range temporal dependencies, so an LSTM may succeed in this setting Our actual models differ from the above description in three important ways. First, we used two different LSTMs: one for the input sequence and another for the output sequence, because doing so increases the number model parameters at negligible computational cost and makes it natural to train the LSTM on multiple language pairs simultaneously [18].","Even though the RNN has all the necessary information, training it would be challenging because of the long term dependencies that would result (figure 1) [14, 4, 16, 15]. But the Long Short-Term Memory (LSTM) [16] can learn problems with long range time dependencies, so an LSTM might work in this case. Our real models have 3 key differences from the above description. First, we utilized two separate LSTMs: one for the input sequence and one for the output, which grows the number of model parameters with little extra computation and makes it easy to train the LSTM on multiple language pairs at once [18].","While the RNN theoretically has the needed info, training would be tough due to the long term dependencies created (figure 1) [14, 4, 16, 15]. However, Long Short-Term Memory networks (LSTMs) [16] can handle problems with long temporal dependencies, so an LSTM may succeed here. Our actual models vary from the description above in 3 main ways. First, we employed two distinct LSTMs: one for the input and one for the output, which increases model parameters at minimal computational cost and enables training the LSTM on multiple language pairs together [18].","Even if the RNN has all the relevant data in principle, training would be difficult because of the resulting long range dependencies (figure 1) [14, 4, 16, 15]. But Long Short-Term Memory networks (LSTMs) [16] can learn problems with long time dependencies, so an LSTM could work in this setting. Our real models differ from the description above in 3 key aspects. First, we used two separate LSTMs: one for the input and one for the output, which grows model parameters with negligible extra computation and allows training the LSTM on multiple language pairs at the same time [18].",A,1
Sequence to Sequence Learning with Neural Networks,"Second, we found that deep LSTMs significantly outperformed shallow LSTMs, so we chose an LSTM with four layers. Third, we found it extremely valuable to reverse the order of the words of the input sentence. So for example, instead of mapping the sentence a, b, c to the sentence α, β, γ, the LSTM is asked to map c, b, a to α, β, γ, where α, β, γ is the translation of a, b, c. This way, a is in close proximity to α, b is fairly close to β, and so on, a fact that makes it easy for SGD to “establish communication” between the input and the output. We found this simple data transformation to greatly improve the performance of the LSTM.","Secondly, we discovered that deep LSTMs significantly surpassed shallow LSTMs, so we opted for an LSTM with four tiers. Thirdly, we found it tremendously beneficial to invert the order of the words in the input sentence. Therefore instead of mapping the sentence a, b, c to the sentence α, β, γ, the LSTM is tasked with mapping c, b, a to α, β, γ, where α, β, γ is the translation of a, b, c. This way, a is close to α, b is fairly near to β, and so forth, a fact that makes it easy for stochastic gradient descent to ""build communication"" between the input and output. We found this simple data change greatly improved the LSTM's performance.","Next, we determined that deep LSTMs substantially outstripped shallow LSTMs, so we went with a 4-layer LSTM. Also, we realized it was extremely valuable to flip the order of the words in the input sentence. So rather than mapping the sentence a, b, c to the sentence α, β, γ, the LSTM is asked to map c, b, a to α, β, γ, where α, β, γ is the translation of a, b, c. This way, a is near α, b is fairly close to β, and so on, which makes it easy for stochastic gradient descent to ""make connections"" between input and output. We found this simple data alteration greatly boosted the LSTM's capabilities.  ","Secondly, we ascertained that deep LSTMs significantly exceeded shallow LSTMs in performance, prompting our selection of a 4-tier LSTM architecture. Additionally, we determined that reversing the sequence of words in the input sentence was tremendously advantageous. Rather than mapping the sentence a, b, c to α, β, γ, the LSTM instead maps c, b, a to α, β, γ, where α, β, γ represents the translation of a, b, c. This juxtaposition of input and output facilitates stochastic gradient descent in forging representational links between them. Applying this basic data transformation substantially enhanced the capabilities of the LSTM.",A,1
Sequence to Sequence Learning with Neural Networks,"We applied our method to the WMT’14 English to French MT task in two ways. We used it to directly translate the input sentence without using a reference SMT system and we it to rescore the n-best lists of an SMT baseline. We report the accuracy of these translation methods, present sample translations, and visualize the resulting sentence representation. We used the WMT’14 English to French dataset. We trained our models on a subset of 12M sentences consisting of 348M French words and 304M English words, which is a clean “selected” subset from [29].","We tested our approach on the WMT'14 English to French machine translation challenge in two ways. We utilized it to translate the input sentence directly without an SMT reference system and we used it to rescore the n-best lists from an SMT baseline. We present the precision of these translation techniques, show example translations, and visualize the resulting sentence representation. We utilized the WMT'14 English to French data. We trained our models on a subset of 12M sentences containing 348M French words and 304M English words, which is a clean ""selected"" subset from [29].","We implemented our technique on the 2014 Workshop on Machine Translation English to French machine translation task in two manners. We employed it to translate the input sentence straightaway without an SMT reference system and we utilized it to rescore the n-best lists of an SMT baseline. We document the accuracy of these translation approaches, provide sample translations, and depict the resulting sentence representation. We leveraged the 2014 WMT English to French data. We trained our models on a subset of 12 million sentences comprising 348 million French words and 304 million English words, which is a clean ""selected"" subset from [29].  ","We applied our approach to the 2014 Workshop on Machine Translation English to French machine translation challenge in two ways. We used it to translate the input sentence directly without a reference statistical machine translation system and we used it to rescore the n-best lists from a statistical machine translation baseline. We present the precision of these translation methods, furnish example translations, and visualize the resulting sentence representation. We employed the 2014 Workshop on Machine Translation English to French data. We trained our models on a subset of 12 million sentences containing 348 million French words and 304 million English words, which is a clean ""selected"" subset from [29].",A,1
Sequence to Sequence Learning with Neural Networks,"We chose this translation task and this specific training set subset because of the public availability of a tokenized training and test set together with 1000-best lists from the baseline SMT [29]. As typical neural language models rely on a vector representation for each word, we used a fixed vocabulary for both languages. We used 160,000 of the most frequent words for the source language and 80,000 of the most frequent words for the target language. Every out-of-vocabulary word was replaced with a special “UNK” token.","We selected this translation assignment and this particular training set portion because the tokenized training and test set were publicly available together with 1000-best lists from the baseline SMT [29]. Since typical neural language models depend on a vector representation for each word, we utilized a fixed vocabulary for both tongues. We employed 160,000 of the most frequent terms for the source tongue and 80,000 of the most frequent terms for the target tongue. We substituted every out-of-vocabulary word with a special “UNK” token.","We opted for this translation task and this specific training set segment due to the public access to a tokenized training and test set along with 1000-best lists from the baseline SMT [29]. As conventional neural language models rely on a vector depiction for each term, we used a fixed vocabulary for both languages. We utilized 160,000 of the most common words for the source language and 80,000 of the most common words for the target language. We replaced every out-of-vocabulary word with a special “UNK” token. ","We chose this translation assignment and this particular training set portion because the tokenized training and test set were available to the public together with 1000-best lists from the baseline SMT [29]. Since typical neural language models depend on a vector representation for each term, we used a fixed vocabulary for both tongues. We utilized 160,000 of the most frequent words for the source tongue and 80,000 of the most frequent words for the target tongue. We substituted every out-of-vocabulary word with a special “UNK” token.",A,1
Sequence to Sequence Learning with Neural Networks,"We search for the most likely translation using a simple left-to-right beam search decoder which maintains a small number B of partial hypotheses, where a partial hypothesis is a prefix of some translation. At each timestep we extend each partial hypothesis in the beam with every possible word in the vocabulary. This greatly increases the number of the hypotheses so we discard all but the B most likely hypotheses according to the model’s log probability. As soon as the “” symbol is appended to a hypothesis, it is removed from the beam and is added to the set of complete hypotheses.","We look for the most probable translation by utilizing a straightforward left-to-right beam search decoder that keeps a small quantity B of incomplete guesses, where an incomplete guess is a prefix of some translation. At each timestep we lengthen each incomplete guess in the beam with every feasible word in the vocabulary. This greatly expands the number of the guesses so we reject all except the B most probable guesses according to the model's log likelihood. As soon as the """" symbol is added to a guess, it is eliminated from the beam and is appended to the set of finished guesses.","We find the most likely translation by using a simple left-to-right beam search decoder which maintains a small amount B of partial hypotheses, in which a partial hypothesis is a prefix of a translation. At every timestep we extend each partial hypothesis in the beam with all possible words in the vocabulary. This significantly increases the number of hypotheses so we remove all except the B most probable hypotheses based on the model's log probability. When the """" symbol is attached to a hypothesis, it's removed from the beam and added to the collection of complete hypotheses.","We determine the most probable translation utilizing a straightforward left-to-right beam search decoder that retains a small number B of incomplete translations, where an incomplete translation is a prefix of a translation. At each timestep we expand each incomplete translation in the beam with every feasible word in the vocabulary. This greatly multiplies the number of translations so we discard all except the B most likely translations according to the model's log likelihood. Once the """" symbol is appended to a translation, it's eliminated from the beam and is included in the set of finished translations.",A,1
Sequence to Sequence Learning with Neural Networks,"While this decoder is approximate, it is simple to implement. Interestingly, our system performs well even with a beam size of 1, and a beam of size 2 provides most of the benefits of beam search (Table 1). We also used the LSTM to rescore the 1000-best lists produced by the baseline system [29]. To rescore an n-best list, we computed the log probability of every hypothesis with our LSTM and took an even average with their score and the LSTM’s score.","Although this decoder is not exact, it is easy to put into practice. Remarkably, our framework functions effectively even utilizing a beam range of 1, and a beam of 2 gives most of the advantages of beam search (Table 1). We also utilized the LSTM to rescore the 1000-best catalogues created by the baseline structure [29]. To rescore an n-best catalogue, we computed the log probability of every theory with our LSTM and took an even average with their score and the LSTM's score.","While this decoder is not fully precise, it can be simply implemented. Interestingly, our framework has good performance even using a beam width of 1, and a beam of 2 provides most of the benefits of beam search (Table 1). We also harnessed the LSTM to re-evaluate the 1000-best indexes produced by the baseline architecture [29]. To re-evaluate an n-best index, we calculated the log probability of every hypothesis with our LSTM and took an even mean with their score and the LSTM's score.  ","Although this decoder is not completely accurate, it is straightforward to put into practice. Remarkably, our system functions well even applying a beam range of 1, and a beam of 2 gives most of the advantages of beam search (Table 1). We also employed the LSTM to re-assess the 1000-best catalogs generated by the baseline design [29]. To re-assess an n-best catalog, we computed the log probability of every theory with our LSTM and took an even average with their score and the LSTM's score.",A,1
Sequence to Sequence Learning with Neural Networks,"While the LSTM is capable of solving problems with long term dependencies, we discovered that the LSTM learns much better when the source sentences are reversed (the target sentences are not reversed). By doing so, the LSTM’s test perplexity dropped from 5.8 to 4.7, and the test BLEU scores of its decoded translations increased from 25.9 to 30.6. While we do not have a complete explanation to this phenomenon, we believe that it is caused by the introduction of many short term dependencies to the dataset. Normally, when we concatenate a source sentence with a target sentence, each word in the source sentence is far from its corresponding word in the target sentence.","Although LSTMs can handle problems involving long-term dependencies, we found they perform much better when the source sentences are reversed (but not the target sentences). Using this approach caused the LSTM's test perplexity to decrease from 5.8 to 4.7, and its translation test BLEU scores rose from 25.9 to 30.6. We don't fully understand why, but we think introducing many short-term dependencies to the data is the reason. Typically, when we join a source sentence to a target sentence, each source word is far from its matching target word.","Despite the LSTM's ability to manage issues with prolonged dependencies, it learns considerably more effectively if the source sentences are flipped (the target sentences remain unflipped). Doing this caused the LSTM's test perplexity to drop from 5.8 to 4.7, and its translation test BLEU scores to increase from 25.9 to 30.6. While we lack a complete clarification for this occurrence, we believe it is owing to the addition of numerous short-term dependencies to the data set. Usually, when we combine a source sentence with a target sentence, every word in the source is distant from its related word in the target.","Although LSTMs can tackle problems with extended dependencies, we discovered they learn much more successfully when the source sentences are reversed (but not the targets). Using this technique made the LSTM's test perplexity decrease from 5.8 to 4.7, and its translation test BLEU scores rise from 25.9 to 30.6. We don't fully grasp why, but we think presenting many short-term dependencies to the data causes this. Ordinarily, when we join a source sentence with a target, each source word is far from its corresponding target word.",A,1
Sequence to Sequence Learning with Neural Networks,"As a result, the problem has a large “minimal time lag” [17]. By reversing the words in the source sentence, the average distance between corresponding words in the source and target language is unchanged. However, the first few words in the source language are now very close to the first few words in the target language, so the problem’s minimal time lag is greatly reduced. Thus, backpropagation has an easier time “establishing communication” between the source sentence and the target sentence, which in turn results in substantially improved overall performance.","Consequently, the issue has a significant ""lowest possible delay"" [17]. By changing the order of the words in the initial sentence, the typical separation between related words in the source and destination language remains the same. But, the first few words in the source language are now very near to the first few words in the target language, so the issue's lowest possible delay is greatly decreased. As a result, backpropagation finds it easier to ""make a connection"" between the source sentence and target sentence, which then produces substantially enhanced overall performance.","Thus, the dilemma has a large ""minimum time gap"" [17]. By flipping the sequence of the terms in the original statement, the mean distance between matching words in the origin and goal language is unaltered. However, the first couple words in the origin language are now very close to the first couple words in the goal language, so the dilemma's minimum time gap is greatly reduced. Consequently, backpropagation has an easier time ""building communication"" between the source sentence and the target sentence, which in turn leads to substantially improved overall performance. ","Therefore, the problem has a large ""smallest time delay"" [17]. By changing the order of the words in the starting sentence, the median separation between related words in the source and end language stays the same. But, the first few words in the source language are now very near to the first few words in the end language, so the problem's smallest time delay is greatly decreased. As a result, backpropagation finds it easier to ""make a link"" between the source sentence and end sentence, which then produces substantially enhanced overall performance.",A,1
Sequence to Sequence Learning with Neural Networks,"Initially, we believed that reversing the input sentences would only lead to more confident predictions in the early parts of the target sentence and to less confident predictions in the later parts. However, LSTMs trained on reversed source sentences did much better on long sentences than LSTMs trained on the raw source sentences (see sec. 3.7), which suggests that reversing the input sentences results in LSTMs with better memory utilization.","At first, we thought turning the input sentences around would just make the predictions more sure at the start of the target sentence and less sure after. But LSTMs trained on backwards source sentences did a lot better on long sentences than LSTMs trained on the original source sentences (see sec. 3.7). This means flipping the input sentences helps LSTMs use their memory better.","Initially, our assumption was that inverting the order of the input sentences would only improve confidence in predictions for the beginning of the target sentence, while decreasing confidence in the later parts. However, LSTMs trained on reversed source sentences significantly outperformed LSTMs trained on unmodified source sentences on long sentences (refer to sec. 3.7). This indicates that reversing the sentences enables more effective memory utilization by LSTMs.  ","At first, we thought that flipping the input sentences around would just make the forecasts more certain at the start of the result sentence and less certain later on. But LSTMs educated on backward origin sentences did far better on long sentences than LSTMs educated on the raw origin sentences (see sec. 3.7). This shows that turning around the input sentences assists LSTMs to better use their memory.",A,1
Sequence to Sequence Learning with Neural Networks,"We found that the LSTM models are fairly easy to train. We used deep LSTMs with 4 layers, with 1000 cells at each layer and 1000 dimensional word embeddings, with an input vocabulary of 160,000 and an output vocabulary of 80,000. Thus the deep LSTM uses 8000 real numbers to represent a sentence. We found deep LSTMs to significantly outperform shallow LSTMs, where each additional layer reduced perplexity by nearly 10%, possibly due to their much larger hidden state. We used a naive softmax over 80,000 words at each output. The resulting LSTM has 384M parameters of which 64M are pure recurrent connections (32M for the “encoder” LSTM and 32M for the “decoder” LSTM).","Our experiments showed that LSTM neural networks can be trained without much difficulty. We implemented deep LSTMs with 4 layers, 1000 nodes per layer, and 1000-dimensional word vectors. The input vocabulary contained 160,000 words and the output vocabulary had 80,000 words. So each sentence was represented by 8000 real values in the deep LSTM. We found that deeper LSTMs performed significantly better than shallow LSTMs, with perplexity reduced by nearly 10% per extra layer, likely because of the much larger hidden state. We used a simple softmax over 80,000 words at each output. The total LSTM had 384 million parameters, with 64 million being recurrent connections (32 million in the ""encoder"" and 32 million in the ""decoder"").","Our tests demonstrated that long short-term memory models are relatively straightforward to optimize. We constructed deep LSTMs with 4 tiers, 1000 neurons per tier, and 1000-dimension word embeddings. The input dictionary had 160,000 terms and the output dictionary contained 80,000 terms. Therefore, the deep LSTM utilizes 8000 real numbers to encode a sentence. We discovered that deeper LSTMs substantially surpassed shallow LSTMs, as each extra tier decreased perplexity by nearly 10%, potentially owing to their far larger hidden condition. We applied a naive softmax over 80,000 words at every output. The resulting LSTM possesses 384 million parameters, of which 64 million are pure recurrent links (32 million for the ""encoder"" LSTM and 32 million for the ""decoder"" LSTM).","Our experiments revealed that long short-term memory networks are fairly uncomplicated to learn. We constructed deep LSTMs with 4 strata, 1000 nodes per stratum, and 1000-measurement word representations. The input lexicon had 160,000 words and the output lexicon possessed 80,000 words. Hence, the deep LSTM employs 8000 real values to characterize a sentence. We ascertained that deeper LSTMs significantly exceeded shallow LSTMs, with each supplementary stratum lessening perplexity by nearly 10%, potentially due to their much vaster hidden state. We utilized a naive softmax over 80,000 words at each output. The consequent LSTM holds 384 million parameters, of which 64 million are pure recurrent connections (32 million for the ""encoder"" LSTM and 32 million for the ""decoder"" LSTM).",A,1
Sequence to Sequence Learning with Neural Networks,"A C++ implementation of deep LSTM with the configuration from the previous section on a single GPU processes a speed of approximately 1,700 words per second. This was too slow for our purposes, so we parallelized our model using an 8-GPU machine. Each layer of the LSTM was executed on a different GPU and communicated its activations to the next GPU / layer as soon as they were computed. Our models have 4 layers of LSTMs, each of which resides on a separate GPU. The remaining 4 GPUs were used to parallelize the softmax, so each GPU was responsible for multiplying by a 1000 × 20000 matrix.","A C++ version of a deep LSTM neural network with the setup from the prior section running on a single graphics card can analyze around 1,700 words each second. This speed was inadequate for our needs, so we made the model run in parallel using a machine with 8 graphics cards. Every layer of the LSTM was executed on a different graphics card and sent its outputs to the next graphics card/layer right after calculating them. Our models have 4 LSTM layers, each on its own graphics card. The other 4 graphics cards were utilized to parallelize the softmax, with each one responsible for multiplying by a 1000 × 20000 matrix.","An implementation in C++ of a deep LSTM neural net with the architecture from before on one GPU can process approximately 1,700 words per second. This was too slow for what we wanted, so we made our model parallel using an 8-GPU computer. Every LSTM layer was run on a separate GPU and communicated its outputs to the next GPU/layer immediately after computing them. Our models have 4 LSTM layers, each on its own GPU. The other 4 GPUs were used to parallelize the softmax, with each one multiplying by a 1000 × 20000 matrix.","A C++ version of a deep long short-term memory neural network with the setup described previously running on a single graphics processing unit can analyze around 1,700 words per second. This speed was not fast enough for our purposes, so we parallelized our model using a machine with 8 graphics processing units. Each long short-term memory layer was executed on a different graphics processing unit and transmitted its outputs to the next graphics processing unit/layer right after calculating them. Our models have 4 long short-term memory layers, each on its own graphics processing unit. The other 4 graphics processing units were utilized to parallelize the softmax function, with each one responsible for multiplying by a 1000 × 20000 matrix.",A,1
Sequence to Sequence Learning with Neural Networks,"The resulting implementation achieved a speed of 6,300 (both English and French) words per second with a minibatch size of 128. Training took about a ten days with this implementation. We used the cased BLEU score [24] to evaluate the quality of our translations. We computed our BLEU scores using multi-bleu.pl1 on the tokenized predictions and ground truth. This way of evaluating the BELU score is consistent with [5] and [2], and reproduces the 33.3 score of [29]. The results are presented in tables 1 and 2. Our best results are obtained with an ensemble of LSTMs that differ in their random initializations and in the random order of minibatches.","The finalized system reached a velocity of 6,300 (for both English and French) words every second utilizing a minibatch magnitude of 128. The implementation required approximately ten days to fully train. We leveraged the cased BLEU metric [24] to gauge the caliber of our translations. We computed our BLEU totals operating multi-bleu.pl1 on the tokenized forecasts and factuals. This technique for assessing the BLEU result aligns with [5] and [2], and reproduces the 33.3 total of [29]. The conclusions are exhibited in tables 1 and 2. Our optimal outcomes were attained with an ensemble of LSTMs that diverge in their arbitrary initializations and in the haphazard order of minibatches.","The completed deployment accomplished a pace of 6,300 (both for English and French) terms per instant with a minibatch size of 128. The training process lasted around ten days utilizing this deployment. We harnessed the cased BLEU score [24] to evaluate the excellence of our translations. We derived our BLEU totals operating multi-bleu.pl1 on the tokenized projections and verities. This approach to gauging the BELU result is consistent with [5] and [2], and reproduces the 33.3 sum of [29]. The conclusions are presented in tables 1 and 2. Our premier results were achieved with a collection of LSTMs that differ in their stochastic initializations and in the random sequence of minibatches.","The implemented system reached a speed of 6,300 (for both English and French) words per moment with a minibatch magnitude of 128. The training took approximately ten days employing this system. We utilized the cased BLEU metric [24] to assess the quality of our translations. We calculated our BLEU scores leveraging multi-bleu.pl1 on the tokenized predictions and actualities. This technique for judging the BELU score aligns with [5] and [2], and reproduces the 33.3 total of [29]. The results are shown in tables 1 and 2. Our best outcomes were obtained with a group of LSTMs that vary in their arbitrary initializations and in the random order of minibatches.",A,1
Sequence to Sequence Learning with Neural Networks,"While the decoded translations of the LSTM ensemble do not outperform the best WMT’14 system, it is the first time that a pure neural translation system outperforms a phrase-based SMT baseline on a large scale MT task by a sizeable margin, despite its inability to handle out-of-vocabulary words. The LSTM is within 0.5 BLEU points of the best WMT’14 result if it is used to rescore the 1000-best list of the baseline system.","Although the translated outputs from the LSTM collection don't surpass the top WMT'14 framework, this is the first occasion that a complete neural translation framework beats a phrase-based SMT baseline by a significant edge on a large scale MT assignment, regardless of its failure to deal with out-of-vocabulary words. The LSTM is inside 0.5 BLEU focuses on the best WMT'14 result if it's utilized to rescore the 1000-best rundown of the benchmark framework.","Despite not exceeding the best WMT'14 structure, the interpreted yields of the LSTM gathering are the first where a pure neural translation framework eclipses a phrase-based SMT benchmark by a generous margin on a huge scale MT task, notwithstanding its powerlessness to handle obscure words. The LSTM is inside 0.5 BLEU points of the best WMT'14 result if used to rescore the top 1000 outcomes from the benchmark framework.  ","Although not surpassing the top WMT'14 system, this is the first time a complete neural translation model beats a phrase-based SMT baseline by a wide edge on a large MT job, even with its inability to process unknown words. The LSTM is within 0.5 BLEU points of the best WMT'14 result if utilized to rescore the top 1000 translations from the baseline system.",A,1
Sequence to Sequence Learning with Neural Networks,"There is a large body of work on applications of neural networks to machine translation. So far, the simplest and most effective way of applying an RNN-Language Model (RNNLM) [23] or a Feedforward Neural Network Language Model (NNLM) [3] to an MT task is by rescoring the nbest lists of a strong MT baseline [22], which reliably improves translation quality. More recently, researchers have begun to look into ways of including information about the source language into the NNLM. Examples of this work include Auli et al. [1], who combine an NNLM with a topic model of the input sentence, which improves rescoring performance.","Extensive research has examined using neural networks for machine translation. Up to this point, the most straightforward and productive approach for applying a Recurrent Neural Network Language Model (RNNLM) [23] or a Feedforward Neural Network Language Model (NNLM) [3] to a machine translation task has been through rescoring the n-best lists from a robust baseline machine translation system [22], which consistently enhances translation accuracy. More recently, investigators have started exploring techniques for incorporating knowledge about the source language into the NNLM. An example of this work is Auli et al. [1], who combine an NNLM with a topic model of the input sentence, which improves rescoring outcomes.","There has been considerable work on leveraging neural networks for automated translation. Thus far, the most basic and effectual way of using a Recurrent Neural Network Language Model (RNNLM) [23] or a Feedforward Neural Network Language Model (NNLM) [3] for a machine translation task is through re-ranking the n-best lists from a strong baseline machine translation system [22], which reliably improves translation quality. In recent times, researchers have begun examining methods for integrating information regarding the source language into the NNLM. An instance of this work is Auli et al. [1], who amalgamate an NNLM with a topic model of the input sentence, which enhances re-ranking performance.  ","A substantial body of research exists on applications of neural networks to automated translation. Up until now, the most uncomplicated and fruitful approach to applying a Recurrent Neural Network Language Model (RNNLM) [23] or a Feedforward Neural Network Language Model (NNLM) [3] to a machine translation task has been through re-ordering the n-best lists from a robust baseline machine translation system [22], which consistently improves translation accuracy. More recently, scientists have started investigating techniques for incorporating knowledge of the source language into the NNLM. One example of this work is Auli et al. [1], who combine an NNLM with a topic model of the input sentence, which improves re-ordering performance.",A,1
Sequence to Sequence Learning with Neural Networks,"Devlin et al. [8] followed a similar approach, but they incorporated their NNLM into the decoder of an MT system and used the decoder’s alignment information to provide the NNLM with the most useful words in the input sentence. Their approach was highly successful and it achieved large improvements over their baseline. Our work is closely related to Kalchbrenner and Blunsom [18], who were the first to map the input sentence into a vector and then back to a sentence, although they map sentences to vectors using convolutional neural networks, which lose the ordering of the words.","Devlin and colleagues [8] used a similar technique, but they combined their neural network language model into the decoder of a machine translation system. They utilized the decoder's alignment data to identify the most relevant words in the input sentence for the language model. Their approach worked very well, substantially improving over their baseline. Our research has strong similarities to Kalchbrenner and Blunsom's [18] work, where they first encoded the input sentence into a vector and then decoded it back into a sentence. However, they used convolutional neural networks for the encoding, which does not preserve word order.","The approach by Devlin et al. [8] was analogous, though they integrated their neural network language model with the decoder of a machine translation system. The alignment data from the decoder allowed them to feed the most useful input words into the language model. This technique succeeded remarkably, achieving major gains over their baseline. Our efforts closely follow Kalchbrenner and Blunsom [18], who pioneered encoding input sentences into vectors and decoding them back into sentences. But they applied convolutional neural networks for the encoding, which loses word order.  ","Devlin and co-authors [8] took a similar approach, incorporating their neural network language model into the decoder of a machine translation system. The alignment information from the decoder enabled them to identify the most relevant words in the input sentence to provide to the language model. Their method performed extremely well, substantially outperforming their baseline. Our work closely parallels Kalchbrenner and Blunsom [18], who first mapped input sentences into vectors and then back into sentences, though they used convolutional neural networks for the mapping, which removes word order.",A,1
Sequence to Sequence Learning with Neural Networks,"Similarly to this work, Cho et al. [5] used an LSTM-like RNN architecture to map sentences into vectors and back, although their primary focus was on integrating their neural network into an SMT system. Bahdanau et al. [2] also attempted direct translations with a neural network that used an attention mechanism to overcome the poor performance on long sentences experienced by Cho et al. [5] and achieved encouraging results. Likewise, Pouget-Abadie et al. [26] attempted to address the memory problem of Cho et al. [5] by translating pieces of the source sentence in way that produces smooth translations, which is similar to a phrase-based approach.","In the same way, Cho and colleagues used a recurrent neural network resembling an LSTM to encode sentences into vectors and decode them back, even though their main goal was to integrate their neural network into a statistical machine translation system. Bahdanau and others also tried direct translation with a neural network having an attention mechanism to get around the poor performance on long sentences seen by Cho et al. and got promising outcomes. Similarly, Pouget-Abadie et al. tried to address the memory issues of Cho et al. by translating chunks of the source sentence to generate fluid translations, akin to a phrase-based method.","Analogously, Cho and co-authors employed an LSTM-style recurrent neural network to map sentences to vectors and back again, despite their primary objective being incorporation of their neural network into a statistical MT system. Bahdanau and colleagues likewise attempted direct translation using a neural network with an attention mechanism to overcome the poor long sentence performance encountered by Cho et al., achieving encouraging results. Correspondingly, Pouget-Abadie et al. endeavored to address the memory problems of Cho et al. by translating segments of the source sentence in a manner producing smooth translations, comparable to a phrase-based approach.  ","In a comparable way, Cho and colleagues utilized a recurrent neural network similar to an LSTM for encoding sentences into vectors and decoding them, even though their principal focus was integrating their neural network into a statistical machine translation system. Bahdanau et al. also tried direct translation using a neural network with an attention mechanism to conquer the poor long sentence performance seen by Cho et al., attaining promising results. In a parallel fashion, Pouget-Abadie et al. sought to address the memory issues of Cho et al. by translating pieces of the source sentence in a way that generates fluid translations, analogous to a phrase-based method.",A,1
Sequence to Sequence Learning with Neural Networks,"We suspect that they could achieve similar improvements by simply training their networks on reversed source sentences. End-to-end training is also the focus of Hermann et al. [12], whose model represents the inputs and outputs by feedforward networks, and map them to similar points in space. However, their approach cannot generate translations directly: to get a translation, they need to do a look up for closest vector in the pre-computed database of sentences, or to rescore a sentence.","We think they might get comparable enhancements just by teaching their neural networks using backward source sentences. Training the full system together is also the emphasis of Hermann et al. [12]. Their system embodies the inputs and outputs with feedforward networks, and relates them to analogous spots in space. However, their tactic can't directly form translations: to obtain a translation, they must search for the closest vector in the pre-computed database of sentences, or rescore a sentence.","We conjecture they could gain similar refinements by simply instructing their neural networks on reversed source sentences. End-to-end learning is also the focus of Hermann et al. [12]. Their model represents the inputs and outputs with feedforward networks, and associates them with comparable points in space. Though, their approach is unable to generate translations directly: to acquire a translation, they must look up the closest vector in the pre-calculated database of sentences, or rerank a sentence.  ","We hypothesize they might attain analogous enhancements by just schooling their neural networks on backward source sentences. Whole system training is also the concentration of Hermann et al. [12]. Their framework characterizes the inputs and outputs with feedforward networks, and relates them to similar spots in space. However, their tactic cannot straightforwardly form translations: to attain a translation, they need to search for the closest vector in the pre-figured database of sentences, or rescore a sentence.",A,1
Sequence to Sequence Learning with Neural Networks,"In this work, we showed that a large deep LSTM, that has a limited vocabulary and that makes almost no assumption about problem structure can outperform a standard SMT-based system whose vocabulary is unlimited on a large-scale MT task. The success of our simple LSTM-based approach on MT suggests that it should do well on many other sequence learning problems, provided they have enough training data. We were surprised by the extent of the improvement obtained by reversing the words in the source sentences.","Our research demonstrated that a large, deep LSTM neural network with limited vocabulary and minimal assumptions about problem structure can surpass a standard statistical machine translation system with unlimited vocabulary on a large-scale machine translation task. The effectiveness of our simple LSTM method for machine translation indicates it may also succeed on many other sequence learning problems, if sufficient training data is available. We did not expect that reversing the word order in the source sentences would improve performance to the extent observed.","In this work, we found that a large, deep LSTM neural network, having restricted vocabulary and making hardly any assumptions regarding the structure of the problem, can outdo a standard SMT system with unlimited vocabulary on a large-scale machine translation problem. The success of our basic LSTM technique on machine translation suggests it may also work well on many other sequence learning tasks, if enough training data exists. We were surprised by how much reversing the words in the source sentences boosted performance.","Our study showed that a large, deep LSTM neural network with limited vocabulary and minimal built-in assumptions about the problem structure can surpass a standard statistical machine translation system with unlimited vocabulary on a large-scale machine translation task. The effectiveness of our simple LSTM approach on machine translation indicates it may generalize well to many other sequence learning problems, provided sufficient training data is available. We did not expect reversing the word order in the source sentences to improve performance to the extent we observed.",A,1
Transformer-XL,"Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, TransformerXL learns dependency that is 80% longer than RNNs and 450% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation.","Transformers can learn long-term connections but their context is restricted to a fixed length when modeling language. We put forward a new neural network called Transformer-XL that can learn beyond a fixed context length without disordering time coherence. It uses segment-level recurrence and a new positional encoding method. Our approach allows capturing longer dependencies and also fixes the context fragmentation issue. Consequently, TransformerXL learns 80% longer dependencies than RNNs and 450% longer than vanilla Transformers, performs better on short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during testing.","Transformers have potential for learning extended dependencies but are constrained by a fixed-length context in language modeling. We present a novel neural network architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal order. It utilizes a segment-level recurrence system and an original positional encoding scheme. Our technique not only allows capturing longer-term dependency, but also solves the context fragmentation problem. As a result, TransformerXL learns dependency that is 80% longer than RNNs and 450% longer than standard Transformers, achieves superior performance on both short and long sequences, and is up to 1,800+ times quicker than standard Transformers during evaluation.","Transformers can learn longer-term connections but are limited by a fixed-length context in language modeling. We introduce a new neural network called Transformer-XL that can learn beyond a fixed context length without disturbing temporal order. It uses segment-level recurrence and a novel positional encoding approach. Our method enables capturing extended dependencies and also resolves context fragmentation. Consequently, TransformerXL learns 80% longer dependencies than RNNs and 450% longer than base Transformers, has better performance on short and long sequences, and is up to 1,800+ times faster than base Transformers during testing.",A,1
Transformer-XL,"Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch. Language modeling is among the important problems that require modeling long-term dependency, with successful applications such as unsupervised pretraining (Dai and Le, 2015; Peters et al., 2018; Radford et al., 2018; Devlin et al., 2018).","Notably, we enhance the best existing results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained exclusively on WikiText-103, Transformer-XL succeeds in generating fairly coherent, new text articles with thousands of tokens. Our code, pre-trained models, and hyperparameters are provided in both Tensorflow and PyTorch. Modeling long-term dependency is among the important problems that require it, with successful applications such as unsupervised pretraining (Dai and Le, 2015; Peters et al., 2018; Radford et al., 2018; Devlin et al., 2018).","Remarkably, we improve the current state-of-the-art results for bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained solely on WikiText-103, Transformer-XL is able to generate relatively coherent, new text articles with thousands of tokens. Our code, pre-trained models, and hyperparameters are available in both Tensorflow and PyTorch. Modeling long-term dependencies is among the important challenges that require it, with successful applications such as unsupervised pre-training (Dai and Le, 2015; Peters et al., 2018; Radford et al., 2018; Devlin et al., 2018).  ","Significantly, we advance the current best results for bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL is capable of producing relatively coherent, novel text articles with thousands of tokens. Our code, pre-trained models, and hyperparameters are provided in both Tensorflow and PyTorch. Modeling long-term dependencies is among the important problems that necessitate it, with successful applications such as unsupervised pre-training (Dai and Le, 2015; Peters et al., 2018; Radford et al., 2018; Devlin et al., 2018).",A,1
Transformer-XL,"However, it has been a challenge to equip neural networks with the capability to model long-term dependency in sequential data. Recurrent neural networks (RNNs), in particular Long Short Term Memory (LSTM) networks (Hochreiter and Schmidhuber, 1997), have been a standard solution to language modeling and obtained strong results on multiple benchmarks. Despite the wide adaption, RNNs are difficult to optimize due to gradient vanishing and explosion (Hochreiter et al., 2001), and the introduction of gating in LSTMs and the gradient clipping technique (Graves, 2013) might not be sufficient to fully address this issue.","Nevertheless, giving neural networks the ability to model long-term dependencies in sequential data has proven difficult. Recurrent neural networks (RNNs), especially Long Short Term Memory (LSTM) networks, have been the conventional approach for language modeling and achieved impressive performance on various benchmarks. However, RNNs are notoriously hard to optimize because of gradient vanishing and explosion. The gating mechanisms in LSTMs and gradient clipping may not completely solve this problem.","Training neural networks to retain information over long sequences has been an ongoing challenge. A prevalent tactic is recurrent neural networks (RNNs) like Long Short Term Memory (LSTM) networks, which have become standard for language modeling and excelled on many benchmarks. But RNNs struggle with vanishing and exploding gradients, so innovations like LSTM gating and gradient clipping have not fully resolved trainability. ","Despite their widespread use, equipping neural networks to remember long-range dependencies in sequential data remains an open problem. Recurrent neural networks (RNNs) and Long Short Term Memory (LSTM) networks specifically have been the go-to for language modeling and achieved strong benchmarks, but they are notoriously difficult to optimize due to exploding and vanishing gradients. Introducing gating in LSTMs and gradient clipping helps but may not completely fix these issues.",A,1
Transformer-XL,"Empirically, previous work has found that LSTM language models use 200 context words on average (Khandelwal et al., 2018), indicating room for further improvement. On the other hand, the direct connections between long-distance word pairs baked in attention mechanisms might ease optimization and enable the learning of long-term dependency (Bahdanau et al., 2014; Vaswani et al., 2017). Recently, Al-Rfou et al. (2018) designed a set of auxiliary losses to train deep Transformer networks for character-level language modeling, which outperform LSTMs by a large margin. Despite the success, the LM training in Al-Rfou et al. (2018) is performed on separated fixed-length segments of a few hundred characters, without any information flow across segments.","Past studies have discovered that LSTM language models typically utilize around 200 context words on average (Khandelwal et al., 2018), showing there is still room for enhancement. However, the direct links between distant word pairs inherent in attention mechanisms may facilitate optimization and learning of long-term dependencies (Bahdanau et al., 2014; Vaswani et al., 2017). More recently, Al-Rfou et al. (2018) created a set of auxiliary losses to train deep Transformer networks for character-level language modeling, substantially outperforming LSTMs. Though successful, the LM training in Al-Rfou et al. (2018) is done on fixed-length segments of a few hundred characters, without any cross-segment information flow.","Previous research has found LSTM language models employ about 200 context words on average (Khandelwal et al., 2018), indicating potential for further progress. Conversely, the direct connections between distant word pairs built into attention mechanisms may ease learning and enable capturing long-term dependencies (Bahdanau et al., 2014; Vaswani et al., 2017). Al-Rfou et al. (2018) recently designed auxiliary losses to train deep Transformer networks for character-level language modeling, substantially surpassing LSTMs. However, their LM training operates on separate fixed-length segments of several hundred characters, without cross-segment information transfer.  ","Studies have shown LSTM language models utilize around 200 context words on average (Khandelwal et al., 2018), suggesting room for improvement. In contrast, the direct links between far-apart word pairs inherent in attention mechanisms may facilitate learning and capturing long-term dependencies (Bahdanau et al., 2014; Vaswani et al., 2017). Al-Rfou et al. (2018) recently engineered auxiliary losses to train deep Transformer networks for character-level language modeling, greatly outperforming LSTMs. But their LM training works on isolated fixed-length segments of a few hundred characters, with no information flow between segments.",A,1
Transformer-XL,"As a consequence of the fixed context length, the model cannot capture any longer-term dependency beyond the predefined context length. In addition, the fixed-length segments are created by selecting a consecutive chunk of symbols without respecting the sentence or any other semantic boundary. Hence, the model lacks necessary contextual information needed to well predict the first few symbols, leading to inefficient optimization and inferior performance. We refer to this problem as context fragmentation. To address the aforementioned limitations of fixed-length contexts, we propose a new architecture called Transformer-XL (meaning extra long).","Because of the fixed context length, the model is unable to capture any longer-term dependency that goes beyond the predefined context length. Also, the fixed-length segments are made by choosing a consecutive chunk of symbols without considering the sentence or any other semantic boundary. As a result, the model does not have the necessary contextual information required to accurately predict the first few symbols, resulting in inefficient optimization and poorer performance. We refer to this issue as context fragmentation. To address the limitations of fixed-length contexts that were mentioned, we propose a new architecture called Transformer-XL (meaning extra long).","Due to the constant context length, the model cannot grasp any longer-term connection further than the pre-defined context length. Furthermore, the fixed-length chunks are formed by taking a successive piece of symbols without regarding the sentence or any other semantic limit. Therefore, the model does not have the required contextual data to properly foresee the first few symbols, causing inefficient enhancement and second-rate presentation. We name this problem context disintegration. To tackle the stated constraints of fixed-length contexts, we suggest a new design called Transformer-XL (meaning additional long). ","Because the context length is fixed, the model is not able to understand any longer-term relationship beyond the pre-set context length. Also, the fixed-length segments are made by taking a series of symbols in a row without considering the sentence or any other semantic boundary. As a result, the model lacks the contextual information needed to accurately predict the first few symbols, leading to ineffective optimization and lower performance. We call this issue context fragmentation. To address the limitations of fixed-length contexts that were described, we put forward a new architecture called Transformer-XL (meaning extra long).",A,1
Transformer-XL,"We introduce the notion of recurrence into our deep self-attention network. In particular, instead of computing the hidden states from scratch for each new segment, we reuse the hidden states obtained in previous segments. The reused hidden states serve as memory for the current segment, which builds up a recurrent connection between the segments. As a result, modeling very longterm dependency becomes possible because information can be propagated through the recurrent connections. Meanwhile, passing information from the previous segment can also resolve the problem of context fragmentation.","We bring in the concept of recurrence into our deep self-attention framework. Specifically, rather than generating the hidden states completely from the beginning for each new portion, we reuse the hidden states obtained in prior portions. The reused hidden states function as memory for the current portion, which constructs a recurrent link between the portions. Consequently, modeling very long-range dependency is achievable because information can be propagated via the recurrent connections. Moreover, transferring information from the previous portion can also fix the issue of context fragmentation.","We introduce the idea of repetition into our deep self-attention model. In particular, instead of producing the hidden states entirely from scratch for every new section, we recycle the hidden states acquired in earlier sections. The recycled hidden states act as remembrance for the current section, which builds a repetitive connection between the sections. As a result, modeling very distant dependency becomes feasible since information can be spread through the repetitive connections. Meanwhile, passing information from the prior section can also resolve the problem of context separation.","We bring in the concept of recurrency into our deep self-attention structure. Specifically, rather than generating the hidden states completely from zero for each new part, we reuse the hidden states obtained in previous parts. The reused hidden states serve as memory for the current part, which constructs a recurrent link between the parts. As a result, modeling very long-span dependency becomes possible because information can be propagated through the recurrent connections. Meanwhile, transferring information from the prior part can also fix the issue of context fragmentation.",A,1
Transformer-XL,"More importantly, we show the necessity of using relative positional encodings rather than absolute ones, in order to enable state reuse without causing temporal confusion. Hence, as an additional technical contribution, we introduce a simple but more effective relative positional encoding formulation that generalizes to attention lengths longer than the one observed during training. Transformer-XL obtained strong results on five datasets, varying from word-level to character level language modeling. Transformer-XL is also able to generate relatively coherent long text articles with thousands of tokens (see Appendix E), trained on only 100M tokens.","Most significantly, we demonstrate the need to use relative positional encodings instead of absolute ones, so that states can be reused without causing temporal disarray. Thus, as an extra technical contribution, we present a simple yet more effective relative positional encoding formulation that extends to attention lengths longer than that seen during training. Transformer-XL achieved strong outcomes on five datasets, ranging from word-level to character level language modeling. Transformer-XL is also capable of generating fairly coherent long text articles with thousands of tokens (see Appendix E), trained on only 100M tokens.","Above all, we exhibit the necessity of utilizing relative positional codings rather than absolute ones, in order to facilitate state reuse without inducing temporal confusion. Therefore, as an additional technical offering, we introduce a simple but more efficacious relative positional coding formulation that generalizes to attention lengths greater than the one observed during training. Transformer-XL obtained robust results on five datasets, varying from word-level to character level language modeling. Transformer-XL is also able to produce relatively coherent long text articles with thousands of tokens (see Appendix E), trained on only 100M tokens.  ","Most importantly, we demonstrate the need to employ relative positional encodings instead of absolute ones, so that states can be reused without causing temporal disarray. Hence, as a further technical contribution, we present a simple yet more effectual relative positional encoding formulation that extends to attention lengths longer than that witnessed during training. Transformer-XL achieved strong outcomes on five datasets, ranging from word-level to character level language modeling. Transformer-XL is also capable of generating relatively coherent long text articles with thousands of tokens (see Appendix E), trained on just 100M tokens.",A,1
Transformer-XL,"Our main technical contributions include introducing the notion of recurrence in a purely self-attentive model and deriving a novel positional encoding scheme. These two techniques form a complete set of solutions, as any one of them alone does not address the issue of fixed-length contexts. Transformer-XL is the first self-attention model that achieves substantially better results than RNNs on both character-level and word-level language modeling.","The key technical innovations we present are bringing in the idea of recurrence into a model using only self-attention and creating a new positional encoding system. These two techniques together provide a full set of solutions, since either one on its own does not solve the problem of fixed-length contexts. Transformer-XL is the first self-attention model to get much better performance than RNNs on language modeling with both characters and words.","Our primary technical contributions are introducing recurrence in a purely self-attentive architecture and developing a novel positional encoding approach. Using both techniques together provides a complete solution, since using only one does not address fixed context lengths. Transformer-XL is the first pure self-attention model to substantially outperform RNNs at language modeling with characters and words.  ","The main technical novelties in our work are bringing in recurrence to a purely self-attentive model and inventing a new positional encoding method. Employing both techniques gives a full set of solutions, since using just one does not handle fixed context sizes. Transformer-XL is the first pure self-attention architecture that achieves much superior performance over RNNs for language modeling at the character and word levels.",A,1
Transformer-XL,"In the last few years, the field of language modeling has witnessed many significant advances, including but not limited to devising novel architectures to better encode the context (Bengio et al., 2003; Mikolov et al., 2010; Merity et al., 2016; Al-Rfou et al., 2018), improving regularization and optimization algorithms (Gal and Ghahramani, 2016) , speeding up the Softmax computation (Grave et al., 2016a) , and enriching the output distribution family (Yang et al., 2017). To capture the long-range context in language modeling, a line of work directly feeds a representation of the wider context into the network as an additional input.","Over the past several years, the area of language modeling has seen many noteworthy improvements, including but not limited to creating new architectures to represent context better (Bengio et al., 2003; Mikolov et al., 2010; Merity et al., 2016; Al-Rfou et al., 2018), enhancing regularization and optimization algorithms (Gal and Ghahramani, 2016), accelerating Softmax computation (Grave et al., 2016a), and expanding the output distribution family (Yang et al., 2017). To incorporate the long-range context in language modeling, one approach directly inputs a representation of the broader context into the network.","In recent times, the domain of language modeling has experienced numerous major advancements, such as designing novel structures to more effectively encode context (Bengio et al., 2003; Mikolov et al., 2010; Merity et al., 2016; Al-Rfou et al., 2018), refining regularization and optimization techniques (Gal and Ghahramani, 2016), speeding up Softmax calculation (Grave et al., 2016a), and enriching the output distribution set (Yang et al., 2017). To capture the long-range context in language modeling, one method directly feeds a representation of the wider context into the system as extra input.","Over the past few years, the field of language modeling has seen many important improvements, including but not limited to inventing new architectures to better represent context (Bengio et al., 2003; Mikolov et al., 2010; Merity et al., 2016; Al-Rfou et al., 2018), enhancing regularization and optimization formulas (Gal and Ghahramani, 2016), accelerating Softmax computation speed (Grave et al., 2016a), and expanding the output distribution options (Yang et al., 2017). To incorporate the long-range context in language modeling, one technique directly inputs a representation of the broader context into the model as additional input.",A,1
Transformer-XL,"Existing works range from ones where context representations are manually defined (Mikolov and Zweig, 2012; Ji et al., 2015; Wang and Cho, 2015) to others that rely on document-level topics learned from data (Dieng et al., 2016; Wang et al., 2017). More broadly, in generic sequence modeling, how to capture long-term dependency has been a long-standing research problem. From this perspective, since the ubiquitous adaption of LSTM, many efforts have been spent on relieving the vanishing gradient problem, including better initialization (Le et al., 2015), additional loss signal (Trinh et al., 2018), augmented memory structure (Ke et al., 2018) and others that modify the internal architecture of RNNs to ease the optimization (Wu et al., 2016; Li et al., 2018).","Existing works include ones where context representations are manually defined (Mikolov and Zweig, 2012; Ji et al., 2015; Wang and Cho, 2015) and others that use document-level topics learned from data (Dieng et al., 2016; Wang et al., 2017). More generally, in generic sequence modeling, how to maintain long-term dependency has been an ongoing research issue. Since the widespread adoption of LSTM, many efforts have focused on addressing the vanishing gradient problem, including better initialization (Le et al., 2015), additional loss signal (Trinh et al., 2018), augmented memory structure (Ke et al., 2018) and others that alter the internal architecture of RNNs to facilitate optimization (Wu et al., 2016; Li et al., 2018).","Current works range from those where context representations are hand-crafted (Mikolov and Zweig, 2012; Ji et al., 2015; Wang and Cho, 2015) to others relying on document-level topics derived from data (Dieng et al., 2016; Wang et al., 2017). Broadly, in generic sequence modeling, capturing long-term dependency has been a persistent research challenge. Since LSTM became ubiquitous, many efforts have targeted the vanishing gradient issue, including superior initialization (Le et al., 2015), extra loss signal (Trinh et al., 2018), expanded memory structure (Ke et al., 2018) and others modifying the internal RNN architecture to ease optimization (Wu et al., 2016; Li et al., 2018).  ","Existing research includes studies where context representations are manually engineered (Mikolov and Zweig, 2012; Ji et al., 2015; Wang and Cho, 2015) and those leveraging document-level topics learned from data (Dieng et al., 2016; Wang et al., 2017). In general sequence modeling, modeling long-term dependencies has been a standing research problem. Since LSTM became prevalent, much work has focused on the vanishing gradient issue, including better initialization (Le et al., 2015), supplementary loss signal (Trinh et al., 2018), augmented memory architectures (Ke et al., 2018) and other RNN architecture modifications to facilitate optimization (Wu et al., 2016; Li et al., 2018).",A,1
Transformer-XL,"In this work, we stick to the standard neural approach to modeling the conditional probability. Specifically, a trainable neural network is used to encode the context x<t into a fixed size hidden state, which is multiplied with the word embeddings to obtain the logits. The logits are then fed into the Softmax function, yielding a categorical probability distribution over the next token. In order to apply Transformer or self-attention to language modeling, the central problem is how to train a Transformer to effectively encode an arbitrarily long context into a fixed size representation. Given infinite memory and computation, a simple solution would be to process the entire context sequence using an unconditional Transformer decoder, similar to a feed-forward neural network.","In this work, we adhere to the conventional neural technique for modeling the conditional likelihood. Namely, a trainable neural network is utilized to encode the context x<t into a static sized latent state, which is multiplied with the word embeddings to get the logits. The logits are then provided to the Softmax function, yielding a categorical probability allocation over the next token. To be able to apply Transformer or self-attention to language modeling, the key issue is how to teach a Transformer to competently encode an arbitrarily long context into a fixed size depiction. Provided unrestricted memory and calculation, a straightforward solution would be to process the whole context sequence utilizing an unconditional Transformer decoder, akin to a feed-forward neural network.","In this work, we follow the standard neural methodology for modeling the conditional probability. In particular, a trainable neural network is employed to encode the context x<t into a constant size hidden representation, which is multiplied with the word vectors to obtain the logits. The logits are then input to the Softmax function, producing a categorical probability distribution over the next token. To apply Transformer or self-attention to language modeling, the central challenge is how to train a Transformer to effectively encode an arbitrarily long context into a fixed size representation. With unlimited memory and computation, a simple solution would be to process the entire context sequence using an unconditional Transformer decoder, similar to a feedforward neural network.","In this work, we adhere to the conventional neural approach for modeling the conditional probability. Specifically, a trainable neural network is used to encode the context x<t into a fixed dimension latent state, which is multiplied with the word embeddings to get the logits. The logits are then passed to the Softmax function, yielding a categorical probability distribution over the next token. To apply Transformer or self-attention to language modeling, the core problem is how to train a Transformer to efficiently encode an arbitrarily long context into a fixed size representation. Given infinite memory and computation, a straightforward solution would be to process the whole context sequence using an unconditional Transformer decoder, akin to a feedforward neural network.",A,1
Transformer-XL,"One feasible but crude approximation is to split the entire corpus into shorter segments of man-ageable sizes, and only train the model within each segment, ignoring all contextual information from previous segments. This is the idea adopted by Al-Rfou et al. (2018). We call it the vanilla model and visualize it in Fig. 1a. Under this training paradigm, information never flows across segments in either the forward or backward pass. There are two critical limitations of using a fixed length context. First, the largest possible dependency length is upper bounded by the segment length, which is a few hundred on character-level language modeling (Al-Rfou et al., 2018).","A possible but unsophisticated way to handle this is to break up the full body of text into smaller, more manageable chunks, and train the model on each chunk separately, not taking into account any context from previous chunks. This is the approach used by Al-Rfou et al. (2018). We refer to this as the basic model and show it in Fig. 1a. With this training method, information never crosses between chunks in either direction. There are two major drawbacks to having a fixed context length. First, the longest attainable dependency is limited by the chunk size, which is a few hundred for character-level language modeling (Al-Rfou et al., 2018).","One rough but workable solution is to split the whole corpus into shorter, more manageable segments and only train the model within those segments, disregarding any contextual data from prior segments. This is the concept used by Al-Rfou et al. (2018). We call this the standard model and illustrate it in Fig. 1a. With this training approach, information never travels between segments in either direction. There are two main limitations to having a fixed context length. First, the maximum possible dependency length is constrained by the segment length, which is a few hundred for character-level language modeling (Al-Rfou et al., 2018).","A basic but viable approximation is to divide the full text corpus into shorter, more manageable sized chunks, and only train the model on each chunk, ignoring any context from previous chunks. This is the idea used by Al-Rfou et al. (2018). We refer to this as the simple model and depict it in Fig. 1a. Under this training method, information never passes between chunks in either the forward or backward direction. There are two major drawbacks to having a fixed context size. First, the longest possible dependency is limited by the chunk size, which is a few hundred for character-level language modeling (Al-Rfou et al., 2018).",A,1
Transformer-XL,"Second, though it is possible to use padding to respect the sentence or other semantic boundaries, in practice it has been standard practice to simply chunk long text into fixed-length segments due to improved efficiency (Peters et al., 2018; Devlin et al., 2018; Al-Rfou et al., 2018). However, simply chunking a sequence into fixed-length segments will lead to the context fragmentation problem as discussed in Section 1.","Next, even though padding could be utilized to regard the limits of sentences or other semantic boundaries, in actuality the standard procedure has been to just split long text into fixed-length chunks because of enhanced speed (Peters et al., 2018; Devlin et al., 2018; Al-Rfou et al., 2018). But simply dividing a sequence into fixed-length pieces will result in the context fragmentation issue as talked about in Section 1.","Furthermore, despite the fact that it's feasible to use padding to respect sentence or other semantic borders, in practice the standard has been to just split long text into segments of a fixed length for better efficiency (Peters et al., 2018; Devlin et al., 2018; Al-Rfou et al., 2018). However, merely breaking up a sequence into fixed-length chunks will lead to the context fragmentation problem as described in Section 1.  ","Additionally, although padding can be used to acknowledge sentence or other semantic limits, the common practice has been to simply separate long text into fixed-size portions for increased speed (Peters et al., 2018; Devlin et al., 2018; Al-Rfou et al., 2018). But just dividing a sequence into fixed-length segments will cause the context fragmentation issue as explained in Section 1.",A,1
Transformer-XL,"To address the limitations of using a fixed-length context, we propose to introduce a recurrence mechanism to the Transformer architecture. During training, the hidden state sequence computed for the previous segment is fixed and cached to be reused as an extended context when the model processes the next new segment, as shown in Fig. 2a. Although the gradient still remains within a segment, this additional input allows the network to exploit information in the history, leading to an ability of modeling longer-term dependency and avoiding context fragmentation.","In order to handle the problems of using a context of a fixed size, we suggest adding a recurring process to the Transformer structure. While training, the hidden state sequence calculated for the prior segment is fixed and stored to be reused as a prolonged context when the model handles the next new segment, as depicted in Fig. 2a. Despite the gradient still staying within a segment, this extra input enables the network to utilize information in the history, resulting in an ability to model longer-term dependency and prevent context fragmentation.","To tackle the issues of utilizing a context of an unchanging length, our proposal is to bring in a repetitive operation into the Transformer design. As training proceeds, the hidden state order computed for the earlier portion is stabilized and cached to be used again as a drawn-out setting when the model tackles the subsequent fresh portion, as presented in Fig. 2a. Although the slope remains inside a portion, this supplementary contribution allows the network to take advantage of particulars in the historical background, conferring an aptitude for modeling more long-term dependency and impeding context disintegration. ","In order to address the shortcomings of employing a context of a rigid dimension, our recommendation is to introduce a recurring action into the Transformer blueprint. During training, the hidden state sequence calculated for the prior segment is fixed and stored to be recycled as an extended backdrop when the model handles the next novel segment, as depicted in Fig. 2a. Despite the gradient still residing within a segment, this additional input enables the network to leverage information in the history, bestowing an ability of modeling longer-lasting dependency and preventing context dissolution.",A,1
Transformer-XL,"During evaluation, at each step, the vanilla model also consumes a segment of the same length as in training, but only makes one prediction at the last position. Then, at the next step, the segment is shifted to the right by only one position, and the new segment has to be processed all from scratch. As shown in Fig. 1b, this procedure ensures that each prediction utilizes the longest possible context exposed during training, and also relieves context fragmentation issue encountered in training. However, this evaluation procedure is extremely expensive. We will show that our proposed architecture is able to substantially improve the evaluation speed.","When assessing performance, the standard model ingests a section of identical size to that used during learning at each phase, but solely generates one forecast at the final spot. Subsequently, in the next phase, the section is moved to the right by just a single location, and the new section needs to be handled completely from the beginning. As depicted in Fig. 1b, this process guarantees that each prediction leverages the longest available context revealed during learning, and also alleviates context fragmentation difficulties experienced during training. However, this evaluation methodology is extremely costly. We will demonstrate that our suggested design can substantially enhance the evaluation velocity.","During testing, the vanilla model takes in a chunk of the same length as in training at each iteration, but only produces one prediction at the final position. At the next iteration, the chunk is shifted right by only one spot, and the new chunk must be processed from scratch. As shown in Fig. 1b, this approach ensures each prediction uses the longest context seen in training, and avoids context fragmentation issues faced in training. But this testing procedure is very expensive. We will show our proposed architecture can greatly improve the testing speed.","When measuring performance, the basic model consumes a segment of equal size to training at each step, but only generates one forecast at the final spot. Next, the segment moves right one place, and the new segment must be handled from the start. As in Fig. 1b, this verifies each prediction harnesses the maximum context from training, and prevents context fragmentation from training. However, this is very costly to evaluate. We will demonstrate our architecture can drastically improve evaluation pace.",A,1
Transformer-XL,"This is analogous to truncated BPTT (Mikolov et al., 2010), a technique developed for training RNNLMs. However, different from truncated BPTT, our method caches a sequence of hidden states instead of the last one, and should be applied together with the relative positional encoding technique described in Section 3.3. Besides achieving extra long context and resolving fragmentation, another benefit that comes with the recurrence scheme is significantly faster evaluation. Specifically, during evaluation, the representations from the previous segments can be reused instead of being computed from scratch as in the case of the vanilla model. In our experiments on enwiki8, Transformer-XL is up to 1,800+ times faster than the vanilla model during evaluation (see Section 4).","This technique is similar to truncated BPTT (Mikolov et al., 2010), a method developed for training RNNLMs. However, unlike truncated BPTT which caches only the last hidden state, our approach stores a sequence of hidden states and should be used with the relative positional encoding method from Section 3.3. In addition to enabling extra long context and resolving fragmentation, another advantage of this recurrent scheme is much faster evaluation. In particular, during evaluation, the representations from previous segments can be reused rather than computed from scratch as with the vanilla model. In our enwiki8 experiments, Transformer-XL was up to 1,800+ times faster than the vanilla model during evaluation (see Section 4).","This approach is analogous to truncated BPTT (Mikolov et al., 2010), a procedure created for training RNNLMs. But our technique differs from truncated BPTT in that it stores a sequence of hidden states instead of just the last one, and needs to be combined with the relative positional encoding approach outlined in Section 3.3. On top of providing extra long context and fixing fragmentation, another benefit of this recurrent system is substantially faster assessment. Specifically, during assessment, the representations from earlier segments can be reused rather than calculated from the beginning as in the vanilla model. In our enwiki8 tests, Transformer-XL was up to 1,800+ times quicker than the vanilla model during assessment (see Section 4).  ","This method is similar to truncated BPTT (Mikolov et al., 2010), a process invented for training RNNLMs. However, our strategy caches a sequence of hidden states rather than only the final one, unlike truncated BPTT, and requires using the relative positional encoding approach described in Section 3.3. Apart from enabling extra long context and solving fragmentation, another advantage of this recurrent framework is much faster appraisal. In particular, during appraisal, the representations from prior segments can be reused instead of computed from zero as in the vanilla model. In our enwiki8 experiments, Transformer-XL was up to 1,800+ times faster than the vanilla model during appraisal (see Section 4).",A,1
Transformer-XL," Finally, notice that the recurrence scheme does not need to be restricted to only the previous segment. In theory, we can cache as many previous segments as the GPU memory allows, and reuse all of them as the extra context when processing the current segment. While we found the idea presented in the previous subsection very appealing, there is a crucial technical challenge we haven’t solved in order to reuse the hidden states. That is, how can we keep the positional information coherent when we reuse the states?","Ultimately, recognize that the repetitive pattern does not have to be limited to just the prior portion. Hypothetically, we could store as many past portions as the GPU memory permits, and make use of all of them as supplementary context when working on the present portion. Although we considered the concept shown in the preceding subsection extremely appealing, there is a vital technical obstacle we have not resolved in order to recycle the concealed states. Specifically, how can we maintain the positional data logical when we recycle the states?","In closing, understand that the repeating arrangement need not be constrained to only the preceding segment. Theoretically, we could cache as many prior segments as the GPU memory provides for, and leverage all of them as extra background when handling the current segment. Despite finding the idea presented in the previous subsection very enticing, there is a crucial technical challenge we have not addressed in order to reuse the hidden states. Namely, how can we keep the positional information coherent when we reuse the states?","Lastly, realize that the recurrent pattern does not have to be limited to just the previous portion. In principle, we could store as many earlier portions as the GPU memory allows for, and utilize all of them as supplementary context when processing the current portion. While we considered the concept outlined in the prior subsection extremely promising, there is a vital technical hurdle we have not conquered in order to recycle the concealed states. Specifically, how can we maintain the positional data consistent when we recycle the states?",A,1
Transformer-XL,"In order to avoid this failure mode, the fundamental idea is to only encode the relative positional information in the hidden states. Conceptually, the positional encoding gives the model a temporal clue or “bias” about how information should be gathered, i.e., where to attend. For the same purpose, instead of incorporating bias statically into the initial embedding, one can inject the same information into the attention score of each layer. More importantly, it is more intuitive and generalizable to define the temporal bias in a relative manner.","To prevent this problem, the key concept is to only encode the relative position data in the hidden layers. In essence, the position encoding provides the model with a temporal hint or ""prejudice"" about how to collect information, namely where to pay attention. Similarly, rather than statically integrating the bias into the initial embedding, one can inject the same details into the attention score of every layer. Most importantly, it is more intuitive and generalizable to characterize the temporal bias relatively.","In order to steer clear of this failure, the fundamental notion is to exclusively encode the comparative positional knowledge in the concealed states. In principle, the positional encoding furnishes the model with a chronological clue or ""partiality"" regarding how material ought to be compiled, specifically where to direct attention. For the same rationale, in lieu of solidifying prejudice into the initial embedding, one could infuse the same intelligence into the attention tally of each stratum. Above all, it is more instinctive and transferable to delineate the chronological bias relatively.","To sidestep this breakdown, the cardinal tenet is to only transcribe the proportional situational gen in the veiled tiers. At its core, the positional encoding endows the model with a temporal hint or ""penchant"" about how info should be amassed, viz. where to spotlight. Toward the same end, rather than ingraining bias statically into the initial embedding, one can suffuse the same dope into the attention score of each echelon. Critically, it is more intuitive and fungible to delineate the temporal bias relatively.",A,1
Transformer-XL,"Meanwhile, we won’t lose any temporal information, as the absolute position can be recovered recursively from relative distances. Previously, the idea of relative positional encodings has been explored in the context of machine translation (Shaw et al., 2018) and music generation (Huang et al., 2018). Here, we offer a different derivation, arriving at a new form of relative positional encodings, which not only has a one-to-one correspondence to its absolute counterpart but also enjoys much better generalization empirically (see Section 4).","In the meantime, we will not miss any details about time, since we can recursively recover the exact position from relative distances. Before this, the concept of relative positional encodings was looked at for machine translation (Shaw et al., 2018) and music creation (Huang et al., 2018). Here, we provide a different analysis, getting a new type of relative positional encodings, which not only has a one-to-one link to its absolute equivalent but also has much better generalization empirically (see Section 4).","Meanwhile, we will not lose any information about when things occurred, because the precise position can be found again recursively from relative separations. Previously, the notion of relative positional encodings was explored for machine translation (Shaw et al., 2018) and generating music (Huang et al., 2018). In this paper, we present a different derivation, arriving at a new form of relative positional encodings, which has a direct correspondence to its absolute counterpart and also performs much better when applied empirically (see Section 4).","In the interim, we will not miss any details about timing, since we can recursively recover the exact position from relative distances. Beforehand, the concept of relative positional encodings was investigated for machine translation (Shaw et al., 2018) and music synthesis (Huang et al., 2018). Here, we provide a different analysis, obtaining a new type of relative positional encodings, which not only has a one-to-one mapping to its absolute equivalent but also has much superior generalization empirically (see Section 4).",A,1
Transformer-XL,"Under the new parameterization, each term has an intuitive meaning: term (a) represents content based addressing, term (b) captures a content dependent positional bias, term (c) governs a global content bias, and (d) encodes a global positional bias. In comparison, the formulation in Shaw et al. (2018) only has terms (a) and (b), dropping the two bias terms (c) and (d). Moreover, Shaw et al. (2018) merge the multiplication WkR into a single trainable matrix Rˆ , which abandons the inductive bias built into the original sinusoid positional encoding (Vaswani et al., 2017). In contrast, our relative positional embedding R adapts the sinusoid formulation.","With the new parameterization, each part has an easy to understand meaning: part (a) stands for content based addressing, part (b) captures a content related positional tendency, part (c) controls a global content inclination, and (d) encodes a global positional tendency. Compared to the formulation in Shaw et al. (2018) which only has parts (a) and (b), dropping the two bias terms (c) and (d). Furthermore, Shaw et al. (2018) combine the multiplication WkR into one trainable matrix Rˆ, which loses the inductive bias constructed into the original sinusoid positional encoding (Vaswani et al., 2017). In contrast, our relative positional embedding R adapts the sinusoid formulation.","In the new parameterization, each component has an intuitive interpretation: component (a) denotes content based addressing, component (b) seizes a content contingent positional bias, component (c) directs a global content bias, and (d) symbolizes a global positional bias. In comparison, the formulation in Shaw et al. (2018) only comprises components (a) and (b), omitting the two bias terms (c) and (d). Additionally, Shaw et al. (2018) consolidate the multiplication WkR into a single trainable matrix Rˆ, which forsakes the inductive bias ingrained in the original sinusoid positional encoding (Vaswani et al., 2017). Conversely, our relative positional embedding R conforms to the sinusoid formulation.  ","Under the new parameterization, each piece has an easy to grasp meaning: piece (a) embodies content based addressing, piece (b) seizes a content related positional tendency, piece (c) governs a global content inclination, and (d) typifies a global positional tendency. Compared to the formulation in Shaw et al. (2018) which only encompasses pieces (a) and (b), excluding the two bias terms (c) and (d). Furthermore, Shaw et al. (2018) coalesce the multiplication WkR into one trainable matrix Rˆ, which relinquishes the inductive bias implanted in the original sinusoid positional encoding (Vaswani et al., 2017). In contrast, our relative positional embedding R adheres to the sinusoid formulation.",A,1
Transformer-XL,"WikiText-103 is the largest available word-level language modeling benchmark with long-term dependency. It contains 103M training tokens from 28K articles, with an average length of 3.6K tokens per article, which allows testing the ability of long-term dependency modeling. We set the attention length to 384 during training and 1600 during evaluation. We adopted adaptive softmax and input representations (Baevski and Auli, 2018; Grave et al., 2016a). As shown in Table 1, Transformer-XL reduces the previous state-of-theart (SoTA) perplexity from 20.5 to 18.3, which demonstrates the superiority of the TransformerXL architecture.","WikiText-103 is the biggest open word-level language modeling benchmark that requires modeling long-range dependencies. It has 103 million training tokens taken from 28,000 articles, with each article containing around 3,600 tokens on average. This allows for testing models on their ability to capture long-term dependencies. During training we used an attention length of 384 and during evaluation we used 1600. We used adaptive softmax and input representations as described in prior work (Baevski and Auli, 2018; Grave et al., 2016a). As shown in Table 1, Transformer-XL reduces the previous state-of-the-art perplexity from 20.5 down to 18.3, demonstrating the advantages of the Transformer-XL architecture.","WikiText-103 remains the most substantial publicly available benchmark for evaluating word-level language models on their capacity to model extended contextual dependencies. Its training set comprises 103 million tokens extracted from 28 thousand articles, with each article containing approximately 3,600 tokens on average, permitting assessment of modeling distant dependencies. Attention length was set to 384 during training and 1,600 during evaluation. Adaptive softmax and input representations were adopted as in previous work (Baevski and Auli, 2018; Grave et al., 2016a). As presented in Table 1, Transformer-XL lowers the prior state-of-the-art perplexity from 20.5 to 18.3, evidencing the benefits of the Transformer-XL design.","WikiText-103 continues to be the biggest open benchmark for testing word-level language models on their ability to capture long-range dependencies. It has 103 million training tokens taken from 28 thousand articles, where each article has around 3,600 tokens on average. This long sequence length allows models to be evaluated on modeling distant dependencies. Attention length was 384 during training and 1,600 during evaluation. We used adaptive softmax and input representations as in previous work (Baevski and Auli, 2018; Grave et al., 2016a). As shown in Table 1, Transformer-XL reduces the previous state-of-the-art perplexity from 20.5 down to 18.3, demonstrating the advantages of the Transformer-XL architecture.",A,1
Transformer-XL,"The dataset enwik8 contains 100M bytes of unprocessed Wikipedia text. We compare our architecture with the previous results in Table 2. Under the model size constraint, the 12-layer Transformer-XL achieves a new SoTA result, outperforming the 12-layer vanilla Transformer from Al-Rfou et al. (2018) by 0.05, while both Transformer variants have a large margin over conventional RNN-based models. Notably, our 12-layer architecture achieves the same result as the 64- layer network from Al-Rfou et al. (2018), using only 17% of the parameter budget. In order to see whether better performances can be obtained by increasing the model size, we train 18-layer and 24-layer Transformer-XLs with increased model sizes.","The unprocessed enwik8 dataset has 100M bytes of Wikipedia text. We make a comparison between our system design and prior outcomes in Table 2. Under the constraint on model size, the 12-layer Transformer-XL achieves a new state-of-the-art result, surpassing the 12-layer vanilla Transformer from Al-Rfou et al. (2018) by 0.05, while both Transformer variants greatly outperform conventional RNN models. Notably, our 12-layer architecture achieves the same result as the 64-layer network from Al-Rfou et al. (2018), utilizing only 17% of the parameter budget. To see if better performance can be obtained by expanding the model size, we train 18-layer and 24-layer Transformer-XLs with increased model sizes.","The raw enwik8 data contains 100M bytes of Wikipedia text. We benchmark our framework against previous scores in Table 2. Subject to the model size limit, the 12-layer Transformer-XL obtains a new best result, exceeding the 12-layer standard Transformer from Al-Rfou et al. (2018) by 0.05, whereas both Transformer versions substantially beat traditional RNN architectures. Remarkably, our 12-layer model equals the 64-layer network from Al-Rfou et al. (2018), with only 17% of the parameters. To check if larger models improve performance, we train 18-layer and 24-layer Transformer-XLs by increasing model size.","The unrefined enwik8 dataset holds 100M bytes of Wikipedia content. We measure our system against prior outputs in Table 2. Under the constraint of model scale, the 12-layer Transformer-XL achieves a new top score, surpassing the 12-layer vanilla Transformer from Al-Rfou et al. (2018) by 0.05, while both Transformer designs far exceed old RNN models. Strikingly, our 12-layer structure matches the 64-layer network from Al-Rfou et al. (2018), utilizing just 17% of the parameters. To see if bigger models boost performance, we train 18-layer and 24-layer Transformer-XLs by expanding model scale.",A,1
Transformer-XL,"With the attention length 784 during training and 3,800 during evaluation, we obtained a new SoTA result and our method is the first to break through 1.0 on widely-studied character level benchmarks. Different from Al-Rfou et al. (2018), Transformer-XL does not need any auxiliary losses, and thus all benefits are credited to a better architecture. Similar to but different from enwik8, text8 contains 100M processed Wikipedia characters created by lowering case the text and removing any character other than the 26 letters a through z, and space.","By using focus times of 784 during preparation and 3,800 during assessment, we achieved a new state-of-the-art outcome and our approach is the first to surpass 1.0 on extensively studied character level benchmarks. In contrast to Al-Rfou et al. (2018), Transformer-XL does not require any supplementary losses, so all advantages are attributed to a superior design. Similar to but distinct from enwik8, text8 has 100M processed Wikipedia characters formed by making the text lowercase and eliminating any character other than the 26 letters a through z, and space.","Through applying attention durations of 784 during training and 3,800 during evaluation, we obtained a new best performance and our method is the first to exceed 1.0 on commonly used character level measures. Unlike Al-Rfou et al. (2018), Transformer-XL does not need any extra losses, so all benefits are credited to a more advanced structure. Comparable to but different from enwik8, text8 comprises 100M processed Wikipedia characters produced by changing the text to lowercase and removing any character apart from the 26 letters a through z, and space.  ","By utilizing attention times of 784 during learning and 3,800 during testing, we achieved a new highest result and our approach is the first to go past 1.0 on extensively used character level benchmarks. In contrast with Al-Rfou et al. (2018), Transformer-XL does not require any supplementary losses, and therefore all advantages are attributed to a more sophisticated architecture. Similar to but distinct from enwik8, text8 holds 100M processed Wikipedia characters formed by changing the text to lowercase and taking out any character other than the 26 letters a through z, and space.",A,1
Transformer-XL,"Due to the similarity, we simply adapt the best model and the same hyper-parameters on enwik8 to text8 without further tuning. The comparison with previous methods is summarized in Table 3. Again, Transformer-XL achieves the new SoTA result with a clear margin. term dependency because sentences have been shuffled. Consequently, this dataset mainly tests the ability of modeling only short-term dependency. The comparison between Transformer-XL and the other methods is shown in Table 4. Although Transformer-XL is mainly designed to better capture longer-term dependency, it dramatically improves the single-model SoTA from 23.7 to 21.8.","Because the datasets are so alike, we just take the optimal model and identical hyperparameters from enwik8 and apply them to text8 without any further adjustment. The contrast with prior approaches is outlined in Table 3. Yet again, Transformer-XL produces the new state-of-the-art result by a wide margin. This dataset primarily examines the capacity for modeling brief term relationships since the sentences have been jumbled. Therefore, this dataset mainly evaluates the ability to model only short-term connections. The comparison of Transformer-XL with the other techniques is displayed in Table 4. Although Transformer-XL is principally intended to better learn longer-term connections, it dramatically improves the single-model state-of-the-art from 23.7 to 21.8.","Since the data is so similar, we simply take the best performing model and identical settings from enwik8 and use them on text8 without any additional tuning. The juxtaposition with earlier methodologies is summarized in Table 3. Transformer-XL once again achieves the new best result by a substantial difference. This dataset mostly tests the skill for modeling brief term associations because the sentences have been shuffled around. Thus, this dataset primarily assesses the skill to model only immediate dependencies. The contrast of Transformer-XL with the other approaches is exhibited in Table 4. While Transformer-XL is primarily created to better learn longer-term dependencies, it dramatically boosts the single-model state-of-the-art from 23.7 to 21.8.  ","Because of the similarities, we just take the optimal model and identical hyperparameters from enwik8 and apply them to text8 without any extra tuning. The comparison with previous techniques is outlined in Table 3. Transformer-XL once again accomplishes the new best result by a wide margin. This dataset mainly evaluates the ability to model short-term connections since the sentences have been reordered. Therefore, this dataset primarily tests the skill to model only immediate dependencies. The contrast of Transformer-XL with the other methods is presented in Table 4. Although Transformer-XL is primarily built to better capture longer-term dependencies, it substantially improves the single-model state-of-the-art from 23.7 to 21.8.",A,1
Transformer-XL,"Specifically, Transformer-XL significantly outperforms a contemporary method using vanilla Transformers (Baevski and Auli, 2018), suggesting the advantage of Transformer-XL is generalizable to modeling short sequences. We also report the results on word-level Penn Treebank in Table 5. Similar to AWD-LSTM (Merity et al., 2017), we apply variational dropout and weight average to Transformer-XL. With proper regularization, Transformer-XL achieves a new SoTA result among models without two-step finetuning. Penn Treebank has only 1M training tokens, which implies that Transformer-XL also generalizes well even on small datasets.","In particular, Transformer-XL substantially surpasses a current approach utilizing standard Transformers (Baevski and Auli, 2018), implying the benefits of Transformer-XL can be extended to modeling brief sequences. We also provide the outcomes on word-level Penn Treebank in Table 5. Analogous to AWD-LSTM (Merity et al., 2017), we implement variational dropout and weight averaging for Transformer-XL. With appropriate regularization, Transformer-XL accomplishes a new state-of-the-art result among models without two-phase fine-tuning. Penn Treebank has only 1M training tokens, which suggests that Transformer-XL also generalizes effectively even on small datasets.","Specifically, Transformer-XL markedly exceeds an existing technique using vanilla Transformers (Baevski and Auli, 2018), indicating the advantages of Transformer-XL are transferable to modeling short sequences. We additionally document the results on word-level Penn Treebank in Table 5. Similar to AWD-LSTM (Merity et al., 2017), we use variational dropout and weight averaging for Transformer-XL. With proper regularization, Transformer-XL achieves a new best result among models without two-step fine-tuning. Penn Treebank has only 1M training tokens, which implies Transformer-XL also generalizes well even on small datasets.  ","In particular, Transformer-XL significantly outperforms a current method employing standard Transformers (Baevski and Auli, 2018), suggesting the benefits of Transformer-XL can extend to modeling short sequences. We also present the outcomes on word-level Penn Treebank in Table 5. Like AWD-LSTM (Merity et al., 2017), we implement variational dropout and weight averaging with Transformer-XL. With suitable regularization, Transformer-XL accomplishes a new state-of-the-art result among models without two-phase fine-tuning. Penn Treebank has only 1M training tokens, which indicates Transformer-XL also generalizes effectively even on small datasets.",A,1
Transformer-XL,"We conduct two sets of ablation studies to examine the effects of two proposed techniques used in Transformer-XL: the recurrence mechanism and the new positional encoding scheme. The first study is performed on WikiText-103, which requires modeling long-term dependency. The results are reported in Table 6. Among the compared encoding schemes, Shaw et al. (2018) is relative, while Vaswani et al. (2017) and Al-Rfou et al. (2018) are absolute. “Full” and “half” losses refer to applying a cross entropy loss to all or the recent half positions in the segment. We found that absolute encodings only work well with half losses because half losses exclude positions with very short attention lengths during training for better generalization.","We perform two groups of analysis studies to investigate the impacts of the two suggested methods utilized in Transformer-XL: the recurrence system and the new positional encoding plan. The initial investigation is led on WikiText-103, which necessitates demonstrating long haul reliance. The discoveries are accounted for in Table 6. Among the thought about encoding plans, Shaw et al. (2018) is relative, while Vaswani et al. (2017) and Al-Rfou et al. (2018) are outright. ""Full"" and ""half"" misfortunes allude to applying a cross entropy misfortune to all or the new half positions in the fragment. We observed that outright encodings just work well with half misfortunes since half misfortunes bar positions with exceptionally short consideration lengths during preparing for better generalization.","We lead two arrangements of removal studies to analyze the consequences of the two proposed procedures utilized in Transformer-XL: the repeat instrument and the new positional encoding plan. The principal investigation is performed on WikiText-103, which requires displaying long haul reliance. The outcomes are accounted for in Table 6. Among the analyzed encoding plans, Shaw et al. (2018) is relative, while Vaswani et al. (2017) and Al-Rfou et al. (2018) are outright. ""Full"" and ""half"" misfortunes allude to applying a cross entropy misfortune to all or the new half positions in the portion. We observed that outright encodings just work well with half misfortunes since half misfortunes prohibit positions with exceptionally short consideration lengths during preparing for better generalization.  ","We lead two sets of end studies to inspect the impacts of the two proposed systems utilized in Transformer-XL: the repeat component and the new positional encoding plan. The initial investigation is acted in WikiText-103, which requires displaying long haul reliance. The discoveries are detailed in Table 6. Among the looked at encoding plans, Shaw et al. (2018) is relative, while Vaswani et al. (2017) and Al-Rfou et al. (2018) are outright. ""Full"" and ""half"" misfortunes allude to applying a cross entropy misfortune to all or the later half positions in the fragment. We observed that outright encodings just work well with half misfortunes since half misfortunes bar positions with exceptionally short consideration lengths during preparing for better generalization.",A,1
Transformer-XL,"Table 6 shows that both the recurrence mechanism and our encoding scheme are necessary to achieve the best performance, as well as generalizing to longer attention sequences during evaluation time. Although the backpropagation length during training is only 128, with the two techniques the attention length can be increased to 640 at test time. In the standard setting with 151M parameters, the perplexity decreases as the attention length increases. Since the recurrence mechanism costs additional memory, we also compare Transformer-XL with baselines under the same GPU memory constraints. As shown in Table 10 in Appendix A, despite using a shorter backpropagation length, Transformer-XL remains superior to the baselines.","Table 6 demonstrates that utilizing both the recurrence technique and our encoding method is imperative for attaining optimal performance, and for generalizing to longer attention sequences during evaluation. Although backpropagation length during training is only 128, the two techniques allow the attention length to rise to 640 at test time. In the standard configuration with 151M parameters, perplexity declines as attention length increases. Because the recurrence mechanism necessitates extra memory, we also contrast Transformer-XL with baselines under identical GPU memory limits. As exhibited in Table 10 in Appendix A, despite employing a shorter backpropagation length, Transformer-XL continues to surpass the baselines.","The data in Table 6 makes clear that leveraging both the recurrence mechanism and our encoding system is crucial for reaching the highest performance, and for extending to longer attention sequences when evaluating. While backpropagation length during training is just 128, the two methods let attention length grow to 640 at test time. With the standard setting of 151M parameters, perplexity lessens as attention length expands. Since the recurrence mechanism needs additional memory, we also compare Transformer-XL to baselines with the same GPU memory constraints. As shown in Table 10 in Appendix A, even using a shorter backpropagation length, Transformer-XL remains superior to the baselines.  ","The information in Table 6 demonstrates that utilizing both the recurrence technique and our encoding framework is vital for achieving optimal performance, and for generalizing to longer attention sequences during evaluation. Although the backpropagation length during training is only 128, the two techniques enable the attention length to increase to 640 at test time. With the standard configuration of 151M parameters, perplexity decreases as attention length increases. Because the recurrence mechanism requires extra memory, we also contrast Transformer-XL with baselines under the same GPU memory limits. As exhibited in Table 10 in Appendix A, despite employing a shorter backpropagation length, Transformer-XL continues to outperform the baselines.",A,1
Transformer-XL,"As shown in Table 7, using segment-level recurrence substantially improves performance even when long-term dependency is not needed, which is consistent with our previous discussion that the recurrence mechanism resolves the context fragmentation problem. Moreover, our relative positional encodings is also superior to Shaw et al. (2018) on short sequences. ECL is the longest length to which increasing the context span would lead to a gain more than a threshold. However, ECL ignores the fact that it is harder to get improvement when a model already achieves a lower perplexity using only a shorter context, and thus it is not suitable for fair comparison among multiple models.","The data in Table 7 demonstrates that utilizing segment-level recurrence significantly enhances performance even without requiring long-term dependency, aligning with our prior analysis showing recurrence mechanisms address context fragmentation issues. Furthermore, our relative positional encodings outperform Shaw et al. (2018) on short sequences. ECL is the maximum length where expanding context span leads to gains surpassing a threshold. However, ECL does not account for the greater difficulty in gaining improvements when a model already reaches lower perplexity using only a shorter context, so it cannot fairly compare multiple models.","As exhibited in Table 7, leveraging segment-level recurrence markedly boosts performance even lacking necessity for long-range dependency, consistent with our earlier discussion that recurrence systems resolve context fragmentation problems. Additionally, our relative positional encodings surpass Shaw et al. (2018) on short sequences. ECL represents the furthest length where increasing context span would produce a gain higher than a set threshold. But ECL overlooks that getting improvement is harder when a model already achieves lower perplexity using just a shorter context, so it cannot equitably compare multiple models.  ","The information in Table 7 shows utilizing segment-level recurrence greatly enhances performance even without needing long-distance dependency, aligning with our prior analysis demonstrating recurrence mechanisms address context fragmentation issues. Furthermore, our relative positional encodings are superior to Shaw et al. (2018) on short sequences. ECL is the maximum length at which expanding context span leads to gains above a set threshold. However, ECL does not consider that getting improvement is more difficult when a model already reaches lower perplexity using only a shorter context, therefore it cannot fairly compare multiple models.",A,1
Transformer-XL,"We instead propose a new metric called Relative Effective Context Length (RECL). RECL is defined on a model group instead of a single model, and the gain of a long context is measure by the relative improvement over the best short context model. As such, the model group shares the same baseline to enable fair comparison. RECL also has a parameter r, which means constraining the comparison on top-r hard examples. See Appendix C for more details about RECL. As shown in Table 8, Transformer-XL manages to model dependency of 900 words long on average with r = 0.1. The RECL of TransformerXL is 80% and 450% longer than recurrent networks and Transformer respectively.","Rather than the existing metric, we put forward a new one termed Comparative Efficiency of Context Length (CECL). CECL is characterized on a collection of models instead of a single one, and the benefit of an extensive context is gauged by the relative enhancement over the best short context model. Therefore, the model group utilizes the same baseline for fair analysis. CECL also possesses a variable r, which implies limiting the comparison to top-r challenging instances. Refer to Appendix C for further information regarding CECL. As depicted in Table 8, Transformer-XL is capable of representing dependency of 900 words on average with r = 0.1. The CECL of TransformerXL is 80% and 450% greater than recurrent networks and Transformer respectively.","We recommend a novel measure named Relative Contextual Efficacy Length (RCEL) as an alternative. RCEL focuses on a model cluster rather than one model, and the value of a long context is assessed via the comparative gain over the top short context model. Hence, the model group has the same reference point for an equitable review. RCEL also contains a factor r, meaning confining the comparison to top-r tough examples. See Appendix C for more specifics on RCEL. As shown in Table 8, Transformer-XL can model dependency of 900 words on average when r = 0.1. The RCEL of TransformerXL is 80% and 450% more than recurrent networks and Transformer respectively.  ","Instead of current metrics, we put forth a new one called Comparative Efficiency of Context Length (CECL). CECL concentrates on a model collection rather than a single model. The profit of an extensive context is gauged through relative enhancement over the best short context model. Thereby, the model group utilizes the same benchmark for fair assessment. CECL has a variable r too, restricting comparison to top r tough cases. Refer Appendix C for CECL details. Per Table 8, Transformer-XL can represent dependency of 900 words on average when r=0.1. The CECL of TransformerXL is 80% and 450% greater than recurrent networks and Transformer respectively.",A,1
Transformer-XL,"Trained only on WikiText-103 which is medium sized, Transformer-XL is already able to generate relatively coherent articles with thousands of tokens without manual cherry picking, despite minor flaws. Please refer to Appendix E for samples. Finally, we compare the evaluation speed of our model with the vanilla Transformer model (AlRfou et al., 2018). As shown in Table 9, due to the state reuse scheme, Transformer-XL achieves an up to 1,874 times speedup during evaluation.","Even though it has some small issues, Transformer-XL can already generate quite logical articles with thousands of words without needing to hand-pick them. This is impressive since it was only trained on WikiText-103 which is medium-sized. See Appendix E for examples. Also, Table 9 shows that Transformer-XL is up to 1,874 times faster than the original Transformer model (AlRfou et al., 2018) during evaluation because of reusing states.","Despite a few minor problems, Transformer-XL can produce relatively coherent, lengthy articles with thousands of tokens without manually selecting them. This is notable given it was only trained on the medium-sized WikiText-103 dataset. Examples are in Appendix E. Additionally, Table 9 demonstrates Transformer-XL's evaluation is up to 1,874x faster than the vanilla Transformer (AlRfou et al., 2018) thanks to state reuse. ","Even with some small flaws, Transformer-XL can generate quite logical, long articles with thousands of words without hand-picking them, which is remarkable considering it was only trained on the medium-sized WikiText-103. See Appendix E for samples. Also, Table 9 shows Transformer-XL evaluates up to 1,874 times faster than the original Transformer (AlRfou et al., 2018) because it reuses states.",A,1
Transformer-XL,"Transformer-XL obtains strong perplexity results, models longer-term dependency than RNNs and Transformer, achieves substantial speedup during evaluation, and is able to generate coherent text articles. We envision interesting applications of Transformer-XL in the fields of text generation, unsupervised feature learning, image and speech modeling. The first visualization aims at revealing the overall trend of where the model is attending. Specifically, for each attention head of each layer, we average the attention distributions of all tokens in the validation set.","Transformer-XL is very effective at language modeling, is able to learn dependencies across longer sequences compared to RNNs and the original Transformer model, evaluates text much faster, and can generate understandable text passages. We see promising uses for Transformer-XL in generating text, learning representations of data without labels, and modeling images and speech. The first visualization shows the general patterns of what the model pays attention to. In particular, for each attention mechanism in each layer, we take the mean of the attention distributions over all tokens in the validation set.","Transformer-XL achieves excellent results on perplexity, is able to model longer-range dependencies than RNNs and the original Transformer, has substantially faster evaluation speed, and can produce coherent text. We foresee interesting uses of Transformer-XL for generating text, unsupervised representation learning, and modeling images and speech. The first visualization is meant to show the high-level trends in what the model focuses its attention on. Specifically, for every attention head in every layer, we take the attention distributions for all tokens in the validation set and average them.","Transformer-XL attains very strong perplexity scores, can capture longer-term dependencies compared to RNNs and the standard Transformer, has much faster evaluation speed, and is capable of producing understandable text. We see promising applications of Transformer-XL in text generation, unsupervised feature learning, and image and speech modeling. The first visualization aims to show the overall patterns in what the model pays attention to. In particular, for every attention mechanism in every layer, we average the attention distributions over all tokens in the validation set.",A,1
Transformer-XL,"In this section, we present some generated text from our best model trained the Wikitext-103 dataset. We seed the our Transformer-XL with a context of at most 512 consecutive tokens randomly sampled from the test set of Wikitext-103. Then, we run Transformer-XL to generate a pre-defined number of tokens (500 or 1,000 in our case). For each generation step, we first find the top-40 probabilities of the next-step distribution and sample from top-40 tokens based on the re-normalized distribution. To help reading, we detokenize the context, the generated text and the reference text. Three generated examples are shown in Tables 11, 12, and 13.","This part displays several automatically produced passages from our most effective model that was trained on the Wikitext-103 data. We initialize the Transformer-XL with a setting of at most 512 successive tokens arbitrarily chosen from the Wikitext-103 test set. We then use Transformer-XL to create a predefined quantity of tokens (500 or 1,000 here). At each generation phase, we first identify the top 40 likelihood values from the next-step distribution and sample from the top 40 tokens based on the renormalized distribution. To assist with readability, we undo the tokenization of the context, the generated text, and the reference text. Three examples of generated text are presented in Tables 11, 12, and 13.","In this portion, we show some computer-generated writing from our top-performing system that was educated on the Wikitext-103 collection. We input the Transformer-XL with an environment of up to 512 back-to-back tokens randomly selected from the Wikitext-103 test collection. We then execute Transformer-XL to produce a pre-set number of tokens (500 or 1,000 here). At each generation move, we first pinpoint the top 40 probabilities in the next-step distribution and sample from the top 40 tokens based on the re-standardized distribution. To enhance legibility, we detokenize the context, the generated writing, and the reference writing. Three instances of generated content are exhibited in Tables 11, 12, and 13.  ","Here, we display some machine-made passages from our optimal architecture trained on the Wikitext-103 set. We initialize the Transformer-XL with a setting of maximum 512 sequential tokens randomly extracted from the Wikitext-103 test set. We then operate Transformer-XL to generate a predefined quantity of tokens (500 or 1,000 here). At each generation step, we first identify the top 40 likelihood values in the next-step distribution and sample from the top 40 tokens based on the renormalized distribution. To improve readability, we undo the tokenization of the context, the generated passages, and the reference passages. Three examples of the generated passages are presented in Tables 11, 12, and 13.",A,1
U-Net_Convolutional Networks for Biomedical Image Segmentation,"There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks.","There is widespread agreement that effectively training deep neural networks necessitates numerous annotated examples. Here, we introduce a network design and training methodology that strongly leverages data augmentation to more efficiently utilize the available labeled data. The model comprises a contracting path to capture context and a symmetric expanding path enabling accurate localization. We demonstrate that such a network can be trained end-to-end from very few images and surpasses the previous best approach (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopy stacks.",It is broadly accepted that thousands of annotated training instances are needed to successfully train deep learning models. We present a network architecture and training procedure that heavily relies on data augmentation to get more value out of the available labeled data. The design has a contracting portion to capture context and a matching expanding portion for precise localization. We show this network can be trained end-to-end from very little data and beats the former top method (a sliding-window convolutional network) on the ISBI challenge for segmenting neuronal structures in electron microscopy stacks.,There is widespread consensus that training deep neural networks well requires many thousands of labeled training examples. We introduce a network design and training strategy that makes strong use of data augmentation to utilize the available annotated data more efficiently. The architecture has a contracting path to capture context and a symmetric expanding path that allows precise localization. We demonstrate this network can be trained end-to-end from very few images and outperforms the previous best approach (a sliding-window convolutional network) on the ISBI challenge for segmenting neuronal structures in electron microscopy image stacks.,A,1
U-Net_Convolutional Networks for Biomedical Image Segmentation,"Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. In the last two years, deep convolutional networks have outperformed the state of the art in many visual recognition tasks, e.g. [ 7 , 3]. While convolutional networks have already existed for a long time [ 8], their success was limited due to the size of the available training sets and the size of the considered networks.","Utilizing an identical neural network educated on pictures from transmitted light microscopy (phase contrast and DIC), we prevailed in the ISBI cell tracking challenge 2015 in these groups decisively. Furthermore, the network operates rapidly. Segmentation of a 512x512 image requires less than one second on a current GPU. In the previous two years, deep convolutional networks have surpassed the state-of-the-art in numerous visual identification tasks, e.g. [7, 3]. Although convolutional networks have been present for a while [8], their triumph was constrained because of the scale of accessible training sets and the size of the studied networks.","Using the same neural network trained on images from light microscopy that passes through the sample (phase contrast and DIC), we won the ISBI cell tracking challenge 2015 in these categories by a wide margin. Also, the network is fast, segmenting a 512x512 image in under a second using a modern GPU. Over the past two years, deep convolutional networks have beaten the state-of-the-art in many visual recognition tasks, for example [7, 3]. While convolutional networks have existed for some time [8], their success was limited by the size of available training sets and networks.  ","Employing an identical deep neural network educated on pictures from transmitted light microscopy (phase contrast and DIC) modalities, we were victorious in the ISBI cell tracking challenge 2015 in these groups by a substantial difference. Furthermore, the network has high throughput, segmenting a 512x512 image in under one second utilizing a contemporary GPU. In the previous two years, deep convolutional networks have exceeded the state-of-the-art in numerous visual recognition tasks, e.g. [7, 3]. Although convolutional networks have been around for a while [8], their achievement was constrained by the scale of existing training sets and network sizes.",A,1
U-Net_Convolutional Networks for Biomedical Image Segmentation,"The breakthrough by Krizhevsky et al. [ 7] was due to supervised training of a large network with 8 layers and millions of parameters on the ImageNet dataset with 1 million training images. Since then, even larger and deeper networks have been trained [12]. The typical use of convolutional networks is on classification tasks, where the output to an image is a single class label. However, in many visual tasks, especially in biomedical image processing, the desired output should include localization, i.e., a class label is supposed to be assigned to each pixel. Moreover, thousands of training images are usually beyond reach in biomedical tasks.","The major advance by Krizhevsky and colleagues [7] resulted from supervised learning of a large 8-layer network with millions of parameters using the ImageNet dataset containing 1 million training images. After that, even bigger and more complex networks have been developed [12]. Convolutional networks are typically used for classification, where an image is assigned a single class label. However, for many visual tasks in biomedical image processing, the ideal output should localize classes, with each pixel assigned a label. Also, biomedical tasks rarely have access to thousands of training images.","The breakthrough of Krizhevsky et al. [7] stemmed from guided training of an 8-layer network with millions of weights on 1 million ImageNet images. Since then, larger and more sophisticated networks were built [12]. Convolutional networks commonly tackle classification, labeling an image with a single class. But for many visual jobs, especially in biomedical image analysis, the output should localize, labeling each pixel. Furthermore, biomedical jobs rarely have thousands of training images.  ","The innovation by Krizhevsky and co-authors [7] resulted from supervised learning of a big 8-layer network with millions of weights using 1 million ImageNet training photos. Afterward, even more massive and intricate networks were developed [12]. Convolutional networks are frequently applied to classification, categorizing an image into one class. However, for numerous visual tasks in biomedical image processing, the desired output is localization, assigning each pixel a label. Additionally, biomedical tasks hardly ever have access to thousands of training photos.",A,1
U-Net_Convolutional Networks for Biomedical Image Segmentation,"Hence, Ciresan et al. [ 1] trained a network in a sliding-window setup to predict the class label of each pixel by providing a local region (patch) around that pixel as input. First, this network can localize. Secondly, the training data in terms of patches is much larger than the number of training images. The resulting network won the EM segmentation challenge at ISBI 2012 by a large margin. Obviously, the strategy in Ciresan et al. [1] has two drawbacks. First, it is quite slow because the network must be run separately for each patch, and there is a lot of redundancy due to overlapping patches. Secondly, there is a trade-off between localization accuracy and the use of context.","Therefore, Ciresan and colleagues [1] taught a neural network in a sliding window configuration to predict the class label of every pixel by feeding a local area (patch) surrounding that pixel as input. Firstly, this network is capable of localization. Secondly, the training data as patches is much more abundant than the number of training images. The resulting network won the EM segmentation challenge at ISBI 2012 by a wide margin. Clearly, the strategy in Ciresan et al. [1] has two flaws. First, it is quite slow since the network must be run independently for each patch, and there is a lot of redundancy due to overlapping patches. Secondly, there is a trade-off between localization accuracy and the use of context.","As such, Ciresan and co-authors [1] trained a neural network using a sliding window method to forecast the class label of each pixel by providing a local region (patch) around that pixel as input. For one, this network is able to localize. Additionally, the training data as patches is far more plentiful than the number of training images. The ensuing network won the EM segmentation challenge at ISBI 2012 by a substantial margin. Obviously, the strategy in Ciresan et al. [1] has two downsides. Firstly, it is quite slow as the network must be run separately for each patch, and there is a lot of redundancy due to overlapping patches. Secondly, there is a trade-off between localization accuracy and the use of context.  ","Therefore, Ciresan and fellow researchers [1] conditioned a neural network using a sliding window technique to predict the class label of every pixel by feeding a local area (patch) surrounding that pixel as input. On one hand, this network has localization capabilities. On the other hand, the training data as patches is much more abundant than the number of training images. The resulting network won the EM segmentation challenge at ISBI 2012 by a wide margin. Clearly, the strategy in Ciresan et al. [1] has two disadvantages. For one, it is quite slow since the network must be run independently for each patch, and there is redundancy due to overlapping patches. For another, there is a trade-off between localization accuracy and context usage.",A,1
U-Net_Convolutional Networks for Biomedical Image Segmentation,"Larger patches require more max-pooling layers that reduce the localization accuracy, while small patches allow the network to see only little context. More recent approaches [11,4] proposed a classifier output that takes into account the features from multiple layers. Good localization and the use of context are possible at the same time. In this paper, we build upon a more elegant architecture, the so-called “fully convolutional network” [9]. We modify and extend this architecture such that it works with very few training images and yields more precise segmentations; see Figure 1.","More extensive image regions need additional max-pooling layers that decrease the precision of localization, whereas small image patches enable the network to observe only a limited context. More recent methods [11,4] suggested a classifier result that considers the features from multiple layers. Accurate localization and utilizing context are achievable concurrently. In this paper, we expand on a more refined architecture, the so-called ""fully convolutional network"" [9]. We adapt and augment this architecture so that it functions with very limited training images and generates more precise segmentations; refer to Figure 1.","Vast image patches necessitate extra max-pooling tiers that reduce localization accuracy, while small patches permit the network to view only minimal context. More current approaches [11,4] put forth a classifier output that incorporates attributes from numerous tiers. Precise localization and leveraging context are feasible simultaneously. Here, we build on a more elegant architecture, the so-termed ""fully convolutional network"" [9]. We tailor and extend this architecture so it operates with very few training images and produces more accurate segmentations; see Figure 1.  ","More sizeable image regions demand additional max-pooling strata that decrease localization precision, whereas small image patches enable the network to observe only restricted context. More contemporary methods [11,4] presented a classifier result that contemplates the attributes from various strata. Accurate localization and harnessing context are achievable at the same time. In this paper, we expand upon a more refined architecture, the so-called ""fully convolutional network"" [9]. We customize and augment this architecture so that it functions with very limited training images and generates more precise segmentations; refer to Figure 1.",A,1
U-Net_Convolutional Networks for Biomedical Image Segmentation,"The main idea in [9] is to supplement a usual contracting network by successive layers, where pooling operators are replaced by upsampling operators. Hence, these layers increase the resolution of the output. In order to localize, high resolution features from the contracting path are combined with the upsampled output. A successive convolution layer can then learn to assemble a more precise output based on this information. One important modification in our architecture is that in the upsampling part we have also a large number of feature channels, which allow the network to propagate context information to higher resolution layers.","The primary concept in [9] is to augment a standard contracting network through progressive tiers, where pooling operators are substituted with upsampling operators. Thus, these strata amplify the resolution of the output. To localize, superior resolution characteristics from the contracting course are integrated with the upsampled production. A following convolution layer can then ascertain to assemble a more precise output founded on this data. One vital modification in our framework is that in the upsampling element we have also a substantial amount of feature channels, which enable the network to broadcast context information to elevated resolution tiers.","The main notion in [9] is to supplement a conventional contracting network using sequential layers, where pooling operators are supplanted by upsampling operators. Accordingly, these layers expand the determination of the output. To pinpoint, high-resolution attributes from the contracting path are combined with the upsampled yield. A resulting convolution layer can then figure out how to assemble a more accurate output based on this knowledge. One crucial change in our design is that in the upsampling portion we also have a large quantity of feature channels, which permit the network to disseminate context information to higher resolution layers.  ","The primary idea in [9] is to enhance a standard contracting network through progressive strata, where pooling operators are replaced with upsampling operators. Therefore, these levels increase the resolution of the output. To localize, high-resolution features from the contracting course are integrated with the upsampled product. A subsequent convolution layer can then learn to construct a more precise output founded on this data. One important alteration in our model is that in the upsampling element we also have a substantial number of feature channels, which allow the network to broadcast context information to higher resolution strata.",A,1
U-Net_Convolutional Networks for Biomedical Image Segmentation,"As a consequence, the expansive path is more or less symmetric to the contracting path, and yields a u-shaped architecture. The network does not have any fully connected layers and only uses the valid part of each convolution, i.e., the segmentation map only contains the pixels, for which the full context is available in the input image. This strategy allows the seamless segmentation of arbitrarily large images by an overlap-tile strategy (see Figure 2). To predict the pixels in the border region of the image, the missing context is extrapolated by mirroring the input image.","Therefore, the expanding course of action mirrors the shrinking course of action, resulting in a u-shaped design. The neural network lacks fully connected strata and solely utilizes the applicable region of each convolution, meaning the segmentation blueprint exclusively encompasses pixels for which the complete setting is present in the input imagery. This plan permits the unbroken segmentation of arbitrarily massive images via an overlap-tile approach (see Figure 2). To predict the edge pixels, the absent context is approximated by reflecting the input image.","As a result, the broadening pathway is more or less a reflection of the narrowing pathway, forming a u-shaped architecture. The network has no fully connected layers and only leverages the valid portion of each convolution, so the segmentation map only includes pixels for which the full context is present in the input image. This approach enables the seamless segmentation of images of any size using an overlapping tile methodology (see Figure 2). To predict pixels near the border of the image, the missing context is extrapolated by mirroring the input image.","In summary, the expanding trajectory mirrors the contracting trajectory, resulting in a u-shaped design. The network lacks fully connected layers and exclusively employs the applicable area of each convolution, meaning the segmentation diagram solely incorporates pixels where the complete backdrop is included in the input visuals. This tactic permits uninterrupted segmentation of images of any dimensions via an overlap-tile plan (see Figure 2). To predict edge pixels, the absent setting is simulated by reflecting the input image.",A,1
U-Net_Convolutional Networks for Biomedical Image Segmentation,"This tiling strategy is important to apply the network to large images, since otherwise the resolution would be limited by the GPU memory. As for our tasks there is very little training data available, we use excessive data augmentation by applying elastic deformations to the available training images. This allows the network to learn invariance to such deformations, without the need to see these transformations in the annotated image corpus. This is particularly important in biomedical segmentation, since deformation used to be the most common variation in tissue and realistic deformations can be simulated efficiently.","This method of splitting up images into smaller tiles is crucial for running the neural network on big images, because otherwise the image resolution would be constrained by the graphics card's memory capacity. Since there are very few labeled training images for our applications, we augment the data extensively by warping the existing training images elastically. This enables the network to become invariant to those distortions, without needing examples of them in the annotated image set. This invariance is especially useful in biomedical segmentation, as deformation tends to be the most prevalent variation in tissue, and realistic deformations can be simulated easily.","This approach of dividing images into tiles is vital for applying the neural network to large images, as the resolution would otherwise be limited by the memory available on the GPU. Given the scarcity of labeled training data for our tasks, we perform aggressive data augmentation by elastically transforming the available training images. That allows the network to learn to be unaffected by those transformations, without requiring instances of them in the annotated training set. That robustness is particularly valuable in biomedical segmentation, since deformation was the most frequent variation in tissue, and realistic deformations can be simulated efficiently.","This strategy of splitting images into tiles is essential for running the neural network on big images, since the resolution would otherwise be constrained by the memory on the graphics processing unit. Because there is very little annotated training data for our applications, we use extensive data augmentation by elastically warping the existing training images. That enables the network to become invariant to those warps, without examples of them needing to be in the labeled image collection. That invariance is especially important in biomedical segmentation, as deformation tended to be the most common variation in tissue, and realistic deformations can be simulated easily.",A,1
U-Net_Convolutional Networks for Biomedical Image Segmentation,"The value of data augmentation for learning invariance has been shown in Dosovitskiy et al. [2] in the scope of unsupervised feature learning. Another challenge in many cell segmentation tasks is the separation of touching objects of the same class; see Figure 3. To this end, we propose the use of a weighted loss, where the separating background labels between touching cells obtain a large weight in the loss function. The resulting network is applicable to various biomedical segmentation problems. In this paper, we show results on the segmentation of neuronal structures in EM stacks (an ongoing competition started at ISBI 2012), where we out-performed the network of Ciresan et al. [1].","The usefulness of augmenting data to learn invariance was demonstrated by Dosovitskiy et al. [2] for unsupervised feature learning. Separating adjoining objects of the same type is another difficulty in many cell segmentation tasks; refer to Figure 3. We suggest utilizing a weighted loss function, where the background labels that separate touching cells are heavily weighted. The resulting network can be used for various biomedical segmentation challenges. Here, we present results segmenting neuronal structures in EM image stacks (a continuing competition begun at ISBI 2012), surpassing the performance of Ciresan et al.'s [1] network.","Dosovitskiy et al. [2] showed that increasing data helps learn invariance, for unsupervised feature learning. Distinguishing merged objects of one class is hard in some cell segmentation; see Figure 3. We recommend a weighted loss function, strongly weighting background tags between adjacent cells. This network applies to multiple biomedical segmentation tasks. We show results segmenting neuron structure in EM stacks (a competition since ISBI 2012), beating Ciresan et al.'s [1] network. ","The usefulness of expanding data to learn invariance was exhibited by Dosovitskiy et al. [2] for unsupervised characteristic learning. Differentiating joined entities of one type is tricky in certain cell division; consult Figure 3. We put forward a weighted loss function, intensely weighting background markers separating adjoining cells. This system is applicable to various biomedical partitioning challenges. We demonstrate results partitioning neurological anatomy in EM piles (a contest since ISBI 2012), surpassing Ciresan et al.'s [1] system.",A,1
U-Net_Convolutional Networks for Biomedical Image Segmentation,"Furthermore, we show results for cell segmentation in light microscopy images from the ISBI cell tracking challenge 2015. Here we won with a large margin on the two most challenging 2D transmitted light datasets. The network architecture is illustrated in Figure 1. It consists of a contracting path (left side) and an expansive path (right side). The contracting path follows the typical architecture of a convolutional network. It consists of the repeated application of two 3x3 convolutions (unpadded convolutions), each followed by a rectified linear unit (ReLU) and a 2x2 max pooling operation with stride 2 for downsampling.","Moreover, we demonstrate results for separating cells in light microscopy pictures from the ISBI cell tracking challenge in 2015. Here our method beat the competition by a wide margin on the two most difficult 2D transmitted light data sets. The structure of the network is shown in Figure 1. It is made up of a contracting path (left side) and an expansive path (right side). The contracting path has the standard design of a convolutional network. It is constructed by repeatedly applying two 3x3 convolutions (unpadded convolutions), each followed by a rectified linear unit (ReLU) and a 2x2 max pooling layer with stride 2 for downsampling.","In addition, we present outcomes for identifying individual cells in light microscope images from the 2015 ISBI cell tracking competition. Our approach won decisively on the two most problematic 2D transmitted light data sets. The architecture of the network is depicted in Figure 1. It is composed of a contracting branch (left) and an expansive branch (right). The contracting branch uses the common structure of a convolutional network. It is built by repeating two 3x3 convolutions (without padding), each followed by a rectified linear unit (ReLU) and a 2x2 max pooling layer with stride 2 to reduce the dimensions.","Furthermore, we demonstrate results for segmenting cells in light microscopy scans from the 2015 ISBI cell tracking challenge. Our method achieved a clear victory on the two most difficult 2D transmitted light datasets. The network design is shown in Figure 1. It has a contracting pathway (left) and an expansive pathway (right). The contracting pathway employs the standard convolutional network structure. It involves repeating two 3x3 convolutions (without padding), each followed by a rectified linear unit (ReLU) and a 2x2 max pooling layer with stride 2 for downsampling.",A,1
U-Net_Convolutional Networks for Biomedical Image Segmentation,"At each downsampling step we double the number of feature channels. Every step in the expansive path consists of an upsampling of the feature map followed by a 2x2 convolution (“up-convolution”) that halves the number of feature channels, a concatenation with the correspondingly cropped feature map from the contracting path, and two 3x3 convolutions, each followed by a ReLU. The cropping is necessary due to the loss of border pixels in every convolution. At the final layer a 1x1 convolution is used to map each 64- component feature vector to the desired number of classes.","In every downsampling step, we multiply the quantity of feature channels by two. Each step in the expansive path is made up of an upsampling of the feature map followed by a 2x2 convolution (""up-convolution"") which halves the number of feature channels, a joining together with the correspondingly cropped feature map from the contracting path, and two 3x3 convolutions, each one succeeded by a ReLU. The cropping is required because of the loss of border pixels in every convolution. At the final layer, a 1x1 convolution is utilized to map each 64-component feature vector to the wanted number of classes.","At every stage of downsampling, we double the amount of feature channels. Every part of the expansive path consists of an upsampling of the feature map, followed by a 2x2 convolution (""up-convolution"") which reduces the feature channels by half, a fusion with the suitably trimmed feature map from the contracting path, and two 3x3 convolutions, each accompanied by a ReLU. The trimming is necessary owing to the loss of edge pixels in each convolution. In the final layer, a 1x1 convolution is used to transform each 64-element feature vector into the desired quantity of classes.  ","In each downsampling step, we increase the feature channels twofold. Each part of the expansive path involves upsampling the feature map, then applying a 2x2 convolution (""up-convolution"") to halve the feature channels, concatenating with the appropriately cropped feature map from the contracting path, followed by two 3x3 convolutions with a ReLU after each one. The cropping is needed because border pixels are lost in every convolution. In the final layer, a 1x1 convolution maps each 64-component feature vector to the target number of classes.",A,1
U-Net_Convolutional Networks for Biomedical Image Segmentation,"In total the network has 23 convolutional layers. To allow a seamless tiling of the output segmentation map (see Figure 2), it is important to select the input tile size such that all 2x2 max-pooling operations are applied to a layer with an even x- and y-size. The input images and their corresponding segmentation maps are used to train the network with the stochastic gradient descent implementation of Caffe [6]. Due to the unpadded convolutions, the output image is smaller than the input by a constant border width. To minimize the overhead and make maximum use of the GPU memory, we favor large input tiles over a large batch size and hence reduce the batch to a single image.","The network contains 23 layers in total that use convolutional operations. To enable a smooth tiling of the output segmentation image (refer to Figure 2), it is vital to choose an input tile dimension so that all 2x2 maximum pooling layers are applied to a layer with even x and y dimensions. The input photos and corresponding segmentation images are utilized to train the network using the stochastic gradient descent implementation from Caffe [6]. Because there is no padding on the convolutions, the output image is smaller than the input by a constant border width. To minimize overhead and maximize GPU memory usage, we prefer large input tiles over a large batch size, so the batch is reduced to a single image.","In full, the network possesses 23 layers that leverage convolutional computations. For seamless tessellation of the output segmentation visual (consult Figure 2), it is essential to elect an input tile measurement so all 2x2 max pooling operations are leveraged on a layer with even x and y magnitudes. The input graphics and associated segmentation graphics are harnessed to drill the network employing stochastic gradient descent from Caffe [6]. Since there is no padding on the convolutions, the output graphic is more compact than the input by a fixed border breadth. To minimize burden and optimize GPU memory utilization, we favor substantial input tiles over a substantial batch amount, hence the batch is narrowed to a single graphic.","The network has a total of 23 layers that use convolutional functions. To enable a smooth covering of the output segmentation image (see Figure 2), it is important to choose an input tile size so that all 2x2 maximum pooling operations are applied to a layer with even x and y dimensions. The input images and matching segmentation images are utilized to train the network using the stochastic gradient descent tool from Caffe [6]. Because the convolutions have no padding, the output image is smaller than the input by a fixed border width. To minimize overhead and maximize GPU memory use, we prefer large input tiles over a large batch size, so the batch is reduced to a single image.",A,1
U-Net_Convolutional Networks for Biomedical Image Segmentation,"Accordingly we use a high momentum (0.99) such that a large number of the previously seen training samples determine the update in the current optimization step. In deep networks with many convolutional layers and different paths through the network, a good initialization of the weights is extremely important. Otherwise, parts of the network might give excessive activations, while other parts never contribute. Ideally the initial weights should be adapted such that each feature map in the network has approximately unit variance.","Therefore, we utilize a high momentum (0.99) so that a large quantity of the training samples previously observed influence the update in the current optimization step. In deep networks containing many convolutional layers and various paths through the network, a proper initialization of the weights is tremendously important. If not, sections of the network may produce excessive activations, while other sections never add anything. Optimally, the initial weights should be adapted so that each feature map in the network has roughly unit variance.","As a result, we use high momentum (0.99) so that a great number of the training samples seen earlier affect the improvement in the current optimization step. In deep networks having numerous convolutional layers and different routes through the network, a suitable initialization of the weights is extremely vital. If not, areas of the network could generate too much activation, whereas other areas never contribute anything. Preferably, the initial weights should be tuned so that each feature map in the network has about unit variance.  ","Thus, we employ high momentum (0.99) so a large quantity of the training samples viewed previously influence the enhancement in the current optimization step. In deep networks possessing many convolutional layers and various paths through the network, a proper initialization of the weights is tremendously crucial. If not, portions of the network may produce excessive activation, while other portions never add anything. Ideally, the initial weights should be adjusted so that each feature map in the network has roughly unit variance.",A,1
U-Net_Convolutional Networks for Biomedical Image Segmentation,"Data augmentation is essential to teach the network the desired invariance and robustness properties, when only few training samples are available. In case of microscopical images we primarily need shift and rotation invariance as well as robustness to deformations and gray value variations. Especially random elastic deformations of the training samples seem to be the key concept to train a segmentation network with very few annotated images. We generate smooth deformations using random displacement vectors on a coarse 3 by 3 grid.","Since labeled data is scarce, augmenting the data is crucial for teaching the model the needed insensitivity and sturdiness when there are only a small number of training examples. For microscopic images we especially require resilience to shifts, rotations, distortions, and changes in grayscale values. It appears that randomly warping the training images is the most effective technique for training a segmentation model using very few annotated photos. We produce smooth distortions by generating arbitrary displacement vectors on a sparse 3x3 grid.","When you only have a handful of labeled instances, artificially expanding the data is imperative for instilling the desired equanimity and hardiness in the network. With microscopic shots, we primarily need it to be indifferent to displacements, spins, deformities, and gray level fluctuations. Distorting the training specimens randomly seems to be the secret sauce for educating a segmentation algorithm using a scarce amount of annotated graphics. We construct fluid contortions by haphazardly moving points on a coarse 3x3 framework.  ","Since annotated data is limited, synthesizing more data is crucial for teaching the neural network the necessary indifference and toughness when there are only a few training examples. For micrographs we especially need robustness to shifts, rotations, warping, and grayscale variations. It appears that randomly deforming the training images is the key technique for training a segmentation model using very few labeled images. We introduce smooth distortions by generating random displacement vectors on a sparse 3 by 3 grid.",A,1
U-Net_Convolutional Networks for Biomedical Image Segmentation,The displacements are sampled from a Gaussian distribution with 10 pixels standard deviation. Per-pixel displacements are then computed using bicubic interpolation. Drop-out layers at the end of the contracting path perform further implicit data augmentation. We demonstrate the application of the u-net to three different segmentation tasks. The first task is the segmentation of neuronal structures in electron microscopic recordings. An example of the data set and our obtained segmentation is displayed in Figure 2. We provide the full result as Supplementary Material. The data set is provided by the EM segmentation challenge [14] that was started at ISBI 2012 and is still open for new contributions.,The movements are taken from a normal distribution with a standard deviation of 10 pixels. The displacements for each pixel are then found using bicubic interpolation. Randomly removing layers at the end of the contracting path adds more implicit data augmentation. We show how the u-net can be used for 3 different segmentation jobs. The first job is identifying neuronal structures in electron microscope images. An example of the data and our segmentation result is shown in Figure 2. The full result is in the Supplementary Material. The data is from the EM segmentation challenge [14] which began at ISBI 2012 and is ongoing for new contributions.,"The shifts are sampled from a Gaussian with 10 pixel standard deviation. The displacements of individual pixels are then computed using bicubic interpolation. Omitting layers randomly at the end of the contracting path provides additional implicit data augmentation. We demonstrate using the u-net for three distinct segmentation tasks. The first task segments neuronal structures in electron microscopy images. An example of the dataset and our segmentation is in Figure 2. The complete result is provided as Supplementary Material. The dataset is from the EM segmentation challenge [14] started at ISBI 2012, which remains open for new contributions.  ","The movements are drawn from a normal distribution with a standard deviation of 10 pixels. The displacements of each pixel are then determined using bicubic interpolation. Randomly dropping out layers at the end of the contracting path provides further implicit data augmentation. We exhibit the application of the u-net to three different segmentation jobs. The first job segments neuronal structures in electron microscopic scans. An instance of the data and our segmentation is shown in Figure 2. The full result is given as Supplementary Material. The data is from the EM segmentation challenge [14] initiated at ISBI 2012, which continues to accept new contributions.",A,1
U-Net_Convolutional Networks for Biomedical Image Segmentation,"The training data is a set of 30 images (512x512 pixels) from serial section transmission electron microscopy of the Drosophila first instar larva ventral nerve cord (VNC). Each image comes with a corresponding fully annotated ground truth segmentation map for cells (white) and membranes (black). The test set is publicly available, but its segmentation maps are kept secret. An evaluation can be obtained by sending the predicted membrane probability map to the organizers. The evaluation is done by thresholding the map at 10 different levels and computation of the “warping error”, the “Rand error” and the “pixel error” [14].","The source material consists of 30 high-resolution images of cell tissue from a fruit fly larva's ventral nerve cord. Each image has a matching fully labeled map outlining the cells and membranes. There is a publicly available test set of images, but the annotation maps are confidential. You can get scored by submitting your predicted membrane layouts to the organizers. They will threshold your maps at 10 levels and calculate the warping, Rand, and pixel errors.","The training images are 30 electron microscopy photos (512x512 pixels) of the ventral nerve cord from a young fruit fly. The images have fully marked ground truth segmentations showing cells (white) and membranes (black). There is a public test set but the segmentations are private. You can get evaluated by sending your predicted membrane probability maps to the organizers. They will threshold the maps at 10 different points and compute the warping, Rand, and pixel errors. ","The data consists of 30 high-resolution electron microscope images of the ventral nerve cord from a Drosophila larva. Each image has a corresponding complete annotation map marking cells in white and membranes in black. There is a test set available publicly but its annotation maps are confidential. You can get scored by submitting your predicted membrane likelihoods to the organizers. They will binarize the maps at 10 thresholds and calculate the warping error, Rand error, and pixel error.",A,1
U-Net_Convolutional Networks for Biomedical Image Segmentation,"The u-net (averaged over 7 rotated versions of the input data) achieves without any further pre- or postprocessing a warping error of 0.0003529 (the new best score, see Table 1) and a rand-error of 0.0382. This is significantly better than the sliding-window convolutional network result by Ciresan et al. [1], whose best submission had a warping error of 0.000420 and a rand error of 0.0504. In terms of rand error the only better performing algorithms on this data set use highly data set specific post-processing methods1 applied to the probability map of Ciresan et al. [1]. We also applied the u-net to a cell segmentation task in light microscopic images. This segmentation task is part of the ISBI cell tracking challenge 2014 and 2015 [10,13].","The u-net (averaged over 7 rotated versions of the input information) accomplishes without any additional pre- or postprocessing a warping error of 0.0003529 (the new top score, refer to Table 1) and a rand-error of 0.0382. This is noticeably superior to the sliding-window convolutional network outcome by Ciresan et al. [1], whose best submission had a warping error of 0.000420 and a rand error of 0.0504. In terms of rand error the only better performing algorithms on this data set utilize highly data set particular post-processing techniques1 applied to the probability map of Ciresan et al. [1]. We also used the u-net on a cell segmentation task in light microscopic images. This segmentation task is part of the ISBI cell tracking challenge 2014 and 2015 [10,13].","The u-net (averaged over 7 rotated versions of the input data) achieves without any extra pre- or postprocessing a warping error of 0.0003529 (the new top score, see Table 1) and a rand-error of 0.0382. This is significantly superior to the sliding-window convolutional network result by Ciresan et al. [1], whose best submission had a warping error of 0.000420 and a rand error of 0.0504. In terms of rand error the only better performing algorithms on this data set use highly data set specific post-processing techniques1 applied to the probability map of Ciresan et al. [1]. We also applied the u-net to a cell segmentation task in light microscopic images. This segmentation task is part of the ISBI cell tracking challenge 2014 and 2015 [10,13]. ","The u-net (averaged over 7 rotated versions of the input information) accomplishes without any further pre- or postprocessing a warping error of 0.0003529 (the new highest score, refer to Table 1) and a rand-error of 0.0382. This is noticeably better than the sliding-window convolutional network result by Ciresan et al. [1], whose best submission had a warping error of 0.000420 and a rand error of 0.0504. In terms of rand error the only superior performing algorithms on this data set use highly data set particular post-processing methods1 applied to the probability map of Ciresan et al. [1]. We also utilized the u-net on a cell segmentation task in light microscopic images. This segmentation task is part of the ISBI cell tracking challenge 2014 and 2015 [10,13].",A,1
U-Net_Convolutional Networks for Biomedical Image Segmentation,"The first data set “PhC-U373”2 contains Glioblastoma-astrocytoma U373 cells on a polyacrylimide substrate recorded by phase contrast microscopy (see Figure 4a,b and Supp. Material). It contains 35 partially annotated training images. Here we achieve an average IOU (“intersection over union”) of 92%, which is significantly better than the second best algorithm with 83% (see Table 2). The second data set “DIC-HeLa”3 are HeLa cells on a flat glass recorded by differential interference contrast (DIC) microscopy (see Figure 3, Figure 4c,d and Supp. Material). It contains 20 partially annotated training images. Here we achieve an average IOU of 77.5% which is significantly better than the second best algorithm with 46%.","The initial information ""PhC-U373"" has Glioblastoma-astrocytoma U373 cells on a polyacrylimide base captured by phase contrast microscopy (refer to Figure 4a,b and Supp. Material). It comprises 35 partly annotated preparation images. In this case we accomplish an average IOU (""intersection over union"") of 92%, which is way better than the runner up algorithm with 83% (refer to Table 2). The next information ""DIC-HeLa"" has HeLa cells on a flat glass captured by differential interference contrast (DIC) microscopy (refer to Figure 3, Figure 4c,d and Supp. Material). It contains 20 partly annotated preparation images. Here we achieve an average IOU of 77.5% which is way better than the runner up algorithm with 46%.","The first dataset ""PhC-U373"" has Glioblastoma-astrocytoma U373 cells on a polyacrylimide substrate imaged using phase contrast microscopy (see Figure 4a,b and Supp. Material). It has 35 partially labeled training images. We achieve an average IOU (intersection over union) of 92% here, significantly exceeding the next best algorithm's 83% (see Table 2). The second dataset ""DIC-HeLa"" has HeLa cells on flat glass imaged by differential interference contrast (DIC) microscopy (see Figure 3, Figure 4c,d and Supp. Material). It has 20 partially labeled training images. We achieve an average IOU of 77.5% here, far surpassing the next best algorithm's 46%.","The initial dataset ""PhC-U373"" contains Glioblastoma-astrocytoma U373 cells on a polyacrylimide substrate captured through phase contrast microscopy (refer to Figure 4a,b and Supp. Material). It has 35 partially marked training photos. Here we attain an average IOU (intersection over union) of 92%, substantially higher than the next top algorithm's 83% (see Table 2). The second dataset ""DIC-HeLa"" has HeLa cells on flat glass imaged by differential interference contrast (DIC) microscopy (refer to Figure 3, Figure 4c,d and Supp. Material). It contains 20 partially marked training photos. Here we attain an average IOU of 77.5%, far exceeding the next top algorithm's 46%.",A,1
U-Net_Convolutional Networks for Biomedical Image Segmentation,"The u-net architecture achieves very good performance on very different biomedical segmentation applications. Thanks to data augmentation with elastic deformations, it only needs very few annotated images and has a very reasonable training time of only 10 hours on a NVidia Titan GPU (6 GB). We provide the full Caffe[6]-based implementation and the trained networks4 . We are sure that the u-net architecture can be applied easily to many more tasks.","The u-net design is very effective for a wide variety of biomedical image segmentation tasks. With data augmentation using elastic distortions, it only requires a small number of labeled images and has a fast training time of just 10 hours on an NVidia Titan GPU (6 GB). We supply the complete Caffe[6]-based implementation and trained networks4. We are confident the u-net architecture can be easily applied to many additional applications.","The u-net model performs excellently on diverse biomedical image segmentation jobs. Thanks to data expansion using elastic tweaks, it needs just a few annotated pictures and trains swiftly in only 10 hours on an NVidia Titan GPU (6 GB). We provide the full Caffe[6]-based code and trained models4. We believe the u-net design can be simply used for numerous extra tasks. ","The u-net framework is highly successful across various biomedical segmentation use cases. With data enhancement via elastic transformations, it requires merely a small labeled dataset and trains rapidly in just 10 hours using an NVidia Titan GPU (6 GB). We make available the complete Caffe[6]-based implementation and pretrained networks4. We are certain the u-net architecture could be easily leveraged for many additional applications.",A,1
Universal Language Model Fine-tuning for Text Classification,"Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18- 24% on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100× more data. We opensource our pretrained models and code.","Transfer learning with induction has made a big difference in computer vision, but current methods in natural language processing still need customization for each task and full training from the beginning. We suggest Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning technique that can be applied to any natural language processing task, and present methods that are key for adjusting a language model. Our approach substantially surpasses the state-of-the-art on six text classification tasks, lowering the error by 18-24% on most datasets. Also, with only 100 labeled examples, it equals the performance of training from nothing on 100× more data. We make our pretrained models and code publicly available.","Inductive transfer learning has greatly helped computer vision, however existing natural language processing approaches still require task-specific changes and complete training from scratch. We put forward Universal Language Model Fine-tuning (ULMFiT), a successful transfer learning approach usable for any natural language task, and introduce techniques crucial for fine-tuning a language model. Our method significantly exceeds the state-of-the-art on six text classification tasks, reducing the error by 18-24% on most datasets. Furthermore, with just 100 labeled examples, it matches the performance of training from nothing on 100× more data. We open source our pretrained models and code.","Transfer learning using induction has had a big positive impact on computer vision, but current natural language processing methods still need customization for each task and full training from the beginning. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning technique applicable to any natural language task, and present techniques key for adjusting a language model. Our approach substantially outperforms the state-of-the-art on six text classification tasks, lowering the error by 18-24% on the majority of datasets. Also, with only 100 labeled examples, it equals the performance of training from scratch on 100× more data. We make our pretrained models and code publicly available.",A,1
Universal Language Model Fine-tuning for Text Classification,"Inductive transfer learning has had a large impact on computer vision (CV). Applied CV models (including object detection, classification, and segmentation) are rarely trained from scratch, but instead are fine-tuned from models that have been pretrained on ImageNet, MS-COCO, and other datasets (Sharif Razavian et al., 2014; Long et al., 2015a; He et al., 2016; Huang et al., 2017). Text classification is a category of Natural Language Processing (NLP) tasks with real-world applications such as spam, fraud, and bot detection (Jindal and Liu, 2007; Ngai et al., 2011; Chu et al., 2012), emergency response (Caragea et al., 2011), and commercial document classification, such as for legal discovery (Roitblat et al., 2010).","Transfer learning from large datasets has greatly benefited computer vision tasks like object recognition, categorization, and segmentation. Rather than training models from nothing, common practice is to adapt models pretrained on ImageNet, MS-COCO, etc. (Sharif Razavian et al., 2014; Long et al., 2015a; He et al., 2016; Huang et al., 2017). Text classification tackles real NLP problems like spam, fraud, bot detection (Jindal and Liu, 2007; Ngai et al., 2011; Chu et al., 2012), crisis response (Caragea et al., 2011), and document organization for legal purposes (Roitblat et al., 2010).","Inductive transfer learning has strongly impacted computer vision fields like object detection, classification, and segmentation. It is rare to train models from scratch. Instead, models pretrained on ImageNet, MS-COCO, etc. are fine-tuned (Sharif Razavian et al., 2014; Long et al., 2015a; He et al., 2016; Huang et al., 2017). Text classification handles real-world NLP tasks such as spam, fraud, bot detection (Jindal and Liu, 2007; Ngai et al., 2011; Chu et al., 2012), emergency response (Caragea et al., 2011), and commercial document classification like legal discovery (Roitblat et al., 2010).  ","Transfer learning from large datasets has been very influential on computer vision tasks including object recognition, categorization, and segmentation. The common practice is to adapt models pretrained on ImageNet, MS-COCO, and other datasets rather than training from scratch (Sharif Razavian et al., 2014; Long et al., 2015a; He et al., 2016; Huang et al., 2017). Text classification tackles real-world NLP applications such as spam, fraud, bot detection (Jindal and Liu, 2007; Ngai et al., 2011; Chu et al., 2012), emergency response (Caragea et al., 2011), and document organization for legal purposes (Roitblat et al., 2010).",A,1
Universal Language Model Fine-tuning for Text Classification,"While Deep Learning models have achieved state-of-the-art on many NLP tasks, these models are trained from scratch, requiring large datasets, and days to converge. Research in NLP focused mostly on transductive transfer (Blitzer et al., 2007). For inductive transfer, fine-tuning pretrained word embeddings (Mikolov et al., 2013), a simple transfer technique that only targets a model’s first layer, has had a large impact in practice and is used in most state-of-the-art models. Recent approaches that concatenate embeddings derived from other tasks with the input at different layers (Peters et al., 2017; McCann et al., 2017; Peters et al., 2018) still train the main task model from scratch and treat pretrained embeddings as fixed parameters, limiting their usefulness.","While Deep Learning models have achieved best in class performance on many NLP tasks, these models are initialized randomly, necessitating large datasets and multiple days to converge. NLP research has focused primarily on transductive transfer learning (Blitzer et al., 2007). For inductive transfer, fine-tuning pre-trained word vectors (Mikolov et al., 2013), a simple transfer technique targeting only a model's first layer, has proven very impactful in practice and is utilized in most state-of-the-art models. More recent approaches concatenating embeddings derived from other tasks with the input at various layers (Peters et al., 2017; McCann et al., 2017; Peters et al., 2018) still initialize the main task model randomly and treat pretrained embeddings as immutable, limiting their utility.","Although Deep Learning models have reached superior performance on numerous NLP tasks, these models start from scratch, needing substantial datasets and days to converge. NLP research concentrated largely on transductive transfer learning (Blitzer et al., 2007). For inductive transfer, fine-tuning pre-trained word representations (Mikolov et al., 2013), a straightforward transfer technique affecting only a model's first layer, has been very impactful in practice and is employed in most cutting-edge models. More recent methods concatenating embeddings from other tasks with the input at multiple layers (Peters et al., 2017; McCann et al., 2017; Peters et al., 2018) still initialize the main task model randomly and consider pretrained embeddings as fixed, restricting their usefulness.  ","Despite Deep Learning models achieving best-in-class results on many NLP tasks, these models start tabula rasa, requiring ample datasets and multiple days to converge. NLP research has focused primarily on transductive transfer learning (Blitzer et al., 2007). For inductive transfer, fine-tuning pretrained word vectors (Mikolov et al., 2013), a simple transfer technique altering only a model's first layer, has proven very beneficial in practice and is used in most advanced models. More recent techniques concatenating embeddings from other tasks with the input across layers (Peters et al., 2017; McCann et al., 2017; Peters et al., 2018) still start the main task model from scratch and treat pretrained embeddings as immutable, limiting their value.",A,1
Universal Language Model Fine-tuning for Text Classification,"In light of the benefits of pretraining (Erhan et al., 2010), we should be able to do better than randomly initializing the remaining parameters of our models. However, inductive transfer via finetuning has been unsuccessful for NLP (Mou et al., 2016). Dai and Le (2015) first proposed finetuning a language model (LM) but require millions of in-domain documents to achieve good performance, which severely limits its applicability. We show that not the idea of LM fine-tuning but our lack of knowledge of how to train them effectively has been hindering wider adoption. LMs overfit to small datasets and suffered catastrophic forgetting when fine-tuned with a classifier.","Considering the advantages of pretraining (Erhan et al., 2010), we ought to be capable of surpassing random initialization for the rest of our model parameters. However, inductive transfer by finetuning has not worked well for NLP (Mou et al., 2016). Dai and Le (2015) first suggested finetuning a language model (LM) but need millions of in-domain documents to perform well, which greatly restricts its usefulness. We demonstrate that not the concept of LM fine-tuning itself but rather our lack of understanding of how to train them effectively has been impeding broader adoption. LMs overfit to small datasets and underwent catastrophic forgetting when fine-tuned jointly with a classifier.","In view of the positive impacts of pretraining (Erhan et al., 2010), we should be able to improve on randomly initializing the other parameters of our models. Still, inductive transfer through finetuning has been unsuccessful for NLP (Mou et al., 2016). Dai and Le (2015) originally proposed finetuning a language model (LM) but require millions of in-domain documents to perform well, which severely constrains its applicability. We show that not the notion of LM fine-tuning itself but rather our lack of knowledge about how to train them effectively has been hindering more widespread adoption. LMs overfit to small datasets and experienced catastrophic forgetting when fine-tuned together with a classifier.","Considering the benefits of pretraining (Erhan et al., 2010), we ought to be able to surpass random initialization for the remaining parameters of our models. However, inductive transfer by means of finetuning has not been effective for NLP (Mou et al., 2016). Dai and Le (2015) first put forward the idea of finetuning a language model (LM) but need millions of in-domain documents to achieve good performance, which greatly limits its usefulness. We demonstrate that not the concept of LM fine-tuning itself but rather our lack of understanding about how to train them effectively has been preventing more widespread adoption. LMs overfit to small datasets and underwent catastrophic forgetting when fine-tuned jointly with a classifier.",A,1
Universal Language Model Fine-tuning for Text Classification,"Compared to CV, NLP models are typically more shallow and thus require different fine-tuning methods. We propose a new method, Universal Language Model Fine-tuning (ULMFiT) that addresses these issues and enables robust inductive transfer learning for any NLP task, akin to fine-tuning ImageNet models: The same 3-layer LSTM architecture— with the same hyperparameters and no additions other than tuned dropout hyperparameters— outperforms highly engineered models and transfer learning approaches on six widely studied text classification tasks. On IMDb, with 100 labeled examples, ULMFiT matches the performance of training from scratch with 10× and—given 50k unlabeled examples—with 100× more data.","In contrast to computer vision, natural language processing models tend to be more superficial and therefore need different refinement approaches. We put forward a new technique, Universal Language Model Fine-tuning (ULMFiT), that tackles these problems and provides robust inductive transfer learning for any NLP task, similar to tuning ImageNet models: The same 3-layer LSTM structure—using the same hyperparameters and no additions besides adjusted dropout hyperparameters—surpasses very engineered models and transfer learning methods on six extensively researched text classification tasks. On IMDb, with 100 labeled examples, ULMFiT equals the performance of training from the beginning with 10× and—given 50k unlabeled examples—with 100× more data.","Compared with CV, NLP models are generally more shallow and thus call for alternative fine-tuning techniques. We introduce a novel approach, Universal Language Model Fine-tuning (ULMFiT), that addresses these challenges and facilitates robust inductive transfer learning for any NLP task, comparable to fine-tuning ImageNet models: The identical 3-layer LSTM design—utilizing the same hyperparameters and no supplements other than tuned dropout hyperparameters—outperforms highly engineered models and transfer learning approaches on six widely analyzed text classification tasks. On IMDb, with 100 labeled examples, ULMFiT matches the performance of training from scratch with 10× and—given 50k unlabeled examples—with 100× more information.","In contrast with CV, NLP models tend to be more superficial and therefore necessitate different refinement procedures. We present a new technique, Universal Language Model Fine-tuning (ULMFiT), that deals with these issues and enables robust inductive transfer learning for any NLP task, similar to tuning ImageNet models: The very same 3-layer LSTM architecture—employing the same hyperparameters and no additions apart from adjusted dropout hyperparameters—surpasses very engineered models and transfer learning methods on six extensively studied text classification tasks. On IMDb, with 100 labeled examples, ULMFiT equals the performance of training from the beginning with 10× and—given 50k unlabeled examples—with 100× more data.",A,1
Universal Language Model Fine-tuning for Text Classification,"Our contributions are the following: 1) We propose Universal Language Model Fine-tuning (ULMFiT), a method that can be used to achieve CV-like transfer learning for any task for NLP. 2) We propose discriminative fine-tuning, slanted triangular learning rates, and gradual unfreezing, novel techniques to retain previous knowledge and avoid catastrophic forgetting during fine-tuning. 3) We significantly outperform the state-of-the-art on six representative text classification datasets, with an error reduction of 18-24% on the majority of datasets. 4) We show that our method enables extremely sample-efficient transfer learning and perform an extensive ablation analysis. 5) We make the pretrained models and our code available to enable wider adoption.","Our innovations are as follows: 1) We put forward Universal Language Model Fine-tuning (ULMFiT), a technique that can be utilized to attain computer vision-like transfer learning for any natural language processing task. 2) We put forward discriminative fine-tuning, slanted triangular learning rates, and gradual unfreezing, new methods to retain prior knowledge and prevent catastrophic forgetting during fine-tuning. 3) We significantly surpass the current state-of-the-art on six exemplary text classification datasets, with an error reduction of 18-24% on most of the datasets. 4) We demonstrate that our method enables extremely sample-efficient transfer learning and perform a thorough ablation analysis. 5) We make the pre-trained models and our code available to facilitate wider adoption.","Our contributions are: 1) We introduce Universal Language Model Fine-tuning (ULMFiT), a procedure that can be used to achieve computer vision-style transfer learning for any natural language task. 2) We introduce discriminative fine-tuning, slanted triangular learning rates, and gradual unfreezing, innovative techniques to maintain previous knowledge and avoid catastrophic forgetting during fine-tuning. 3) We substantially outperform the current state-of-the-art on six representative text classification datasets, with an error reduction of 18-24% on most datasets. 4) We show that our method allows extremely sample-efficient transfer learning and perform a comprehensive ablation analysis. 5) We make the pre-trained models and our code available to enable broader adoption.","Our innovations include: 1) We present Universal Language Model Fine-tuning (ULMFiT), a technique that can be leveraged to attain computer vision-style transfer learning for any natural language understanding task. 2) We present discriminative fine-tuning, slanted triangular learning rates, and gradual unfreezing, novel methods to retain prior knowledge and prevent catastrophic forgetting during fine-tuning. 3) We significantly exceed the current state-of-the-art on six exemplary text classification datasets, with an error reduction of 18-24% on the majority of datasets. 4) We demonstrate that our method enables extremely sample-efficient transfer learning and perform an extensive ablation analysis. 5) We make the pretrained models and our code available to facilitate wider adoption.",A,1
Universal Language Model Fine-tuning for Text Classification,"Features in deep neural networks in CV have been observed to transition from general to task-specific from the first to the last layer (Yosinski et al., 2014). For this reason, most work in CV focuses on transferring the first layers of the model (Long et al., 2015b). Sharif Razavian et al. (2014) achieve state-of-theart results using features of an ImageNet model as input to a simple classifier. In recent years, this approach has been superseded by fine-tuning either the last (Donahue et al., 2014) or several of the last layers of a pretrained model and leaving the remaining layers frozen (Long et al., 2015a).","Characteristics in deep neural networks for computer vision have been noticed to change from universal to job-particular from the initial to the final layer (Yosinski et al., 2014). Because of this, most work in computer vision concentrates on moving the first layers of the model (Long et al., 2015b). Sharif Razavian et al. (2014) attain best-in-class outcomes utilizing features of an ImageNet model as input to a simple classifier. In recent times, this tactic has been replaced by tuning either the last (Donahue et al., 2014) or a few of the last layers of a pre-trained model and keeping the rest of the layers fixed (Long et al., 2015a).","Qualities in profound neural systems for computerized picture handling have been seen to change from wide-ranging to assignment-explicit from the early to the last layer (Yosinski et al., 2014). Thus, a great part of the work in computerized picture handling centers around moving the initial layers of the model (Long et al., 2015b). Sharif Razavian et al. (2014) accomplish top tier results utilizing highlights of an ImageNet model as information to a basic classifier. As of late, this methodology has been supplanted by adjusting either the last (Donahue et al., 2014) or a couple of the last layers of a pre-prepared model and keeping the leftover layers frozen (Long et al., 2015a).  ","Attributes in profound neural organizations for automated visual depiction have been noticed to change from expansive to errand-explicit from the initial to the last layer (Yosinski et al., 2014). Therefore, a great part of the work in automated visual portrayal centers around moving the early layers of the model (Long et al., 2015b). Sharif Razavian et al. (2014) accomplish first class results using highlights of an ImageNet model as data to a clear classifier. Starting late, this system has been displaced by changing either the last (Donahue et al., 2014) or two or three of the last layers of a pre-set up model and keeping the excess layers frozen (Long et al., 2015a).",A,1
Universal Language Model Fine-tuning for Text Classification,"In NLP, only recently have methods been proposed that go beyond transferring word embeddings. The prevailing approach is to pretrain embeddings that capture additional context via other tasks. Embeddings at different levels are then used as features, concatenated either with the word embeddings or with the inputs at intermediate layers. This method is known as hypercolumns (Hariharan et al., 2015) in CV and is used by Peters et al. (2017), Peters et al. (2018), Wieting and Gimpel (2017), Conneau et al. (2017), and McCann et al. (2017) who use language modeling, paraphrasing, entailment, and Machine Translation (MT) respectively for pretraining.","In the field of natural language processing, techniques that go past simply transferring word embeddings have only recently started being developed. The most common approach is to pre-train embeddings that capture extra context through other tasks. Embeddings at various levels are then utilized as features, concatenated either with the word embeddings or with the inputs at middle layers. This technique is referred to as hypercolumns (Hariharan et al., 2015) in computer vision and is utilized by Peters et al. (2017), Peters et al. (2018), Wieting and Gimpel (2017), Conneau et al. (2017), and McCann et al. (2017) who use language modeling, paraphrasing, entailment, and machine translation respectively for pretraining.","In NLP, methods that surpass just transferring word embeddings have only been proposed recently. The prevailing technique is to pre-train embeddings that capture more context through other tasks. Embeddings at multiple levels are then employed as features, concatenated either with the word embeddings or with the inputs at intermediate layers. This approach is known as hypercolumns (Hariharan et al., 2015) in computer vision and is utilized by Peters et al. (2017), Peters et al. (2018), Wieting and Gimpel (2017), Conneau et al. (2017), and McCann et al. (2017) who use language modeling, paraphrasing, entailment, and machine translation respectively for pre-training.","In natural language processing, approaches going past simply transferring word embeddings have only been put forward recently. The common technique is to pre-train embeddings capturing additional context through other tasks. Embeddings at various levels are then used as features, concatenated either with the word embeddings or with the inputs at middle layers. This method is referred to as hypercolumns (Hariharan et al., 2015) in computer vision and is employed by Peters et al. (2017), Peters et al. (2018), Wieting and Gimpel (2017), Conneau et al. (2017), and McCann et al. (2017) who utilize language modeling, paraphrasing, entailment, and machine translation respectively for pre-training.",A,1
Universal Language Model Fine-tuning for Text Classification," Specifically, Peters et al. (2018) require engineered custom architectures, while we show state-of-the-art performance with the same basic architecture across a range of tasks. In CV, hypercolumns have been nearly entirely superseded by end-to-end fine-tuning (Long et al., 2015a). A related direction is multi-task learning (MTL) (Caruana, 1993). This is the approach taken by Rei (2017) and Liu et al. (2018) who add a language modeling objective to the model that is trained jointly with the main task model. MTL requires the tasks to be trained from scratch every time, which makes it inefficient and often requires careful weighting of the task specific objective functions (Chen et al., 2017).","In particular, Peters et al. (2018) need specially designed architectures, while we demonstrate cutting-edge performance using the same fundamental architecture for various tasks. In computer vision, hypercolumns have been almost completely superseded by end-to-end fine-tuning (Long et al., 2015a). A related approach is multi-task learning (MTL) (Caruana, 1993). This is the method used by Rei (2017) and Liu et al. (2018) who incorporate a language modeling goal into the model that is trained together with the main task model. MTL requires the tasks to be trained from the beginning every time, making it inefficient and often needing careful weighting of the task-specific objective functions (Chen et al., 2017).","Specifically, Peters et al. (2018) require custom-built architectures, whereas we show state-of-the-art results with the same basic architecture for multiple tasks. In computer vision, hypercolumns have been nearly completely replaced by end-to-end fine-tuning (Long et al., 2015a). A related technique is multi-task learning (MTL) (Caruana, 1993). This is the approach taken by Rei (2017) and Liu et al. (2018) who add a language modeling target to the model that is trained jointly with the main task model. MTL necessitates retraining the tasks from scratch each time, making it inefficient and often requiring careful balancing of the task-specific loss functions (Chen et al., 2017).  ","In particular, Peters et al. (2018) need specially engineered architectures, while we show best-in-class performance using the same fundamental architecture across various tasks. In computer vision, hypercolumns have been almost entirely superseded by end-to-end fine-tuning (Long et al., 2015a). A related technique is multi-task learning (MTL) (Caruana, 1993). This is the method employed by Rei (2017) and Liu et al. (2018) who incorporate a language modeling objective into the model that is trained together with the main task model. MTL necessitates retraining the tasks from scratch every time, which makes it inefficient and often requires careful weighting of the task-specific loss functions (Chen et al., 2017).",A,1
Universal Language Model Fine-tuning for Text Classification,"Fine-tuning has been used successfully to transfer between similar tasks, e.g. in QA (Min et al., 2017), for distantly supervised sentiment analysis (Severyn and Moschitti, 2015), or MT domains (Sennrich et al., 2015) but has been shown to fail between unrelated ones (Mou et al., 2016). Dai and Le (2015) also fine-tune a language model, but overfit with 10k labeled examples and require millions of in-domain documents for good performance. In contrast, ULMFiT leverages general-domain pretraining and novel finetuning techniques to prevent overfitting even with only 100 labeled examples and achieves state-of-the-art results also on small datasets.","Fine-tuning has been utilized effectively to transfer knowledge between similar tasks, for example in QA (Min et al., 2017), for distantly supervised sentiment analysis (Severyn and Moschitti, 2015), or MT domains (Sennrich et al., 2015) but has demonstrated failure between unrelated tasks (Mou et al., 2016). Dai and Le (2015) also fine-tune a language model, but overfit with 10k labeled examples and need millions of in-domain documents to achieve good performance. In contrast, ULMFiT leverages general-domain pretraining and new finetuning techniques to avoid overfitting even with only 100 labeled examples and attains state-of-the-art results even on small datasets.","Fine-tuning has been fruitfully employed to transfer learning between analogous tasks, such as in QA (Min et al., 2017), for distantly supervised sentiment analysis (Severyn and Moschitti, 2015), or MT domains (Sennrich et al., 2015) but has proven unsuccessful between dissimilar tasks (Mou et al., 2016). Dai and Le (2015) also fine-tune a language model, but overfit with 10k labeled examples and require millions of in-domain documents to obtain good performance. By contrast, ULMFiT takes advantage of general-domain pretraining and innovative finetuning techniques to prevent overfitting even with only 100 labeled examples and realizes state-of-the-art results even on small datasets.  ","Fine-tuning has been profitably applied to transfer knowledge between comparable tasks, for instance in QA (Min et al., 2017), for distantly supervised sentiment analysis (Severyn and Moschitti, 2015), or MT domains (Sennrich et al., 2015) but has been shown to be ineffective between unrelated tasks (Mou et al., 2016). Dai and Le (2015) also fine-tune a language model, but overfit with 10k labeled examples and need millions of in-domain documents to achieve decent performance. Conversely, ULMFiT capitalizes on general-domain pretraining and novel finetuning techniques to avoid overfitting even with only 100 labeled examples and attains state-of-the-art results even on small datasets.",A,1
Universal Language Model Fine-tuning for Text Classification,"Language modeling can be seen as the ideal source task and a counterpart of ImageNet for NLP: It captures many facets of language relevant for downstream tasks, such as long-term dependencies (Linzen et al., 2016), hierarchical relations (Gulordava et al., 2018), and sentiment (Radford et al., 2017). In contrast to tasks like MT (McCann et al., 2017) and entailment (Conneau et al., 2017), it provides data in near-unlimited quantities for most domains and languages. Additionally, a pretrained LM can be easily adapted to the idiosyncrasies of a target task, which we show significantly improves performance (see Section 5).","Language modeling has the potential to be the perfect foundation task and a natural language processing equivalent of ImageNet: It captures numerous aspects of language that are useful for downstream tasks, like long-term connections (Linzen et al., 2016), hierarchical links (Gulordava et al., 2018), and emotion (Radford et al., 2017). Unlike tasks such as machine translation (McCann et al., 2017) and textual entailment (Conneau et al., 2017), it makes available nearly unlimited data for most topics and tongues. Also, a pre-trained language model can be easily tailored to the peculiarities of a target task, which we demonstrate substantially improves performance (see Section 5).","Language modeling can serve as the ideal source task and a natural language counterpart to ImageNet: It encompasses many facets of language relevant for downstream tasks, including long-range dependencies (Linzen et al., 2016), hierarchical relationships (Gulordava et al., 2018), and affect (Radford et al., 2017). In contrast with tasks such as machine translation (McCann et al., 2017) and inference (Conneau et al., 2017), it provides near-limitless data for most domains and languages. Furthermore, a pre-trained language model can be readily adapted to the idiosyncrasies of a target task, which we exhibit significantly boosts performance (see Section 5).  ","Language modeling has the potential to be the optimal foundation task and a linguistic equivalent of ImageNet: It captures numerous language aspects useful for downstream tasks, including long-distance connections (Linzen et al., 2016), hierarchical links (Gulordava et al., 2018), and feeling (Radford et al., 2017). Unlike tasks like machine translation (McCann et al., 2017) and implication (Conneau et al., 2017), it furnishes near-endless data for most subjects and tongues. Additionally, a pre-trained language model can be easily tailored to the quirks of a target task, which we demonstrate substantially improves results (see Section 5).",A,1
Universal Language Model Fine-tuning for Text Classification,"Moreover, language modeling already is a key component of existing tasks such as MT and dialogue modeling. Formally, language modeling induces a hypothesis space H that should be useful for many other NLP tasks (Vapnik and Kotz, 1982; Baxter, 2000). We propose Universal Language Model Finetuning (ULMFiT), which pretrains a language model (LM) on a large general-domain corpus and fine-tunes it on the target task using novel techniques. The method is universal in the sense that it meets these practical criteria: 1) It works across tasks varying in document size, number, and label type; 2) it uses a single architecture and training process; 3) it requires no custom feature engineering or preprocessing; and 4) it does not require additional in-domain documents or labels.","Furthermore, language modeling is already a vital part of present tasks like machine translation and conversation modeling. Theoretically, language modeling produces a hypothesis space H that should be useful for many other natural language processing tasks (Vapnik and Kotz, 1982; Baxter, 2000). We suggest Universal Language Model Finetuning (ULMFiT), which pre-trains a language model (LM) on a large general-domain corpus and fine-tunes it on the target task utilizing novel techniques. The method is universal in that it meets these practical criteria: 1) It is effective across tasks varying in document size, number, and label type; 2) it employs a single architecture and training process; 3) it requires no custom feature engineering or preprocessing; and 4) it does not need additional in-domain documents or labels.","In addition, language modeling is already a crucial element of current tasks such as machine translation and conversation modeling. Theoretically speaking, language modeling produces a hypothesis space H that should be beneficial for many other natural language processing tasks (Vapnik and Kotz, 1982; Baxter, 2000). We put forward Universal Language Model Finetuning (ULMFiT), which pre-trains a language model (LM) on a large general-domain corpus and fine-tunes it on the target task using innovative techniques. The method is universal in that it satisfies these practical criteria: 1) It is effective across tasks varying in document size, number, and label type; 2) it uses a single architecture and training process; 3) it requires no custom feature engineering or preprocessing; and 4) it does not call for additional in-domain documents or labels.","Also, language modeling is already a vital component of present tasks like machine translation and conversation modeling. In theory, language modeling generates a hypothesis space H that should be beneficial for many other natural language processing tasks (Vapnik and Kotz, 1982; Baxter, 2000). We present Universal Language Model Finetuning (ULMFiT), which pre-trains a language model (LM) on a large general-domain corpus and fine-tunes it on the target task employing innovative techniques. The method is universal in that it meets these practical criteria: 1) It works across tasks varying in document size, number, and label type; 2) it utilizes a single architecture and training process; 3) it requires no custom feature engineering or preprocessing; and 4) it does not call for extra in-domain documents or labels.",A,1
Universal Language Model Fine-tuning for Text Classification,"In our experiments, we use the state-of-the-art language model AWD-LSTM (Merity et al., 2017a), a regular LSTM (with no attention, short-cut connections, or other sophisticated additions) with various tuned dropout hyperparameters. Analogous to CV, we expect that downstream performance can be improved by using higher-performance language models in the future. ULMFiT consists of the following steps, which we show in Figure 1: a) General-domain LM pretraining (§3.1); b) target task LM fine-tuning (§3.2); and c) target task classifier fine-tuning (§3.3). We discuss these in the following sections.","For our tests, we utilize the cutting-edge language model AWD-LSTM (Merity et al., 2017a), which is a standard LSTM (without attention mechanisms, shortcut connections, or other complex augmentations) with several optimized dropout hyperparameters. Similar to computer vision, we anticipate that performance on downstream tasks can be enhanced by employing more capable language models moving forward. ULMFiT consists of the following phases, displayed in Figure 1: a) Pretraining a general-domain LM (§3.1); b) Fine-tuning the LM on the target task (§3.2); and c) Fine-tuning a classifier for the target task (§3.3). We elaborate on these in the next sections.","In our experiments, we make use of the state-of-the-art language model AWD-LSTM (Merity et al., 2017a), which is a vanilla LSTM (without attention, shortcut connections, or other advanced features) with various tuned dropout settings. As in computer vision, we expect downstream performance to improve by utilizing higher-capability language models in the future. ULMFiT has the following steps, shown in Figure 1: a) Pretraining a general-purpose LM (§3.1); b) Fine-tuning the LM on the target task (§3.2); and c) Fine-tuning a classifier on the target task (§3.3). We discuss these in the following sections.","For our experiments, we employ the cutting-edge language model AWD-LSTM (Merity et al., 2017a), which is a standard LSTM (without attention mechanisms, shortcut connections, or other sophisticated enhancements) with several optimized dropout parameters. Similar to computer vision, we anticipate downstream performance can be boosted by leveraging more capable language models moving forward. ULMFiT consists of the following phases, depicted in Figure 1: a) Pretraining a general-purpose LM (§3.1); b) Tuning the LM on the target task (§3.2); and c) Tuning a classifier on the target task (§3.3). We elaborate on these in the subsequent sections.",A,1
Universal Language Model Fine-tuning for Text Classification,"An ImageNet-like corpus for language should be large and capture general properties of language. We pretrain the language model on Wikitext-103 (Merity et al., 2017b) consisting of 28,595 preprocessed Wikipedia articles and 103 million words. Pretraining is most beneficial for tasks with small datasets and enables generalization even with 100 labeled examples. We leave the exploration of more diverse pretraining corpora to future work, but expect that they would boost performance. While this stage is the most expensive, it only needs to be performed once and improves performance and convergence of downstream models.","A large dataset resembling ImageNet that captures the general attributes of language should be created for pretraining language models. We pretrain the language model using Wikitext-103 (Merity et al., 2017b), which contains 28,595 preprocessed Wikipedia articles totaling 103 million words. Pretraining is most useful for tasks with small training sets, allowing the model to generalize even with just 100 labeled examples. We leave exploring more varied pretraining datasets for future work, but anticipate they would further improve performance. Although this pretraining stage is computationally expensive, it only needs to be done once and enhances downstream models' performance and convergence.","An extensive corpus similar to ImageNet that represents the broad properties of language is needed to pretrain language models. We pretrain the language model on Wikitext-103 (Merity et al., 2017b), which has 28,595 Wikipedia articles preprocessed into 103 million words. Pretraining is most beneficial for tasks with scarce labeled data, permitting generalization even with only 100 labeled samples. We defer exploring more diverse pretraining data to future work, but expect further gains in performance. While pretraining is costly, it is a one-time investment that improves downstream models' accuracy and convergence.  ","A large-scale dataset like ImageNet capturing the general traits of language should be created to pretrain language models. We pretrain the language model using Wikitext-103 (Merity et al., 2017b), composed of 28,595 processed Wikipedia articles totaling 103 million words. Pretraining is most advantageous for tasks with minimal labeled data, enabling generalization even with just 100 labeled examples. We leave examining more varied pretraining data for future work, but predict further performance improvements. Although expensive, pretraining only needs to be conducted once and boosts downstream models' performance and convergence.",A,1
Universal Language Model Fine-tuning for Text Classification,"No matter how diverse the general-domain data used for pretraining is, the data of the target task will likely come from a different distribution. We thus fine-tune the LM on data of the target task. Given a pretrained general-domain LM, this stage converges faster as it only needs to adapt to the idiosyncrasies of the target data, and it allows us to train a robust LM even for small datasets. We propose discriminative fine-tuning and slanted triangular learning rates for fine-tuning the LM, which we introduce in the following.","Even if the general-purpose data used for pre-training the model covers a wide range of topics, the data for the specific task we want to solve will probably come from a different distribution. So we further fine-tune the language model on examples from the target task. Starting with a model pre-trained on diverse generic data, this fine-tuning stage can converge quickly since it only needs to adapt the model to the quirks of the task-specific data. It also lets us train a robust language model even with small task-specific datasets. We suggest two techniques for fine-tuning the model - discriminative fine-tuning and slanted triangular learning rates - which we will explain next.","Despite the broad variety of general domain data used for pre-training, the data for the given task is likely to have a different distribution. Therefore, we fine-tune the language model using examples from the target task. With a model pre-trained on diverse generic data, this fine-tuning is faster because the model only needs to become accustomed to the unique aspects of the task data. It also enables training a robust language model even with small task datasets. We put forward discriminative fine-tuning and slanted triangular learning rates for fine-tuning the model, which we will describe below. ","Even with pre-training data covering a wide range of general domains, the data for a specific target task will probably come from a different distribution. As a result, we further fine-tune the language model on target task data. Starting from a model pre-trained on diverse generic data, this fine-tuning is faster as the model only needs to adapt to the particularities of the target data. It also allows training a robust language model even with small target task datasets. We propose two techniques for fine-tuning - discriminative fine-tuning and slanted triangular learning rates - which we explain next.",A,1
Universal Language Model Fine-tuning for Text Classification,"For adapting its parameters to task-specific features, we would like the model to quickly converge to a suitable region of the parameter space in the beginning of training and then refine its parameters. Using the same learning rate (LR) or an annealed learning rate throughout training is not the best way to achieve this behaviour. Instead, we propose slanted triangular learning rates (STLR), which first linearly increases the learning rate and then linearly decays it according to the following update schedule, which can be seen in Figure 2.","To tune its parameters to the specific characteristics of the task, we want the model to swiftly move to a proper area of the parameter space at the start of training, and then fine-tune its parameters. Having an unchanging learning rate (LR) or a cooling learning rate during the whole training is not the optimal way to get this performance. Rather, we put forward tilted triangular learning rates (STLR), which first linearly grows the learning rate and then linearly decreases it based on the following update schedule, visible in Figure 2.","For adapting its weights to the distinctive aspects of the task, our aim is for the model to quickly reach a suitable zone of the parameter space at the beginning of training, and then refine its weights. Keeping the same learning rate (LR) or a decreasing learning rate throughout training is not the most effective approach to realize this conduct. As an alternative, we suggest slanted triangular learning rates (STLR), which firstly increases the learning rate in a linear fashion and then decreases it linearly according to the following update schedule, shown in Figure 2.  ","To adjust its weights to the particular features of the task, we desire the model to rapidly converge to a proper region of the parameter space at the start of training, and then fine-tune its weights. Maintaining an unchanged learning rate (LR) or a cooling learning rate during the entire training is not the most optimal way to attain this performance. Rather, we propose tilted triangular learning rates (STLR), which first increases the learning rate linearly and then decreases it linearly based on the following update schedule, depicted in Figure 2.",A,1
Universal Language Model Fine-tuning for Text Classification,"STLR modifies triangular learning rates (Smith, 2017) with a short increase and a long decay period, which we found key for good performance. In Section 5, we compare against aggressive cosine annealing, a similar schedule that has recently been used to achieve state-of-the-art performance in CV (Loshchilov and Hutter, 2017). Finally, for fine-tuning the classifier, we augment the pretrained language model with two additional linear blocks. Following standard practice for CV classifiers, each block uses batch normalization (Ioffe and Szegedy, 2015) and dropout, with ReLU activations for the intermediate layer and a softmax activation that outputs a probability distribution over target classes at the last layer.","STLR alters three-sided learning rates (Smith, 2017) with a brief upsurge and an extended decay episode, which we established as crucial for favorable results. In Section 5, we contrast with forceful cosine annealing, a comparable timetable recently utilized to accomplish state-of-the-art execution in CV (Loshchilov and Hutter, 2017). At last, for tuning the classifier, we expand the pretrained language model with two extra linear blocks. As per standard practice for CV classifiers, each block employs batch normalization (Ioffe and Szegedy, 2015) and dropout, with ReLU activations for the middle layer and a softmax activation that yields a probability distribution over target classes at the last layer.","STLR tweaks triangular learning rates (Smith, 2017) with a short spike and a long decay period, which we found vital for good performance. In Section 5, we compare to aggressive cosine annealing, a similar schedule recently used to achieve cutting-edge performance in CV (Loshchilov and Hutter, 2017). Finally, for fine-tuning the classifier, we augment the pretrained language model with two extra linear blocks. Following standard practice for CV classifiers, each block utilizes batch normalization (Ioffe and Szegedy, 2015) and dropout, with ReLU activations for the intermediate layer and a softmax activation that outputs a probability distribution over target classes at the final layer.","STLR changes triangular learning rates (Smith, 2017) with a short increase and an extended decay duration, which we determined key for satisfactory results. In Section 5, we contrast with forceful cosine annealing, a comparable timetable recently employed to accomplish best-in-class performance in CV (Loshchilov and Hutter, 2017). Lastly, for tuning the classifier, we expand the pre-trained language model with two additional linear blocks. Per standard practice for CV classifiers, each block uses batch normalization (Ioffe and Szegedy, 2015) and dropout, with ReLU activations for the middle layer and a softmax activation that yields a probability distribution over target classes at the final layer.",A,1
Universal Language Model Fine-tuning for Text Classification,"Note that the parameters in these task-specific classifier layers are the only ones that are learned from scratch. The first linear layer takes as the input the pooled last hidden layer states. The signal in text classification tasks is often contained in a few words, which may occur anywhere in the document. As input documents can consist of hundreds of words, information may get lost if we only consider the last hidden state of the model.","Observe that the variables in these assignment-explicit categorizer tiers are the sole ones acquired from the beginning. The primary direct stratum takes as the contribution the pooled final obscure stratum circumstances. The gesture in text sorting assignments is regularly comprised in a couple of terms, which may happen anyplace in the report. As info reports can comprise of many words, data might become lost if we just think about the last hidden condition of the model.","Note that the parameters in these job-particular classifier layers are the only ones learned from nothing. The first linear part accepts as the info the pooled last hidden layer states. The sign in text classification tasks is often present in a few terms, which could appear anywhere in the document. As documents can have hundreds of words, information might vanish if we only examine the final hidden state of the model.  ","Recognize that the variables in these work-explicit classifier levels are the solitary ones secured from scratch. The initial straight layer takes as the contribution the pooled last concealed layer states. The flag in content order assignments is frequently contained in a couple of words, which might happen anyplace in the report. As information reports can comprise of many words, data could get lost if we just consider the last concealed condition of the model.",A,1
Universal Language Model Fine-tuning for Text Classification,"Fine-tuning the target classifier is the most critical part of the transfer learning method. Overly aggressive fine-tuning will cause catastrophic forgetting, eliminating the benefit of the information captured through language modeling; too cautious fine-tuning will lead to slow convergence (and resultant overfitting). Besides discriminative finetuning and triangular learning rates, we propose gradual unfreezing for fine-tuning the classifier. Rather than fine-tuning all layers at once, which risks catastrophic forgetting, we propose to gradually unfreeze the model starting from the last layer as this contains the least general knowledge (Yosinski et al., 2014): We first unfreeze the last layer and fine-tune all unfrozen layers for one epoch.","Adjusting the target classifier is the most important part of the transfer learning technique. Being too aggressive with adjusting will result in forgetting everything that was learned, removing any benefit from the information obtained through language modeling. Being too cautious will lead to slow improvements (and overfitting). In addition to discriminative fine-tuning and triangular learning rates, we suggest steadily unfreezing for adjusting the classifier. Rather than adjusting all layers at once, which risks forgetting everything, we propose to slowly unfreeze the model starting with the last layer since it contains the least general knowledge (Yosinski et al., 2014): We first unfreeze the final layer and tune all unfrozen layers for one epoch.","Modifying the target classifier is the most crucial aspect of the transfer learning process. Modifying too drastically will make the model forget everything it learned, eliminating the usefulness of the information from language modeling. Modifying too little will result in slow progress (and overfitting). On top of discriminative modification and triangular learning rates, we recommend gradually unfreezing for modifying the classifier. Instead of modifying all layers simultaneously, which risks forgetting, we suggest gradually unfreezing the model beginning with the last layer as it has the least general knowledge (Yosinski et al., 2014): We first unfreeze the final layer and adjust all unfrozen layers for one epoch. ","Tuning the target classifier is the most important part of transfer learning. Tuning too aggressively will make the model forget everything, removing any benefit from the language modeling information. Tuning too cautiously will lead to slow improvements (and overfitting). In addition to discriminative tuning and triangular learning rates, we propose progressively unfreezing for tuning the classifier. Rather than tuning all layers at once, which risks forgetting everything, we suggest progressively unfreezing the model starting with the last layer since it has the least general knowledge (Yosinski et al., 2014): We first unfreeze the final layer and refine all unfrozen layers for one epoch.",A,1
Universal Language Model Fine-tuning for Text Classification,"We then unfreeze the next lower frozen layer and repeat, until we finetune all layers until convergence at the last iteration. This is similar to ‘chain-thaw’ (Felbo et al., 2017), except that we add a layer at a time to the set of ‘thawed’ layers, rather than only training a single layer at a time. While discriminative fine-tuning, slanted triangular learning rates, and gradual unfreezing all are beneficial on their own, we show in Section 5 that they complement each other and enable our method to perform well across diverse datasets.","We subsequently thaw the next lower solidified stratum and redo, until we refine all tiers until union at the final repetition. This resembles 'chain-defrost' (Felbo et al., 2017), excluding that we supplement a tier at a time to the set of 'thawed' tiers, rather than only coaching a single tier at once. While discriminative refinement, slanted triangular learning paces, and gradual unfreezing all are advantageous on their own, we demonstrate in Section 5 that they complement each other and empower our method to execute well across diverse datasets.","Next, we unsolidify the following lower frozen layer and reiterate, until we fine-adjust all layers until convergence at the final iteration. This is similar to 'chain-melt' (Felbo et al., 2017), except we add a layer at a time to the set of 'melted' layers, rather than only training a single layer at once. Although discriminative fine-tuning, tilted triangular learning rates, and gradual unfreezing are all beneficial alone, we show in Section 5 they complement each other and allow our method to perform well across different datasets.  ","Subsequently, we defrost the next lower frozen stratum and repeat, until we refine all strata until union at the final repetition. This resembles 'chain-thaw' (Felbo et al., 2017), barring that we append a stratum at a time to the set of 'thawed' strata, rather than only coaching a single stratum at once. While discriminative refinement, slanted triangular learning velocities, and gradual unfreezing are all favorable alone, we exhibit in Section 5 they complement each other and empower our method to execute well across diverse datasets.",A,1
Universal Language Model Fine-tuning for Text Classification,"Language models are trained with backpropagation through time (BPTT) to enable gradient propagation for large input sequences. In order to make fine-tuning a classifier for large documents feasible, we propose BPTT for Text Classification (BPT3C): We divide the document into fixed length batches of size b. At the beginning of each batch, the model is initialized with the final state of the previous batch; we keep track of the hidden states for mean and max-pooling; gradients are back-propagated to the batches whose hidden states contributed to the final prediction. In practice, we use variable length backpropagation sequences (Merity et al., 2017a).","Neural network language models learn through backpropagation across time steps (BPTT), which allows gradient flow over long input sequences. To enable fine-tuning a text classifier on large documents, we put forward BPTT for Text Classification (BPT3C): We separate the document into set-length batches of size b. At the start of each batch, we initialize the model with the final state of the prior batch; we track the hidden states for mean and max pooling; we propagate gradients back to the batches whose hidden states added to the final prediction. In practice, we utilize variable length backpropagation sequences (Merity et al., 2017a).","Language models trained with neural networks use backpropagation through time (BPTT) to enable gradient flow over long input sequences. To make tuning a classifier on big documents possible, we introduce BPTT for Text Classification (BPT3C): We split the document into fixed size batches of length b. At the beginning of each batch, we initialize the model with the final state of the previous batch; we monitor the hidden states for mean and max pooling; we backpropagate gradients to the batches whose hidden states contributed to the final prediction. In practice, we employ variable length backpropagation sequences (Merity et al., 2017a). ","Neural network language models are trained using backpropagation through time (BPTT) to allow gradient flow over long input sequences. To enable fine-tuning a text classifier on large documents, we present BPTT for Text Classification (BPT3C): We separate the document into set-length batches of size b. At the start of each batch, we initialize the model with the final state of the prior batch; we keep track of the hidden states for mean and max pooling; we backpropagate gradients to the batches whose hidden states contributed to the final prediction. In practice, we utilize variable length backpropagation sequences (Merity et al., 2017a).",A,1
Universal Language Model Fine-tuning for Text Classification,"Similar to existing work (Peters et al., 2017, 2018), we are not limited to fine-tuning a unidirectional language model. For all our experiments, we pretrain both a forward and a backward LM. We fine-tune a classifier for each LM independently using BPT3C and average the classifier predictions. While our approach is equally applicable to sequence labeling tasks, we focus on text classification tasks in this work due to their important real world applications.","Like previous research (Peters et al., 2017, 2018), we are not constrained to fine-tuning a one-directional language model. For all of our experiments, we pre-train both a forward and a backward LM. We fine-tune a classifier for each LM separately using BPT3C and take the average of the classifier predictions. Although our approach can also be applied to sequence labeling tasks, we concentrate on text classification tasks in this work because of their significant real world uses.","Similar to existing studies (Peters et al., 2017, 2018), we are not limited to adapting a unidirectional language model. For all our tests, we pretrain both a forward and backward LM. We customize a classifier for each LM independently utilizing BPT3C and average the classifier forecasts. While our method is just as relevant to sequence labeling tasks, we focus on text classification tasks in this work due to their important practical applications. ","Like previous work (Peters et al., 2017, 2018), we are not constrained to fine-tuning a one-way language model. For all our experiments, we pre-train both a forward and backward LM. We adapt a classifier for each LM separately using BPT3C and take the mean of the classifier predictions. Although our approach can also apply to sequence labeling tasks, we concentrate on text classification tasks in this work because of their significant real world purposes.",A,1
Universal Language Model Fine-tuning for Text Classification,"We evaluate our method on six widely-studied datasets, with varying numbers of documents and varying document length, used by state-of-the-art text classification and transfer learning approaches (Johnson and Zhang, 2017; McCann et al., 2017) as instances of three common text classification tasks: sentiment analysis, question classification, and topic classification. We show the statistics for each dataset and task in Table 1. For sentiment analysis, we evaluate our approach on the binary movie review IMDb dataset (Maas et al., 2011) and on the binary and five-class version of the Yelp review dataset compiled by Zhang et al. (2015).","We test our technique on six commonly used data sets that have different amounts of documents and document lengths. These data sets have been used in other state-of-the-art text classification and transfer learning studies (Johnson and Zhang, 2017; McCann et al., 2017). They represent three typical text classification tasks: sentiment analysis, question classification, and topic classification. Table 1 shows the statistics for each data set and task. For sentiment analysis, we assess our approach using the binary movie review IMDb data set (Maas et al., 2011) and the binary and five-class versions of the Yelp review data set put together by Zhang et al. (2015).","Our method is evaluated using six extensively studied data sets with varying document counts and document lengths. These data sets have been utilized in cutting-edge text classification and transfer learning research (Johnson and Zhang, 2017; McCann et al., 2017) as examples of three common text classification tasks: sentiment analysis, question classification, and topic classification. The statistics for each data set and task are displayed in Table 1. For sentiment analysis, we test our approach on the binary movie review IMDb data set (Maas et al., 2011) as well as the binary and five-class versions of the Yelp review data set assembled by Zhang et al. (2015).","We assess our method's performance using six widely analyzed data sets that have different numbers of documents and document lengths. These data sets have been employed in state-of-the-art text classification and transfer learning studies (Johnson and Zhang, 2017; McCann et al., 2017) as cases of three prevalent text classification tasks: sentiment analysis, question classification, and topic classification. The statistics for each data set and task are shown in Table 1. For sentiment analysis, we evaluate our approach using the binary movie review IMDb data set (Maas et al., 2011) and the binary and five-class variants of the Yelp review data set compiled by Zhang et al. (2015).",A,1
Universal Language Model Fine-tuning for Text Classification,"We are interested in a model that performs robustly across a diverse set of tasks. To this end, if not mentioned otherwise, we use the same set of hyperparameters across tasks, which we tune on the IMDb validation set. We use the AWD-LSTM language model (Merity et al., 2017a) with an embedding size of 400, 3 layers, 1150 hidden activations per layer, and a BPTT batch size of 70. We apply dropout of 0.4 to layers, 0.3 to RNN layers, 0.4 to input embedding layers, 0.05 to embedding layers, and weight dropout of 0.5 to the RNN hidden-to-hidden matrix. The classifier has a hidden layer of size 50.","Our goal is to create a system that works consistently well on many different tasks. With this goal in mind, unless stated otherwise, we utilize the same hyperparameters across all tasks, tuning them using the validation set from IMDb. We implement the AWD-LSTM neural network language model (Merity et al., 2017a), with 400-dimensional embeddings, 3 hidden layers, 1150 activations per layer, and backpropagation batches of 70 time steps. We regularize with dropout rates of 0.4 on layers, 0.3 on RNN layers, 0.4 on input embeddings, 0.05 on word embeddings, and weight dropout of 0.5 on the RNN weight matrices. The classifier has 1 hidden layer with 50 units.","We want to build a model that is robust and performs well on a wide variety of tasks. As a general rule, we use the same hyperparameter settings for all tasks, tuning them on the IMDb validation data, unless mentioned otherwise. Our model architecture is the AWD-LSTM language model from Merity et al. (2017a), with 400-dim embeddings, 3 hidden layers, 1150 hidden units per layer, and backpropagation through time batches of 70. For regularization, we use dropout of 0.4 on layers, 0.3 on RNN layers, 0.4 on input embeddings, 0.05 on word embeddings, and weight dropout of 0.5 on the RNN-to-RNN weight matrices. The classifier has a 50-unit hidden layer.","Our objective is a model that works well across many different tasks, not just one. Therefore, unless stated otherwise, we utilize the same hyperparameters for all tasks, tuning them on the IMDb validation set. The model is the AWD-LSTM language model from Merity et al. (2017a), which has 400-dimensional embeddings, 3 hidden layers, 1150 activations per hidden layer, and backpropagation through time batches of 70 time steps. For regularization, we use dropout of 0.4 on layers, 0.3 on RNN layers, 0.4 on input embeddings, 0.05 on word vectors, and weight dropout of 0.5 on the RNN weight matrices. The classifier contains a 50-unit hidden layer.",A,1
Universal Language Model Fine-tuning for Text Classification,"For consistency, we report all results as error rates (lower is better). We show the test error rates on the IMDb and TREC-6 datasets used by McCann et al. (2017) in Table 2. Our method outperforms both CoVe, a state-of-the-art transfer learning method based on hypercolumns, as well as the state-of-the-art on both datasets. On IMDb, we reduce the error dramatically by 43.9% and 22% with regard to CoVe and the state-of-the-art respectively. This is promising as the existing state-of-the-art requires complex architectures (Peters et al., 2018), multiple forms of attention (McCann et al., 2017) and sophisticated embedding schemes (Johnson and Zhang, 2016), while our method employs a regular LSTM with dropout.","For consistency, we present all findings as error percentages (lower is superior). We display the test error percentages on the IMDb and TREC-6 data sets utilized by McCann et al. (2017) in Table 2. Our approach surpasses both CoVe, a cutting-edge transfer learning technique founded on hypercolumns, and the state-of-the-art on both data sets. On IMDb, we dramatically cut the error by 43.9% and 22% compared to CoVe and the state-of-the-art respectively. This is encouraging as the current state-of-the-art necessitates intricate architectures (Peters et al., 2018), multiple attention forms (McCann et al., 2017) and refined embedding systems (Johnson and Zhang, 2016), while our technique uses a regular LSTM with dropout.","To be consistent, we communicate all outcomes as error rates (lower is superior). We exhibit the test error rates on the IMDb and TREC-6 collections used by McCann et al. (2017) in Table 2. Our approach outperforms both CoVe, an advanced transfer learning approach based on hypercolumns, and the state-of-the-art on both collections. On IMDb, we dramatically reduce the error by 43.9% and 22% relative to CoVe and the state-of-the-art respectively. This is encouraging as the current state-of-the-art needs complex architectures (Peters et al., 2018), multiple attention forms (McCann et al., 2017) and refined embedding systems (Johnson and Zhang, 2016), while our approach uses a standard LSTM with dropout.","For consistency, we report all outcomes as error percentages (lower is better). We display the test error percentages on the IMDb and TREC-6 data sets used by McCann et al. (2017) in Table 2. Our method surpasses both CoVe, a leading-edge transfer learning technique based on hypercolumns, and the state-of-the-art on both data sets. On IMDb, we significantly decrease the error by 43.9% and 22% relative to CoVe and the state-of-the-art respectively. This is promising as the existing state-of-the-art requires intricate architectures (Peters et al., 2018), multiple attention forms (McCann et al., 2017) and sophisticated embedding schemes (Johnson and Zhang, 2016), while our method uses a regular LSTM with dropout.",A,1
Universal Language Model Fine-tuning for Text Classification,"We note that the language model fine-tuning approach of Dai and Le (2015) only achieves an error of 7.64 vs. 4.6 for our method on IMDb, demonstrating the benefit of transferring knowledge from a large ImageNet-like corpus using our fine-tuning techniques. IMDb in particular is reflective of real world datasets: Its documents are generally a few paragraphs long—similar to emails (e.g for legal discovery) and online comments (e.g for community management); and sentiment analysis is similar to many commercial applications, e.g. product response tracking and support email routing. On TREC-6, our improvement—similar as the improvements of state-of-the-art approaches—is not statistically significant, due to the small size of the 500-examples test set.","We point out that the language model tuning method from Dai and Le (2015) only reaches a mistake rate of 7.64 vs 4.6 for our technique on IMDb. This shows the benefit of moving knowledge from a large ImageNet-like collection using our tuning procedures. IMDb specifically reflects real world data sets: Its documents usually contain a few paragraphs - like emails (e.g. for legal investigation) and online remarks (e.g. for community administration). Also, sentiment analysis is similar to many business uses, e.g. product feedback tracking and support email routing. On TREC-6, our enhancement - similar to improvements of state-of-the-art methods - is not statistically significant, because of the small 500-example test set size.","We indicate that the language model adaptation approach by Dai and Le (2015) only accomplishes an error of 7.64 vs 4.6 for our approach on IMDb. This demonstrates the advantage of transferring expertise from a large ImageNet-like library using our adaptation techniques. IMDb is illustrative of real world data sets: Its documents generally contain a few paragraphs - comparable to emails (for instance legal investigation) and online comments (for instance community moderation). Also, sentiment analysis is similar to many commercial uses, for instance product response tracking and support email routing. On TREC-6, our improvement - similar to enhancements of cutting edge approaches - is not statistically significant, due to the small 500-example test set size.  ","We highlight that the language model fine-tuning method by Dai and Le (2015) only realizes an error of 7.64 vs 4.6 for our method on IMDb. This exhibits the benefit of transferring knowledge from a large ImageNet-like collection using our fine-tuning techniques. IMDb is reflective of real world data sets: Its documents usually comprise a few paragraphs - akin to emails (e.g. for legal discovery) and online remarks (e.g. for community administration). Additionally, sentiment analysis is similar to many business applications, e.g. product feedback tracking and support email routing. On TREC-6, our enhancement - similar to improvements of state-of-the-art methods - is not statistically significant, owing to the small 500-example test set size.",A,1
Universal Language Model Fine-tuning for Text Classification,"Nevertheless, the competitive performance on TREC-6 demonstrates that our model performs well across different dataset sizes and can deal with examples that range from single sentences—in the case of TREC-6— to several paragraphs for IMDb. Note that despite pretraining on more than two orders of magnitude less data than the 7 million sentence pairs used by McCann et al. (2017), we consistently outperform their approach on both datasets. We show the test error rates on the larger AG, DBpedia, Yelp-bi, and Yelp-full datasets in Table 3. Our method again outperforms the state-of-the-art significantly. On AG, we observe a similarly dramatic error reduction by 23.7% compared to the state-of-the-art. On DBpedia, Yelp-bi, and Yelp-full, we reduce the error by 4.8%, 18.2%, 2.0% respectively.","Despite this, the competitive results on TREC-6 show that our model is effective across different dataset sizes and can manage examples ranging from single sentences in TREC-6 to multiple paragraphs in IMDb. Even though we pre-trained on over two orders of magnitude less data than the 7 million sentence pairs used by McCann et al. (2017), we consistently surpass their approach on both datasets. We display the test error rates on the larger AG, DBpedia, Yelp-bi, and Yelp-full datasets in Table 3. Our method again significantly outperforms the state-of-the-art. On AG, we see a similarly remarkable error reduction of 23.7% compared to the state-of-the-art. On DBpedia, Yelp-bi, and Yelp-full, we decrease the error by 4.8%, 18.2%, 2.0% respectively.","However, the competitive results on TREC-6 indicate that our model is successful across different dataset sizes and can handle examples ranging from single sentences in TREC-6 to multiple paragraphs in IMDb. Although we pre-trained on over two orders of magnitude less data than the 7 million sentence pairs used by McCann et al. (2017), we consistently exceed their approach on both datasets. We present the test error rates on the larger AG, DBpedia, Yelp-bi, and Yelp-full datasets in Table 3. Our method again substantially surpasses the state-of-the-art. On AG, we observe a similarly remarkable error reduction of 23.7% compared to the state-of-the-art. On DBpedia, Yelp-bi, and Yelp-full, we decrease the error by 4.8%, 18.2%, 2.0% respectively.  ","Still, the competitive performance on TREC-6 shows that our model is effective across different dataset sizes and can manage examples ranging from single sentences in TREC-6 to multiple paragraphs in IMDb. Even though we pre-trained on over two orders of magnitude less data than the 7 million sentence pairs used by McCann et al. (2017), we consistently outdo their approach on both datasets. We present the test error rates on the larger AG, DBpedia, Yelp-bi, and Yelp-full datasets in Table 3. Our method again significantly exceeds the state-of-the-art. On AG, we see a similarly remarkable error reduction of 23.7% compared to the state-of-the-art. On DBpedia, Yelp-bi, and Yelp-full, we decrease the error by 4.8%, 18.2%, 2.0% respectively.",A,1
Universal Language Model Fine-tuning for Text Classification,"In order to assess the impact of each contribution, we perform a series of analyses and ablations. We run experiments on three corpora, IMDb, TREC6, and AG that are representative of different tasks, genres, and sizes. For all experiments, we split off 10% of the training set and report error rates on this validation set with unidirectional LMs. We fine-tune the classifier for 50 epochs and train all methods but ULMFiT with early stopping.","To evaluate the effect of each part, we conduct multiple examinations and removals. We do tests on three groups of data, IMDb, TREC6, and AG which are typical of various tasks, styles, and sizes. For all tests, we separate 10% of the training set and document mistake percentages on this confirmation set with one-directional LMs. We adjust the classifier for 50 epochs and prepare all techniques except ULMFiT with early stopping.","In order to gauge the influence of each contribution, we perform a series of analyses and eliminations. We execute trials on three collections, IMDb, TREC6, and AG which represent different objectives, types, and magnitudes. For all trials, we detach 10% of the training set and document error rates on this validation set with one-way LMs. We fine-tune the classifier for 50 epochs and train all approaches excluding ULMFiT with early halting.","To assess the impact of each part, we do multiple reviews and removals. We conduct experiments on three groups, IMDb, TREC6, and AG that typify various tasks, styles, and sizes. For all experiments, we separate 10% of the training set and report mistake percentages on this confirmation set with single-direction LMs. We adjust the classifier for 50 epochs and prepare all methods except ULMFiT with early ending.",A,1
Universal Language Model Fine-tuning for Text Classification,"One of the main benefits of transfer learning is being able to train a model for a task with a small number of labels. We evaluate ULMFiT on different numbers of labeled examples in two settings: only labeled examples are used for LM fine-tuning (‘supervised’); and all task data is available and can be used to fine-tune the LM (‘semi-supervised’). We compare ULMFiT to training from scratch—which is necessary for hypercolumn-based approaches. We split off balanced fractions of the training data, keep the validation set fixed, and use the same hyperparameters as before. We show the results in Figure 3.","Transfer learning allows models to be trained for tasks even when there are only a few labeled examples available. We tested ULMFiT with different amounts of labeled data in two scenarios: using only the labeled data to fine-tune the language model (supervised) and using all the task data, both labeled and unlabeled, to fine-tune the language model (semi-supervised). We compared ULMFiT to training a model from scratch, which is required for approaches based on hypercolumns. We held out balanced portions of the training data while keeping the validation set the same, and used the same hyperparameters as before. The results are shown in Figure 3.","One major advantage of transfer learning is the ability to train a model for a task when there are limited labeled examples. We evaluated ULMFiT using varying numbers of labeled examples in two settings: utilizing only the labeled data for language model fine-tuning (supervised) and leveraging all task data, labeled and unlabeled, for language model fine-tuning (semi-supervised). We compared ULMFiT against training from scratch, which hypercolumn-based approaches need. We separated out balanced fractions of the training data while keeping the validation set unchanged, and used the same hyperparameters. The results are presented in Figure 3.  ","Transfer learning lets you train a model for a task even with few labeled examples available. We tested ULMFiT on different amounts of labeled data in two scenarios: using just the labeled data to fine-tune the language model (supervised) and using all task data, labeled and unlabeled, to fine-tune the language model (semi-supervised). We compared ULMFiT to training a model from scratch, which hypercolumn approaches require. We held out balanced parts of the training data while keeping the validation set constant, and used the same hyperparameters. The results are shown in Figure 3.",A,1
Universal Language Model Fine-tuning for Text Classification,"On IMDb and AG, supervised ULMFiT with only 100 labeled examples matches the performance of training from scratch with 10× and 20× more data respectively, clearly demonstrating the benefit of general-domain LM pretraining. If we allow ULMFiT to also utilize unlabeled examples (50k for IMDb, 100k for AG), at 100 labeled examples, we match the performance of training from scratch with 50× and 100× more data on AG and IMDb respectively. On TREC-6, ULMFiT significantly improves upon training from scratch; as examples are shorter and fewer, supervised and semi-supervised ULMFiT achieve similar results.","On the movie review websites IMDb and AG, the ULMFiT model that was pre-trained on general text data and then fine-tuned with only 100 labeled examples performed just as well as models trained from scratch with 10 and 20 times more training data. This clearly shows the benefit of pre-training language models on unlabeled general domain text before fine-tuning them for a specific task. When ULMFiT was also allowed to use 50,000 and 100,000 unlabeled examples from IMDb and AG respectively, with just 100 labeled examples it matched the performance of models trained from scratch with 50 and 100 times more labeled data. On the question answering dataset TREC-6, ULMFiT significantly outperformed training from scratch. Since the TREC examples are shorter with less data, the supervised and semi-supervised versions of ULMFiT achieved similar results.","The ULMFiT model pre-trained on generic text and fine-tuned with only 100 labeled samples from the movie websites IMDb and AG performed just as well as models built from scratch with 10 and 20 times more training data. This demonstrates the clear advantage of pre-training language models on unlabeled general domain text before adapting them to a specific task. When ULMFiT also utilized 50,000 and 100,000 unlabeled examples from IMDb and AG, with 100 labeled samples it equaled models trained from scratch with 50 and 100 times more labeled data. On the question answering dataset TREC-6, ULMFiT greatly outperformed training from scratch. Since TREC examples are shorter with less data, supervised and semi-supervised ULMFiT had similar results.  ","On the movie review sites IMDb and AG, the ULMFiT model that was first pre-trained on generic text then fine-tuned with only 100 labeled samples matched the performance of models trained from scratch with 10 and 20 times more data. This shows the clear benefit of pre-training language models on unlabeled general domain text before tailoring them to a specific task. When ULMFiT also used 50,000 and 100,000 unlabeled examples from IMDb and AG, with 100 labeled samples it equaled models trained from scratch with 50 and 100 times more labeled data. On the question answering dataset TREC-6, ULMFiT greatly surpassed training from scratch. Since TREC examples are shorter with less data, supervised and semi-supervised ULMFiT had comparable results.",A,1
Universal Language Model Fine-tuning for Text Classification,"We compare using no pretraining with pretraining on WikiText-103 (Merity et al., 2017b) in Table 4. Pretraining is most useful for small and medium-sized datasets, which are most common in commercial applications. However, even for large datasets, pretraining improves performance. In order to gauge the importance of choosing an appropriate LM, we compare a vanilla LM with the same hyperparameters without any dropout with the AWD-LSTM LM with tuned dropout parameters in Table 5. Using our fine-tuning techniques, even a regular LM reaches surprisingly good performance on the larger datasets. On the smaller TREC-6, a vanilla LM without dropout runs the risk of overfitting, which decreases performance.","We make a comparison between not using any pretraining versus using pretraining on WikiText-103 (Merity et al., 2017b) in Table 4. Pretraining is most beneficial for small and medium datasets, which are very common in business uses. However, even for large datasets, pretraining improves the results. To measure the importance of picking a suitable LM, we compare a basic LM with the same hyperparameters but no dropout to the AWD-LSTM LM with optimized dropout settings in Table 5. Using our fine-tuning methods, even a normal LM without dropout achieves very good performance on the larger datasets. On the smaller TREC-6, a vanilla LM without dropout risks overfitting, which lowers performance.","We do a comparison of not pretraining at all versus pretraining using WikiText-103 (Merity et al., 2017b) in Table 4. Pretraining helps the most for small and medium-size datasets, which are very prevalent in real-world applications. But even for large datasets, pretraining improves the outcomes. To gauge the significance of selecting a good LM, we compare a standard LM with the same hyperparameters but no dropout to the AWD-LSTM LM with tuned dropout parameters in Table 5. Using our fine-tuning techniques, even a regular LM without dropout accomplishes surprisingly strong performance on the bigger datasets. On the smaller TREC-6, a basic LM without dropout risks overfitting, which worsens performance.","We make a comparison between not doing any pretraining and doing pretraining using WikiText-103 (Merity et al., 2017b) in Table 4. Pretraining is most beneficial for small and medium datasets, which are very common in practical applications. However, even for large datasets, pretraining boosts the performance. To assess the importance of picking a suitable LM, we compare a vanilla LM with the same hyperparameters without dropout to the AWD-LSTM LM with optimized dropout settings in Table 5. Using our fine-tuning methods, even a regular LM without dropout achieves very good results on the larger datasets. On the smaller TREC-6, a basic LM without dropout risks overfitting, which decreases the performance.",A,1
Universal Language Model Fine-tuning for Text Classification,"We have proposed ULMFiT, an effective and extremely sample-efficient transfer learning method that can be applied to any NLP task. We have also proposed several novel fine-tuning techniques that in conjunction prevent catastrophic forgetting and enable robust learning across a diverse range of tasks. Our method significantly outperformed existing transfer learning techniques and the state-of-the-art on six representative text classification tasks. We hope that our results will catalyze new developments in transfer learning for NLP.","We have put forward ULMFiT, an efficacious and tremendously sample-efficient transfer learning technique that is applicable to all natural language processing objectives. We have also suggested multiple original fine-tuning methods that together impede catastrophic forgetting and make possible sturdy learning over a broad scope of assignments. Our approach substantially surpassed prevailing transfer learning techniques and the state-of-the-art on six prototypical text classification challenges. We aspire that our outcomes will stimulate novel advancements in transfer learning for NLP.","We have introduced ULMFiT, a potent and extremely sample-thrifty transfer learning procedure that can be utilized for any NLP goal. Additionally, we have brought forward numerous innovative fine-tuning strategies that collectively prevent catastrophic forgetting and enable robust acquisition across a wide variety of tasks. Our methodology significantly outstripped existing transfer learning approaches and the state-of-the-art on six representative text categorization problems. We are hopeful that our findings will galvanize new developments in transfer learning for natural language processing.  ","We have presented ULMFiT, an effectual and tremendously sample-economical transfer learning technique applicable to all natural language handling aims. We have also tabled several novel fine-tuning protocols that together forbid catastrophic forgetting and make feasible sturdy acquisition over a diverse set of assignments. Our procedure substantially exceeded prevailing transfer learning methodologies and the state-of-the-art on six archetypal text classification challenges. We are optimistic our outputs will catalyze novel advancements in transfer learning for NLP.",A,1
VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION,"In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small ( 3 × 3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16–19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results.","In this research, we examine how the depth of convolutional networks impacts their precision in large-scale image recognition. Our primary contribution is a comprehensive assessment of networks with increasing depth using an architecture with very small (3 × 3) convolution filters. This demonstrates that major improvements over prior setups can be achieved by increasing the depth to 16-19 weight layers. These discoveries formed the basis for our ImageNet Challenge 2014 submission, where our team earned the first and second places in the localization and classification tracks respectively. We also illustrate that our representations generalize effectively to other data sets, where they produce state-of-the-art outcomes.","This work investigates how the number of layers in convolutional networks affects their accuracy in large-scale image identification. We thoroughly test networks of varying depth using a model with tiny (3 x 3) convolution filters. We find pushing the depth to 16-19 weight layers significantly improves on previous configurations. Our ImageNet Challenge 2014 submission using these findings won first and second place in the localization and classification tracks. Our representations also achieve state-of-the-art results when applied to other datasets, showing they generalize well.  ","In this study, we analyze how convolutional network depth impacts performance on large-scale image recognition tasks. We extensively evaluate networks with increasing layers using an architecture with very small (3x3) convolution filters. We demonstrate that going deeper to 16-19 weight layers substantially outperforms previous setups. Our ImageNet Challenge 2014 submission leveraging these insights took first and second place in the localization and classification tracks. Our representations also produce state-of-the-art outcomes on other datasets, exhibiting strong generalization.",A,1
VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION,"Convolutional networks (ConvNets) have recently enjoyed a great success in large-scale image and video recognition (Krizhevsky et al., 2012; Zeiler & Fergus, 2013; Sermanet et al., 2014; Simonyan & Zisserman, 2014) which has become possible due to the large public image repositories, such as ImageNet (Deng et al., 2009), and high-performance computing systems, such as GPUs or large-scale distributed clusters (Dean et al., 2012). In particular, an important role in the advance of deep visual recognition architectures has been played by the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC) (Russakovsky et al., 2014), which has served as a testbed for a few generations of large-scale image classification systems, from high-dimensional shallow feature encodings (Perronnin et al., 2010) (the winner of ILSVRC-2011) to deep ConvNets (Krizhevsky et al., 2012) (the winner of ILSVRC-2012).","Recently, convolutional neural networks (ConvNets) have achieved great success in large-scale image and video recognition tasks (Krizhevsky et al., 2012; Zeiler & Fergus, 2013; Sermanet et al., 2014; Simonyan & Zisserman, 2014). This breakthrough has become feasible thanks to the availability of large public image datasets like ImageNet (Deng et al., 2009), and high-performance computing systems like GPUs or large distributed clusters (Dean et al., 2012). Specifically, the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC) (Russakovsky et al., 2014) has played an important role in advancing deep visual recognition architectures. ILSVRC has served as a testbed for several generations of large-scale image classification systems, from high-dimensional shallow feature encodings (Perronnin et al., 2010) (the ILSVRC-2011 winner) to deep ConvNets (Krizhevsky et al., 2012) (the ILSVRC-2012 winner).","In recent years, convolutional neural networks (ConvNets) have achieved tremendous success in large-scale image and video recognition tasks (Krizhevsky et al., 2012; Zeiler & Fergus, 2013; Sermanet et al., 2014; Simonyan & Zisserman, 2014). This has become feasible due to the availability of large public image datasets like ImageNet (Deng et al., 2009), and high-performance computing systems like GPUs or large distributed clusters (Dean et al., 2012). In particular, the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC) (Russakovsky et al., 2014) has played a crucial role in advancing deep visual recognition architectures. ILSVRC has served as a testing ground for several generations of large-scale image classification systems, from high-dimensional shallow feature encodings (Perronnin et al., 2010) (the ILSVRC-2011 winner) to deep ConvNets (Krizhevsky et al., 2012) (the ILSVRC-2012 winner).  ","In recent times, convolutional neural networks (ConvNets) have achieved immense success in large-scale image and video recognition tasks (Krizhevsky et al., 2012; Zeiler & Fergus, 2013; Sermanet et al., 2014; Simonyan & Zisserman, 2014). This has been made possible due to the availability of large public image datasets such as ImageNet (Deng et al., 2009), and high-performance computing systems like GPUs or large distributed clusters (Dean et al., 2012). Specifically, the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC) (Russakovsky et al., 2014) has played a pivotal role in advancing deep visual recognition architectures. ILSVRC has served as an experimental platform for several generations of large-scale image classification systems, from high-dimensional shallow feature encodings (Perronnin et al., 2010) (the ILSVRC-2011 winner) to deep ConvNets (Krizhevsky et al., 2012) (the ILSVRC-2012 winner).",A,1
VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION,"With ConvNets becoming more of a commodity in the computer vision field, a number of attempts have been made to improve the original architecture of Krizhevsky et al. (2012) in a bid to achieve better accuracy. For instance, the best-performing submissions to the ILSVRC2013 (Zeiler & Fergus, 2013; Sermanet et al., 2014) utilised smaller receptive window size and smaller stride of the first convolutional layer. Another line of improvements dealt with training and testing the networks densely over the whole image and over multiple scales (Sermanet et al., 2014; Howard, 2014).","As ConvNets have become more commonplace in computer vision, there have been numerous efforts to enhance the original ConvNet architecture from Krizhevsky et al. (2012) to get better results. For example, the top-scoring models for ILSVRC2013 (Zeiler & Fergus, 2013; Sermanet et al., 2014) used smaller receptive field size and stride for the first conv layer. Other advancements involved dense training and testing of networks across the full image and at multiple scales (Sermanet et al., 2014; Howard, 2014).","With convolutional neural networks turning into a standard tool in the computer vision domain, many tries have been made to improve on the original ConvNet design by Krizhevsky et al. (2012) to achieve superior accuracy. The best ILSVRC2013 submissions (Zeiler & Fergus, 2013; Sermanet et al., 2014) for instance used smaller receptive field dimensions and stride in the first conv layer. Other enhancements focused on dense training and testing of networks over the whole image and at multiple scales (Sermanet et al., 2014; Howard, 2014).  ","As convolutional neural networks have become more mainstream in computer vision, there have been numerous attempts to enhance the seminal ConvNet architecture from Krizhevsky et al. (2012) in hopes of obtaining better performance. For example, the top-scoring ILSVRC2013 entries (Zeiler & Fergus, 2013; Sermanet et al., 2014) employed smaller receptive field size and stride for the initial convolutional layer. Other improvements involved dense training and evaluation of networks across the entire image and across multiple scales (Sermanet et al., 2014; Howard, 2014).",A,1
VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION,"In this paper, we address another important aspect of ConvNet architecture design – its depth. To this end, we fix other parameters of the architecture, and steadily increase the depth of the network by adding more convolutional layers, which is feasible due to the use of very small ( 3 × 3) convolution filters in all layers. As a result, we come up with significantly more accurate ConvNet architectures, which not only achieve the state-of-the-art accuracy on ILSVRC classification and localisation tasks, but are also applicable to other image recognition datasets, where they achieve excellent performance even when used as a part of a relatively simple pipelines (e.g. deep features classified by a linear SVM without fine-tuning).","This research examines a critical component of ConvNet design - the network's depth. We hold other architectural parameters constant and incrementally increase depth by stacking more convolutional layers, enabled by very small (3x3) filters throughout. Our substantially deeper ConvNets attain state-of-the-art accuracy on ILSVRC classification and localization while generalizing well to other datasets with simple pipelines (e.g. deep features classified by linear SVM without fine-tuning).","In this study, we tackle a key aspect of ConvNet architecture - depth. Fixing other network parameters, we gradually grow depth by adding more convolutional layers, feasible thanks to tiny (3x3) filters everywhere. Our much deeper ConvNets not only achieve cutting-edge performance on ILSVRC classification and localization, but also transfer well to other image tasks using basic pipelines (like deep features with linear SVM and no fine-tuning).  ","Here we address depth, a critical ConvNet design choice. Holding other architecture factors steady, we increment depth by stacking more convolutional layers, viable given very small (3x3) filters throughout. Our substantially deeper ConvNets deliver state-of-the-art ILSVRC classification and localization accuracy while excelling on other image datasets using simple pipelines (e.g. deep features with linear SVM, no fine-tuning).",A,1
VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION,"We have released our two best-performing models 1 to facilitate further research. The rest of the paper is organised as follows. In Sect. 2, we describe our ConvNet configurations. The details of the image classification training and evaluation are then presented in Sect. 3, and the configurations are compared on the ILSVRC classification task in Sect. 4. Sect. 5 concludes the paper. For completeness, we also describe and assess our ILSVRC-2014 object localisation system in Appendix A, and discuss the generalisation of very deep features to other datasets in Appendix B. Finally, Appendix C contains the list of major paper revisions.","We have made public our top two models to enable more research. The remainder of the paper is structured like this. In Section 2, we explain our ConvNet setups. Section 3 then provides the specifics of the image classification training and testing, and Section 4 compares the configurations on the ILSVRC classification task. Section 5 wraps up the paper. For thoroughness, we also depict and evaluate our ILSVRC-2014 object localization system in Appendix A, and examine the generalization of very deep features to other data sets in Appendix B. Appendix C lists the major paper revisions.","We have released our two best-performing models to promote more research. The rest of the article is organized as follows. Section 2 describes our ConvNet architectures. Section 3 then presents the details of image classification training and assessment, and Section 4 compares the configurations on the ILSVRC classification task. Section 5 concludes the article. For completeness, we also portray and appraise our ILSVRC-2014 object localization system in Appendix A, and investigate the extension of very deep features to other data sets in Appendix B. Appendix C contains the list of major article revisions.  ","We have made public our top two models to facilitate additional research. The remainder of the manuscript is structured as follows. Section 2 delineates our ConvNet configurations. The specifics of image classification training and evaluation are then provided in Section 3, and Section 4 compares the configurations on the ILSVRC classification task. Section 5 concludes the manuscript. For thoroughness, we also depict and evaluate our ILSVRC-2014 object localization system in Appendix A, and examine the generalization of very deep features to other datasets in Appendix B. Appendix C enumerates the major manuscript revisions.",A,1
VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION,"To measure the improvement brought by the increased ConvNet depth in a fair setting, all our ConvNet layer configurations are designed using the same principles, inspired by Ciresan et al. (2011); Krizhevsky et al. (2012). In this section, we first describe a generic layout of our ConvNet configurations (Sect. 2.1) and then detail the specific configurations used in the evaluation (Sect. 2.2). Our design choices are then discussed and compared to the prior art in Sect. 2.3. During training, the input to our ConvNets is a fixed-size 224 × 224 RGB image.","To evaluate the benefits of deeper ConvNet architectures in an unbiased way, we designed all our ConvNet layers based on the same ideas from previous work by Ciresan et al. and Krizhevsky et al. First, we explain the general design of our ConvNets (Section 2.1). Then we specify the exact architectures used in our experiments (Section 2.2). We discuss our design decisions and compare to prior work in Section 2.3. Our ConvNets take 224 x 224 RGB images as input during training.","To fairly assess the gains from increased ConvNet depth, our ConvNet configurations adhere to common principles inspired by prior work like Ciresan et al. and Krizhevsky et al. We first outline the generic architecture of our ConvNets (Section 2.1). We then detail the precise configurations tested (Section 2.2). Our choices are analyzed and contrasted with previous approaches in Section 2.3. The input to our ConvNets during training is a 224 x 224 RGB image of fixed size.","To measure the improvements from deeper ConvNets neutrally, our ConvNet layers follow shared guidelines based on previous work by Ciresan et al. and Krizhevsky et al. We first describe the general form of our ConvNets (Section 2.1). We then specify the exact models used in experiments (Section 2.2). Our decisions are discussed and compared to past work in Section 2.3. Our ConvNets take 224 x 224 RGB images as input when training.",A,1
VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION,"The only preprocessing we do is subtracting the mean RGB value, computed on the training set, from each pixel. The image is passed through a stack of convolutional (conv.) layers, where we use filters with a very small receptive field: 3 × 3 (which is the smallest size to capture the notion of left/right, up/down, center). In one of the configurations we also utilise 1 × 1 convolution filters, which can be seen as a linear transformation of the input channels (followed by non-linearity). The convolution stride is fixed to 1 pixel; the spatial padding of conv. layer input is such that the spatial resolution is preserved after convolution, i.e. the padding is 1 pixel for 3 × 3 conv. layers.","The sole data preparation step is taking the average RGB value from the training images and subtracting that from each pixel. The image goes through multiple convolutional filters, using very small 3 x 3 receptive fields (the smallest size that can sense left/right, up/down, center directions). In one setup we also use 1 x 1 convolution filters, essentially linear transformations of the input channels (with a non-linearity after). The convolution stride is 1 pixel; the spatial padding of the input to convolutional layers preserves spatial resolution after convolution, i.e. 1 pixel padding for 3 x 3 filters.","The only data preprocessing is subtracting the mean RGB value computed on the training set from each pixel. The image passes through stacked convolutional layers, utilizing filters with a very small 3 x 3 receptive field (the smallest size that can capture notions of left/right, up/down, center). In one configuration we also use 1 x 1 convolution filters, which can be seen as linear transformations of the input channels (followed by non-linearity). The convolution stride is fixed at 1 pixel; the spatial padding of convolutional layer inputs preserves spatial resolution after convolution, meaning 1 pixel padding for 3 x 3 convolutional layers.  ","The sole data pre-processing step is subtracting the average RGB value, calculated on the training images, from each pixel. The image goes through a stack of convolutional layers, where very small 3 x 3 receptive field filters are used (the smallest size that can capture left/right, up/down, center notions). In one configuration we also use 1 x 1 convolution filters, which are essentially linear transformations of the input channels (with non-linearity after). The convolution stride is 1 pixel; the spatial padding of convolutional layer inputs is such that spatial resolution is maintained after convolution, i.e. 1 pixel padding for 3 x 3 convolutional filters.",A,1
VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION,"Spatial pooling is carried out by five max-pooling layers, which follow some of the conv. layers (not all the conv. layers are followed by max-pooling). Max-pooling is performed over a 2 × 2 pixel window, with stride 2. A stack of convolutional layers (which has a different depth in different architectures) is followed by three Fully-Connected (FC) layers: the first two have 4096 channels each, the third performs 1000- way ILSVRC classification and thus contains 1000 channels (one for each class). The final layer is the soft-max layer. The configuration of the fully connected layers is the same in all networks.","Spatial pooling is implemented through 5 max-pooling layers, which come after some of the convolutional layers (not all convolutional layers are followed by max-pooling). Max-pooling takes place over a 2 x 2 pixel window, with a stride of 2. A stack of convolutional layers (which has varying depth across architectures) is followed by 3 Fully-Connected (FC) layers: the first two have 4096 channels each, the third does 1000-way ILSVRC classification and thus has 1000 channels (one per class). The final layer is soft-max. The configuration of the fully connected layers is consistent across all networks.","Spatial pooling happens via 5 max-pooling layers, which go after certain conv. layers (not all conv. layers are trailed by max-pooling). Max-pooling operates on a 2 x 2 pixel window, with a stride of 2. A pile of convolutional layers (which has differing depth in various architectures) is followed by 3 Fully-Connected (FC) layers: the first two have 4096 channels apiece, the third conducts 1000-way ILSVRC categorization and hence contains 1000 channels (one for every class). The final layer is soft-max. The setup of the fully connected layers is the same in all networks. ","Spatial pooling is done through 5 max-pooling layers, which come after some of the conv. layers (not every conv. layer is followed by max-pooling). Max-pooling takes place across a 2 x 2 pixel window, with a stride of 2. A collection of convolutional layers (which has varying depth in different architectures) is followed by 3 Fully-Connected (FC) layers: the first two have 4096 channels each, the third does 1000-way ILSVRC classification and thus has 1000 channels (one per class). The final layer is soft-max. The configuration of the fully connected layers is identical across all networks.",A,1
VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION,"All hidden layers are equipped with the rectification (ReLU (Krizhevsky et al., 2012)) non-linearity. We note that none of our networks (except for one) contain Local Response Normalisation (LRN) normalisation (Krizhevsky et al., 2012): as will be shown in Sect. 4, such normalisation does not improve the performance on the ILSVRC dataset, but leads to increased memory consumption and computation time. Where applicable, the parameters for the LRN layer are those of (Krizhevsky et al., 2012).","Every single concealed neural network layer makes use of the rectified linear unit (ReLU) activation function (Krizhevsky et al., 2012). It should be noted that none of our neural networks (with one exception) utilize local response normalization (LRN) (Krizhevsky et al., 2012): as will be demonstrated in Section 4, this type of normalization does not enhance performance on the ILSVRC dataset, but does increase memory usage and computation time. When relevant, the parameters for the LRN layer match those from (Krizhevsky et al., 2012).","All hidden neural network layers contain the rectified linear unit (ReLU) non-linear activation function (Krizhevsky et al., 2012). We point out that local response normalization (LRN) (Krizhevsky et al., 2012) is not present in any of our neural networks (except one): Section 4 will show LRN does not improve accuracy on the ILSVRC dataset, but does increase memory and compute requirements. The parameters for the LRN layer, when used, are the same as in (Krizhevsky et al., 2012). ","Every single hidden neural network layer utilizes the rectified linear activation function (ReLU) (Krizhevsky et al., 2012). It is noted that local response normalization (LRN) (Krizhevsky et al., 2012) is absent from all our neural networks (with one exception): Section 4 will demonstrate LRN does not enhance performance on the ILSVRC dataset, but does increase memory usage and computation time. When applicable, the LRN layer parameters match those in (Krizhevsky et al., 2012).",A,1
VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION,"The ConvNet configurations, evaluated in this paper, are outlined in Table 1, one per column. In the following we will refer to the nets by their names (A–E). All configurations follow the generic design presented in Sect. 2.1, and differ only in the depth: from 11 weight layers in the network A (8 conv. and 3 FC layers) to 19 weight layers in the network E (16 conv. and 3 FC layers). The width of conv. layers (the number of channels) is rather small, starting from 64 in the first layer and then increasing by a factor of 2 after each max-pooling layer, until it reaches 512.","The ConvNet designs tested in this paper are shown in Table 1, with one design per column. We will refer to the networks by their names (A-E) going forward. All of the configurations follow the general architecture described in Section 2.1, and only vary in their depth, ranging from 11 weighted layers in network A (8 convolutional and 3 fully connected layers) to 19 weighted layers in network E (16 convolutional and 3 fully connected layers). The width of the convolutional layers (the number of channels) starts small at 64 in the first layer, then doubles after each max pooling layer until reaching 512.","The ConvNet models evaluated in this paper can be found in Table 1, with each column representing one model. We will use the names A through E when discussing these models. Every model uses the generic architecture from Section 2.1, only differing in how deep they are: Model A has 11 layers with weights (8 convolutional and 3 fully connected) while Model E has 19 weighted layers (16 convolutional and 3 fully connected). The width of the convolutional layers (number of channels) begins at 64 in the first layer, then doubles after each max pooling layer until reaching 512.  ","The designs of the ConvNets tested in this paper are shown in Table 1, one per column. We will refer to the networks using their names A-E moving forward. All of the designs follow the general design presented in Section 2.1, and only vary in their depth, ranging from 11 layers with weights in Network A (8 convolutional and 3 fully connected layers) to 19 weighted layers in Network E (16 convolutional and 3 fully connected layers). The width of the convolutional layers (number of channels) starts small with 64 channels in the first layer, then doubles after every max pooling layer until reaching 512 channels.",A,1
VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION,"Our ConvNet configurations are quite different from the ones used in the top-performing entries of the ILSVRC-2012 (Krizhevsky et al., 2012) and ILSVRC-2013 competitions (Zeiler & Fergus, 2013; Sermanet et al., 2014). Rather than using relatively large receptive fields in the first conv. layers (e.g. 11×11 with stride 4 in (Krizhevsky et al., 2012), or 7×7 with stride 2 in (Zeiler & Fergus, 2013; Sermanet et al., 2014)), we use very small 3 × 3 receptive fields throughout the whole net, which are convolved with the input at every pixel (with stride 1).","The structure of our Convolutional Neural Networks differs significantly from the top models in the ILSVRC-2012 (Krizhevsky et al., 2012) and ILSVRC-2013 (Zeiler & Fergus, 2013; Sermanet et al., 2014) competitions. Instead of utilizing fairly large receptive fields in the initial convolutional layers (for instance 11×11 with stride 4 in (Krizhevsky et al., 2012), or 7×7 with stride 2 in (Zeiler & Fergus, 2013; Sermanet et al., 2014)), we employ very small 3 × 3 receptive fields throughout the entire network, convolving them with the input at each pixel (with stride 1).","Our Convolutional Neural Network architectures are quite dissimilar from the best models in ILSVRC-2012 (Krizhevsky et al., 2012) and ILSVRC-2013 (Zeiler & Fergus, 2013; Sermanet et al., 2014). Rather than having relatively large receptive fields in the first convolution layers (such as 11×11 with stride 4 in (Krizhevsky et al., 2012), or 7×7 with stride 2 in (Zeiler & Fergus, 2013; Sermanet et al., 2014)), we utilize very small 3 × 3 receptive fields in all layers, convolving them with the input at each pixel (with stride 1). ","The designs of our Convolutional Neural Networks are very different from the top-ranked models in ILSVRC-2012 (Krizhevsky et al., 2012) and ILSVRC-2013 (Zeiler & Fergus, 2013; Sermanet et al., 2014) competitions. Instead of employing fairly large receptive fields in the initial conv layers (for example 11×11 with stride 4 in (Krizhevsky et al., 2012), or 7×7 with stride 2 in (Zeiler & Fergus, 2013; Sermanet et al., 2014)), we use very small 3 × 3 receptive fields throughout the network, sliding them over the input at each pixel (with stride 1).",A,1
VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION,"It is easy to see that a stack of two 3×3 conv. layers (without spatial pooling in between) has an effective receptive field of 5×5; three such layers have a 7 × 7 effective receptive field. So what have we gained by using, for instance, a stack of three 3×3 conv. layers instead of a single 7×7 layer? First, we incorporate three non-linear rectification layers instead of a single one, which makes the decision function more discriminative. Second, we decrease the number of parameters: assuming that both the input and the output of a three-layer 3 × 3 convolution stack has C channels, the stack is parametrised by 3 3 2C 2  = 27C 2 weights; at the same time, a single 7 × 7 conv. layer would require 7 2C 2 = 49C 2 parameters, i.e. 81% more.","It's evident that a pile of two 3x3 convolutional layers (without spatial pooling between them) has a working receptive field of 5x5; three such layers have a 7x7 effective receptive field. So what have we profited by utilizing, for example, a pile of three 3x3 conv. layers rather than a solitary 7x7 layer? To begin with, we join three non-direct rectification layers rather than a solitary one, which makes the choice work more discriminative. Besides, we decrease the quantity of boundaries: expecting that both the info and the yield of a three-layer 3x3 convolution stack has C channels, the stack is parameterized by 3x3x2C^2 = 27C^2 loads; simultaneously, a solitary 7x7 conv. layer would require 7^2C^2 = 49C^2 parameters, i.e. 81% more.","It's clear that a set of two 3x3 conv. layers (without spatial pooling between) has a receptive field of 5x5; three such layers have a 7x7 receptive field. So what have we gained by using, for example, a set of three 3x3 conv. layers rather than one 7x7 layer? First, we include three non-linear rectification layers rather than one, making the decision function more discriminative. Second, we reduce the parameter count: assuming both input and output of a three-layer 3x3 convolution stack have C channels, the stack has 3x3x2C^2 = 27C^2 weights; a single 7x7 conv. layer needs 7^2C^2 = 49C^2 parameters, 81% more.  ","Evidently, a collection of two 3x3 convolutional layers (with no spatial pooling between) possesses a receptive field of 5x5; three such layers possess a 7x7 receptive field. Therefore, what have we accomplished by employing, for example, a collection of three 3x3 conv. layers rather than a single 7x7 layer? Initially, we incorporate three non-linear rectification layers rather than one, rendering the decision function more discriminative. Additionally, we decrease the parameter quantity: assuming both input and output of a three-layer 3x3 convolution collection have C channels, the collection possesses 3x3x2C^2 = 27C^2 weights; a single 7x7 conv. layer necessitates 7^2C^2 = 49C^2 parameters, 81% more.",A,1
VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION,"This can be seen as imposing a regularisation on the 7 × 7 conv. filters, forcing them to have a decomposition through the 3 × 3 filters (with non-linearity injected in between). The incorporation of 1 × 1 conv. layers (configuration C, Table 1) is a way to increase the nonlinearity of the decision function without affecting the receptive fields of the conv. layers. Even though in our case the 1 × 1 convolution is essentially a linear projection onto the space of the same dimensionality (the number of input and output channels is the same), an additional non-linearity is introduced by the rectification function. It should be noted that 1×1 conv. layers have recently been utilised in the “Network in Network” architecture of Lin et al. (2014).","This can be viewed as enforcing a regularization on the 7 × 7 convolution filters, compelling them to have a decomposition through the 3 × 3 filters (with non-linearity added in between). The inclusion of 1 × 1 convolution layers (configuration C, Table 1) is a technique to boost the non-linearity of the decision function without changing the receptive fields of the convolution layers. Although in our case the 1 × 1 convolution is basically a linear projection onto the space of the same dimensionality (the input and output channel numbers are the same), extra non-linearity is introduced by the rectifier function. It should be noted that 1×1 convolution layers have recently been used in the ""Network in Network"" architecture of Lin et al. (2014).","This can be considered as imposing a constraint on the 7 × 7 convolutional filters, requiring them to have a factorization through the 3 × 3 filters (with nonlinearity inserted in between). The addition of 1 × 1 convolutional layers (configuration C, Table 1) is a way to increase the nonlinear nature of the decision function without modifying the receptive fields of the convolutional layers. Even though in our case the 1 × 1 convolution is fundamentally a linear projection onto the space of the same dimension (the input and output channel numbers are the same), supplementary nonlinearity is introduced by the rectifier function. It should be noted that 1×1 convolutional layers have recently been utilized in the ""Network in Network"" architecture of Lin et al. (2014).  ","This can be viewed as enforcing a structure on the 7 × 7 convolutional filters, necessitating them to have a decomposition via the 3 × 3 filters (with nonlinearity added between). The incorporation of 1 × 1 convolutional layers (configuration C, Table 1) is a technique to amplify the nonlinear nature of the decision function without altering the receptive fields of the convolutional layers. Despite the fact that in our case the 1 × 1 convolution is essentially a linear projection onto the space of the same dimension (the input and output channel numbers are identical), extra nonlinearity is introduced by the rectifier function. It should be noted that 1×1 convolutional layers have recently been employed in the ""Network in Network"" architecture of Lin et al. (2014).",A,1
VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION," Small-size convolution filters have been previously used by Ciresan et al. (2011), but their nets are significantly less deep than ours, and they did not evaluate on the large-scale ILSVRC dataset. Goodfellow et al. (2014) applied deep ConvNets (11 weight layers) to the task of street number recognition, and showed that the increased depth led to better performance. GoogLeNet (Szegedy et al., 2014), a top-performing entry of the ILSVRC-2014 classification task, was developed independently of our work, but is similar in that it is based on very deep ConvNet (22 weight layers) and small convolution filters (apart from 3 × 3, they also use 1 × 1 and 5 × 5 convolutions). ","Previously, Ciresan and colleagues (2011) utilized small-sized convolution filters, however their networks were far less deep compared to ours. Moreover, they did not assess performance on the large-scale ILSVRC dataset. Goodfellow's group (2014) applied deep convolutional neural networks (11 weight layers) to street number recognition, demonstrating improved performance with greater depth. Separate from our work, GoogLeNet (Szegedy et al., 2014) was a top performer in the ILSVRC-2014 classification task. Similar to our approach, GoogLeNet relies on an extremely deep convolutional network (22 weight layers) and small convolution filters (not only 3x3 but also 1x1 and 5x5 convolutions).  ","In prior work, Ciresan and co-authors (2011) used small convolution filters, but their networks were much shallower than ours and they did not test on the large ILSVRC dataset. Goodfellow and colleagues (2014) applied deep convolutional neural networks (11 weight layers) to street number recognition and showed increased depth led to better performance. Developed independently from our work, GoogLeNet (Szegedy et al., 2014), a top performer in the ILSVRC-2014 classification task, was alike in using very deep convolutional networks (22 weight layers) and small convolution filters (along with 3x3, they utilized 1x1 and 5x5 convolutions).","Previously, Ciresan and team (2011) employed small convolution filters, however their networks were far less deep versus ours, and they did not evaluate using the substantial ILSVRC dataset. Goodfellow's group (2014) leveraged deep convolutional neural networks (11 weight layers) for street number recognition, demonstrating enhanced performance with greater depth. Separately from our efforts, GoogLeNet (Szegedy et al., 2014), a top finisher in the ILSVRC-2014 classification task, was similar in leveraging very deep convolutional networks (22 weight layers) and small convolution filters (not just 3x3 but also 1x1 and 5x5 convolutions).",A,1
VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION,"Their network topology is, however, more complex than ours, and the spatial resolution of the feature maps is reduced more aggressively in the first layers to decrease the amount of computation. As will be shown in Sect. 4.5, our model is outperforming that of Szegedy et al. (2014) in terms of the single-network classification accuracy. The ConvNet training procedure generally follows Krizhevsky et al. (2012) (except for sampling the input crops from multi-scale training images, as explained later).","However, their network design is more complicated than our own, and the detail of the feature maps is decreased more strongly in the initial layers to reduce the amount of processing needed. As described in Section 4.5, our model surpasses the one by Szegedy et al. (2014) regarding the classification accuracy of a single network. Our ConvNet training process is generally similar to Krizhevsky et al. (2012) (excluding sampling the input crops from training images of multiple sizes, which is clarified later).","Nonetheless, their network structure is more intricate than our own, and the spatial clarity of the feature maps is reduced more aggressively in the preliminary tiers to minimize the quantity of calculations. As demonstrated in Part 4.5, our archetype is outshining that of Szegedy et al. (2014) regarding the individual-system categorization precision. The ConvNet preparation routine broadly shadows Krizhevsky et al. (2012) (leaving out testing the contribution harvests from multi-scale preparation pictures, as explained subsequently).  ","However, their network layout is more complicated than our own, and the spatial distinctness of the feature diagrams is decreased more forcefully in the initial layers to reduce the measure of processing. As will be exhibited in Area 4.5, our model is exceeding expectations in comparison to Szegedy et al. (2014) as far as the single-organization order exactness. The ConvNet preparing system for the most part takes after Krizhevsky et al. (2012) (beside testing the info crops from multi-scale preparing pictures, as clarified later).",A,1
VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION,"Namely, the training is carried out by optimising the multinomial logistic regression objective using mini-batch gradient descent (based on back-propagation (LeCun et al., 1989)) with momentum. The batch size was set to 256, momentum to 0.9. The training was regularised by weight decay (the L2 penalty multiplier set to 5 · 10−4 ) and dropout regularisation for the first two fully-connected layers (dropout ratio set to 0.5). The learning rate was initially set to 10−2 , and then decreased by a factor of 10 when the validation set accuracy stopped improving. In total, the learning rate was decreased 3 times, and the learning was stopped after 370K iterations (74 epochs).","Specifically, the training utilizes mini-batch gradient descent with momentum to optimize the multinomial logistic regression objective (based on backpropagation (LeCun et al., 1989)). The batch size was 256, momentum was 0.9. Weight decay regularization (L2 penalty multiplier of 5 x 10^-4) and dropout regularization for the first two fully-connected layers (dropout ratio of 0.5) were used to regularize the training. The initial learning rate was 10^-2, then decreased by a factor of 10 when validation set accuracy plateaued. The learning rate decreased 3 times total, and training stopped after 370K iterations (74 epochs).","In particular, the training minimizes the multinomial logistic regression loss using mini-batch gradient descent with momentum (utilizing backpropagation (LeCun et al., 1989)). The batch size was 256, momentum was 0.9. Regularization was done with weight decay (L2 penalty multiplier of 5 x 10^-4) and dropout on the first two fully-connected layers (dropout ratio of 0.5). The initial learning rate was 10^-2, then reduced by a factor of 10 when validation accuracy stopped improving. The learning rate reduced 3 times total, and training ended after 370K iterations (74 epochs).","Specifically, the multinomial logistic regression loss was minimized via mini-batch gradient descent with momentum (leveraging backpropagation (LeCun et al., 1989)). The batch size was 256, momentum was 0.9. Regularization utilized weight decay (L2 penalty of 5 x 10^-4) and dropout for the first two fully-connected layers (dropout rate of 0.5). The learning rate started at 10^-2, then was reduced by 10x when validation accuracy plateaued. The learning rate was reduced 3 times total, and training finished after 370K iterations (74 epochs).",A,1
VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION,"We conjecture that in spite of the larger number of parameters and the greater depth of our nets compared to (Krizhevsky et al., 2012), the nets required less epochs to converge due to (a) implicit regularisation imposed by greater depth and smaller conv. filter sizes; (b) pre-initialisation of certain layers. The initialisation of the network weights is important, since bad initialisation can stall learning due to the instability of gradient in deep nets. To circumvent this problem, we began with training the configuration A (Table 1), shallow enough to be trained with random initialisation. Then, when training deeper architectures, we initialised the first four convolutional layers and the last three fully connected layers with the layers of net A (the intermediate layers were initialised randomly).","We hypothesize that despite the larger quantity of parameters and greater depth of our neural networks compared to (Krizhevsky et al., 2012), the networks needed fewer epochs to converge because of (a) implicit regularisation imposed by greater depth and smaller convolutional filter sizes; (b) pre-initialisation of certain layers. The initialisation of the network weights is crucial, since poor initialisation can impede learning due to the instability of gradient in deep nets. To avoid this issue, we started by training the configuration A (Table 1), shallow enough to be trained with random initialisation. Then, when training deeper architectures, we initialised the first four convolutional layers and the last three fully connected layers with the layers of net A (the intermediate layers were initialised randomly).","Our conjecture is that even with the larger number of parameters and increased depth of our neural networks versus (Krizhevsky et al., 2012), the networks required fewer epochs to converge owing to (a) implicit regularisation from greater depth and smaller convolutional filter sizes; (b) pre-initialisation of some layers. The initialisation of the network weights is important, because bad initialisation can hinder learning due to gradient instability in deep nets. To get around this problem, we began by training configuration A (Table 1), shallow enough for random initialisation. Then, when training deeper architectures, we initialised the first four convolutional layers and last three fully connected layers with net A's layers (the intermediate layers were randomly initialised).  ","We posit that notwithstanding the greater quantity of parameters and increased depth of our neural networks compared to (Krizhevsky et al., 2012), the networks needed fewer epochs to converge due to (a) implicit regularisation from greater depth and smaller convolutional filter sizes; (b) pre-initialisation of certain layers. The initialisation of the network weights is critical, since poor initialisation can impede learning because of gradient instability in deep nets. To circumvent this issue, we started by training configuration A (Table 1), shallow enough for random initialisation. Subsequently, when training deeper architectures, we initialised the first four convolutional layers and final three fully connected layers with net A's layers (the intermediate layers were randomly initialised).",A,1
VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION,"We did not decrease the learning rate for the pre-initialised layers, allowing them to change during learning. For random initialisation (where applicable), we sampled the weights from a normal distribution with the zero mean and 10−2 variance. The biases were initialised with zero. It is worth noting that after the paper submission we found that it is possible to initialise the weights without pre-training by using the random initialisation procedure of Glorot & Bengio (2010). To obtain the fixed-size 224×224 ConvNet input images, they were randomly cropped from rescaled training images (one crop per image per SGD iteration). To further augment the training set, the crops underwent random horizontal flipping and random RGB colour shift (Krizhevsky et al., 2012).","We kept the learning rate unchanged for the pre-trained layers, allowing them to be modified during training. For random initialization (when relevant), we took the weights from a normal distribution with zero mean and 10−2 variance. The biases started at zero. After submitting the paper, we realized we could initialize the weights without pre-training using the random initialization method of Glorot & Bengio (2010). To get the fixed 224×224 ConvNet input images, we randomly cropped the resized training images (one crop per image per SGD iteration). To further increase the training set size, the crops were randomly flipped horizontally and had random RGB color shifts (Krizhevsky et al., 2012).","The learning rate was not lowered for the pre-initialized layers, so they could adapt during learning. With random initialization (where used), the weights were sampled from a normal distribution with zero mean and 10−2 variance. Biases were set to zero initially. Notably, after submitting the paper we found weights could be initialized without pre-training using the random initialization of Glorot & Bengio (2010). To get the fixed 224×224 ConvNet input images, random crops were taken from scaled training images (one per image per SGD iteration). To further expand the training set, crops underwent random horizontal flips and RGB color shifts (Krizhevsky et al., 2012).  ","We did not reduce the learning rate for the pre-trained layers, permitting them to change during training. For random starting values (where relevant), we drew the weights from a normal distribution with zero mean and 10−2 variance. The biases began at zero. After submitting the paper, we realized weights could be initialized without pre-training using the random initialization of Glorot & Bengio (2010). To obtain the fixed 224×224 ConvNet input images, random crops were taken from resized training images (one per image per SGD iteration). To further grow the training set, crops were randomly flipped horizontally and had random RGB color changes (Krizhevsky et al., 2012).",A,1
VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION,"At test time, given a trained ConvNet and an input image, it is classified in the following way. First, it is isotropically rescaled to a pre-defined smallest image side, denoted as Q (we also refer to it as the test scale). We note that Q is not necessarily equal to the training scale S (as we will show in Sect. 4, using several values of Q for each S leads to improved performance). Then, the network is applied densely over the rescaled test image in a way similar to (Sermanet et al., 2014). Namely, the fully-connected layers are first converted to convolutional layers (the first FC layer to a 7 × 7 conv. layer, the last two FC layers to 1 × 1 conv. layers).","When classifying a new image using a trained ConvNet model, the image is first resized to have its smallest side be length Q (the test scale). Note that Q may not match the training scale S. The resized image is then processed by applying the ConvNet densely across the image, similarly to (Sermanet et al., 2014). Specifically, the fully connected layers are converted to convolutional layers (the first FC layer becomes a 7x7 conv layer, and the last two FC layers become 1x1 conv layers).","To classify a new image with a trained ConvNet, the image is first isotropically rescaled so its shortest side is length Q (the test scale). Q may differ from the training scale S. The network is then applied densely over the rescaled image as in (Sermanet et al., 2014). That is, the fully-connected layers are converted to convolutional layers (the first FC layer becomes a 7x7 conv layer, and the last two FC layers become 1x1 conv layers). ","When classifying a new image using a trained convolutional neural network model, the image is first resized isotropically so its smallest dimension is Q (the test scale). Note that Q may not equal the training scale S. The resized image is then processed by applying the model densely across the image, as in (Sermanet et al., 2014). Specifically, the fully-connected layers are converted to convolutional layers (the first fully-connected layer becomes a 7x7 convolutional layer, and the last two fully-connected layers become 1x1 convolutional layers).",A,1
VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION,"The first is to fix S, which corresponds to single-scale training (note that image content within the sampled crops can still represent multiscale image statistics). In our experiments, we evaluated models trained at two fixed scales: S = 256 (which has been widely used in the prior art (Krizhevsky et al., 2012; Zeiler & Fergus, 2013; Sermanet et al., 2014)) and S = 384. Given a ConvNet configuration, we first trained the network using S = 256. To speed-up training of the S = 384 network, it was initialised with the weights pre-trained with S = 256, and we used a smaller initial learning rate of 10−3 .","The initial approach is to establish S, which is analogous to single-scale preparation (keep in mind image particulars inside the sampled crops can still exemplify multiscale image data). In our assessments, we gauged models prepared at two fixed scales: S = 256 (which has been extensively utilized in preceding work (Krizhevsky et al., 2012; Zeiler & Fergus, 2013; Sermanet et al., 2014)) and S = 384. Provided a ConvNet arrangement, we first conditioned the network employing S = 256. To accelerate training of the S = 384 network, it was initialized with the weights pre-trained with S = 256, and we utilized a smaller initial learning rate of 10−3.","The first tactic is to determine S, which matches to single-scale schooling (remember that image specifics inside the sampled crops can still represent multiscale image statistics). In our trials, we reviewed models schooled at two fixed scales: S = 256 (which has been widely used in prior art (Krizhevsky et al., 2012; Zeiler & Fergus, 2013; Sermanet et al., 2014)) and S = 384. Given a ConvNet configuration, we first drilled the network employing S = 256. To hasten training of the S = 384 network, it was initialized with the weights pre-trained with S = 256, and we exercised a smaller initial learning rate of 10−3.","The initial approach is to pinpoint S, which is tantamount to single-scale education (keep in mind image minutiae within the sampled crops can still constitute multiscale image statistics). In our assessments, we gauged models educated at two fixed scales: S = 256 (which has been extensively utilized in preceding work (Krizhevsky et al., 2012; Zeiler & Fergus, 2013; Sermanet et al., 2014)) and S = 384. Provided a ConvNet arrangement, we first indoctrinated the network employing S = 256. To expedite training of the S = 384 network, it was initialized with the weights pre-trained with S = 256, and we exerted a smaller initial learning rate of 10−3.",A,1
VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION,"The resulting fully-convolutional net is then applied to the whole (uncropped) image. The result is a class score map with the number of channels equal to the number of classes, and a variable spatial resolution, dependent on the input image size. Finally, to obtain a fixed-size vector of class scores for the image, the class score map is spatially averaged (sum-pooled). We also augment the test set by horizontal flipping of the images; the soft-max class posteriors of the original and flipped images are averaged to obtain the final scores for the image. Since the fully-convolutional network is applied over the whole image, there is no need to sample multiple crops at test time (Krizhevsky et al., 2012), which is less efficient as it requires network re-computation for each crop.","The resulting entirely convolutional neural network is then used on the complete (not cropped) image. This produces a class probability map with a number of channels equal to the quantity of classes, and a variable spatial resolution, dependent on the input image dimensions. Lastly, to get a fixed-size vector of class probabilities for the image, the class probability map is spatially averaged (sum-pooled). We also augment the test set by flipping the images horizontally; the soft-max class probabilities of the original and flipped images are averaged to obtain the final probabilities for the image. Since the fully convolutional network is applied over the entire image, there is no need to sample multiple crops at test time (Krizhevsky et al., 2012), which is less efficient as it requires re-running the network for each crop.","The final fully convolutional network is then utilized on the whole (uncropped) image. This generates a class likelihood map with the number of channels matching the number of classes, and a variable spatial resolution, based on the input image size. Finally, to get a fixed-size vector of class likelihoods for the image, the class likelihood map is spatially averaged (sum-pooled). We also enhance the test set by flipping the images horizontally; the soft-max class likelihoods of the original and flipped images are averaged to obtain the final likelihoods for the image. Since the fully convolutional network is applied over the complete image, there is no need to sample multiple crops at test time (Krizhevsky et al., 2012), which is less efficient as it requires re-computing the network for each crop.","The resulting completely convolutional neural network is then used on the whole (not cropped) image. This creates a class probability map with the number of channels equaling the number of classes, and a variable spatial resolution, based on the input image size. Lastly, to obtain a fixed-size vector of class probabilities for the image, the class probability map is spatially averaged (sum-pooled). We also augment the test set by flipping the images horizontally; the soft-max class probabilities of the original and flipped images are averaged to get the final probabilities for the image. Since the fully convolutional network is applied over the complete image, there is no need to sample multiple crops at test time (Krizhevsky et al., 2012), which is less efficient as it requires re-running the network for each crop.",A,1
VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION,"At the same time, using a large set of crops, as done by Szegedy et al. (2014), can lead to improved accuracy, as it results in a finer sampling of the input image compared to the fully-convolutional net. Also, multi-crop evaluation is complementary to dense evaluation due to different convolution boundary conditions: when applying a ConvNet to a crop, the convolved feature maps are padded with zeros, while in the case of dense evaluation the padding for the same crop naturally comes from the neighbouring parts of an image (due to both the convolutions and spatial pooling), which substantially increases the overall network receptive field, so more context is captured.","Furthermore, utilizing a large collection of crops, as implemented by Szegedy et al. (2014), can enhance precision, since it enables finer examination of the input image versus the fully-convolutional structure. Additionally, multi-crop assessment complements dense evaluation owing to differing convolution limits: applying a ConvNet to a crop involves zero-padding the convolved attribute charts, whereas dense evaluation provides natural padding for the same crop from adjacent image regions (due to both the convolutions and spatial pooling), substantially expanding the overall network receptive scope, capturing more context.","Moreover, leveraging a substantial crop set, per Szegedy et al. (2014), may improve performance, because it allows more thorough sampling of the input image compared to the fully-convolutional architecture. Also, multi-crop testing provides complementary benefits to dense testing due to different convolution constraints: convnet crop testing pads convolved maps with zeros, but dense testing naturally pads the same crop using surrounding image areas (via convolutions and spatial pooling), greatly increasing the overall receptive range and capturing more context.","Additionally, employing numerous crops, following Szegedy et al. (2014), can boost accuracy, since it enables finer probing of the input image versus fully-convolutional models. Furthermore, multi-crop analysis complements dense analysis owing to distinct convolution limits: applying a ConvNet to a crop zero-pads the convolved maps, but dense analysis organically pads the same crop using adjacent image areas (through convolutions and spatial pooling), substantially extending the overall receptive scope and incorporating more context.",A,1
VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION,"First, we note that using local response normalisation (A-LRN network) does not improve on the model A without any normalisation layers. We thus do not employ normalisation in the deeper architectures (B–E). Second, we observe that the classification error decreases with the increased ConvNet depth: from 11 layers in A to 19 layers in E. Notably, in spite of the same depth, the configuration C (which contains three 1 × 1 conv. layers), performs worse than the configuration D, which uses 3 × 3 conv. layers throughout the network.","Initially, we see that applying local response normalization (A-LRN network) does not enhance the model A without any normalization layers. Therefore, we do not use normalization in the deeper architectures (B–E). Next, we notice that the classification error reduces as the ConvNet depth increases: from 11 layers in A to 19 layers in E. Significantly, despite having the same depth, the configuration C (which has three 1 × 1 conv. layers), performs worse than the configuration D, which utilizes 3 × 3 conv. layers throughout the network.","To start, implementing local response normalization (A-LRN network) does not improve the model A without any normalization layers. As a result, we do not use normalization in the deeper architectures (B–E). Additionally, we find that the classification error decreases as the ConvNet becomes deeper: from 11 layers in A to 19 layers in E. Importantly, even though they have the same depth, the configuration C (which has three 1 × 1 conv. layers), does worse than the configuration D, which uses 3 × 3 conv. layers in the whole network.  ","First off, applying local response normalization (A-LRN network) does not enhance the model A without any normalization layers. Therefore, we do not employ normalization in the deeper architectures (B–E). Secondly, we see that the classification error reduces as the ConvNet becomes deeper: from 11 layers in A to 19 layers in E. Significantly, despite having the same depth, the configuration C (which contains three 1 × 1 conv. layers), performs worse than the configuration D, which utilizes 3 × 3 conv. layers throughout the network.",A,1
VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION,"The former is a multi-class classification error, i.e. the proportion of incorrectly classified images; the latter is the main evaluation criterion used in ILSVRC, and is computed as the proportion of images such that the ground-truth category is outside the top-5 predicted categories. For the majority of experiments, we used the validation set as the test set. Certain experiments were also carried out on the test set and submitted to the official ILSVRC server as a “VGG” team entry to the ILSVRC-2014 competition (Russakovsky et al., 2014).","The first metric is a measurement of misclassified images across all categories; the second is the primary evaluation used in ILSVRC, calculated as the proportion of images where the true category is not in the top 5 predicted classes. For most experiments, we utilized the validation set for testing. Some experiments were also executed on the test set and submitted to the official ILSVRC server as a ""VGG"" team entry in the ILSVRC-2014 competition (Russakovsky et al., 2014).","The former evaluates incorrect categorizations across all classes; the latter is the key benchmark in ILSVRC, computed by finding images where the actual type is excluded from the top 5 forecasted types. In most tests, we leveraged the validation set for evaluation. Additional experiments were conducted on the test set and provided to the ILSVRC server as a ""VGG"" team submission for the ILSVRC-2014 challenge (Russakovsky et al., 2014).  ","The first is a measurement of misclassified images across all categories; the second is the primary metric used in ILSVRC, calculated by determining the proportion of images where the real category is missing from the top 5 predicted categories. For the bulk of experiments, we utilized the validation set for testing. Some experiments were also performed on the test set and submitted to the official ILSVRC server as a ""VGG"" team entry in the ILSVRC-2014 competition (Russakovsky et al., 2014).",A,1
VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION,"On a system equipped with four NVIDIA Titan Black GPUs, training a single net took 2–3 weeks depending on the architecture. In this section, we present the image classification results achieved by the described ConvNet architectures on the ILSVRC-2012 dataset (which was used for ILSVRC 2012–2014 challenges). The dataset includes images of 1000 classes, and is split into three sets: training (1.3M images), validation (50K images), and testing (100K images with held-out class labels). The classification performance is evaluated using two measures: the top-1 and top-5 error.","Using a system with 4 NVIDIA Titan Black GPUs, it took around 2-3 weeks to train one neural network based on its design. This part shows the image classification results from the ConvNet models on the ILSVRC-2012 dataset (used in the ILSVRC 2012-2014 contests). The dataset has pictures from 1000 categories, divided into 3 groups: training (1.3 million images), validation (50,000 images), and testing (100,000 images with held-out labels). The classification accuracy is measured by two metrics: top-1 and top-5 error rate.","On a computer with 4 NVIDIA Titan Black graphics cards, training a single neural network took between 2-3 weeks depending on its architecture. In this section, we present the image classification accuracy achieved by the described Convolutional Neural Network models on the ILSVRC-2012 image dataset (used in the ILSVRC 2012-2014 competitions). The dataset contains images from 1000 classes, split into three subsets: training (1.3 million images), validation (50 thousand images), and testing (100 thousand images with hidden class labels). The classification performance is evaluated using two measures: the top-1 and top-5 error rate.","Using a machine equipped with 4 NVIDIA Titan Black GPUs, it took 2-3 weeks to train one network based on its design. Here we show the image classification results from the Convolutional Neural Network architectures on the ILSVRC-2012 image dataset (used in the ILSVRC 2012-2014 challenges). The dataset has images from 1000 categories, divided into 3 subsets: training (1.3 million images), validation (50 thousand images), and testing (100 thousand images with undisclosed labels). The classification accuracy is measured by two metrics: top-1 and top-5 error percentage.",A,1
VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION,"Multi-GPU training exploits data parallelism, and is carried out by splitting each batch of training images into several GPU batches, processed in parallel on each GPU. After the GPU batch gradients are computed, they are averaged to obtain the gradient of the full batch. Gradient computation is synchronous across the GPUs, so the result is exactly the same as when training on a single GPU. While more sophisticated methods of speeding up ConvNet training have been recently proposed (Krizhevsky, 2014), which employ model and data parallelism for different layers of the net, we have found that our conceptually much simpler scheme already provides a speedup of 3.75 times on an off-the-shelf 4-GPU system, as compared to using a single GPU.","Training a neural network using multiple graphics processing units (GPUs) takes advantage of data parallelism. The training images are split into smaller batches, with each GPU processing one batch at the same time. After the gradients are calculated for each GPU batch, they are combined to get the overall gradient for the full batch. The gradient calculations happen at the same time across all GPUs, so the result is the same as training on just one GPU. While more complex methods to speed up neural network training have been developed (Krizhevsky, 2014), which use model and data parallelism for different layers, our simpler approach already provides 3.75 times faster training on a standard 4-GPU system compared to using one GPU.","Using many GPUs together for neural network training makes use of data parallelism. The full set of training images is divided into smaller batches, with each GPU handling one batch simultaneously. Once the gradients are found for each GPU's batch, they are averaged together to get the gradient for the entire batch. The gradient math happens at the same time on all GPUs, so it's the same as training on a single GPU. Though more intricate techniques to accelerate neural network training have been created recently (Krizhevsky, 2014), which utilize model and data parallelism for different layers, our more straightforward method already speeds up training by 3.75 times on a typical 4-GPU system, compared to a single GPU.  ","Training neural networks with multiple GPUs leverages data parallelism. The full batch of training images is split up, with each GPU processing a smaller batch at the same time. After the gradients are calculated for each GPU's batch, they are combined to determine the gradient for the whole batch. The gradient calculations occur simultaneously on all GPUs, giving the same result as training on one GPU. While more advanced approaches to speeding up neural network training have been developed recently (Krizhevsky, 2014), using model and data parallelism for different layers, our simpler technique already achieves 3.75x faster training on a standard 4-GPU setup compared to a single GPU.",A,1
VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION,"While we believe that in practice the increased computation time of multiple crops does not justify the potential gains in accuracy, for reference we also evaluate our networks using 50 crops per scale (5 × 5 regular grid with 2 flips), for a total of 150 crops over 3 scales, which is comparable to 144 crops over 4 scales used by Szegedy et al. (2014). Our implementation is derived from the publicly available C++ Caffe toolbox (Jia, 2013) (branched out in December 2013), but contains a number of significant modifications, allowing us to perform training and evaluation on multiple GPUs installed in a single system, as well as train and evaluate on full-size (uncropped) images at multiple scales (as described above).","Although we think that in real world use the increased processing time of multiple crops does not warrant the possible improvements in precision, for reference we also assess our neural networks utilizing 50 crops per scale (5 × 5 standard grid with 2 flips), totaling 150 crops over 3 scales, which is similar to 144 crops over 4 scales utilized by Szegedy et al. (2014). Our implementation is derived from the publicly available C++ Caffe toolkit (Jia, 2013) (branched out in December 2013), but has various significant modifications, enabling us to carry out training and evaluation on multiple GPUs installed in a single system, as well as train and evaluate on full-size (uncropped) images at multiple scales (as described above).","While we believe that in actual practice the extra computation time of multiple crops does not justify the potential increases in accuracy, for reference we also test our networks using 50 crops per scale (5 × 5 regular grid with 2 flips), for a total of 150 crops over 3 scales, which is comparable to 144 crops over 4 scales used by Szegedy et al. (2014). Our implementation is derived from the publicly available C++ Caffe toolkit (Jia, 2013) (branched out in December 2013), but contains several significant modifications, allowing us to perform training and evaluation on multiple GPUs installed in one system, as well as train and evaluate on full-size (uncropped) images at multiple scales (as described above).  ","Although we think that in real world usage the increased processing time of multiple crops does not warrant the possible improvements in precision, for reference we also evaluate our neural networks using 50 crops per scale (5 × 5 standard grid with 2 flips), totaling 150 crops over 3 scales, which is similar to 144 crops over 4 scales utilized by Szegedy et al. (2014). Our implementation is derived from the publicly available C++ Caffe toolkit (Jia, 2013) (branched out in December 2013), but contains a number of significant modifications, enabling us to execute training and evaluation on multiple GPUs installed in a single system, as well as train and evaluate on full-size (uncropped) images at multiple scales (as described above).",A,1
VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION,"This indicates that while the additional non-linearity does help (C is better than B), it is also important to capture spatial context by using conv. filters with non-trivial receptive fields (D is better than C). The error rate of our architecture saturates when the depth reaches 19 layers, but even deeper models might be beneficial for larger datasets. We also compared the net B with a shallow net with five 5 × 5 conv. layers, which was derived from B by replacing each pair of 3 × 3 conv. layers with a single 5 × 5 conv. layer (which has the same receptive field as explained in Sect. 2.3). The top-1 error of the shallow net was measured to be 7% higher than that of B (on a center crop), which confirms that a deep net with small filters outperforms a shallow net with larger filters.","This shows that while adding more nonlinearity helps (C performs better than B), capturing spatial context through using convolution filters with meaningful receptive fields is also important (D outdoes C). The error rate of our model plateaus when it reaches 19 layers, but deeper models could prove beneficial given larger datasets. We also pitted net B against a shallow net with five 5x5 convolution layers, created by substituting each pair of 3x3 convolution layers from B with one 5x5 convolution layer (having the same receptive field as described in Section 2.3). The top-1 error of the shallow net was 7% higher than B's (on a center crop), confirming that a deep net with small filters is superior to a shallow net with larger filters.","This demonstrates that although incorporating extra nonlinearity is advantageous (C surpasses B), capturing spatial relationships by utilizing convolution filters with meaningful receptive fields is also crucial (D beats C). The error rate of our architecture maxes out at 19 layers, but deeper models may be useful for larger datasets. We also compared net B to a shallow net with five 5x5 convolution layers, formed by replacing each pair of 3x3 convolution layers in B with a single 5x5 convolution layer (having the same receptive field as explained in Section 2.3). The top-1 error of the shallow net was 7% higher than B's (on a center crop), showing that a deep net with small filters is better than a shallow net with bigger filters.","This shows that while adding more non-linearity is helpful (C is superior to B), capturing spatial context through convolution filters with meaningful receptive fields is also important (D is better than C). The error rate of our model peaks at 19 layers, but deeper models may prove beneficial for larger datasets. We also tested net B against a shallow net with five 5x5 convolution layers, created by swapping each pair of 3x3 convolution layers in B for a single 5x5 convolution layer (having the same receptive field as described in Section 2.3). The top-1 error of the shallow net was 7% higher than B's (on a center crop), demonstrating that a deep net with small filters surpasses a shallow net with larger filters.",A,1
VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION,"Up until now, we evaluated the performance of individual ConvNet models. In this part of the experiments, we combine the outputs of several models by averaging their soft-max class posteriors. This improves the performance due to complementarity of the models, and was used in the top ILSVRC submissions in 2012 (Krizhevsky et al., 2012) and 2013 (Zeiler & Fergus, 2013; Sermanet et al., 2014). The results are shown in Table 6. By the time of ILSVRC submission we had only trained the single-scale networks, as well as a multi-scale model D (by fine-tuning only the fully-connected layers rather than all layers).","Until this point, we looked at how each ConvNet model performed on its own. Now, we will combine the predictions from several models by taking the average of their softmax class probabilities. This boosts performance because the models complement each other, and was a technique used by the top submissions to ILSVRC in 2012 (Krizhevsky et al., 2012) and 2013 (Zeiler & Fergus, 2013; Sermanet et al., 2014). The results are presented in Table 6. By the time we submitted to ILSVRC, we had only trained the single-scale networks, as well as a multi-scale model D (by fine-tuning just the fully-connected layers rather than all layers).","Up to this juncture, we assessed the capabilities of individual ConvNet architectures. In this portion of the experiments, we merge the outputs of multiple models by averaging their softmax class posteriors. This enhances performance due to the complementary nature of the models, and was utilized in the highest ranking ILSVRC entries in 2012 (Krizhevsky et al., 2012) and 2013 (Zeiler & Fergus, 2013; Sermanet et al., 2014). The results are displayed in Table 6. At the time of our ILSVRC submission, we had only trained the single-scale networks, and also a multi-scale model D (by fine-tuning solely the fully-connected layers rather than all layers).","So far, we have evaluated how each individual ConvNet model performs. Now, we will combine the predictions from several models by taking the mean of their softmax class probabilities. This improves performance because the models complement each other, and was a technique used by the top scoring ILSVRC submissions in 2012 (Krizhevsky et al., 2012) and 2013 (Zeiler & Fergus, 2013; Sermanet et al., 2014). The results are shown in Table 6. When we submitted to ILSVRC, we had only trained the single-scale networks, and also a multi-scale model D (by fine-tuning just the fully-connected layers rather than all layers).",A,1
VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION,"The resulting ensemble of 7 networks has 7.3% ILSVRC test error. After the submission, we considered an ensemble of only two best-performing multi-scale models (configurations D and E), which reduced the test error to 7.0% using dense evaluation and 6.8% using combined dense and multi-crop evaluation. For reference, our best-performing single model achieves 7.1% error (model E, Table 5).","The group of 7 networks produces a 7.3% error rate on the ILSVRC test data. After submitting, we looked at an ensemble with just the 2 top multi-scale models (versions D and E), which lowered the test error to 7.0% with dense evaluation and 6.8% with both dense and multi-crop evaluation. For comparison, our best single model has a 7.1% error rate (model E, Table 5).","The collection of 7 neural networks results in a 7.3% error on the ILSVRC testing set. Post-submission, we analyzed a combination of only the 2 best multi-scale architectures (D and E), which reduced the test error to 7.0% using dense assessment and 6.8% using both dense and multi-crop assessment. As a benchmark, our top standalone model has a 7.1% error (model E, Table 5).  ","The group of 7 neural networks produces a 7.3% error on the ILSVRC test set. After turning in results, we evaluated an ensemble of just the 2 best multi-scale networks (D and E), lowering test error to 7.0% with dense evaluation and 6.8% with dense plus multi-crop evaluation. For reference, our top single network has a 7.1% error rate (model E, Table 5).",A,1
VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION,"Finally, we compare our results with the state of the art in Table 7. In the classification task of ILSVRC-2014 challenge (Russakovsky et al., 2014), our “VGG” team secured the 2nd place with 7.3% test error using an ensemble of 7 models. After the submission, we decreased the error rate to 6.8% using an ensemble of 2 models. As can be seen from Table 7, our very deep ConvNets significantly outperform the previous generation of models, which achieved the best results in the ILSVRC-2012 and ILSVRC-2013 competitions. Our result is also competitive with respect to the classification task winner (GoogLeNet with 6.7% error) and substantially outperforms the ILSVRC-2013 winning submission Clarifai, which achieved 11.2% with outside training data and 11.7% without it.","In conclusion, we make a comparison of our findings with the current state of the art in Table 7. In the image classification challenge of the ILSVRC-2014 competition (Russakovsky et al., 2014), our ""VGG"" team got 2nd place with a 7.3% test error rate using an ensemble of 7 models. After submitting, we reduced the error rate to 6.8% using just 2 models. As evident in Table 7, our very deep Convolutional Networks significantly surpass the previous generation of models, which had the top results in the ILSVRC-2012 and ILSVRC-2013 contests. Our result is also competitive with the classification winner (GoogLeNet with 6.7% error) and substantially exceeds the ILSVRC-2013 winning submission Clarifai, which achieved 11.2% with external training data and 11.7% without it.","Finally, we make a comparison between our results and the current state of the art in Table 7. In the image classification task of the ILSVRC-2014 challenge (Russakovsky et al., 2014), our ""VGG"" team came in 2nd place with a 7.3% test error rate using an ensemble of 7 models. After submission, we lowered the error rate to 6.8% using just 2 models. As shown in Table 7, our very deep Convolutional Networks greatly surpass the previous generation of models, which had the best results in the ILSVRC-2012 and ILSVRC-2013 contests. Our result is also competitive with the image classification winner (GoogLeNet with 6.7% error) and substantially beats the ILSVRC-2013 winning submission Clarifai, which achieved 11.2% with outside training data and 11.7% without it.","In closing, we make a comparison of our results with the current state of the art in Table 7. In the image classification task of the ILSVRC-2014 challenge (Russakovsky et al., 2014), our ""VGG"" team took 2nd place with a 7.3% test error rate using an ensemble of 7 models. After submission, we reduced the error rate to 6.8% using just 2 models. As evident in Table 7, our very deep Convolutional Networks greatly exceed the previous generation of models, which had the top results in the ILSVRC-2012 and ILSVRC-2013 contests. Our result is also competitive with the classification winner (GoogLeNet with 6.7% error) and substantially beats the ILSVRC-2013 winning submission Clarifai, which achieved 11.2% with external training data and 11.7% without it.",A,1
VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION,"This is remarkable, considering that our best result is achieved by combining just two models – significantly less than used in most ILSVRC submissions. In terms of the single-net performance, our architecture achieves the best result (7.0% test error), outperforming a single GoogLeNet by 0.9%. Notably, we did not depart from the classical ConvNet architecture of LeCun et al. (1989), but improved it by substantially increasing the depth.","This is striking, taking into account that our top outcome is reached by mixing only two models - much less than utilized in most ILSVRC entries. Regarding the single-net act, our design accomplishes the finest result (7.0% test error), surpassing a single GoogLeNet by 0.9%. Significantly, we did not diverge from the classical ConvNet building of LeCun et al. (1989), but bettered it by extensively expanding the depth.","It is amazing that our best performance is obtained by integrating just two models - far fewer than used in most ILSVRC submissions. In terms of the performance of a single net, our structure achieves the top result (7.0% test error), outdoing a single GoogLeNet by 0.9%. It is notable that we did not stray from the conventional ConvNet architecture of LeCun et al. (1989), but enhanced it by substantially increasing the depth.  ","This is astounding, considering our optimal outcome is reached by combining only two models - much less than utilized in most ILSVRC entries. Regarding the performance of a single net, our design produces the best result (7.0% test error), exceeding a single GoogLeNet by 0.9%. Importantly, we did not diverge from the classic ConvNet framework of LeCun et al. (1989), but improved it by significantly expanding the depth.",A,1
XLNet_Generalized Autoregressive Pretraining for Language Understanding,"With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation.","BERT uses denoising autoencoding for pretraining, which models bidirectional contexts better than autoregressive language modeling pretraining like GPT. However, BERT's masking approach fails to capture dependencies between masked words, causing a gap between pretraining and finetuning. We present XLNet, a new autoregressive pretraining method that learns bidirectional contexts by maximizing likelihood over all factorization orders, avoiding BERT's limitations.","BERT's denoising autoencoder pretraining surpasses autoregressive language model pretraining in modeling bidirectional contexts. But BERT's reliance on masking overlooks dependencies between masked words, creating a pretrain-finetune discrepancy. We introduce XLNet, a generalized autoregressive pretraining approach that learns bidirectional contexts by maximizing expected likelihood across all permutation orders, overcoming BERT's weaknesses through its autoregressive formulation.","BERT uses a denoising autoencoder for pretraining which better models bidirectional contexts versus autoregressive language model pretraining like GPT. However, BERT's masking neglects dependencies between masked words, causing a pretrain-finetune gap. We propose XLNet, a new autoregressive pretraining method maximizing likelihood over all factorization orders to learn bidirectional contexts, avoiding BERT's limitations through its autoregressive nature.",A,1
XLNet_Generalized Autoregressive Pretraining for Language Understanding,"Unsupervised representation learning has been highly successful in the domain of natural language processing [7, 22, 27, 28, 10]. Typically, these methods first pretrain neural networks on large-scale unlabeled text corpora, and then finetune the models or representations on downstream tasks. Under this shared high-level idea, different unsupervised pretraining objectives have been explored in literature. Among them, autoregressive (AR) language modeling and autoencoding (AE) have been the two most successful pretraining objectives.","Self-directed representation learning has seen great success in natural language processing [7, 22, 27, 28, 10]. These techniques usually first train neural networks on massive unlabeled text data, then adjust the models or representations for specific tasks. With this common high-level concept, different unsupervised pretraining goals have been studied. Of these, autoregressive (AR) language modeling and autoencoding (AE) have been the two most effective pretraining objectives.","Unsupervised neural network training has excelled in natural language processing [7, 22, 27, 28, 10]. These approaches first teach neural networks using huge unlabeled text datasets, then refine the models or embeddings for particular jobs. Under this shared top-level idea, various unsupervised pretraining aims have been explored in research. Among them, autoregressive (AR) language modeling and autoencoding (AE) have been the two most fruitful pretraining goals.  ","Self-directed neural network learning has been tremendously successful in natural language processing [7, 22, 27, 28, 10]. Typically, these techniques first develop neural networks using massive unlabeled text corpora, then fine-tune the models or representations for specific tasks. With this common high-level concept, different unsupervised pretraining targets have been studied in literature. Of these, autoregressive (AR) language modeling and autoencoding (AE) have been the two most effective pretraining objectives.",A,1
XLNet_Generalized Autoregressive Pretraining for Language Understanding,"In comparison, AE based pretraining does not perform explicit density estimation but instead aims to reconstruct the original data from corrupted input. A notable example is BERT [10], which has been the state-of-the-art pretraining approach. Given the input token sequence, a certain portion of tokens are replaced by a special symbol [MASK], and the model is trained to recover the original tokens from the corrupted version. Since density estimation is not part of the objective, BERT is allowed to utilize bidirectional contexts for reconstruction.","In contrast, AE based pretraining does not explicitly model the data distribution but instead tries to reconstruct the original data from altered input. A prominent instance is BERT [10], which has been the top performing pretraining method. With the input token sequence, some tokens are swapped with a special symbol [MASK], and the model is optimized to predict the original tokens from the modified version. As density modeling is not part of the goal, BERT can leverage bidirectional contexts for reconstruction.","On the other hand, AE based pretraining does not directly estimate densities but rather aims to reproduce the original data from distorted input. A well-known example is BERT [10], which has been the state-of-the-art pretraining technique. Given the input token sequence, a portion of tokens are substituted with a special symbol [MASK], and the model is trained to restore the original tokens from the altered version. Since density estimation is not an objective, BERT can use bidirectional contexts for reconstruction.","In contrast, AE based pretraining does not explicitly learn the data distribution but instead reconstructs the original data from corrupted input. A prominent case is BERT [10], which has been the top performing pretraining approach. In the input token sequence, some tokens are replaced with a special symbol [MASK], and the model is optimized to predict the original tokens from the modified version. As density modeling is not a goal, BERT can leverage bidirectional contexts for reconstruction.",A,1
XLNet_Generalized Autoregressive Pretraining for Language Understanding,"As an immediate benefit, this closes the aforementioned bidirectional information gap in AR language modeling, leading to improved performance. However, the artificial symbols like [MASK] used by BERT during pretraining are absent from real data at finetuning time, resulting in a pretrain-finetune discrepancy. Moreover, since the predicted tokens are masked in the input, BERT is not able to model the joint probability using the product rule as in AR language modeling. In other words, BERT assumes the predicted tokens are independent of each other given the unmasked tokens, which is oversimplified as high-order, long-range dependency is prevalent in natural language [9].","This immediately helps address the previously mentioned two-way lack of information in autoregressive language modeling, improving its capabilities. But the artificial symbols like [MASK] that BERT uses while pretraining don't exist in real data during finetuning, creating inconsistency between pretraining and finetuning. Also, as the predicted tokens are masked in the input, BERT can't model the joint probability using the product rule as in autoregressive language modeling. In other words, BERT assumes the predicted tokens are unrelated to each other when given the unmasked tokens, which is an oversimplification since high-order, long-distance dependency is common in natural language [9].","This right away bridges the two-way knowledge gap in autoregressive language modeling mentioned earlier, enhancing its performance. However, the synthetic symbols such as [MASK] utilized by BERT during pretraining are not found in real data during finetuning, resulting in divergence between pretraining and finetuning. Furthermore, since the predicted tokens are obscured in the input, BERT cannot model the joint probability using the product rule as in autoregressive language modeling. To put it another way, BERT presumes the predicted tokens are independent of one another conditional on the unmasked tokens, which is an oversimplification as high-order, long-range dependency prevails in natural language [9].","This immediately addresses the previously stated two-way information deficit in autoregressive language modeling, improving its abilities. However, the artificial symbols such as [MASK] employed by BERT during pretraining do not exist in real data during finetuning, producing an inconsistency between pretraining and finetuning. In addition, as the predicted tokens are obscured in the input, BERT is unable to model the joint probability using the product rule as in autoregressive language modeling. In other words, BERT assumes the predicted tokens are unrelated given the unmasked tokens, which is an oversimplification since high-order, long-distance dependency is prevalent in natural language [9].",A,1
XLNet_Generalized Autoregressive Pretraining for Language Understanding,"Faced with the pros and cons of existing language pretraining objectives, in this work, we propose XLNet, a generalized autoregressive method that leverages the best of both AR language modeling and AE while avoiding their limitations. Firstly, instead of using a fixed forward or backward factorization order as in conventional AR models, XLNet maximizes the expected log likelihood of a sequence w.r.t. all possible permutations of the factorization order. Thanks to the permutation operation, the context for each position can consist of tokens from both left and right. In expectation, each position learns to utilize contextual information from all positions, i.e., capturing bidirectional context.","When looking at the advantages and disadvantages of current language pretraining goals, we put forward XLNet in this work, a generalized autoregressive technique that utilizes the best of both autoregressive language modeling and autoencoding while avoiding their constraints. First, rather than employing a fixed forward or backward factorization sequence like in conventional autoregressive models, XLNet maximizes the expected log probability of a sequence with respect to all feasible permutations of the factorization order. Due to the permutation operation, the context for each position can contain tokens from both left and right. In expectation, each position learns to use contextual information from all positions, that is, seizing bidirectional context.","Considering the pros and cons of existing language pretraining aims, we introduce XLNet, a general autoregressive approach that harnesses the strengths of both autoregressive language modeling and autoencoding while circumventing their limitations. Instead of utilizing a fixed forward or backward factorization sequence as in standard autoregressive models, XLNet maximizes the expected log likelihood of a sequence relative to all potential permutations of the factorization order. Because of the permutation operation, the context for each position can include tokens from both left and right. On average, each position learns to leverage contextual information from all positions, capturing bidirectional context.  ","When examining the advantages and disadvantages of current language pretraining goals, we present XLNet here, a generalized autoregressive technique that capitalizes on the best aspects of both autoregressive language modeling and autoencoding while avoiding their constraints. Rather than employing a fixed forward or backward factorization sequence as in conventional autoregressive models, XLNet maximizes the expected log probability of a sequence with respect to all possible permutations of the factorization order. Owing to the permutation operation, the context for each position can comprise tokens from both left and right. On average, each position learns to employ contextual information from all positions, grasping bidirectional context.",A,1
XLNet_Generalized Autoregressive Pretraining for Language Understanding,"Secondly, as a generalized AR language model, XLNet does not rely on data corruption. Hence, XLNet does not suffer from the pretrain-finetune discrepancy that BERT is subject to. Meanwhile, the autoregressive objective also provides a natural way to use the product rule for factorizing the joint probability of the predicted tokens, eliminating the independence assumption made in BERT. In addition to a novel pretraining objective, XLNet improves architectural designs for pretraining.","Next, since XLNet is a general autoregressive language model, it does not depend on distorting data. Therefore, XLNet does not have the problem BERT has where pretraining and finetuning are different. Also, the autoregressive goal provides a straightforward way to use the product rule to factor the joint probability of the predicted words, removing the independence assumption BERT makes. On top of a new pretraining aim, XLNet enhances architectural designs for pretraining.","Secondly, as XLNet is a broad autoregressive language model, it does not use corrupted data. As a result, XLNet does not have the discrepancy between pretraining and finetuning that BERT has. Additionally, the autoregressive objective naturally allows using the product rule to factorize the joint likelihood of the predicted words, eliminating the independence assumption in BERT. Apart from a novel pretraining goal, XLNet improves architectural designs for pretraining.","Additionally, since XLNet is a general AR language model, it does not use altered data. Therefore, XLNet does not have the pretrain-finetune inconsistency BERT has. Also, the autoregressive goal provides a natural way to utilize the product rule to factor the joint probability of the predicted words, eliminating the independence assumption BERT makes. On top of a new pretraining objective, XLNet enhances architectural designs for pretraining.",A,1
XLNet_Generalized Autoregressive Pretraining for Language Understanding,"Inspired by the latest advancements in AR language modeling, XLNet integrates the segment recurrence mechanism and relative encoding scheme of Transformer-XL [9] into pretraining, which empirically improves the performance especially for tasks involving a longer text sequence. Naively applying a Transformer(-XL) architecture to permutation-based language modeling does not work because the factorization order is arbitrary and the target is ambiguous. As a solution, we propose to reparameterize the Transformer(-XL) network to remove the ambiguity.","Motivated by the most recent progress in AR language modeling, XLNet combines the segment repetition system and relative encoding plan of Transformer-XL [9] into pretraining. This empirically enhances the performance particularly for tasks involving longer text sequences. Simply applying a Transformer(-XL) architecture to permutation-based language modeling fails because the factorization order is random and the target is unclear. To address this, we suggest reparameterizing the Transformer(-XL) network to eliminate the ambiguity.","Inspired by the newest improvements in AR language modeling, XLNet integrates the segment recurrence tool and relative encoding scheme of Transformer-XL [9] into pretraining, which demonstrably boosts the performance especially for jobs involving longer text strings. Naively implementing a Transformer(-XL) architecture to permutation-based language modeling does not function because the factorization order is arbitrary and the goal is ambiguous. As a solution, we put forward reparameterizing the Transformer(-XL) network to remove the ambiguity. ","Motivated by the most cutting-edge advancements in AR language modeling, XLNet combines the segment repetition mechanism and relative encoding plan of Transformer-XL [9] into pretraining, which empirically enhances the performance particularly for tasks involving longer text sequences. Simply applying a Transformer(-XL) architecture to permutation-based language modeling does not work because the factorization order is random and the target is unclear. As a remedy, we propose reparameterizing the Transformer(-XL) network to eliminate the ambiguity.",A,1
XLNet_Generalized Autoregressive Pretraining for Language Understanding," Empirically, under comparable experiment setting, XLNet consistently outperforms BERT [10] on a wide spectrum of problems including GLUE language understanding tasks, reading comprehension tasks like SQuAD and RACE, text classification tasks such as Yelp and IMDB, and the ClueWeb09-B document ranking task. The idea of permutation-based AR modeling has been explored in [32, 12], but there are several key differences. Firstly, previous models aim to improve density estimation by baking an “orderless” inductive bias into the model while XLNet is motivated by enabling AR language models to learn bidirectional contexts.","Through empirical testing under similar experimental conditions, XLNet repeatedly exceeds BERT [10] across a broad range of tasks including language comprehension like GLUE, reading comprehension like SQuAD and RACE, text classification like Yelp and IMDB, and ClueWeb09-B document ranking. Though permutation-based AR modeling has been studied before [32, 12], XLNet differs in important ways. It aims to enable AR language models to learn bidirectional contexts, unlike previous models focused on improving density estimation by integrating an ""orderless"" inductive bias.","Studies show XLNet consistently outperforms BERT [10] on many tests with comparable settings, including language understanding benchmarks like GLUE, reading tests like SQuAD and RACE, classifying text genres like Yelp reviews and IMDB comments, and ranking ClueWeb09-B documents. While permutation-based AR modeling has prior work [32, 12], XLNet is unique. It intends to allow AR language models to learn bidirectional contexts, unlike past models targeting density estimation improvements through an ""orderless"" inductive bias.  ","Empirical evaluations under similar experimental controls reveal XLNet repeatedly surpasses BERT [10] on diverse tasks including GLUE language comprehension, SQuAD and RACE reading, Yelp and IMDB text classification, and ClueWeb09-B document ranking. Though permutation-based AR modeling has predecessors [32, 12], XLNet differs significantly. It aims to enable AR language models to learn bidirectional contexts, contrasting previous models focused on boosting density estimation by integrating an ""orderless"" inductive bias.",A,1
XLNet_Generalized Autoregressive Pretraining for Language Understanding,"Technically, to construct a valid target-aware prediction distribution, XLNet incorporates the target position into the hidden state via two-stream attention while previous permutation-based AR models relied on implicit position awareness inherent to their MLP architectures. Finally, for both orderless NADE and XLNet, we would like to emphasize that “orderless” does not mean that the input sequence can be randomly permuted but that the model allows for different factorization orders of the distribution. Another related idea is to perform autoregressive denoising in the context of text generation [11], which only considers a fixed order though.","Fundamentally, XLNet uses two-stream attention to include the target position in the hidden state to build a correct target-aware prediction distribution, while previous permutation-based AR models depended on the implicit position awareness built into their MLP architectures. Additionally, we want to stress that ""orderless"" does not imply the input sequence can be arbitrarily reordered for both orderless NADE and XLNet, but rather that the model permits different factorization orders of the distribution. A related concept is to do autoregressive denoising for text generation [11], which only looks at a fixed order however.","In essence, XLNet utilizes two-stream attention to incorporate the target position into the hidden state in order to construct a valid target-aware prediction distribution, whereas earlier permutation-based AR models leveraged the implicit position awareness inherent in their MLP designs. Moreover, we would like to emphasize that ""orderless"" does not mean the input sequence can be randomly shuffled for either orderless NADE or XLNet, but rather that the model allows for different factorization orders of the distribution. Another associated idea is to perform autoregressive denoising for text generation [11], which only considers a fixed order though.  ","At its core, XLNet uses two-stream attention to integrate the target position into the hidden state to build a sound target-aware prediction distribution, while previous permutation-based AR models capitalized on the implicit position awareness built into their MLP architectures. Furthermore, we want to stress that ""orderless"" does not mean the input sequence can be arbitrarily reordered for either orderless NADE or XLNet, but rather that the model accommodates different factorization orders of the distribution. A related notion is to conduct autoregressive denoising for text generation [11], which only examines a fixed order however.",A,1
XLNet_Generalized Autoregressive Pretraining for Language Understanding,"According to the comparison above, AR language modeling and BERT possess their unique advantages over the other. A natural question to ask is whether there exists a pretraining objective that brings the advantages of both while avoiding their weaknesses. Borrowing ideas from orderless NADE [32], we propose the permutation language modeling objective that not only retains the benefits of AR models but also allows models to capture bidirectional contexts. Specifically, for a sequence x of length T, there are T! different orders to perform a valid autoregressive factorization. Intuitively, if model parameters are shared across all factorization orders, in expectation, the model will learn to gather information from all positions on both sides.","The comparison shows that AR language modeling and BERT have unique strengths compared to each other. It's natural to wonder if there's a pretraining goal that combines the benefits of both while avoiding the downsides. Inspired by orderless NADE [32], we suggest the permutation language modeling objective. It keeps the advantages of AR models and lets models use bidirectional contexts. For a sequence x of length T, there are T! valid ways to factorize autoregressively. If parameters are shared across all orders, the model can learn to use info from all positions on both sides.","The preceding analysis indicates AR language modeling and BERT have distinct pros over the other. A logical question is whether some pretraining aim unifies the upsides of both while skipping the weaknesses. Borrowing from orderless NADE [32], we put forward permutation language modeling. This retains AR model strengths and enables capturing bidirectional contexts. Specifically, for a sequence x of length T, there are T! valid autoregressive factorizations. Intuitively, sharing parameters across all orders, the model can gather information from all positions on both sides. ","The earlier comparison shows AR language modeling and BERT have unique benefits over the other. A natural inquiry is if some pretraining goal combines the advantages of both while avoiding the drawbacks. Taking inspiration from orderless NADE [32], we propose permutation language modeling. This keeps AR model strengths and allows capturing bidirectional contexts. Namely, for a sequence x of length T, there are T! valid ways to autoregressively factorize. If parameters are shared across orders, the model can learn to utilize information from all positions on both sides.",A,1
XLNet_Generalized Autoregressive Pretraining for Language Understanding,"The proposed objective only permutes the factorization order, not the sequence order. In other words, we keep the original sequence order, use the positional encodings corresponding to the original sequence, and rely on a proper attention mask in Transformers to achieve permutation of the factorization order. Note that this choice is necessary, since the model will only encounter text sequences with the natural order during finetuning. To provide an overall picture, we show an example of predicting the token x3 given the same input sequence x but under different factorization orders in the Appendix A.7 with Figure 4.","The suggested goal merely changes the factorization sequence, not the order of the sequence. That is, we retain the original sequence order, utilize the positional encodings that correspond to the original sequence, and depend on a suitable attention mask in Transformers to accomplish a permutation of the factorization order. Note that this decision is required, since the model will only see text sequences with the natural order during fine-tuning. To give a complete overview, we provide an example of predicting the token x3 given the same input sequence x but under varying factorization orders in Appendix A.7 with Figure 4.","The proposed aim only alters the factorization arrangement, keeping the sequence order intact. In other words, we maintain the original sequence order, use the position encodings matching the original sequence, and rely on a proper attention mask in Transformers to achieve rearrangement of the factorization order. This choice is necessary, since the model will only experience text sequences in their natural order during fine-tuning. To present the full picture, we include an example of forecasting the token x3 given the same input sequence x but under different factorization orders in Appendix A.7 with Figure 4.  ","The suggested target merely modifies the factorization pattern, preserving the sequence order. That is, we keep the original sequence order, utilize the position codes corresponding to the original sequence, and depend on a suitable attention mask in Transformers to accomplish a reorganization of the factorization order. This decision is required, since the model will only encounter text sequences in their natural order during fine-tuning. To provide the complete perspective, we show an example of predicting the token x3 given the same input sequence x but under varying factorization orders in Appendix A.7 with Figure 4.",A,1
XLNet_Generalized Autoregressive Pretraining for Language Understanding,"Since our objective function fits in the AR framework, we incorporate the state-of-the-art AR language model, Transformer-XL [9], into our pretraining framework, and name our method after it. We integrate two important techniques in Transformer-XL, namely the relative positional encoding scheme and the segment recurrence mechanism. We apply relative positional encodings based on the original sequence as discussed earlier, which is straightforward. Now we discuss how to integrate the recurrence mechanism into the proposed permutation setting and enable the model to reuse hidden states from previous segments.","Because our goal function is compatible with the AR structure, we include the cutting-edge AR language model, Transformer-XL [9], into our pretraining system, and title our approach after it. We combine two key methods from Transformer-XL: the relative positional encoding plan and the segment repetition mechanism. We implement relative positional encodings founded on the original sequence as described before, which is simple. Now we explain how to integrate the recurrence mechanism into the proposed permutation configuration and empower the model to reuse hidden states from preceding segments.","Since our objective function is suitable for the AR framework, we incorporate the state-of-the-art AR language model, Transformer-XL [9], into our pretraining framework, and name our approach after it. We unite two vital techniques in Transformer-XL: the relative positional encoding scheme and the segment recurrence mechanism. We put into practice relative positional encodings based on the original sequence as previously discussed, which is straightforward. We now discuss how to integrate the recurrence mechanism into the proposed permutation setting and enable the model to reuse hidden states from previous segments.","Because our goal function works with the AR architecture, we include the cutting-edge AR language model, Transformer-XL [9], into our pretraining system, and title our method after it. We combine two key techniques from Transformer-XL: the relative positional encoding plan and the segment repetition mechanism. We implement relative positional encodings based on the original sequence as previously described, which is simple. We now explain how to integrate the recurrence mechanism into the proposed permutation configuration and empower the model to reuse hidden states from prior segments.",A,1
XLNet_Generalized Autoregressive Pretraining for Language Understanding,"This allows caching and reusing the memory without knowing the factorization order of the previous segment. In expectation, the model learns to utilize the memory over all factorization orders of the last segment. The query stream can be computed in the same way. Finally, Figure 1 (c) presents an overview of the proposed permutation language modeling with two-stream attention (see Appendix A.7 for more detailed illustration).","This enables storing and reusing the memory without being aware of the factorization sequence of the prior portion. On average, the model learns to leverage the memory across all factorization sequences of the most recent portion. The query flow can be determined similarly. Lastly, Figure 1 (c) gives an overview of the suggested permutation language modeling with two-stream focus (refer to Appendix A.7 for a more in-depth diagram).","This allows saving and reapplying the memory without knowing the decomposition order of the preceding section. In prospect, the model learns to employ the memory over all decomposition orders of the last section. The query stream can be produced in the same fashion. Finally, Figure 1 (c) provides a summary of the proposed permutation language modeling with two-channel attention (see Appendix A.7 for a more detailed illustration).  ","This permits caching and reusing the memory without being cognizant of the factorization sequence of the earlier segment. Broadly speaking, the model learns to take advantage of the memory across all factorization sequences of the most recent segment. The query flow can be generated similarly. In closing, Figure 1 (c) gives a synopsis of the suggested permutation language modeling with dual attention streams (refer to Appendix A.7 for a more comprehensive representation).",A,1
XLNet_Generalized Autoregressive Pretraining for Language Understanding,"Many downstream tasks have multiple input segments, e.g., a question and a context paragraph in question answering. We now discuss how we pretrain XLNet to model multiple segments in the autoregressive framework. During the pretraining phase, following BERT, we randomly sample two segments (either from the same context or not) and treat the concatenation of two segments as one sequence to perform permutation language modeling. We only reuse the memory that belongs to the same context.","Numerous later tasks have more than one input part, like a question and background section in querying. We will now talk about how we pre-train XLNet to represent multiple parts in the self-regressing structure. During pre-training, after BERT, we arbitrarily choose two parts (either from the same context or not) and handle putting the two parts together as one sequence to do permutation language modeling. We only reuse the memory that is part of the same context.","Many subsequent jobs have multiple input chunks, for instance, an ask and a context paragraph in asking. We will now examine how we pre-prepare XLNet to model multiple chunks in the self-referencing framework. During the pre-prep phase, following BERT, we randomly pick two chunks (either from the same context or not) and treat the combination of two chunks as one sequence to perform permutation language modeling. We only reuse the memory that belongs to the same context.","A lot of later tasks have multiple input segments, like a question and background paragraph in questioning. We will now discuss how we pre-train XLNet to represent multiple segments in the self-referential structure. During pre-training, after BERT, we randomly select two segments (either from the same context or not) and handle combining the two segments as one sequence to do permutation language modeling. We only reuse the memory that is part of the same context.",A,1
XLNet_Generalized Autoregressive Pretraining for Language Understanding,"Specifically, the input to our model is the same as BERT: [CLS, A, SEP, B, SEP], where “SEP” and “CLS” are two special symbols and “A” and “B” are the two segments. Although we follow the two-segment data format, XLNet-Large does not use the objective of next sentence prediction [10] as it does not show consistent improvement in our ablation study (see Section 3.4). Architecturally, different from BERT that adds an absolute segment embedding to the word embedding at each position, we extend the idea of relative encodings from Transformer-XL to also encode the segments.","In particular, we feed the same input to our model as BERT does: [CLS, A, SEP, B, SEP]. The tokens ""SEP"" and ""CLS"" are special symbols, while ""A"" and ""B"" represent the two segments. Despite using the two-segment format, XLNet-Large does not utilize next sentence prediction [10] since it did not consistently improve performance in our ablation experiments (refer to Section 3.4). Unlike BERT which adds an absolute segment embedding to each word embedding, we build on Transformer-XL's relative encodings to also encode the segments.","Specifically, our model takes as input: [CLS, A, SEP, B, SEP], identical to BERT. ""SEP"" and ""CLS"" are special tokens, and ""A"" and ""B"" are two segments. Although we use the two-segment setup, XLNet-Large skips next sentence prediction [10] because it didn't consistently boost results in our ablation tests (see Section 3.4). Architecturally, rather than BERT's absolute segment embedding added to each word embedding, we extend Transformer-XL's relative encodings to also encode segments.","In particular, we input [CLS, A, SEP, B, SEP] into our model, the same as BERT. ""SEP"" and ""CLS"" are special symbols, while ""A"" and ""B"" represent two segments. Despite employing the two-segment format, XLNet-Large omits next sentence prediction [10] since it didn't reliably improve performance in our ablation studies (refer to Section 3.4). Unlike BERT's absolute segment embedding added to each word embedding, we build on Transformer-XL's relative encodings to also encode the segments.",A,1
XLNet_Generalized Autoregressive Pretraining for Language Understanding,"Comparing Eq. (2) and (5), we observe that both BERT and XLNet perform partial prediction, i.e., only predicting a subset of tokens in the sequence. This is a necessary choice for BERT because if all tokens are masked, it is impossible to make any meaningful predictions. In addition, for both BERT and XLNet, partial prediction plays a role of reducing optimization difficulty by only predicting tokens with sufficient context. However, the independence assumption discussed in Section 2.1 disables BERT to model dependency between targets. To better understand the difference, let’s consider a concrete example [New, York, is, a, city].","By examining Equation 2 and 5, we see that BERT and XLNet both do partial forecasting, meaning they only predict some of the tokens in the sequence. This is essential for BERT since predicting all tokens is not possible if they are all masked. Furthermore, for BERT and XLNet, partial prediction helps make optimization less difficult by only predicting tokens with enough context. However, as discussed in Section 2.1, the independence assumption prevents BERT from modeling dependencies between targets. To illustrate the difference, consider the concrete example [New, York, is, a, city].","Analyzing Equations 2 and 5 reveals that BERT and XLNet both make incomplete predictions, predicting just some tokens in the sequence rather than all of them. This incomplete prediction is necessary for BERT because predicting every token is impossible if they are all obscured. Additionally, for both BERT and XLNet, incomplete prediction plays a role in decreasing the difficulty of optimization by only predicting tokens with adequate context. However, as mentioned in Section 2.1, the assumption of independence stops BERT from modeling dependencies between predictions. To better grasp the difference, examine the specific example [New, York, is, a, city].","Looking at Equation 2 and Equation 5 shows that BERT and XLNet both do partial foretelling, meaning they only anticipate a subset of the tokens in the sequence. This is essential for BERT since anticipating all tokens is impossible if they are all concealed. Also, for both BERT and XLNet, partial foretelling helps make optimization less troublesome by only anticipating tokens with sufficient context. However, as talked about in Section 2.1, the assumption of autonomy hinders BERT from modeling connections between anticipations. To illustrate the difference, think about the concrete example [New, York, is, a, city].",A,1
XLNet_Generalized Autoregressive Pretraining for Language Understanding,"Following BERT [10], we use the BooksCorpus [40] and English Wikipedia as part of our pretraining data, which have 13GB plain text combined. In addition, we include Giga5 (16GB text) [26], ClueWeb 2012-B (extended from [5]), and Common Crawl [6] for pretraining. We use heuristics to aggressively filter out short or low-quality articles for ClueWeb 2012-B and Common Crawl, which results in 19GB and 110GB text respectively. After tokenization with SentencePiece [17], we obtain 2.78B, 1.09B, 4.75B, 4.30B, and 19.97B subword pieces for Wikipedia, BooksCorpus, Giga5, ClueWeb, and Common Crawl respectively, which are 32.89B in total.","Following BERT's approach, we utilize the BooksCorpus and English Wikipedia for part of our pretraining information, totaling 13GB of plain text. Additionally, we make use of Giga5 (16GB), ClueWeb 2012-B (built on past work), and Common Crawl, for pretraining. We apply filters to aggressively remove short or low-quality articles from ClueWeb 2012-B and Common Crawl, resulting in 19GB and 110GB of text respectively. After tokenizing with SentencePiece, we get 2.78B, 1.09B, 4.75B, 4.30B, and 19.97B subword pieces for Wikipedia, BooksCorpus, Giga5, ClueWeb, and Common Crawl respectively, totaling 32.89B overall.","Adopting the methodology of BERT, our pretraining data incorporates the BooksCorpus and English Wikipedia, together supplying 13GB of plain text. We also utilize Giga5 (16GB text), ClueWeb 2012-B (expanding on previous work), and Common Crawl for pretraining. We filter out short or low-quality articles from ClueWeb 2012-B and Common Crawl aggressively, yielding 19GB and 110GB of text. Tokenizing with SentencePiece gives us 2.78B, 1.09B, 4.75B, 4.30B, and 19.97B subword pieces for Wikipedia, BooksCorpus, Giga5, ClueWeb, and Common Crawl respectively, amounting to 32.89B total.","As BERT did, our pretraining uses the BooksCorpus and English Wikipedia, which provide 13GB of plain text combined. Additionally, we use Giga5 (16GB text), ClueWeb 2012-B (building on past research), and Common Crawl for pretraining data. We apply aggressive filtering to remove short or low-quality articles from ClueWeb 2012-B and Common Crawl, getting 19GB and 110GB of text. Tokenizing with SentencePiece produces 2.78B, 1.09B, 4.75B, 4.30B, and 19.97B subword pieces for Wikipedia, BooksCorpus, Giga5, ClueWeb, and Common Crawl respectively, totaling 32.89B.",A,1
XLNet_Generalized Autoregressive Pretraining for Language Understanding,"Our largest model XLNet-Large has the same architecture hyperparameters as BERT-Large, which results in a similar model size. During pretraining, we always use a full sequence length of 512. Firstly, to provide a fair comparison with BERT (section 3.2), we also trained XLNet-Large-wikibooks on BooksCorpus and Wikipedia only, where we reuse all pretraining hyper-parameters as in the original BERT. Then, we scale up the training of XLNet-Large by using all the datasets described above. Specifically, we train on 512 TPU v3 chips for 500K steps with an Adam weight decay optimizer, linear learning rate decay, and a batch size of 8192, which takes about 5.5 days.","Our biggest model XLNet-Large contains the same structural settings as BERT-Large, so it has a comparable model capacity. When pretraining, we constantly utilize the full sequence length of 512. First, to fairly contrast with BERT (section 3.2), we trained XLNet-Large-wikibooks using just BooksCorpus and Wikipedia, reusing all pretraining settings from original BERT. Then, we expanded the training of XLNet-Large by employing all the datasets described previously. In particular, we train on 512 TPU v3 chips for 500K steps with an Adam weight decay optimizer, linear learning rate decay, and a batch size of 8192, which takes about 5.5 days.","Our most extensive model XLNet-Large has identical architecture hyperparameters to BERT-Large, resulting in a similar model magnitude. During pretraining, we perpetually employ the complete sequence length of 512. Initially, to give an impartial comparison with BERT (section 3.2), we also educated XLNet-Large-wikibooks solely on BooksCorpus and Wikipedia, where we reuse all pretraining settings from the original BERT. Subsequently, we scale up the training of XLNet-Large by utilizing all the datasets delineated above. Precisely, we train on 512 TPU v3 chips for 500K steps with an Adam weight decay optimizer, linear learning rate decay, and a batch size of 8192, which consumes about 5.5 days.  ","Our biggest model XLNet-Large contains the same architectural hyperparameters as BERT-Large, leading to a comparable model size. During pretraining, we always employ the full sequence length of 512. At first, to provide an equitable comparison with BERT (section 3.2), we also trained XLNet-Large-wikibooks exclusively on BooksCorpus and Wikipedia, reusing all pretraining settings from the original BERT. Afterward, we expand the training of XLNet-Large by leveraging all the datasets described previously. Specifically, we train on 512 TPU v3 chips for 500K steps with an Adam weight decay optimizer, linear learning rate decay, and a batch size of 8192, which consumes approximately 5.5 days.",A,1
XLNet_Generalized Autoregressive Pretraining for Language Understanding,"Here, we first compare the performance of BERT and XLNet in a fair setting to decouple the effects of using more data and the improvement from BERT to XLNet. In Table 1, we compare (1) best performance of three different variants of BERT and (2) XLNet trained with the same data and hyperparameters. As we can see, trained on the same data with an almost identical training recipe, XLNet outperforms BERT by a sizable margin on all the considered datasets.","In this section, we start by analyzing the performance of BERT and XLNet under equal conditions, in order to separate out the effects of using more data from the enhancements XLNet provides over BERT. Table 1 shows a comparison of (1) the best results from three different versions of BERT and (2) XLNet when trained using the same data and hyperparameters. As evident, when trained on identical data using nearly the same training process, XLNet substantially surpasses BERT across all the datasets considered.","We first juxtapose the capabilities of BERT and XLNet in an impartial environment, disentangling the influences of utilizing more information and the advancements from BERT to XLNet. Table 1 contrasts (1) the best exhibitions of three unique variants of BERT and (2) XLNet prepared with a similar information and hyperparameters. As should be obvious, prepared on a similar information with an almost indistinguishable preparing recipe, XLNet exceeds expectations BERT by a significant edge on every one of the considered datasets. ","Here, we start by weighing BERT against XLNet under even conditions, to isolate the effects of using more data from XLNet's improvements over BERT. Table 1 shows (1) the best results from three BERT variants and (2) XLNet when trained on the same data and hyperparameters. Clearly, when trained using virtually identical data and methods, XLNet substantially outdoes BERT across all the datasets tested.",A,1
XLNet_Generalized Autoregressive Pretraining for Language Understanding,"After the initial publication of our manuscript, a few other pretrained models were released such as RoBERTa [21] and ALBERT [19]. Since ALBERT involves increasing the model hidden size from 1024 to 2048/4096 and thus substantially increases the amount of computation in terms of FLOPs, we exclude ALBERT from the following results as it is hard to lead to scientific conclusions. To obtain relatively fair comparison with RoBERTa, the experiment in this section is based on full data and reuses the hyper-parameters of RoBERTa, as described in section 3.1.","Following the first publication of our paper, some other pre-trained models were made public like RoBERTa [21] and ALBERT [19]. We do not include ALBERT in the subsequent results given it greatly expands the model hidden dimension from 1024 to 2048/4096 which substantially raises the computational cost in FLOPs. Hence it is problematic to derive scientific conclusions. For a relatively equitable comparison with RoBERTa, the experiment here utilizes full data and recycles the hyper-parameters of RoBERTa, as delineated in section 3.1.","After our initial manuscript release, additional pretrained models emerged such as RoBERTa [21] and ALBERT [19]. Since ALBERT grows the model hidden size substantially from 1024 to 2048/4096, drastically increasing computational load in FLOPs, we omit ALBERT from the ensuing outcomes as drawing scientific conclusions is difficult. To obtain a relatively fair benchmark against RoBERTa, the trial in this portion leverages full data and reuses the hyperparameters of RoBERTa, as described in section 3.1.  ","Subsequent to the first publication of our paper, other pretrained models like RoBERTa [21] and ALBERT [19] were made public. As ALBERT expands the model hidden dimension markedly from 1024 to 2048/4096, leading to a major rise in computational cost per FLOPs, we do not include ALBERT in the following results as scientific conclusions are problematic. For a relatively equitable evaluation against RoBERTa, the experiment here employs full data and recycles the hyperparameters of RoBERTa, as outlined in section 3.1.",A,1
XLNet_Generalized Autoregressive Pretraining for Language Understanding,"We perform an ablation study to understand the importance of each design choice based on four datasets with diverse characteristics. Specifically, there are three main aspects we hope to study: 1) The effectiveness of the permutation language modeling objective alone, especially compared to the denoising auto-encoding objective used by BERT. 2) The importance of using Transformer-XL as the backbone neural architecture. 3) The necessity of some implementation details including span-based prediction, the bidirectional input pipeline, and next-sentence prediction.","We conduct an analysis removing different components to comprehend the value of each design decision using four datasets with varying properties. In particular, there are three primary facets we aspire to examine: 1) The potency of the permutation language modeling goal by itself, especially compared to the denoising auto-encoding purpose utilized by BERT. 2) The significance of employing Transformer-XL as the backbone neural building block. 3) The need for some implementation information including span-based forecasting, the bidirectional input workflow, and next-sentence prediction.","We perform an exploratory analysis eliminating various elements to understand the importance of each design selection based on four datasets with diverse qualities. Specifically, there are three key aspects we hope to investigate: 1) The effectiveness of just the permutation language modeling aim, contrasted with the denoising auto-encoding goal used in BERT. 2) The importance of utilizing Transformer-XL as the backbone neural architecture. 3) The necessity of some implementation details like span-based projection, the bidirectional input system, and next-sentence prediction.  ","We do a study removing different parts to grasp the value of each design decision using four datasets with varying natures. In particular, there are three primary facets we want to examine: 1) The potency of solely the permutation language modeling purpose, especially compared to the denoising auto-encoding goal employed by BERT. 2) The significance of using Transformer-XL as the backbone neural structure. 3) The need for some implementation information including span-based forecasting, the bidirectional input workflow, and next-sentence prediction.",A,1
XLNet_Generalized Autoregressive Pretraining for Language Understanding,"With these purposes in mind, in Table 6, we compare 6 XLNet-Base variants with different implementation details (rows 3 - 8), the original BERT-Base model (row 1), and an additional Transformer-XL baseline trained with the denoising auto-encoding (DAE) objective used in BERT but with the bidirectional input pipeline (row 2). For fair comparison, all models are based on a 12-layer architecture with the same model hyper-parameters as BERT-Base and are trained on only Wikipedia and the BooksCorpus. All results reported are the median of 5 runs.","To make fair comparisons between models, Table 6 shows 6 versions of XLNet-Base with various implementation specifics (rows 3-8), the original BERT-Base (row 1), and another Transformer-XL baseline taught using BERT's denoising autoencoding but with bidirectional input (row 2). All models have 12 layers with BERT-Base hyperparameters and were only trained on Wikipedia and BooksCorpus. The results are the median of 5 trials.","With fair testing in mind, Table 6 contrasts 6 variants of XLNet-Base with different details (rows 3-8), the original BERT-Base (row 1), and a supplementary Transformer-XL baseline educated utilizing BERT's denoising autoencoding however with bidirectional input (row 2). Every one of the models depend on a 12-layer design with BERT-Base hyperparameters and were prepared just on Wikipedia and BooksCorpus. The outcomes reported are the median of 5 runs. ","To enable equitable comparisons, Table 6 exhibits 6 forms of XLNet-Base with various execution subtleties (lines 3-8), the first BERT-Base (line 1), and an extra Transformer-XL benchmark prepared utilizing BERT's clamor evacuation autoencoding however with two-sided information (line 2). All models utilize a 12-layer design with BERT-Base hyperparameters and were prepared exclusively on Wikipedia and BooksCorpus. The outcomes introduced are the median of 5 trials.",A,1
XLNet_Generalized Autoregressive Pretraining for Language Understanding,"Examining rows 1 - 4 of Table 6, we can see both Transformer-XL and the permutation LM clearly contribute the superior performance of XLNet over BERT. Moreover, if we remove the memory caching mechanism (row 5), the performance clearly drops, especially for RACE which involves the longest context among the 4 tasks. In addition, rows 6 - 7 show that both span-based prediction and the bidirectional input pipeline play important roles in XLNet. Finally, we unexpectedly find the next-sentence prediction objective proposed in the original BERT does not necessarily lead to an improvement in our setting. Hence, we exclude the next-sentence prediction objective from XLNet.","Looking at rows 1 - 4 in Table 6, it is evident that both Transformer-XL and the permutation language model are important for XLNet's superior performance over BERT. Also, removing the memory caching mechanism (row 5) clearly decreases performance, especially on RACE which has the longest context of the 4 tasks. Furthermore, rows 6 - 7 demonstrate that span-based prediction and bidirectional input are both crucial components of XLNet. Interestingly, we find that the next-sentence prediction objective from original BERT does not improve performance here. Therefore, we omitted next-sentence prediction from XLNet.","Analyzing rows 1 through 4 in Table 6 makes it clear that Transformer-XL and the permutation language model are the main reasons XLNet outperforms BERT. Eliminating the memory caching mechanism (row 5) noticeably reduces performance, particularly on RACE which has the longest context among the 4 tasks. Additionally, rows 6 and 7 indicate that both span-based prediction and bidirectional input are integral parts of XLNet. Surprisingly, the original BERT's next-sentence prediction objective does not enhance performance in our experiments. As a result, we removed next-sentence prediction from XLNet.  ","Looking at the first 4 rows in Table 6, we can discern that Transformer-XL and the permutation language model are the primary factors behind XLNet's superior performance versus BERT. Also, taking away the memory caching mechanism (row 5) visibly decreases performance, especially for RACE which has the longest context of the 4 tasks. Moreover, rows 6 and 7 demonstrate that span-based prediction and bidirectional input are key components of XLNet. Unexpectedly, BERT's original next-sentence prediction objective does not improve performance here. Therefore, we took out next-sentence prediction from XLNet.",A,1
XLNet_Generalized Autoregressive Pretraining for Language Understanding,"XLNet is a generalized AR pretraining method that uses a permutation language modeling objective to combine the advantages of AR and AE methods. The neural architecture of XLNet is developed to work seamlessly with the AR objective, including integrating Transformer-XL and the careful design of the two-stream attention mechanism. XLNet achieves substantial improvement over previous pretraining objectives on various tasks.","XLNet is a general autoregressive pretraining technique that utilizes a permutation language modeling goal to combine the benefits of autoregressive and autoencoding approaches. The neural network architecture of XLNet is designed to seamlessly work with the autoregressive objective, including integrating Transformer-XL and the meticulous design of the two-stream attention system. XLNet accomplishes considerable enhancement over prior pretraining goals on a variety of tasks.","XLNet is a broad autoregressive pretraining procedure that leverages a permutation language modeling aim to unite the strengths of autoregressive and autoencoding methodologies. The neural network design of XLNet is engineered to smoothly interoperate with the autoregressive aim, encompassing assimilating Transformer-XL and the diligent conception of the two-stream attention framework. XLNet reaches significant betterment over earlier pretraining aims on numerous undertakings. ","XLNet is a wide-ranging autoregressive pretraining technique that harnesses a permutation language modeling purpose to combine the advantages of autoregressive and autoencoding techniques. The neural network architecture of XLNet is constructed to seamlessly collaborate with the autoregressive purpose, encompassing integrating Transformer-XL and the careful creation of the two-stream attention structure. XLNet accomplishes substantial enhancement over prior pretraining purposes on a variety of tasks.",A,1
XLNet_Generalized Autoregressive Pretraining for Language Understanding,"The RACE dataset [18] contains near 100K questions taken from the English exams for middle and high school Chinese students in the age range between 12 to 18, with the answers generated by human experts. This is one of the most difficult reading comprehension datasets that involve challenging reasoning questions. Moreover, the average length of the passages in RACE are longer than 300, which is significantly longer than other popular reading comprehension datasets such as SQuAD [29]. As a result, this dataset serves as a challenging benchmark for long text understanding. We use a sequence length of 512 during finetuning.","The RACE dataset [18] has almost 100,000 questions from English tests for Chinese students aged 12-18, with human-generated answers. It is one of the hardest reading comprehension datasets with tricky reasoning questions. Also, RACE passages average over 300 words, much longer than other popular datasets like SQuAD [29]. So it's a tough benchmark for understanding long text. We used a sequence length of 512 during fine-tuning.","The RACE dataset [18] includes nearly 100K English exam questions for Chinese middle and high schoolers from 12 to 18 years old, answered by experts. It is among the most challenging reading comprehension datasets featuring complex reasoning questions. Furthermore, RACE passages are significantly longer than 300 words on average, much more than other well-known datasets such as SQuAD [29]. Therefore, this dataset is a difficult test for comprehending long text. During fine-tuning, we utilized a sequence length of 512.  ","The RACE dataset [18] has around 100,000 English test questions for Chinese students in grades 7-12, ages 12-18, with human-generated answers. It is one of the toughest reading comprehension datasets with tricky reasoning questions. Also, RACE passages are over 300 words on average, much longer than other widely used datasets like SQuAD [29]. As a result, this dataset provides a challenging test for understanding lengthy text. During fine-tuning, we used a sequence length of 512.",A,1
XLNet_Generalized Autoregressive Pretraining for Language Understanding,"The GLUE dataset [34] is a collection of 9 natural language understanding tasks. The test set labels are removed from the publicly released version, and all the practitioners must submit their predictions on the evaluation server to obtain test set results. In Table 5, we present results of multiple settings, including single-task and multi-task, as well as single models and ensembles. In the multi-task setting, we jointly train an XLNet on the four largest datasets—MNLI, SST-2, QNLI, and QQP—and finetune the network on the other datasets. Only single-task training is employed for the four large datasets.","The GLUE benchmark [34] is a set of 9 tasks for evaluating natural language understanding. The test labels are not publicly available, so researchers must submit predictions to the evaluation server to get test results. Table 5 shows the performance of various settings on GLUE, including single-task and multi-task learning, as well as individual models and ensembles. For multi-task learning, we train an XLNet jointly on the 4 largest datasets - MNLI, SST-2, QNLI, and QQP - then fine-tune on the other datasets. We only use single-task learning for the 4 big datasets.","The GLUE collection [34] contains 9 natural language understanding tasks. The test set labels are kept private, so practitioners need to upload their predictions to the evaluation server to see test results. Table 5 presents the performance of different approaches on GLUE, including training on individual tasks vs multiple tasks, and single models vs ensembles. For multi-task learning, we train one XLNet model together on the 4 biggest datasets - MNLI, SST-2, QNLI, and QQP - and then fine-tune on the other datasets. We only train separately on each task for the 4 large datasets.  ","GLUE [34] is a benchmark with 9 natural language understanding tasks. The test labels are not released publicly, so teams must submit predictions to the evaluation server to get test results. Table 5 shows the performance of various methods on GLUE, including single-task and multi-task training, and individual models versus ensembles. For multi-task learning, we jointly train one XLNet on the 4 largest datasets - MNLI, SST-2, QNLI, and QQP - then fine-tune on the other datasets. We use single-task training exclusively for the 4 big datasets.",A,1
XLNet_Generalized Autoregressive Pretraining for Language Understanding,"Following the setting in previous work [8], we use the ClueWeb09-B dataset to evaluate the performance on document ranking. The queries were created by the TREC 2009-2012 Web Tracks based on 50M documents and the task is to rerank the top 100 documents retrieved using a standard retrieval method. Since document ranking, or ad-hoc retrieval, mainly concerns the low-level representations instead of high-level semantics, this dataset serves as a testbed for evaluating the quality of word embeddings. We use a pretrained XLNet to extract word embeddings for the documents and queries without fine tuning, and employ a kernel pooling network [36] to rank the documents.","As established in prior research [8], we utilize the ClueWeb09-B data collection to assess performance on ranking documents. The inquiries were formed by the TREC 2009-2012 Web Tracks founded on 50M texts and the objective is to reorganize the top 100 articles obtained utilizing a standard retrieval technique. Because document ranking, or ad-hoc retrieval, chiefly deals with low-level representations rather than high-level semantics, this data collection functions as a testbed for evaluating the caliber of word embeddings. We employ a pre-trained XLNet to derive word embeddings for the documents and questions without fine-tuning, and use a kernel pooling network [36] to rank the documents.","In line with earlier work [8], we make use of the ClueWeb09-B dataset to gauge effectiveness on document ranking. The questions were generated by the TREC 2009-2012 Web Tracks built on 50M papers and the goal is to re-rank the top 100 papers retrieved using a standard retrieval method. Since document ranking, or ad-hoc retrieval, primarily concerns low-level representations instead of high-level semantics, this dataset serves as a testbed for assessing the quality of word embeddings. We utilize a pre-trained XLNet to extract word embeddings for the documents and questions without fine-tuning, and utilize a kernel pooling network [36] to rank the documents.","As established in previous research [8], we employ the ClueWeb09-B dataset to evaluate performance on ranking documents. The queries were formed by the TREC 2009-2012 Web Tracks based on 50M texts and the task is to rearrange the top 100 texts obtained using a standard retrieval approach. Because document ranking, or ad-hoc retrieval, mostly relates to low-level representations rather than high-level semantics, this dataset functions as a testbed for assessing the quality of word embeddings. We use a pre-trained XLNet to derive word embeddings for the documents and queries without fine-tuning, and use a kernel pooling network [36] to rank the documents.",A,1
XLNet_Generalized Autoregressive Pretraining for Language Understanding,"Such a limitation of AR language modeling can be critical in real-world applications. For example, consider a span extraction question answering task with the context “Thom Yorke is the singer of Radiohead” and the question “Who is the singer of Radiohead”. The representations of “Thom Yorke” are not dependent on “Radiohead” with AR language modeling and thus they will not be chosen as the answer by the standard approach that employs softmax over all token representation.","This kind of constraint on autoregressive language modeling can be very important in real uses. As an illustration, think about a span extraction question answering task with the context ""Thom Yorke is the vocalist of Radiohead"" and the question ""Who is the vocalist of Radiohead"". The representations of ""Thom Yorke"" do not rely on ""Radiohead"" with AR language modeling, so they will not be selected as the answer by the standard method that uses softmax over all token representations.","Such a restriction of AR language modeling could be crucial in practical applications. For instance, imagine a span extraction QA task where the context is ""Thom Yorke sings for the band Radiohead"" and the question is ""Who sings for Radiohead?"". Since the representations for ""Thom Yorke"" are independent of ""Radiohead"" in AR language models, they would not be predicted as the answer using the typical approach of applying softmax over all token representations.","This kind of limitation on autoregressive language models can be very significant in real-world uses. For example, consider a span extraction question answering task where the background is ""Thom Yorke is the lead singer of the band Radiohead"" and the question is ""Who leads the vocals for Radiohead?"". Because the representations for ""Thom Yorke"" do not depend on ""Radiohead"" in AR language models, they would not be selected as the response by the standard technique of using softmax over all token representations.",A,1
XLNet_Generalized Autoregressive Pretraining for Language Understanding,"With a deep root in density estimation4 [4, 32, 24], language modeling has been a rapidly-developing research area [9, 1, 3]. However, there has been a gap between language modeling and pretraining due to the lack of the capability of bidirectional context modeling, as analyzed in Section A.5.2. It has even been challenged by some machine learning practitioners whether language modeling is a meaningful pursuit if it does not directly improve downstream tasks. XLNet generalizes language modeling and bridges such a gap. As a result, it further “justifies” language modeling research.","Language modeling has developed quickly as an area of research and is firmly rooted in density estimation techniques. However, there has been a divide between language modeling and pretraining because language models could not model context bidirectionally, as discussed in Section A.5.2. Some machine learning experts have even questioned whether language modeling is worthwhile if it does not directly improve performance on downstream tasks. XLNet expands language modeling capabilities and connects language modeling with pretraining. As a result, it provides further validation for research on language modeling.","With foundations in density approximation, language modeling has seen fast progress as a research field. But language modeling and pretraining have been disconnected due to language models' inability to model context bidirectionally, per Section A.5.2. Some machine learning professionals have challenged if language modeling is meaningful if it does not boost downstream performance. XLNet generalizes language modeling and links it to pretraining. Thus, it additionally ""legitimizes"" language modeling work.","Language modeling research has developed rapidly, building on density estimation methods. However, a separation has existed between language modeling and pretraining because of language models' lack of bidirectional context capabilities, as discussed in Section A.5.2. Some machine learning experts have even questioned the value of language modeling if it does not directly enhance downstream tasks. XLNet expands language modeling and connects it to pretraining. Therefore, it further ""validates"" language modeling as an area of research.",A,1
XLNet_Generalized Autoregressive Pretraining for Language Understanding,"More interestingly, in Fig. 3, we present 3 patterns that only appear in XLNet but not BERT: (a) The self-exclusion pattern attends to all other tokens but itself, probably offering a fast way to gather global information; (b) The relative-stride pattern attends to positions every a few stride apart relative to the query position; (c) The one-side masked pattern is very similar to the lower-left part of Fig. 1-(d), with the upper-right triangle masked out. It seems that the model learns not to attend the relative right half.","In a more fascinating way, Fig. 3 displays 3 models that are unique to XLNet but absent in BERT: (a) The self-exclusion model pays attention to all tokens except itself, likely providing a rapid approach to collect global details; (b) The relative-stride model attends to locations periodically spaced compared to the query location; (c) The one-side masked model closely resembles the lower-left section of Fig. 1-(d), with the upper-right triangle concealed. It appears the model learns to not focus on the relative right half.","More intriguingly, Fig. 3 exhibits 3 designs existent only in XLNet and not BERT: (a) The self-exclusion design regards all other symbols but itself, probably offering a quick means to accumulate comprehensive information; (b) The relative-stride design considers positions spaced apart at a fixed interval relative to the query position; (c) The one-side masked design highly resembles the lower-left area of Fig. 1-(d), with the upper-right triangle obscured. It seems the model learns to not pay attention to the relative right section.","In a more interesting way, Fig. 3 displays 3 patterns present only in XLNet and absent in BERT: (a) The self-exclusion pattern notes all other marks except itself, likely providing a fast approach to gather global details; (b) The relative-stride pattern focuses on spots periodically spaced compared to the query spot; (c) The one-side masked pattern is very similar to the lower-left part of Fig. 1-(d), with the upper-right triangle concealed. It appears the model learns to not concentrate on the relative right portion.",A,1
XLNet_Generalized Autoregressive Pretraining for Language Understanding,"Note that all these three unique patterns involve the relative positions rather than absolute ones, and hence are likely enabled by the “relative attention” mechanism in XLNet. We conjecture these unique patterns contribute to the performance advantage of XLNet. On the other hand, the proposed permutation LM objective mostly contributes to a better data efficiency, whose effects may not be obvious from qualitative visualization.","Observe that all three of these distinctive designs depend on comparative rather than absolute placements, so they are probably facilitated by the ""relative attention"" component in XLNet. We hypothesize these unusual designs help explain the superior performance of XLNet. However, the suggested permutation language model goal appears to mainly assist with better data efficiency, which may not be evident from qualitative inspection.","Notice that all of these three novel blueprints involve positional relationships rather than absolute positions, so they are plausibly empowered by the ""relative attention"" apparatus in XLNet. We theorize these novel blueprints lend a hand to the enhanced capabilities of XLNet. Though, the advised permutation language archetype ambition seems to chiefly lend a hand with superior data thrift, whose goods may not be noticeable from qualitative scanning. ","Recognize that all three of these unique outlines hinge on correlated rather than absolute placements, hence they are potentially enabled by the ""relative attention"" mechanism within XLNet. We posit these distinctive outlines contribute to the heightened effectiveness of XLNet. However, the suggested permutation language model objective apparently mostly assists with superior data economy, whose benefits may not be discernible from qualitative examination.",A,1
"You Only Look Once_Unified, Real-Time Object Detection","We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast.","We introduce YOLO, a novel method for recognizing objects in images. Earlier object detection systems modify classifiers for detection tasks. Our approach formulates object detection as a regression problem to locate bounding boxes and compute class probabilities. A single neural network estimates bounding boxes and class probabilities straight from full images in one pass. Since the complete detection system is a single network, it can be trained end-to-end directly for detection accuracy. Our unified design is remarkably fast.","We present YOLO, an innovative approach to detecting objects. Prior object detection systems repurpose classifiers to do detection. We pose object detection as a regression problem to find bounding boxes and class probabilities. One neural net predicts bounding boxes and probabilities straight from entire images in one shot. The full pipeline being one network means it can optimize detection performance end-to-end. Our unified architecture is extremely fast. ","We introduce YOLO, a new technique for object detection. Earlier detection systems adapt classifiers for detection. We formulate detection as regression to localize bounding boxes and predict class probabilities. A single neural network estimates boxes and probabilities directly from whole images in one evaluation. The full pipeline being one network enables end-to-end optimization for detection. Our unified design is very fast.",A,1
"You Only Look Once_Unified, Real-Time Object Detection","Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is less likely to predict false positives on background. Finally, YOLO learns very general representations of objects. It outperforms other detection methods, including DPM and R-CNN, when generalizing from natural images to other domains like artwork.","The initial YOLO architecture can analyze images in real-time at a rate of 45 frames per second. A smaller variant called Fast YOLO can remarkably process 155 frames each second while still attaining double the mAP of other real-time systems. In comparison with other state-of-the-art detectors, YOLO makes more mistakes localizing objects but is not as likely to incorrectly identify background regions. Moreover, YOLO acquires very broad representations of objects. It surpasses other detection approaches like DPM and R-CNN when transferring from natural images to other areas such as artwork.","Our baseline YOLO framework can evaluate images instantly at 45 fps. A reduced form, Fast YOLO, can astoundingly assess 155 fps while maintaining 2x the mAP of other real-time tools. Relative to cutting-edge detection, YOLO has more errors localizing but fewer false positives on background. Additionally, YOLO learns very wide-ranging representations of objects. It exceeds other methods including DPM and R-CNN when shifting from natural images to domains such as artwork.  ","The original YOLO architecture is capable of processing images rapidly at 45 frames per second. A compact version called Fast YOLO remarkably evaluates 155 frames per second while preserving twice the mAP of other real-time systems. In comparison to state-of-the-art detection, YOLO makes more localization mistakes but has fewer incorrect identifications of background. Furthermore, YOLO develops very flexible representations of objects. It outperforms other approaches like DPM and R-CNN when moving from natural images to areas like artwork.",A,1
"You Only Look Once_Unified, Real-Time Object Detection","Humans glance at an image and instantly know what objects are in the image, where they are, and how they interact. The human visual system is fast and accurate, allowing us to perform complex tasks like driving with little conscious thought. Fast, accurate algorithms for object detection would allow computers to drive cars without specialized sensors, enable assistive devices to convey real-time scene information to human users, and unlock the potential for general purpose, responsive robotic systems. Current detection systems repurpose classifiers to perform detection.","People look at a picture and right away can identify the items in it, where they are situated, and how they connect. The human ability to see is quick and precise, letting us do intricate jobs like operating a vehicle without much conscious effort. Speedy, correct programs for noticing objects would let computers steer autos minus specific devices, help assistive gadgets communicate real-time scene info to human users, and unleash the capability for general rationale, receptive robotic frameworks. Existing identification frameworks repurpose classifiers to play out identification.","Humans glimpse at a visual and instantly comprehend what objects exist in the visual, their locations, and their interactions. The human sense of sight is fast and accurate, enabling us to execute complex tasks such as driving with minimal conscious deliberation. Rapid, precise algorithms for object recognition would enable computers to operate vehicles without specialized sensors, allow assistive technologies to convey real-time scene data to human users, and unlock the potential for general purpose, responsive robotic systems. Current detection technologies reuse classifiers to conduct detection.","People look at a picture and right away know what things are in it, where they are, and how they connect. The human ability to see is quick and correct, letting us perform intricate jobs like driving a car without much conscious thinking. Fast, accurate programs for spotting objects would let computers steer cars without special sensors, help assistive gadgets relay real-time scene info to people, and release the potential for general purpose, responsive robot systems. Existing identification systems repurpose classifiers to do identification.",A,1
"You Only Look Once_Unified, Real-Time Object Detection","To detect an object, these systems take a classifier for that object and evaluate it at various locations and scales in a test image. Systems like deformable parts models (DPM) use a sliding window approach where the classifier is run at evenly spaced locations over the entire image [10]. More recent approaches like R-CNN use region proposal methods to first generate potential bounding boxes in an image and then run a classifier on these proposed boxes. After classification, post-processing is used to refine the bounding boxes, eliminate duplicate detections, and rescore the boxes based on other objects in the scene [13].","In order to identify an object, these frameworks utilize a classifier for that object and assess it across different positions and proportions in a test image. Methods such as deformable parts models (DPM) employ a sliding window tactic where the classifier is executed at evenly distributed spots over the whole image [10]. More contemporary frameworks like R-CNN leverage region proposal techniques to first create potential bounding boxes in an image and afterward run a classifier on these suggested boxes. After classification, post-processing is utilized to refine the bounding boxes, eliminate duplicate identifications, and rescore the boxes dependent on other objects in the scene [13].","To recognize an object, these frameworks use a classifier for that object and evaluate it at different locations and sizes in a test image. Approaches like deformable parts models (DPM) utilize a sliding window strategy where the classifier is applied at evenly spaced spots across the entire image [10]. Newer methods like R-CNN first generate candidate bounding boxes in an image using region proposal techniques and then apply a classifier to these proposed boxes. After classification, additional processing is used to improve the bounding boxes, remove duplicate detections, and rescore the boxes based on other objects in the scene [13].","In order to detect an object, these systems employ a classifier for that object and assess it at various positions and scales in a test image. Techniques such as deformable parts models (DPM) use a sliding window tactic where the classifier is executed at evenly distributed locations over the whole image [10]. More recent techniques like R-CNN first generate potential bounding boxes in an image using region proposal methods and then execute a classifier on these proposed boxes. Following classification, post-processing is utilized to refine the bounding boxes, eliminate repeated detections, and rescore the boxes based on other objects in the scene [13].",A,1
"You Only Look Once_Unified, Real-Time Object Detection","These complex pipelines are slow and hard to optimize because each individual component must be trained separately. We reframe object detection as a single regression problem, straight from image pixels to bounding box coordinates and class probabilities. Using our system, you only look once (YOLO) at an image to predict what objects are present and where they are. YOLO is refreshingly simple: see Figure 1. A single convolutional network simultaneously predicts multiple bounding boxes and class probabilities for those boxes. YOLO trains on full images and directly optimizes detection performance.","These intricate pipelines are sluggish and difficult to enhance because each separate part needs to be educated on its own. We restructure object identification as a sole regression issue, straight from image pixels to bounding container coordinates and class probabilities. Utilizing our framework, you just observe once (YOLO) at a photo to predict what items exist and where they are situated. YOLO is remarkably simple: see Figure 1. A single convolutional system concurrently predicts multiple bounding containers and class odds for those containers. YOLO learns on complete images and directly enhances detection performance.","These complex networks are slow and challenging to improve because every component has to be trained independently. We reimagine object recognition as a single regression task, directly from image pixels to bounding box locations and class likelihoods. With our approach, you only inspect once (YOLO) at an image to determine what objects are there and where they are. YOLO is refreshingly uncomplicated: see Figure 1. A single convolutional architecture simultaneously anticipates multiple bounding boxes and class probabilities for those boxes. YOLO is trained on full images and directly enhances detection accuracy.","These elaborate pipelines are sluggish and tough to optimize since each part has to be educated separately. We recast object identification as a solitary regression problem, straight from image pixels to bounding box coordinates and class probabilities. Using our framework, you just look once (YOLO) at a picture to predict what objects exist and where they are located. YOLO is surprisingly straightforward: see Figure 1. A single convolutional network at the same time forecasts multiple bounding boxes and class odds for those boxes. YOLO learns on entire images and directly improves detection performance.",A,1
"You Only Look Once_Unified, Real-Time Object Detection","This unified model has several benefits over traditional methods of object detection. First, YOLO is extremely fast. Since we frame detection as a regression problem we don’t need a complex pipeline. We simply run our neural network on a new image at test time to predict detections. Our base network runs at 45 frames per second with no batch processing on a Titan X GPU and a fast version runs at more than 150 fps. This means we can process streaming video in real-time with less than 25 milliseconds of latency. Furthermore, YOLO achieves more than twice the mean average precision of other real-time systems.","This combined model has multiple advantages compared to conventional approaches for detecting objects. To start, YOLO is incredibly quick. Because we formulate detection as a regression task, we don't require an elaborate pipeline. We just run our neural network on a new image during testing to predict detections. Our foundation network operates at 45 frames per second without batch processing on a Titan X GPU and a faster version functions at over 150 fps. This signifies we can handle streaming video in real time with less than 25 milliseconds of delay. Additionally, YOLO attains more than two times the mean average precision of other real-time frameworks.","This unified system has several benefits versus old-fashioned techniques for recognizing objects. First off, YOLO is extremely fast. Since we present detection as a regression challenge, we don't need a complicated sequence. We simply execute our neural net on a new pic at test time to predict detections. Our base network performs at 45 frames per second without batch handling on a Titan X GPU and a quicker version runs at over 150 fps. This implies we can manage streaming video in real-time with less than 25 milliseconds of lag. Furthermore, YOLO accomplishes more than double the mean average precision of other real-time setups.","This combined model has multiple advantages compared to traditional methods of spotting objects. To begin, YOLO is extremely quick. Because we frame detection as a regression issue, we don't require a complex pipeline. We just execute our neural network on a new image during testing to predict detections. Our foundation network functions at 45 frames per second without batch processing on a Titan X GPU and a speedier version operates at over 150 fps. This means we can handle streaming video in real-time with less than 25 milliseconds of delay. Additionally, YOLO reaches more than twice the mean average precision of other real-time systems.",A,1
"You Only Look Once_Unified, Real-Time Object Detection","Second, YOLO reasons globally about the image when making predictions. Unlike sliding window and region proposal-based techniques, YOLO sees the entire image during training and test time so it implicitly encodes contextual information about classes as well as their appearance. Fast R-CNN, a top detection method [14], mistakes background patches in an image for objects because it can’t see the larger context. YOLO makes less than half the number of background errors compared to Fast R-CNN. Third, YOLO learns generalizable representations of objects.","Furthermore, YOLO considers the full image when making forecasts. In contrast to methods that use sliding windows or region proposals, YOLO observes the whole image during training and testing, so it inherently represents context about classes and their looks. Fast R-CNN, a leading detection technique [14], misidentifies background sections in an image as objects since it can't view the bigger context. YOLO makes fewer than half as many background errors as Fast R-CNN. Additionally, YOLO acquires generalizable representations of objects.","Moreover, YOLO examines the complete image when predicting. Unlike approaches using sliding windows or regional proposals, YOLO sees the whole image during training and inference thus encoding implicit information about classes and appearances. Fast R-CNN, a top detection method [14], incorrectly labels background parts of an image as objects because it lacks the larger context. YOLO has less than half the background errors of Fast R-CNN. Also, YOLO develops adaptable representations of objects.  ","In addition, YOLO evaluates the entire image when forecasting. In contrast with techniques using sliding windows or regional proposals, YOLO observes the full image during training and testing, thereby inherently capturing contextual information about classes and looks. Fast R-CNN, a leading detection approach [14], misclassifies background sections of an image as objects since it lacks the broader context. YOLO has fewer than half the background errors as Fast R-CNN. Furthermore, YOLO forms generalizable representations of objects.",A,1
"You Only Look Once_Unified, Real-Time Object Detection","When trained on natural images and tested on artwork, YOLO outperforms top detection methods like DPM and R-CNN by a wide margin. Since YOLO is highly generalizable it is less likely to break down when applied to new domains or unexpected inputs. YOLO still lags behind state-of-the-art detection systems in accuracy. While it can quickly identify objects in images it struggles to precisely localize some objects, especially small ones. We examine these tradeoffs further in our experiments. All of our training and testing code is open source. A variety of pretrained models are also available to download.","After being taught using natural pictures and then evaluated on pieces of art, YOLO is much better at detecting objects compared to other excellent detection systems such as DPM and R-CNN. Since YOLO can generalize very well, it is not as likely to fail when used on new areas or with unexpected inputs. However, YOLO is still not as accurate as the most advanced detection programs available right now. Although it can swiftly recognize objects in images, it has trouble precisely locating some objects, particularly small ones. We look further into these tradeoffs in our experiments. Our training and testing code is open source and available to the public. There are also several pretrained models that can be downloaded.","When YOLO was trained using normal images and then tested on art, it substantially outperformed top object detection methods including DPM and R-CNN. Because YOLO is highly adaptable, it is less prone to problems when applied to new domains or unexpected inputs. However, YOLO still does not match state-of-the-art detection systems in terms of accuracy. While it can quickly identify objects in images, it struggles to precisely pinpoint the location of some objects, especially small ones. We examine these tradeoffs more in our experiments. All our training and testing code is open source and accessible. Multiple pretrained models can also be downloaded.  ","After being trained on natural images and evaluated on art, YOLO significantly exceeds other leading detection methods such as DPM and R-CNN in its ability to detect objects. Since YOLO is extremely generalizable, it is less likely to fail when used on new areas or with unexpected inputs. However, YOLO still falls short of the most advanced detection systems in terms of accuracy. Although it can rapidly recognize objects in images, it has difficulty precisely localizing some objects, particularly small ones. We delve further into these tradeoffs in our experiments. Our training and testing code is open source and available to the public. There are several pretrained models that are also available to download.",A,1
"You Only Look Once_Unified, Real-Time Object Detection","We unify the separate components of object detection into a single neural network. Our network uses features from the entire image to predict each bounding box. It also predicts all bounding boxes across all classes for an image simultaneously. This means our network reasons globally about the full image and all the objects in the image. The YOLO design enables end-to-end training and real time speeds while maintaining high average precision. Our system divides the input image into an S × S grid. If the center of an object falls into a grid cell, that grid cell is responsible for detecting that object.","We combine the different parts of object detection into one neural network. Our network utilizes features from the whole image to forecast each bounding box. It also anticipates all bounding boxes across all classes for an image at the same time. This means our network thinks globally about the complete image and all the objects in the image. The YOLO design allows end-to-end training and real time speeds while keeping high average precision. Our system splits the input image into an S × S grid. If the center of an object is inside a grid cell, that grid cell is accountable for spotting that object.","We unify the individual elements of object detection into a single neural network. Our network leverages information from the full image to predict each bounding box. It also foresees all bounding boxes across all classes for an image simultaneously. This signifies our network reasons holistically about the entire image and all the objects in the image. The YOLO architecture permits end-to-end learning and real time velocities while maintaining high average precision. Our system divides the input image into an S × S grid. If the center of an object falls within a grid cell, that grid cell is responsible for identifying that object.","We consolidate the separate pieces of object detection into one neural network. Our network exploits data from the whole image to project each bounding box. It also anticipates all bounding boxes across all classes for an image at once. This denotes our network thinks comprehensively about the complete image and all the objects in the image. The YOLO model allows end-to-end education and real time paces while keeping high average precision. Our system splits the input image into an S × S grid. If the center of an object is inside a grid cell, that grid cell is accountable for detecting that object.",A,1
"You Only Look Once_Unified, Real-Time Object Detection","The (x, y) coordinates represent the center of the box relative to the bounds of the grid cell. The width and height are predicted relative to the whole image. Finally the confidence prediction represents the IOU between the predicted box and any ground truth box. We implement this model as a convolutional neural network and evaluate it on the PASCAL VOC detection dataset [9]. The initial convolutional layers of the network extract features from the image while the fully connected layers predict the output probabilities and coordinates.","The (x, y) values denote the middle of the box in relation to the edges of the grid unit. The width and height are anticipated proportional to the full image. Lastly the confidence forecast represents the IOU between the expected box and any actual box. We build this model as a convolutional neural system and assess it on the PASCAL VOC detection data set [9]. The early convolutional tiers of the system draw out features from the image while the completely linked tiers predict the production probabilities and locations.","The (x, y) points symbolize the center of the box with regards to the boundaries of the grid segment. The width and height are foreseen relative to the entire image. Finally, the confidence prediction characterizes the IOU between the expected box and any factual box. We construct this model as a convolutional neural network and evaluate it on the PASCAL VOC detection dataset [9]. The initial convolutional layers of the network derive features from the image while the fully connected layers anticipate the output probabilities and coordinates.  ","The (x, y) coordinates depict the middle of the box in context to the confines of the grid unit. The width and height are projected proportional to the whole image. Ultimately, the confidence forecast represents the IOU between the anticipated box and any real box. We build this model as a convolutional neural network and assess it on the PASCAL VOC detection dataset [9]. The early convolutional tiers of the network derive features from the image while the fully linked tiers predict the output probabilities and locations.",A,1
"You Only Look Once_Unified, Real-Time Object Detection","Our network architecture is inspired by the GoogLeNet model for image classification [34]. Our network has 24 convolutional layers followed by 2 fully connected layers. Instead of the inception modules used by GoogLeNet, we simply use 1 × 1 reduction layers followed by 3 × 3 convolutional layers, similar to Lin et al [22]. The full network is shown in Figure 3. We also train a fast version of YOLO designed to push the boundaries of fast object detection. Fast YOLO uses a neural network with fewer convolutional layers (9 instead of 24) and fewer filters in those layers. Other than the size of the network, all training and testing parameters are the same between YOLO and Fast YOLO.","The design of our neural network framework takes inspiration from the GoogLeNet architecture for categorizing images [34]. Our framework contains 24 layers for detecting features plus 2 fully connected layers. Rather than using the inception modules from GoogLeNet, we utilize 1 × 1 reduction layers followed by 3 × 3 convolutional layers, similar to the approach by Lin et al [22]. Figure 3 outlines the full structure of our network. Additionally, we train a rapid version of YOLO intended to push the limits of quick object detection. This Fast YOLO uses a neural network with fewer convolutional layers (9 rather than 24) and fewer filters per layer. Aside from the network size, all training and testing parameters are identical between YOLO and Fast YOLO.","Our neural network design derives inspiration from the GoogLeNet model for image classification tasks [34]. It consists of 24 convolutional layers and 2 fully connected layers. Instead of the inception modules in GoogLeNet, we use 1 × 1 reduction layers followed by 3 × 3 convolutional layers, akin to the work by Lin et al [22]. Figure 3 shows the complete network architecture. We also trained a faster variant of YOLO meant to extend the boundaries of rapid object detection. This Fast YOLO utilizes a neural network with fewer convolutional layers (9 instead of 24) and fewer filters in those layers. Other than the network size, all training and testing settings are the same across YOLO and Fast YOLO.  ","We based the architecture of our neural network on the GoogLeNet model for image classification [34]. It contains 24 convolutional layers and 2 fully connected layers. Rather than using the inception modules of GoogLeNet, we employ 1 × 1 reduction layers followed by 3 × 3 convolutional layers, similar to Lin et al [22]. The full network structure is diagrammed in Figure 3. Additionally, we trained a speedy version of YOLO designed to push the limits of fast object detection. This Fast YOLO uses a neural network with fewer convolutional layers (9 instead of 24) and fewer filters per layer. Aside from the network dimensions, all training and testing parameters are identical for YOLO and Fast YOLO.",A,1
"You Only Look Once_Unified, Real-Time Object Detection","We pretrain our convolutional layers on the ImageNet 1000-class competition dataset [30]. For pretraining we use the first 20 convolutional layers from Figure 3 followed by a average-pooling layer and a fully connected layer. We train this network for approximately a week and achieve a single crop top-5 accuracy of 88% on the ImageNet 2012 validation set, comparable to the GoogLeNet models in Caffe’s Model Zoo [24]. We use the Darknet framework for all training and inference [26]. We then convert the model to perform detection.","We pre-train the layers in our convolutional neural network that are responsible for detecting visual features using the dataset from the ImageNet image classification competition, which contains 1000 different categories. To do this pre-training, we utilize the first 20 convolutional layers from our network architecture, along with an average pooling layer and a fully-connected layer. We train this portion of the network for about a week and attain a top-5 accuracy of 88% on a single crop of the 2012 ImageNet validation images, which is on par with GoogLeNet models from the Caffe Model Zoo. We use the Darknet framework to handle training and making predictions. After that, we modify the model to perform object detection tasks.","Before using our convolutional neural network for detection, we first pre-condition the convolutional layers using the 1000-class ImageNet dataset. For this initial training, we take the first 20 convolutional layers from our network design, as well as an average pooling layer and fully connected layer. After approximately 1 week of training, this portion of the network achieves a top-5 accuracy of 88% on single crops from the 2012 ImageNet validation set, which matches GoogLeNet models from the Caffe Model Zoo. We carry out all training and predictions with the Darknet framework. Subsequently, we adapt the model to do detection.","As an initial step, we pre-train the convolutional layers of our network using the 1000-category ImageNet competition dataset. For this pre-training phase, we utilize the first 20 convolutional layers shown in our network architecture, along with an average pooling layer and fully connected layer. After roughly 1 week of training, we reach a top-5 accuracy of 88% on individual crops of images from the 2012 ImageNet validation set, on par with GoogLeNet models in the Caffe Model Zoo. We use the Darknet framework for training and making inferences. We then modify the model to handle object detection tasks.",A,1
"You Only Look Once_Unified, Real-Time Object Detection","Ren et al. show that adding both convolutional and connected layers to pretrained networks can improve performance [29]. Following their example, we add four convolutional layers and two fully connected layers with randomly initialized weights. Detection often requires fine-grained visual information so we increase the input resolution of the network from 224 × 224 to 448 × 448. Our final layer predicts both class probabilities and bounding box coordinates. We normalize the bounding box width and height by the image width and height so that they fall between 0 and 1.","Ren and colleagues demonstrate that appending convolutional and fully connected layers to pre-trained networks enhances system performance [29]. Pursuing their approach, we supplement four convolutional strata and two fully connected strata possessing arbitrarily initialized weights. Identifying objects frequently necessitates fine-grained visual data, so we boost the input resolution of the network from 224 × 224 to 448 × 448. Our final stratum predicts class likelihoods and bounding box coordinates. We standardize the bounding box width and height by the image width and height so they range between 0 and 1.","The study by Ren et al. exhibits that integrating both convolutional and dense layers into pretrained models can improve outcomes [29]. Mirroring their methodology, we incorporate four convolutional tiers and two fully connected tiers with randomly assigned weights. Detection often needs precise visual information hence we expand the input dimensions of the network from 224 × 224 to 448 × 448. Our final tier estimates both class probabilities and bounding box coordinates. We normalize the bounding box width and height against the image width and height so they fall in the interval 0 to 1.","Research by Ren and coauthors reveals that appending both convolutional and fully connected strata to pretrained architectures can enhance performance [29]. Emulating their approach, we add four convolutional layers and two fully connected layers possessing arbitrarily initialized weights. Identifying objects frequently necessitates fine-grained visual data, so we increase the input size of the network from 224 × 224 to 448 × 448. Our final layer predicts both class likelihoods and bounding box coordinates. We standardize the bounding box width and height by the image width and height so they are bounded between 0 and 1.",A,1
"You Only Look Once_Unified, Real-Time Object Detection","We parametrize the bounding box x and y coordinates to be offsets of a particular grid cell location so they are also bounded between 0 and 1. We use sum-squared error because it is easy to optimize, however it does not perfectly align with our goal of maximizing average precision. It weights localization error equally with classification error which may not be ideal. Also, in every image many grid cells do not contain any object. This pushes the “confidence” scores of those cells towards zero, often overpowering the gradient from cells that do contain objects. This can lead to model instability, causing training to diverge early on.","We express the x and y coordinates of the bounding box as offsets of a specific grid cell position. This constrains them to be between 0 and 1. We apply sum of squared errors since it's simple to enhance, although it's not a flawless match to our aim of maximizing average precision. It equally weights localization and classification mistakes, which may not be best. Furthermore, in each image numerous grid cells lack any object. This suppresses the ""confidence"" values of those cells towards zero, frequently overpowering the gradient from cells containing objects. This can prompt model variability, resulting in training divergence early on.","We formulate the x and y bounding box coordinates as shifts of a particular grid cell location, bounding them between 0 and 1. We utilize sum of squared errors for straightforward optimization, but it's imperfect for our goal of maximizing average precision. It equates localization and classification errors, potentially suboptimal. Also, many grid cells in each image lack objects. This diminishes their ""confidence"" scores towards zero, often exceeding the gradient from occupied cells. This may cause model instability and early training divergence. ","The bounding box x and y are expressed as offsets of a specific grid cell, constraining them between 0 and 1. Sum of squared errors is used for easy optimization but does not fully match the goal of maximizing average precision. It equally weights localization and classification errors, possibly suboptimally. Additionally, many grid cells in each image are empty. This reduces their ""confidence"" towards zero, frequently surpassing the gradient of filled cells. This can create model instability and premature training divergence.",A,1
"You Only Look Once_Unified, Real-Time Object Detection",Sum-squared error also equally weights errors in large boxes and small boxes. Our error metric should reflect that small deviations in large boxes matter less than in small boxes. To partially address this we predict the square root of the bounding box width and height instead of the width and height directly. YOLO predicts multiple bounding boxes per grid cell. At training time we only want one bounding box predictor to be responsible for each object.,The sum of squared errors puts equal importance on mistakes in large and small boxes. Our error measurement should show that small mistakes in big boxes are less important than in small boxes. To partially fix this we predict the square root of the bounding box width and height instead of the width and height directly. YOLO guesses multiple bounding boxes per grid cell. When training we only want one bounding box predictor to be accountable for each object.,The total of squared errors weights slips in large boxes and small boxes the same. Our error gauge should demonstrate that tiny deviations in substantial boxes matter less than in diminutive boxes. To somewhat address this we calculate the square root of the bounding box width and height rather than the width and height straightforwardly. YOLO estimates multiple bounding boxes per grid cell. During training we only desire one bounding box forecaster to be responsible for each item. ,The amount of squared mistakes places equal significance on errors in big and little boxes. Our mistake measurement should indicate that small deviations in large boxes are less consequential than in small boxes. To partially resolve this we figure the square root of the bounding box width and height rather than the width and height directly. YOLO predicts multiple bounding boxes per grid cell. When teaching we only want one bounding box estimator to be accountable for each object.,A,1
"You Only Look Once_Unified, Real-Time Object Detection",Note that the loss function only penalizes classification error if an object is present in that grid cell (hence the conditional class probability discussed earlier). It also only penalizes bounding box coordinate error if that predictor is “responsible” for the ground truth box (i.e. has the highest IOU of any predictor in that grid cell). We train the network for about 135 epochs on the training and validation data sets from PASCAL VOC 2007 and 2012. When testing on 2012 we also include the VOC 2007 test data for training.,"The loss function only applies a penalty if an object is present in a grid cell, based on the conditional class probability. Also, it only penalizes the bounding box coordinates if that predictor has the highest IOU of any predictor in the grid cell. We trained the network for around 135 epochs using the training and validation data from PASCAL VOC 2007 and 2012. When testing on 2012 we also used the 2007 test data for training.","The loss function only punishes classification mistakes if an object exists in a grid cell, related to the conditional class probability. Bounding box coordinate errors are only penalized if that predictor has the maximum IOU of any predictor in the grid cell. We trained the network for about 135 epochs with the training and validation data from PASCAL VOC 2007 and 2012. When testing in 2012 we also utilized the 2007 test data for training.  ",Note that the loss function only applies a penalty for classification errors if an object is present in a grid cell (based on the conditional class probability). It also only punishes bounding box coordinate mistakes if that predictor has the highest IOU of any predictor in the grid cell. We trained the network for approximately 135 epochs using the training and validation datasets from PASCAL VOC 2007 and 2012. When testing in 2012 we also included the 2007 test data for training.,A,1
"You Only Look Once_Unified, Real-Time Object Detection","Throughout training we use a batch size of 64, a momentum of 0.9 and a decay of 0.0005. Our learning rate schedule is as follows: For the first epochs we slowly raise the learning rate from 10−3 to 10−2 . If we start at a high learning rate our model often diverges due to unstable gradients. We continue training with 10−2 for 75 epochs, then 10−3 for 30 epochs, and finally 10−4 for 30 epochs. To avoid overfitting we use dropout and extensive data augmentation. A dropout layer with rate = .5 after the first connected layer prevents co-adaptation between layers [18].","During training, we utilize a batch amount of 64, momentum of 0.9 and decay of 0.0005. Our learning rate agenda is: For the initial epochs, we gradually increase the learning rate from 10−3 to 10−2. Starting with a high learning rate frequently causes our model to diverge owing to volatile gradients. We persist training with 10−2 for 75 epochs, followed by 10−3 for 30 epochs, and eventually 10−4 for 30 epochs. To prevent overfitting, we employ dropout and considerable data augmentation. A dropout layer with rate = .5 after the first connected layer hinders co-adaptation amid layers [18].","Over the course of training, we employ a batch size of 64, momentum of 0.9 and deterioration of 0.0005. Our learning rate timetable is: For the early epochs, we slowly boost the learning rate from 10−3 to 10−2. Initiating with a high learning rate often makes our model diverge because of unsteady gradients. We keep training with 10−2 for 75 epochs, then 10−3 for 30 epochs, and finally 10−4 for 30 epochs. To avoid overfitting, we utilize dropout and extensive data augmentation. A dropout layer with rate = .5 after the first connected layer impedes co-adaptation between layers [18].","During training, we use a batch amount of 64, momentum of 0.9 and decay of 0.0005. Our learning rate schedule is: For the initial epochs, we gradually increase the learning rate from 10−3 to 10−2. Starting at a high learning rate frequently causes our model to diverge due to erratic gradients. We keep training with 10−2 for 75 epochs, then 10−3 for 30 epochs, and lastly 10−4 for 30 epochs. To prevent overfitting, we employ dropout and considerable data augmentation. A dropout layer with rate = .5 after the first connected layer inhibits co-adaptation among layers [18].",A,1
"You Only Look Once_Unified, Real-Time Object Detection","For data augmentation we introduce random scaling and translations of up to 20% of the original image size. We also randomly adjust the exposure and saturation of the image by up to a factor of 1.5 in the HSV color space. Just like in training, predicting detections for a test image only requires one network evaluation. On PASCAL VOC the network predicts 98 bounding boxes per image and class probabilities for each box. YOLO is extremely fast at test time since it only requires a single network evaluation, unlike classifier-based methods.","To expand the data we present arbitrary resizing and movements of up to 20% of the first image dimensions. We also randomly tweak the revelation and vividness of the image by up to 1.5 times in the HSV color range. Identical to preparation, anticipating identifications for an evaluation image solely necessitates one system appraisal. On PASCAL VOC the structure envisages 98 bounding containers per visualization and category likelihoods for each enclosure. YOLO is tremendously rapid at examination interval as it solely entails a singular system weighing, differently from classifier-founded procedures.","For augmenting the information we bring in haphazard scaling and shifts of up to 20% of the primary image extent. We furthermore randomly calibrate the exposure and saturation of the image by up to an influence of 1.5 in the HSV chromaticity space. Precisely like in coaching, prognosticating detections for a test image only entails one network valuation. On PASCAL VOC the network predicts 98 bounding boxes per figure and category probabilities for each box. YOLO is extremely fast at test time since it only needs a single network assessment, contrary to classifier-based techniques. ","To increase the data we present arbitrary resizing and movements of up to 20% of the original image magnitude. We also randomly adjust the revelation and vividness of the image by up to 1.5 times in the HSV color space. Identical to training, predicting identifications for an evaluation image only necessitates one network evaluation. On PASCAL VOC the network anticipates 98 bounding boxes per visualization and category probabilities for each enclosure. YOLO is extremely rapid at examination time as it solely requires a single network appraisal, differently from classifier-established methods.",A,1
"You Only Look Once_Unified, Real-Time Object Detection","The grid design enforces spatial diversity in the bounding box predictions. Often it is clear which grid cell an object falls in to and the network only predicts one box for each object. However, some large objects or objects near the border of multiple cells can be well localized by multiple cells. Non-maximal suppression can be used to fix these multiple detections. While not critical to performance as it is for R-CNN or DPM, non-maximal suppression adds 2- 3% in mAP.","The grid layout creates variety in the predicted bounding box locations. It's frequently evident which grid section an object is in and the network just predicts one box per object. Though, some large objects or objects close to the edge of multiple cells can be accurately located by numerous cells. Non-maximum suppression can fix these multiple detections. While not as critical as for R-CNN or DPM, non-max suppression provides 2-3% in mAP.","The grid pattern generates diversity in the forecasted bounding box spots. Often it's clear which grid square an object falls into and the network only foretells one box for each object. However, some big objects or objects near the border of various squares can be precisely pinpointed by multiple squares. Non-maximal suppression can amend these multiple detections. Although not as vital as for R-CNN or DPM, non-maximum suppression contributes 2-3% in mAP.  ","The grid arrangement produces variation in the expected bounding box places. It's frequently obvious which grid piece an object is inside and the network just predicts one box per object. Though, some large objects or objects close to the edge of various pieces can be accurately located by multiple pieces. Non-maximum suppression can fix these multiple detections. While not as crucial as for R-CNN or DPM, non-maximum suppression provides 2-3% in mAP.",A,1
"You Only Look Once_Unified, Real-Time Object Detection","YOLO imposes strong spatial constraints on bounding box predictions since each grid cell only predicts two boxes and can only have one class. This spatial constraint limits the number of nearby objects that our model can predict. Our model struggles with small objects that appear in groups, such as flocks of birds. Since our model learns to predict bounding boxes from data, it struggles to generalize to objects in new or unusual aspect ratios or configurations. Our model also uses relatively coarse features for predicting bounding boxes since our architecture has multiple downsampling layers from the input image.","The YOLO model severely limits where bounding boxes can be predicted because each part of the image grid can only guess two boxes and assign one class. This strict space limit reduces how many close objects our model can detect. Our model has trouble with small things in groups, like bird flocks. Since our model learns from data how to predict bounding box locations, it struggles to generalize to objects in new or odd proportions or arrangements. Our model also uses relatively rough features for bounding box prediction because our design downsamples the input image multiple times.","YOLO puts very tight constraints on where bounding boxes can be anticipated because each image grid section is restricted to predicting two boxes and one class. This tight spatial control decreases the quantity of nearby items our system can identify. Our system has difficulty with little objects clustered together, like bird swarms. Because our system learns from data how to predict bounding box positions, it has trouble generalizing to objects in unfamiliar or peculiar shapes or layouts. Our system also utilizes relatively coarse features for bounding box forecasting since our architecture downsamples the input image multiple times.","The YOLO architecture imposes rigid limitations on where bounding boxes can be expected since each image grid portion is limited to guessing two boxes and one class. This strict spatial regulation reduces the number of adjacent objects our model can detect. Our model struggles with small objects grouped together, such as flocks of birds. Because our model is trained on data to predict bounding box locations, it has difficulty generalizing to objects in new or odd proportions or configurations. Our model also employs relatively rough features for bounding box prediction because our design downsamples the input image multiple times.",A,1
"You Only Look Once_Unified, Real-Time Object Detection"," Finally, while we train on a loss function that approximates detection performance, our loss function treats errors the same in small bounding boxes versus large bounding boxes. A small error in a large box is generally benign but a small error in a small box has a much greater effect on IOU. Our main source of error is incorrect localizations. Object detection is a core problem in computer vision. Detection pipelines generally start by extracting a set of robust features from input images (Haar [25], SIFT [23], HOG [4], convolutional features [6]).","In conclusion, although we use a loss function that estimates detection accuracy, this loss function considers mistakes equally for small bounding boxes and large bounding boxes. A minor error in a large box is typically harmless but a small error in a small box greatly impacts IOU. Our primary source of mistakes is incorrect localizations. Detecting objects is a fundamental issue in computer vision. Detection systems usually begin by extracting a set of robust features from input images (Haar [25], SIFT [23], HOG [4], convolutional features [6]).","To summarize, while our loss function that approximates detection performance treats all errors the same in small bounding boxes and large bounding boxes, a small error in a large box is generally not problematic but a small error in a small box significantly affects IOU. We mostly make errors from incorrect localizations. Spotting objects is a key challenge in computer vision. Detection pipelines start by obtaining a set of robust features from input images (Haar [25], SIFT [23], HOG [4], convolutional features [6]).","In closing, even though we utilize a loss function that estimates detection accuracy, this loss function does not distinguish between mistakes in small bounding boxes versus large bounding boxes. A minor error in a large box is usually not an issue but a small error in a small box greatly impacts IOU. We primarily make mistakes from wrong localizations. Detecting objects is a fundamental problem in computer vision. Detection systems begin by extracting a set of robust features from input images (Haar [25], SIFT [23], HOG [4], convolutional features [6]).",A,1
"You Only Look Once_Unified, Real-Time Object Detection","Then, classifiers [36, 21, 13, 10] or localizers [1, 32] are used to identify objects in the feature space. These classifiers or localizers are run either in sliding window fashion over the whole image or on some subset of regions in the image [35, 15, 39]. We compare the YOLO detection system to several top detection frameworks, highlighting key similarities and differences. Deformable parts models (DPM) use a sliding window approach to object detection [10]. DPM uses a disjoint pipeline to extract static features, classify regions, predict bounding boxes for high scoring regions, etc.","Next, categorizers [36, 21, 13, 10] or finders [1, 32] are utilized to pinpoint items in the characteristic space. These categorizers or finders are executed either in a sliding window style over the full image or on a subset of areas in the image [35, 15, 39]. We contrast the YOLO identification framework with a few top detection structures, emphasizing key similarities and differences. Deformable component models (DPM) employ a sliding window tactic for object detection [10]. DPM uses a separate pipeline to extract static features, categorize regions, anticipate bounding boxes for high scoring areas, etc.","Subsequently, classifiers [36, 21, 13, 10] or locators [1, 32] are leveraged to identify objects within the feature space. These classifiers or locators are implemented either in a sliding window manner across the entire image or on some selection of regions within the image [35, 15, 39]. We compare the YOLO detection system with several leading detection frameworks, highlighting key likenesses and differences. Deformable parts models (DPM) utilize a sliding window approach for object detection [10]. DPM employs a disconnected pipeline to extract static features, classify areas, predict bounding boxes for high scoring regions, etc.","After that, categorizers [36, 21, 13, 10] or finders [1, 32] are used to pinpoint objects within the feature space. These categorizers or finders are executed either in a sliding window fashion over the whole image or on a subset of areas within the image [35, 15, 39]. We contrast the YOLO detection framework with several top detection systems, emphasizing key similarities and differences. Deformable parts models (DPM) use a sliding window tactic for object detection [10]. DPM utilizes a separate pipeline to extract static features, classify regions, forecast bounding boxes for high scoring areas, etc.",A,1
"You Only Look Once_Unified, Real-Time Object Detection","Our system replaces all of these disparate parts with a single convolutional neural network. The network performs feature extraction, bounding box prediction, nonmaximal suppression, and contextual reasoning all concurrently. Instead of static features, the network trains the features in-line and optimizes them for the detection task. Our unified architecture leads to a faster, more accurate model than DPM. R-CNN and its variants use region proposals instead of sliding windows to find objects in images. Selective Search [35] generates potential bounding boxes, a convolutional network extracts features, an SVM scores the boxes, a linear model adjusts the bounding boxes, and non-max suppression eliminates duplicate detections.","Our framework substitutes all of these different components with a sole convolutional neural network. The network concurrently carries out feature extraction, bounding box forecasting, nonmaximal suppression, and contextual analysis. Rather than fixed features, the network educates the features inline and enhances them for the detection objective. Our combined architecture results in a quicker, more precise model compared to DPM. R-CNN and its variations utilize region proposals instead of moving windows to identify objects in images. Selective Search [35] produces potential bounding boxes, a convolutional network extracts features, an SVM evaluates the boxes, a linear model calibrates the bounding boxes, and non-max suppression removes duplicate detections.","Our system swaps out all of these various parts for a single convolutional neural network. The network performs feature extraction, bounding box prediction, nonmaximal suppression, and contextual reasoning all at the same time. Instead of static features, the network trains the features on the fly and optimizes them for the detection task. Our unified architecture leads to a faster, more accurate model compared to DPM. R-CNN and its variants use region proposals instead of sliding windows to locate objects in images. Selective Search [35] generates candidate bounding boxes, a convolutional network extracts features, an SVM scores the boxes, a linear model adjusts the bounding boxes, and non-max suppression eliminates duplicate detections.  ","Our framework replaces all of these different components with a single convolutional neural network. The network concurrently conducts feature extraction, bounding box forecasting, nonmaximal suppression, and contextual analysis. Rather than fixed features, the network develops the features inline and enhances them for the detection objective. Our combined architecture produces a quicker, more accurate model compared to DPM. R-CNN and its variations employ region proposals instead of sliding windows to pinpoint objects in images. Selective Search [35] creates potential bounding boxes, a convolutional network extracts features, an SVM evaluates the boxes, a linear model calibrates the bounding boxes, and non-max suppression removes duplicate detections.",A,1
"You Only Look Once_Unified, Real-Time Object Detection"," Each stage of this complex pipeline must be precisely tuned independently and the resulting system is very slow, taking more than 40 seconds per image at test time [14]. YOLO shares some similarities with R-CNN. Each grid cell proposes potential bounding boxes and scores those boxes using convolutional features. However, our system puts spatial constraints on the grid cell proposals which helps mitigate multiple detections of the same object. Our system also proposes far fewer bounding boxes, only 98 per image compared to about 2000 from Selective Search. Finally, our system combines these individual components into a single, jointly optimized model.","The multiple steps in this intricate framework need to be tuned meticulously one by one, and the complete structure is extremely slow, requiring over 40 seconds for each image during testing [14]. YOLO has some common aspects with R-CNN. Every grid unit puts forward possible bounding boxes and evaluates those regions using convolutional characteristics. However, our framework enforces spatial limits on the grid unit proposals which assists in reducing multiple identifications of the identical item. Our framework also recommends far fewer bounding boxes, only 98 per image compared to around 2000 from Selective Search. Lastly, our framework unites these distinct elements into a single, collectively enhanced model.","This complex pipeline has many phases that must be carefully calibrated separately, making the final system very inefficient, needing more than 40 seconds to process a single image at test time [14]. YOLO is similar to R-CNN in some ways. It uses grid cells to propose potential bounding boxes, scoring them using convolutional features. But YOLO's spatial constraints on grid cells reduce duplicate detections of the same object. It also proposes far fewer boxes, only 98 per image versus ~2000 for Selective Search. Finally, YOLO combines these parts into one jointly optimized model.  ","The numerous steps in this intricate pipeline need individual meticulous tuning, resulting in an extremely slow final system, taking over 40 seconds per image during testing [14]. YOLO shares some parallels with R-CNN. Grid units offer possible bounding boxes, evaluating them via convolutional features. However, YOLO imposes spatial limits on the grid units to decrease duplicate detections of identical objects. It also suggests far fewer boxes, only 98 per image rather than around 2000 from Selective Search. Ultimately, YOLO integrates these components into one jointly enhanced model.",A,1
"You Only Look Once_Unified, Real-Time Object Detection","Fast and Faster R-CNN focus on speeding up the R-CNN framework by sharing computation and using neural networks to propose regions instead of Selective Search [14] [28]. While they offer speed and accuracy improvements over R-CNN, both still fall short of real-time performance. Many research efforts focus on speeding up the DPM pipeline [31] [38] [5]. They speed up HOG computation, use cascades, and push computation to GPUs. However, only 30Hz DPM [31] actually runs in real-time. Instead of trying to optimize individual components of a large detection pipeline, YOLO throws out the pipeline entirely and is fast by design.","The R-CNN models Fast R-CNN and Faster R-CNN aim to accelerate the R-CNN framework by sharing computations and utilizing neural networks rather than Selective Search to propose regions [14] [28]. Although they enhance speed and accuracy over R-CNN, both still fall short of real-time performance. Much research strives to expedite the DPM pipeline [31] [38] [5] by speeding up HOG calculation, employing cascades, and shifting computation to GPUs. However, only 30Hz DPM [31] truly operates in real-time. Rather than attempting to optimize individual parts of a large detection pipeline, YOLO discards the entire pipeline and is designed to be fast.","Fast R-CNN and Faster R-CNN focus on increasing the speed of R-CNN by sharing computations and using neural networks instead of Selective Search to propose regions [14] [28]. While they improve speed and accuracy compared to R-CNN, they still do not achieve real-time performance. Many efforts aim to accelerate the DPM pipeline [31] [38] [5] by speeding up HOG, using cascades, and moving computation to GPUs. But only 30Hz DPM [31] actually runs in real-time. Instead of optimizing individual components of a complex detection pipeline, YOLO abandons the entire pipeline and is inherently fast. ","Fast R-CNN and Faster R-CNN concentrate on boosting the speed of R-CNN through shared computation and neural network region proposals instead of Selective Search [14] [28]. Although they enhance speed and accuracy over R-CNN, they still fall short of real-time capability. Numerous works focus on expediting the DPM pipeline [31] [38] [5] via faster HOG, cascades, and GPU computation. However, only 30Hz DPM [31] truly achieves real-time operation. Rather than optimizing individual parts of a large detection pipeline, YOLO eschews the entire pipeline and is designed for speed.",A,1
"You Only Look Once_Unified, Real-Time Object Detection","Detectors for single classes like faces or people can be highly optimized since they have to deal with much less variation [37]. YOLO is a general purpose detector that learns to detect a variety of objects simultaneously. Unlike R-CNN, Szegedy et al. train a convolutional neural network to predict regions of interest [8] instead of using Selective Search. MultiBox can also perform single object detection by replacing the confidence prediction with a single class prediction. However, MultiBox cannot perform general object detection and is still just a piece in a larger detection pipeline, requiring further image patch classification. Both YOLO and MultiBox use a convolutional network to predict bounding boxes in an image but YOLO is a complete detection system.","Single class detectors like those for faces or people can be greatly enhanced since they need to manage far less diversity [37]. YOLO is a wide-ranging detector that learns to spot many types of objects at the same time. In contrast to R-CNN, Szegedy et al. teach a convolutional neural network to foresee areas of interest [8] rather than applying Selective Search. MultiBox can also do single object detection by substituting the confidence forecast with a single class prediction. However, MultiBox cannot perform general object detection and is still just a part in a larger detection workflow, needing extra image patch classification. Both YOLO and MultiBox employ a convolutional network to predict bounding boxes in an image but YOLO is a full detection system.","Detectors for solitary classes such as faces or people can be extremely optimized as they only have to handle much less variation [37]. YOLO is a general detector that learns to detect many different objects at the same time. Unlike R-CNN, Szegedy et al. train a convolutional neural network to anticipate regions of interest [8] rather than utilizing Selective Search. MultiBox can also do single object detection by substituting the confidence prediction with a single class prediction. However, MultiBox cannot do general object detection and is still just a component in a larger detection pipeline, necessitating additional image patch classification. Both YOLO and MultiBox employ a convolutional network to predict bounding boxes in an image but YOLO is a complete detection system.","Detectors for individual classes like faces or people can be highly enhanced since they only have to address far less diversity [37]. YOLO is a general-purpose detector that learns to spot a variety of objects at once. In contrast to R-CNN, Szegedy et al. train a convolutional neural network to forecast regions of interest [8] rather than applying Selective Search. MultiBox can also execute single object detection by substituting the confidence prediction with a single class prediction. However, MultiBox cannot execute general object detection and is still just a part in a bigger detection pipeline, necessitating extra image patch classification. Both YOLO and MultiBox use a convolutional network to predict bounding boxes in an image but YOLO is a full detection system.",A,1
"You Only Look Once_Unified, Real-Time Object Detection","Sermanet et al. train a convolutional neural network to perform localization and adapt that localizer to perform detection [32]. OverFeat efficiently performs sliding window detection but it is still a disjoint system. OverFeat optimizes for localization, not detection performance. Like DPM, the localizer only sees local information when making a prediction. OverFeat cannot reason about global context and thus requires significant post-processing to produce coherent detections.","Sermanet and colleagues educated a convolutional neural network to do localization and tailored that localizer to execute detection [32]. OverFeat competently executes sliding window detection but it is still a separate system. OverFeat enhances for localization, not detection capability. Similar to DPM, the localizer only perceives local details when forming a prediction. OverFeat cannot deduce about comprehensive situation and hence necessitates considerable post-processing to generate coherent detections.","Sermanet and co-workers trained a convolutional neural network to carry out localization and adapted that localizer to conduct detection [32]. OverFeat efficiently implements sliding window detection however it is still a disconnected system. OverFeat focuses on localization, not detection performance. Analogous to DPM, the localizer only perceives local facts when making a forecast. OverFeat cannot reason regarding global context thus requires significant after-processing to produce coherent detections. ","Sermanet and colleagues taught a convolutional neural network to execute localization and tailored that localizer to perform detection [32]. OverFeat competently implements sliding window detection however it is still a separate system. OverFeat enhances for localization, not detection capability. Similar to DPM, the localizer only sees local details when forming a prediction. OverFeat cannot deduce about comprehensive situation thus necessitates considerable after-processing to generate coherent detections.",A,1
"You Only Look Once_Unified, Real-Time Object Detection","Our work is similar in design to work on grasp detection by Redmon et al [27]. Our grid approach to bounding box prediction is based on the MultiGrasp system for regression to grasps. However, grasp detection is a much simpler task than object detection. MultiGrasp only needs to predict a single graspable region for an image containing one object. It doesn’t have to estimate the size, location, or boundaries of the object or predict it’s class, only find a region suitable for grasping. YOLO predicts both bounding boxes and class probabilities for multiple objects of multiple classes in an image.","Our work has a similar structure to the grasp detection work done by Redmon and colleagues [27]. Our grid-based approach to predicting bounding boxes is modeled after the MultiGrasp system for regressing to grasps. However, grasp detection is a much more straightforward task than object detection. MultiGrasp only needs to identify a single graspable area in an image with one object. It doesn’t need to estimate the object's size, location, boundaries, or predict its class, only find a region good for grasping. YOLO predicts both bounding boxes and class probabilities for multiple objects of multiple classes in one image.","Our research has a comparable design to the grasp detection research by Redmon's group [27]. Our grid methodology for bounding box prediction is based on the MultiGrasp framework for regression to grasps. However, grasp detection is a much more basic task than object detection. MultiGrasp only needs to predict one graspable zone in an image with a single object. It doesn’t need to estimate the object's dimensions, position, edges, or anticipate its class, only identify an area suitable for grasping. YOLO anticipates both bounding boxes and class probabilities for numerous objects of numerous classes in a single image.  ","Our work has a similar structure to the grasp detection research conducted by Redmon and team [27]. Our grid-structured approach to bounding box prediction is modeled on the MultiGrasp system for regressing to grasps. However, grasp detection is a far simpler task than object detection. MultiGrasp only needs to determine one graspable region in an image containing a single object. It doesn’t need to estimate the object's size, location, boundaries, or predict its class, only identify an area good for grasping. YOLO predicts both bounding boxes and class probabilities for multiple objects across multiple classes in one image.",A,1
"You Only Look Once_Unified, Real-Time Object Detection","First we compare YOLO with other real-time detection systems on PASCAL VOC 2007. To understand the differences between YOLO and R-CNN variants we explore the errors on VOC 2007 made by YOLO and Fast R-CNN, one of the highest performing versions of R-CNN [14]. Based on the different error profiles we show that YOLO can be used to rescore Fast R-CNN detections and reduce the errors from background false positives, giving a significant performance boost. We also present VOC 2012 results and compare mAP to current state-of-the-art methods. Finally, we show that YOLO generalizes to new domains better than other detectors on two artwork datasets.","Initially we contrast YOLO with additional real-time identification frameworks on PASCAL VOC 2007. To comprehend the variances between YOLO and R-CNN variants we investigate the mistakes on VOC 2007 created by YOLO and Fast R-CNN, one of the highest functioning iterations of R-CNN [14]. Grounded on the divergent error profiles we exhibit that YOLO can be utilized to rescore Fast R-CNN identifications and diminish the errors from background false positives, providing a significant performance enhancement. We also proffer VOC 2012 outcomes and compare mAP to current state-of-the-art techniques. Finally, we demonstrate that YOLO generalizes to new domains superior to other detectors on two artwork datasets.","We first juxtapose YOLO with other instantaneous detection systems on PASCAL VOC 2007. To grasp the differences between YOLO and R-CNN versions we explore the inaccuracies on VOC 2007 made by YOLO and Fast R-CNN, one of the top performing variants of R-CNN [14]. Based on the contrasting error profiles we show that YOLO can be employed to rescore Fast R-CNN detections and decrease the errors from background false positives, providing a significant performance boost. We also present VOC 2012 results and contrast mAP to current most advanced methods. Finally, we exhibit that YOLO generalizes to new domains better than other detectors on two artwork datasets. ","Initially we compare YOLO with additional real-time identification systems on PASCAL VOC 2007. To comprehend the divergences between YOLO and R-CNN variants we inspect the mistakes on VOC 2007 created by YOLO and Fast R-CNN, one of the highest functioning iterations of R-CNN [14]. Grounded on the differing error profiles we demonstrate that YOLO can be utilized to rescore Fast R-CNN identifications and reduce the errors from background false positives, providing a significant performance enhancement. We also offer VOC 2012 outcomes and contrast mAP to current most advanced techniques. Finally, we establish that YOLO generalizes to new domains superior to other detectors on two artwork datasets.",A,1
"You Only Look Once_Unified, Real-Time Object Detection","Many research efforts in object detection focus on making standard detection pipelines fast. [5] [38] [31] [14] [17] [28] However, only Sadeghi et al. actually produce a detection system that runs in real-time (30 frames per second or better) [31]. We compare YOLO to their GPU implementation of DPM which runs either at 30Hz or 100Hz. While the other efforts don’t reach the real-time milestone we also compare their relative mAP and speed to examine the accuracy-performance tradeoffs available in object detection systems.","A lot of work in object detection tries to speed up standard detection systems. [5] [38] [31] [14] [17] [28] But only Sadeghi et al. built a system that can run in real-time (at least 30 fps). [31] We compare YOLO to their GPU version of DPM that goes either 30Hz or 100Hz. Though the other projects aren't real-time, we also look at their mAP and speed to see the tradeoffs between accuracy and speed for object detectors.","Many object detection research projects aim to make regular detection pipelines fast. [5] [38] [31] [14] [17] [28] However, Sadeghi et al. is the only one that achieves real-time performance (30+ fps). [31] We benchmark YOLO against their GPU DPM at 30Hz and 100Hz. Despite the other projects not meeting real-time thresholds, we still analyze their mAP and speed to understand the accuracy vs performance balances in object detection.  ","A lot of object detection research strives to speed up standard systems. [5] [38] [31] [14] [17] [28] But only Sadeghi et al. managed to build a real-time system (≥30 fps). [31] We test YOLO against their GPU DPM at 30Hz and 100Hz. Although the other projects aren't real-time, we still look at their mAP and speed to grasp the accuracy vs speed tradeoffs in object detection.",A,1
"You Only Look Once_Unified, Real-Time Object Detection","Fast YOLO is the fastest object detection method on PASCAL; as far as we know, it is the fastest extant object detector. With 52.7% mAP, it is more than twice as accurate as prior work on real-time detection. YOLO pushes mAP to 63.4% while still maintaining real-time performance. We also train YOLO using VGG-16. This model is more accurate but also significantly slower than YOLO. It is useful for comparison to other detection systems that rely on VGG-16 but since it is slower than real-time the rest of the paper focuses on our faster models.","Fast YOLO is the quickest object detection approach on PASCAL; as far as we are aware, it is the swiftest existing object detector. With 52.7% mAP, it is over two times as precise as previous work on real-time detection. YOLO pushes mAP to 63.4% while still keeping real-time speed. We also educate YOLO employing VGG-16. This prototype is more correct but also considerably slower than YOLO. It is beneficial for contrast to other detection systems that depend on VGG-16 but since it is slower than real-time the rest of the paper concentrates on our faster models.","Fast YOLO is the most rapid object identification technique on PASCAL; to our knowledge, it is the most expeditious current object detector. With 52.7% mAP, it is more than double as accurate as prior real-time detection work. YOLO increases mAP to 63.4% while still retaining real-time velocity. We also train YOLO with VGG-16. This model is more precise but also significantly slower than YOLO. It is useful for comparison to other detection frameworks that use VGG-16 but since it is slower than real-time the remainder of the paper focuses on our swifter models.  ","Fast YOLO is the most speedy object spotting approach on PASCAL; as far as we're aware, it is the most fleet present object detector. With 52.7% mAP, it is over twice as precise as previous real-time detection work. YOLO pushes mAP to 63.4% while still keeping real-time pace. We also school YOLO employing VGG-16. This prototype is more accurate but also considerably slower than YOLO. It is beneficial for contrast to other detection systems that utilize VGG-16 but since it is slower than real-time the rest of the paper concentrates on our faster models.",A,1
"You Only Look Once_Unified, Real-Time Object Detection","Fastest DPM effectively speeds up DPM without sacrificing much mAP but it still misses real-time performance by a factor of 2 [38]. It also is limited by DPM’s relatively low accuracy on detection compared to neural network approaches. R-CNN minus R replaces Selective Search with static bounding box proposals [20]. While it is much faster than R-CNN, it still falls short of real-time and takes a significant accuracy hit from not having good proposals. Fast R-CNN speeds up the classification stage of R-CNN but it still relies on selective search which can take around 2 seconds per image to generate bounding box proposals.","The quickest DPM method efficiently hastens DPM without giving up too much mAP, but it is still not quite twice as slow as real-time performance [38]. It is also constrained by DPM's relatively inferior detection accuracy compared to neural network techniques. R-CNN without the region proposal module substitutes Selective Search with fixed bounding box suggestions [20]. Although much faster than R-CNN, it still does not achieve real-time speed and has a considerable accuracy decrease from not utilizing good proposals. Fast R-CNN accelerates the classification part of R-CNN however it still depends on selective search which can take around 2 seconds per image to generate bounding box proposals.","The fastest DPM approach meaningfully speeds up DPM without sacrificing a lot of mAP, but it is still twice as slow as real-time capability [38]. It is also limited by DPM's relatively low detection accuracy compared to neural network methods. R-CNN without the region proposal component switches Selective Search with static bounding box ideas [20]. While much quicker than R-CNN, it still does not reach real-time performance and takes a significant accuracy reduction from not having quality proposals. Fast R-CNN expedites the categorization portion of R-CNN however it still relies on selective search which can require around 2 seconds per image to create bounding box proposals.  ","The quickest DPM technique notably accelerates DPM without giving up much mAP, but it is still twice as lethargic as real-time ability [38]. It is also constrained by DPM's relatively inferior detection precision compared to neural network approaches. R-CNN without the region proposal module substitutes Selective Search with fixed bounding box concepts [20]. Although far speedier than R-CNN, it still does not attain real-time velocity and endures a considerable accuracy decrease from not utilizing good proposals. Fast R-CNN expedites the classification part of R-CNN however it still depends on selective search which can demand around 2 seconds per image to generate bounding box proposals.",A,1
